<?xml version="1.0" ?>
<!DOCTYPE PubmedArticleSet PUBLIC "-//NLM//DTD PubMedArticle, 1st January 2023//EN" "https://dtd.nlm.nih.gov/ncbi/pubmed/out/pubmed_230101.dtd">
<PubmedArticleSet>
<PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36637719</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>13</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1865-0341</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>13</Day></PubDate></JournalIssue><Title>Radiological physics and technology</Title><ISOAbbreviation>Radiol Phys Technol</ISOAbbreviation></Journal><ArticleTitle>Prediction of body weight from chest radiographs using deep learning with a convolutional neural network.</ArticleTitle><ELocationID EIdType="doi" ValidYN="Y">10.1007/s12194-023-00697-3</ELocationID><Abstract><AbstractText>Accurate body weights are not necessarily available in routine clinical practice. This study aimed to investigate whether body weight can be predicted from chest radiographs using deep learning. Deep-learning models with a convolutional neural network (CNN) were trained and tested using chest radiographs from 85,849 patients. The CNN models were evaluated by calculating the mean absolute error (MAE) and Spearman's rank correlation coefficient (&#x3c1;). The MAEs of the CNN models were 2.63&#xa0;kg and 3.35&#xa0;kg for female and male patients, respectively. The predicted body weight was significantly correlated with the actual body weight (&#x3c1;&#x2009;=&#x2009;0.917, p&#x2009;&lt;&#x2009;0.001 for females; &#x3c1;&#x2009;=&#x2009;0.915, p&#x2009;&lt;&#x2009;0.001 for males). The body weight was predicted using chest radiographs by applying deep learning. Our method is potentially useful for radiation dose management, determination of the contrast medium dose, and estimation of the specific absorption rate in patients with unknown body weights.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s), under exclusive licence to Japanese Society of Radiological Technology and Japan Society of Medical Physics.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Ichikawa</LastName><ForeName>Shota</ForeName><Initials>S</Initials><Identifier Source="ORCID">0000-0002-4275-1422</Identifier><AffiliationInfo><Affiliation>Graduate School of Health Sciences, Hokkaido University, Kita-12, Nishi-5, Kita-ku, Sapporo, 060-0812, Japan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiological Technology, Kurashiki Central Hospital, 1-1-1 Miwa, Kurashiki, Okayama, 710-8602, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Itadani</LastName><ForeName>Hideki</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>Department of Radiological Technology, Kurashiki Central Hospital, 1-1-1 Miwa, Kurashiki, Okayama, 710-8602, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Sugimori</LastName><ForeName>Hiroyuki</ForeName><Initials>H</Initials><Identifier Source="ORCID">0000-0002-7796-5113</Identifier><AffiliationInfo><Affiliation>Faculty of Health Sciences, Hokkaido University, Kita-12, Nishi-5, Kita-ku, Sapporo, 060-0812, Japan. sugimori@hs.hokudai.ac.jp.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>13</Day></ArticleDate></Article><MedlineJournalInfo><Country>Japan</Country><MedlineTA>Radiol Phys Technol</MedlineTA><NlmUniqueID>101467995</NlmUniqueID><ISSNLinking>1865-0333</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Artificial intelligence</Keyword><Keyword MajorTopicYN="N">Body weight</Keyword><Keyword MajorTopicYN="N">Chest radiography</Keyword><Keyword MajorTopicYN="N">Convolutional neural network</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>11</Month><Day>27</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2023</Year><Month>1</Month><Day>9</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2023</Year><Month>1</Month><Day>7</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>11</Hour><Minute>22</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>14</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>14</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36637719</ArticleId><ArticleId IdType="doi">10.1007/s12194-023-00697-3</ArticleId><ArticleId IdType="pii">10.1007/s12194-023-00697-3</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Fukunaga M, Matsubara K, Ichikawa S, Mitsui H, Yamamoto H, Miyati T. CT dose management of adult patients with unknown body weight using an effective diameter. Eur J Radiol. 2021;135:109483. https://doi.org/10.1016/j.ejrad.2020.109483 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejrad.2020.109483</ArticleId></ArticleIdList></Reference><Reference><Citation>O&#x2019;Neill S, Kavanagh RG, Carey BW, Moore N, Maher M, O&#x2019;Connor OJ. Using body mass index to estimate individualised patient radiation dose in abdominal computed tomography. Eur Radiol Exp. 2018;2:38. https://doi.org/10.1186/s41747-018-0070-5 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s41747-018-0070-5</ArticleId></ArticleIdList></Reference><Reference><Citation>Bae KT. Intravenous contrast medium administration and scan timing at CT: considerations and approaches. Radiology. 2010;256:32&#x2013;61. https://doi.org/10.1148/radiol.10090908 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.10090908</ArticleId></ArticleIdList></Reference><Reference><Citation>Awai K, Hori S. Effect of contrast injection protocol with dose tailored to patient weight and fixed injection duration on aortic and hepatic enhancement at multidetector-row helical CT. Eur Radiol. 2003;13:2155&#x2013;60. https://doi.org/10.1007/s00330-003-1904-x .</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-003-1904-x</ArticleId></ArticleIdList></Reference><Reference><Citation>Seo Y, Wang ZJ. Measurement and evaluation of specific absorption rate and temperature elevation caused by an artificial hip joint during MRI scanning. Sci Rep. 2021;11:1134. https://doi.org/10.1038/s41598-020-80828-7 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-020-80828-7</ArticleId></ArticleIdList></Reference><Reference><Citation>Hall WL, Larkin GL, Trujillo MJ, Hinds JL, Delaney KA. Errors in weight estimation in the emergency department: comparing performance by providers and patients. J Emerg Med. 2004;27:219&#x2013;24. https://doi.org/10.1016/j.jemermed.2004.04.008 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jemermed.2004.04.008</ArticleId></ArticleIdList></Reference><Reference><Citation>Menon S, Kelly A-M. How accurate is weight estimation in the emergency department? Emerg Med Australas. 2005;17:113&#x2013;6. https://doi.org/10.1111/j.1742-6723.2005.00701.x .</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/j.1742-6723.2005.00701.x</ArticleId></ArticleIdList></Reference><Reference><Citation>Fernandes CMB, Clark S, Price A, Innes G. How accurately do we estimate patients&#x2019; weight in emergency departments? Can Fam Physician. 1999;45:2373&#x2013;6.</Citation></Reference><Reference><Citation>Ichikawa S, Hamada M, Sugimori H. A deep-learning method using computed tomography scout images for estimating patient body weight. Sci Rep. 2021;11:15627. https://doi.org/10.1038/s41598-021-95170-9 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-021-95170-9</ArticleId></ArticleIdList></Reference><Reference><Citation>Yasaka K, Akai H, Kunimatsu A, Kiryu S, Abe O. Prediction of bone mineral density from computed tomography: application of deep learning with a convolutional neural network. Eur Radiol. 2020;30:3549&#x2013;57. https://doi.org/10.1007/s00330-020-06677-0 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-020-06677-0</ArticleId></ArticleIdList></Reference><Reference><Citation>Kojita Y, Matsuo H, Kanda T, Nishio M, Sofue K, Nogami M, et al. Deep learning model for predicting gestational age after the first trimester using fetal MRI. Eur Radiol. 2021;31:3775&#x2013;82. https://doi.org/10.1007/s00330-021-07915-9 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-021-07915-9</ArticleId></ArticleIdList></Reference><Reference><Citation>Sabottke CF, Breaux MA, Spieler BM. Estimation of age in unidentified patients via chest radiography using convolutional neural network regression. Emerg Radiol. 2020;27:463&#x2013;8. https://doi.org/10.1007/s10140-020-01782-5 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10140-020-01782-5</ArticleId></ArticleIdList></Reference><Reference><Citation>Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. In: Proceedings of the 3rd International Conference on Learning Representations 2014. http://arxiv.org/abs/1409.1556 .</Citation></Reference><Reference><Citation>Tang YX, Tang YB, Peng Y, et al. Automated abnormality classification of chest radiographs using deep convolutional neural networks. npj Digit Med. 2020. https://doi.org/10.1038/s41746-020-0273-z .</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41746-020-0273-z</ArticleId></ArticleIdList></Reference><Reference><Citation>Keidar D, Yaron D, Goldstein E, et al. COVID-19 classification of X-ray images using deep neural networks. Eur Radiol. 2021;31:9654&#x2013;63. https://doi.org/10.1007/s00330-021-08050-1 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-021-08050-1</ArticleId></ArticleIdList></Reference><Reference><Citation>Usman M, Zia T, Tariq A. Analyzing transfer learning of vision transformers for interpreting chest radiography. J Digit Imaging. 2022;35:1445&#x2013;62. https://doi.org/10.1007/s10278-022-00666-z .</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10278-022-00666-z</ArticleId></ArticleIdList></Reference><Reference><Citation>Krizhevsky A, Sutskever I, Hinton GE. ImageNet classification with deep convolutional neural networks. Commun ACM. 2017;60:84&#x2013;90. https://doi.org/10.1145/3065386 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1145/3065386</ArticleId></ArticleIdList></Reference><Reference><Citation>Kingma DP, Ba J. Adam: a method for stochastic optimization. In: Proceedings of the 3rd International Conference for Learning Representations 2014. https://arxiv.org/abs/1412.6980 .</Citation></Reference><Reference><Citation>Selvaraju RR, Cogswell M, Das A, Vedantam R, Parikh D, Batra D. Grad-CAM: visual explanations from deep networks via gradient-based localization. In: Proceedings of the IEEE International Conference on Computer Vision. 2017. https://doi.org/10.1109/ICCV.2017.74 .</Citation></Reference><Reference><Citation>Buckley RG, Stehman CR, Dos Santos FL, Riffenburgh RH, Swenson A, Mjos N, et al. Bedside method to estimate actual body weight in the emergency department. J Emerg Med. 2012;42:100&#x2013;4. https://doi.org/10.1016/j.jemermed.2010.10.022 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jemermed.2010.10.022</ArticleId></ArticleIdList></Reference><Reference><Citation>Geraghty EM, Boone JM. Determination of height, weight, body mass index, and body surface area with a single abdominal CT image. Radiology. 2003;228:857&#x2013;63. https://doi.org/10.1148/radiol.2283020095 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2283020095</ArticleId></ArticleIdList></Reference><Reference><Citation>Gascho D, Ganzoni L, Kolly P, Zoelch N, Hatch GM, Thali MJ, et al. A new method for estimating patient body weight using CT dose modulation data. Eur Radiol Exp. 2017;1:23. https://doi.org/10.1186/s41747-017-0028-z .</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s41747-017-0028-z</ArticleId></ArticleIdList></Reference><Reference><Citation>Yamada Y, Yamada M, Chubachi S, Yokoyama Y, Matsuoka S, Tanabe A, et al. Comparison of inspiratory and expiratory lung and lobe volumes among supine, standing, and sitting positions using conventional and upright CT. Sci Rep. 2020;10:16203. https://doi.org/10.1038/s41598-020-73240-8 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-020-73240-8</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36637464</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>13</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1432-1998</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>13</Day></PubDate></JournalIssue><Title>Pediatric radiology</Title><ISOAbbreviation>Pediatr Radiol</ISOAbbreviation></Journal><ArticleTitle>Dating birth-related clavicular fractures: pediatric radiologists versus artificial intelligence.</ArticleTitle><ELocationID EIdType="doi" ValidYN="Y">10.1007/s00247-023-05590-0</ELocationID><Abstract><AbstractText Label="BACKGROUND" NlmCategory="BACKGROUND">Fracture dating from skeletal surveys is crucial in the diagnosis and investigation of infant abuse. However, this task is challenging because of the subjective nature of the radiologic interpretation and the lack of ground truth. Researchers have used birth-related clavicle fractures as a surrogate to study the radiographic pattern of healing; however, they did not elucidate the accuracy performance of the radiologists in dating fractures.</AbstractText><AbstractText Label="OBJECTIVE" NlmCategory="OBJECTIVE">To determine the accuracy of radiologists in dating birth-related clavicle fractures and compare their performance to that achieved by computer algorithm.</AbstractText><AbstractText Label="MATERIALS AND METHODS" NlmCategory="METHODS">We used a previously assembled birth-related clavicle fracture database consisting of 416 anteroposterior clavicle radiographs as the study cohort. The average and standard deviation of the fracture age within this database were 24&#xa0;days and 18&#xa0;days, respectively. Three blinded radiologists independently estimated the ages of the clavicle fractures depicted in the radiographs within the database. We compared these estimation results to those made by a recently published deep-learning (DL) model conducted with the identical infant cohort. We calculated standard error metrics to compare the accuracy performances of the radiologists and the computer model.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">The intra- and inter-reader agreements of the fracture age estimates by the radiologists were moderate to good. The radiologists estimated the fracture ages with a mean absolute error (MAE) of 6.1-7.1&#xa0;days, and standard deviation of the absolute error of 6.3-8.3&#xa0;days. The accuracy performances of the three radiologists were not significantly different from one another. In comparison, the DL model estimated the age of clavicle fractures with an MAE of 4.2&#xa0;days, significantly lower than all of the radiologists (P&#xa0;&lt; 0.001).</AbstractText><AbstractText Label="CONCLUSION" NlmCategory="CONCLUSIONS">Three experienced pediatric radiologists dated clavicular fractures with moderate-good intra- and inter-reader agreements. The correlations between the radiologists' estimates and the ground truth were moderate to good. The fracture ages assigned by the DL model showed superior correlation with the ground truth compared to radiologists' dating estimates.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Tsai</LastName><ForeName>Andy</ForeName><Initials>A</Initials><Identifier Source="ORCID">0000-0002-0089-6463</Identifier><AffiliationInfo><Affiliation>Department of Radiology, Boston Children's Hospital, Harvard Medical School, 300 Longwood Ave., Boston, MA, 02115, USA. andy.tsai@childrens.harvard.edu.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>P&#xe9;rez-Rossell&#xf3;</LastName><ForeName>Jeannette M</ForeName><Initials>JM</Initials><AffiliationInfo><Affiliation>Department of Radiology, Boston Children's Hospital, Harvard Medical School, 300 Longwood Ave., Boston, MA, 02115, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Ecklund</LastName><ForeName>Kirsten</ForeName><Initials>K</Initials><AffiliationInfo><Affiliation>Department of Radiology, Boston Children's Hospital, Harvard Medical School, 300 Longwood Ave., Boston, MA, 02115, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Walters</LastName><ForeName>Michele M</ForeName><Initials>MM</Initials><AffiliationInfo><Affiliation>Department of Radiology, Boston Children's Hospital, Harvard Medical School, 300 Longwood Ave., Boston, MA, 02115, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kleinman</LastName><ForeName>Paul K</ForeName><Initials>PK</Initials><AffiliationInfo><Affiliation>Department of Radiology, Boston Children's Hospital, Harvard Medical School, 300 Longwood Ave., Boston, MA, 02115, USA.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>13</Day></ArticleDate></Article><MedlineJournalInfo><Country>Germany</Country><MedlineTA>Pediatr Radiol</MedlineTA><NlmUniqueID>0365332</NlmUniqueID><ISSNLinking>0301-0449</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Artificial intelligence</Keyword><Keyword MajorTopicYN="N">Child abuse</Keyword><Keyword MajorTopicYN="N">Clavicle</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Fracture</Keyword><Keyword MajorTopicYN="N">Fracture dating</Keyword><Keyword MajorTopicYN="N">Healing</Keyword><Keyword MajorTopicYN="N">Infants</Keyword><Keyword MajorTopicYN="N">Radiography</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>10</Month><Day>20</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>30</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>12</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>11</Hour><Minute>12</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>14</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>14</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36637464</ArticleId><ArticleId IdType="doi">10.1007/s00247-023-05590-0</ArticleId><ArticleId IdType="pii">10.1007/s00247-023-05590-0</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Cumming WA (1979) Neonatal skeletal fractures. Birth trauma or child abuse? J Can Assoc Radiol 30:30&#x2013;33</Citation></Reference><Reference><Citation>O&#x2019;Connor JF, Cohen J (1987) Dating fractures. In: Kleinman PK (ed) Diagnostic imaging of child abuse. Williams &amp; Wilkins, Baltimore, pp 103&#x2013;113</Citation></Reference><Reference><Citation>Yeo LI, Reed MH (1994) Staging of healing of femoral fractures in children. Can Assoc Radiol J 45:16&#x2013;19</Citation></Reference><Reference><Citation>Islam O, Soboleski D, Symons S et al (2000) Development and duration of radiographic signs of bone healing in children. AJR Am J Roentgenol 175:75&#x2013;78</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/ajr.175.1.1750075</ArticleId></ArticleIdList></Reference><Reference><Citation>Malone CA, Sauer NJ, Fenton TW (2011) A radiographic assessment of pediatric fracture healing and time since injury. J Forensic Sci 56:1123&#x2013;1130</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/j.1556-4029.2011.01820.x</ArticleId></ArticleIdList></Reference><Reference><Citation>Halliday K, Broderick N, Somers J, Hawkes R (2011) Dating fractures in infants. Clin Radiol 66:1049&#x2013;1054</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.crad.2011.06.001</ArticleId></ArticleIdList></Reference><Reference><Citation>Prosser I, Lawon Z, Evans A et al (2012) A timetable for the radiologic features of fracture healing in young children. AJR Am J Roentgenol 198:1014&#x2013;1120</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/AJR.11.6734</ArticleId></ArticleIdList></Reference><Reference><Citation>Sanchez TR, Nguyen H, Palacios W et al (2013) Retrospective evaluation and dating of non-accidental rib fractures in infants. Clin Radiol 68:e467&#x2013;e471</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.crad.2013.03.017</ArticleId></ArticleIdList></Reference><Reference><Citation>Warner C, Maguire S, Trefan L et al (2017) A study of radiological features of healing in long bone fractures among infants less than a year. Skeletal Radiol 46:333&#x2013;341</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00256-016-2563-8</ArticleId></ArticleIdList></Reference><Reference><Citation>Crompton S, Messina F, Klafkowski G et al (2021) Validating scoring systems for fracture healing in infants and young children: pilot study. Pediatr Radiol 51:1682&#x2013;1689</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00247-021-05038-3</ArticleId></ArticleIdList></Reference><Reference><Citation>Kyllonen KM, Monson KL, Smith MA (2022) Postmortem and antemortem forensic assessment of pediatric fracture healing from radiographs and machine learning classification. Biology 11:749</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/biology11050749</ArticleId></ArticleIdList></Reference><Reference><Citation>Walters MM, Forbes PW, Buonomo C, Kleinman PK (2014) Healing patterns of clavicular birth injuries as a guide to fracture dating in cases of possible infant abuse. Pediatr Radiol 44:1224&#x2013;1229</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00247-014-2995-z</ArticleId></ArticleIdList></Reference><Reference><Citation>Fadell M, Miller A, Trefan L et al (2017) Radiological features of healing in newborn clavicular fractures. Eur Radiol 27:2180&#x2013;2187</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-016-4569-y</ArticleId></ArticleIdList></Reference><Reference><Citation>Tsai A, Grant PE, Warfield SK et al (2022) Deep learning of birth-related infant clavicle fractures: a potential virtual consultant for fracture dating. Pediatr Radiol 52:2206&#x2013;2214</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00247-022-05380-0</ArticleId></ArticleIdList></Reference><Reference><Citation>Koch GG (1982) Intraclass correlation coefficient. In: Kotz S, Johnson NL (eds) Encyclopedia of statistical sciences, vol 4. Wiley, New York, pp 213&#x2013;217</Citation></Reference><Reference><Citation>Koo TK, Li MY (2016) A guideline of selecting and reporting intraclass correlation coefficients for reliability research. J Chiropr Med 15:155&#x2013;163</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jcm.2016.02.012</ArticleId></ArticleIdList></Reference><Reference><Citation>Barber I, Perez-Rossello JM, Wilson CR, Kleinman PK (2015) The yield of high-detail radiographic skeletal surveys in suspected infant abuse. Pediatr Radiol 45:69&#x2013;80</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00247-014-3064-3</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36636489</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>13</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Print">2329-4302</ISSN><JournalIssue CitedMedium="Print"><Volume>10</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month></PubDate></JournalIssue><Title>Journal of medical imaging (Bellingham, Wash.)</Title><ISOAbbreviation>J Med Imaging (Bellingham)</ISOAbbreviation></Journal><ArticleTitle>Identification of infarct core and ischemic penumbra using computed tomography perfusion and deep learning.</ArticleTitle><Pagination><StartPage>014001</StartPage><MedlinePgn>014001</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1117/1.JMI.10.1.014001</ELocationID><Abstract><AbstractText Label="PURPOSE" NlmCategory="UNASSIGNED">The size and location of infarct and penumbra are key to decision-making for acute ischemic stroke (AIS) management. CT perfusion (CTP) software estimate infarct and penumbra volume using contralateral hemisphere relative thresholding. This approach is not robust and widely contested by the scientific community. In this study, we investigate the use of deep learning-based algorithms to efficiently locate infarct and penumbra tissue on CTP hemodynamic maps.</AbstractText><AbstractText Label="APPROACH" NlmCategory="UNASSIGNED">CTP scans were retrospectively collected for 60 and 59 patients in the infarct only and infarct + penumbra substudies respectively. Commercial CTP software was used to generate cerebral blood flow, cerebral blood volume, mean transit time, time to peak, and delay time maps. U-Net-shaped architectures were trained to segment infarct or infarct + penumbra. Test-time-augmentation, ensembling, and watershed segmentation were used as postprocessing techniques. Segmentation performance was evaluated using Dice coefficients (DC) and mean absolute volume errors (MAVE).</AbstractText><AbstractText Label="RESULTS" NlmCategory="UNASSIGNED">The algorithm segmented infarct tissue resulted in DC of <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.64</mml:mn> <mml:mo>&#xb1;</mml:mo> <mml:mn>0.03</mml:mn></mml:mrow> </mml:math> (0.63, 0.65), and MAVE of <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>4.91</mml:mn> <mml:mo>&#xb1;</mml:mo> <mml:mn>0.94</mml:mn></mml:mrow> </mml:math> (4.5, 5.32) mL. In comparison, the commercial software predicted infarct with a DC of <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.31</mml:mn> <mml:mo>&#xb1;</mml:mo> <mml:mn>0.17</mml:mn></mml:mrow> </mml:math> (0.26, 0.36) and MAVE of <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>9.77</mml:mn> <mml:mo>&#xb1;</mml:mo> <mml:mn>8.35</mml:mn></mml:mrow> </mml:math> (7.12, 12.42) mL. The algorithm was able to segment infarct + penumbra with a DC of <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.61</mml:mn> <mml:mo>&#xb1;</mml:mo> <mml:mn>0.04</mml:mn></mml:mrow> </mml:math> (0.6, 0.63), and MAVE of <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>6.51</mml:mn> <mml:mo>&#xb1;</mml:mo> <mml:mn>1.37</mml:mn></mml:mrow> </mml:math> (5.91, 7.11) mL. In comparison, the commercial software predicted infarct + penumbra with a DC of <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.3</mml:mn> <mml:mo>&#xb1;</mml:mo> <mml:mn>0.19</mml:mn></mml:mrow> </mml:math> (0.25, 0.35) and MAVE of <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>9.18</mml:mn> <mml:mo>&#xb1;</mml:mo> <mml:mn>7.55</mml:mn></mml:mrow> </mml:math> (7.25, 11.11) mL.</AbstractText><AbstractText Label="CONCLUSIONS" NlmCategory="UNASSIGNED">Use of deep learning algorithms to assess severity of AIS in terms of infarct and penumbra volume is precise and outperforms current relative thresholding methods. Such an algorithm would enhance the reliability of CTP in guiding treatment decisions.</AbstractText><CopyrightInformation>&#xa9; 2023 Society of Photo-Optical Instrumentation Engineers (SPIE).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Bhurwani</LastName><ForeName>Mohammad Mahdi Shiraz</ForeName><Initials>MMS</Initials><Identifier Source="ORCID">0000-0001-6226-4392</Identifier><AffiliationInfo><Affiliation>University at Buffalo, Department of Biomedical Engineering, Buffalo, New York, United States.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Canon Stroke and Vascular Research Center, Buffalo, New York, United States.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Boutelier</LastName><ForeName>Timothe</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>Olea Medical, La Ciotat, France.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Davis</LastName><ForeName>Adam</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Olea Medical, La Ciotat, France.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Gautier</LastName><ForeName>Guillaume</ForeName><Initials>G</Initials><AffiliationInfo><Affiliation>Olea Medical, La Ciotat, France.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Swetz</LastName><ForeName>Dennis</ForeName><Initials>D</Initials><AffiliationInfo><Affiliation>University at Buffalo, Department of Biomedical Engineering, Buffalo, New York, United States.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Canon Stroke and Vascular Research Center, Buffalo, New York, United States.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Rava</LastName><ForeName>Ryan A</ForeName><Initials>RA</Initials><Identifier Source="ORCID">0000-0001-6456-8445</Identifier><AffiliationInfo><Affiliation>University at Buffalo, Department of Biomedical Engineering, Buffalo, New York, United States.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Canon Stroke and Vascular Research Center, Buffalo, New York, United States.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Raguenes</LastName><ForeName>Dorian</ForeName><Initials>D</Initials><AffiliationInfo><Affiliation>Olea Medical, La Ciotat, France.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Waqas</LastName><ForeName>Muhammad</ForeName><Initials>M</Initials><Identifier Source="ORCID">0000-0003-4500-7954</Identifier><AffiliationInfo><Affiliation>Canon Stroke and Vascular Research Center, Buffalo, New York, United States.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>University at Buffalo, Department of Neurosurgery, Buffalo, New York, United States.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Snyder</LastName><ForeName>Kenneth V</ForeName><Initials>KV</Initials><AffiliationInfo><Affiliation>Canon Stroke and Vascular Research Center, Buffalo, New York, United States.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>University at Buffalo, Department of Neurosurgery, Buffalo, New York, United States.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Siddiqui</LastName><ForeName>Adnan H</ForeName><Initials>AH</Initials><AffiliationInfo><Affiliation>Canon Stroke and Vascular Research Center, Buffalo, New York, United States.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>University at Buffalo, Department of Neurosurgery, Buffalo, New York, United States.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Ionita</LastName><ForeName>Ciprian N</ForeName><Initials>CN</Initials><Identifier Source="ORCID">0000-0001-7049-0592</Identifier><AffiliationInfo><Affiliation>University at Buffalo, Department of Biomedical Engineering, Buffalo, New York, United States.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Canon Stroke and Vascular Research Center, Buffalo, New York, United States.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>University at Buffalo, Department of Neurosurgery, Buffalo, New York, United States.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>09</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>J Med Imaging (Bellingham)</MedlineTA><NlmUniqueID>101643461</NlmUniqueID><ISSNLinking>2329-4302</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">computed tomography perfusion</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword><Keyword MajorTopicYN="N">infarct core</Keyword><Keyword MajorTopicYN="N">ischemic penumbra</Keyword><Keyword MajorTopicYN="N">ischemic stroke</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>5</Month><Day>11</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>20</Day></PubMedPubDate><PubMedPubDate PubStatus="pmc-release"><Year>2024</Year><Month>1</Month><Day>9</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>2</Hour><Minute>3</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>14</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>14</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>ppublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36636489</ArticleId><ArticleId IdType="pmc">PMC9826796</ArticleId><ArticleId IdType="doi">10.1117/1.JMI.10.1.014001</ArticleId><ArticleId IdType="pii">22116GRR</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36635582</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>12</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1573-7373</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>13</Day></PubDate></JournalIssue><Title>Journal of neuro-oncology</Title><ISOAbbreviation>J Neurooncol</ISOAbbreviation></Journal><ArticleTitle>Application of artificial intelligence to stereotactic radiosurgery for intracranial lesions: detection, segmentation, and outcome prediction.</ArticleTitle><ELocationID EIdType="doi" ValidYN="Y">10.1007/s11060-022-04234-x</ELocationID><Abstract><AbstractText Label="BACKGROUND" NlmCategory="BACKGROUND">Rapid evolution of artificial intelligence (AI) prompted its wide application in healthcare systems. Stereotactic radiosurgery served as a good candidate for AI model development and achieved encouraging result in recent years. This article aimed at demonstrating current AI application in radiosurgery.</AbstractText><AbstractText Label="METHODS" NlmCategory="METHODS">Literatures published in PubMed during 2010-2022, discussing AI application in stereotactic radiosurgery were reviewed.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">AI algorithms, especially machine learning/deep learning models, have been administered to different aspect of stereotactic radiosurgery. Spontaneous tumor detection and automated lesion delineation or segmentation were two of the promising application, which could be further extended to longitudinal treatment follow-up. Outcome prediction utilized machine learning algorithms with radiomic-based analysis was another well-established application.</AbstractText><AbstractText Label="CONCLUSIONS" NlmCategory="CONCLUSIONS">Stereotactic radiosurgery has taken a lead role in AI development. Current achievement, limitation, and further investigation was summarized in this article.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Lin</LastName><ForeName>Yen-Yu</ForeName><Initials>YY</Initials><AffiliationInfo><Affiliation>Department of Neurosurgery, Neurological Institute, Taipei Veterans General Hospital, Taipei, Taiwan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Guo</LastName><ForeName>Wan-Yuo</ForeName><Initials>WY</Initials><AffiliationInfo><Affiliation>Department of Radiology, Taipei Veterans General Hospital, Taipei, Taiwan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>School of Medicine, National Yang Ming Chiao Tung University, Taipei, Taiwan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lu</LastName><ForeName>Chia-Feng</ForeName><Initials>CF</Initials><AffiliationInfo><Affiliation>Department of Biomedical Imaging and Radiological Sciences, National Yang Ming Chiao Tung University, Taipei, Taiwan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Peng</LastName><ForeName>Syu-Jyun</ForeName><Initials>SJ</Initials><AffiliationInfo><Affiliation>Professional Master Program in Artificial Intelligence in Medicine, College of Medicine, Taipei Medical University, Taipei, Taiwan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wu</LastName><ForeName>Yu-Te</ForeName><Initials>YT</Initials><AffiliationInfo><Affiliation>Brain Research Center, National Yang Ming Chiao Tung University, Taipei, Taiwan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Biomedical Imaging and Radiological Sciences, National Yang Ming Chiao Tung University, Taipei, Taiwan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Institute of Biophotonics, National Yang Ming Chiao Tung University, Taipei, Taiwan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lee</LastName><ForeName>Cheng-Chia</ForeName><Initials>CC</Initials><AffiliationInfo><Affiliation>Department of Neurosurgery, Neurological Institute, Taipei Veterans General Hospital, Taipei, Taiwan. yfnaughty@gmail.com.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Brain Research Center, National Yang Ming Chiao Tung University, Taipei, Taiwan. yfnaughty@gmail.com.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>School of Medicine, National Yang Ming Chiao Tung University, Taipei, Taiwan. yfnaughty@gmail.com.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType><PublicationType UI="D016454">Review</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>13</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>J Neurooncol</MedlineTA><NlmUniqueID>8309335</NlmUniqueID><ISSNLinking>0167-594X</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Artificial intelligence</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Radiomics</Keyword><Keyword MajorTopicYN="N">Stereotactic radiosurgery</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>12</Month><Day>9</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>30</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>23</Hour><Minute>27</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36635582</ArticleId><ArticleId IdType="doi">10.1007/s11060-022-04234-x</ArticleId><ArticleId IdType="pii">10.1007/s11060-022-04234-x</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Panesar SS, Kliot M, Parrish R, Fernandez-Miranda J, Cagle Y, Britz GW (2020) Promises and perils of artificial intelligence in neurosurgery. Neurosurgery 87:33&#x2013;44. https://doi.org/10.1093/neuros/nyz471</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/neuros/nyz471</ArticleId></ArticleIdList></Reference><Reference><Citation>Rajkomar A, Dean J, Kohane I (2019) Machine learning in medicine. N Engl J Med 380:1347&#x2013;1358. https://doi.org/10.1056/NEJMra1814259</Citation><ArticleIdList><ArticleId IdType="doi">10.1056/NEJMra1814259</ArticleId></ArticleIdList></Reference><Reference><Citation>Barrag&#xe1;n-Montero A, Javaid U, Vald&#xe9;s G, Nguyen D, Desbordes P, Macq B, Willems S, Vandewinckele L, Holmstr&#xf6;m M, L&#xf6;fman F, Michiels S, Souris K, Sterpin E, Lee JA (2021) Artificial intelligence and machine learning for medical imaging: a technology review. Phys Med 83:242&#x2013;256. https://doi.org/10.1016/j.ejmp.2021.04.016</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejmp.2021.04.016</ArticleId></ArticleIdList></Reference><Reference><Citation>Yamamoto M, Serizawa T, Shuto T, Akabane A, Higuchi Y, Kawagishi J, Yamanaka K, Sato Y, Jokura H, Yomo S, Nagano O, Kenai H, Moriki A, Suzuki S, Kida Y, Iwai Y, Hayashi M, Onishi H, Gondo M, Sato M, Akimitsu T, Kubo K, Kikuchi Y, Shibasaki T, Goto T, Takanashi M, Mori Y, Takakura K, Saeki N, Kunieda E, Aoyama H, Momoshima S, Tsuchiya K (2014) Stereotactic radiosurgery for patients with multiple brain metastases (JLGK0901): a multi-institutional prospective observational study. Lancet Oncol 15:387&#x2013;395. https://doi.org/10.1016/s1470-2045(14)70061-0</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/s1470-2045(14)70061-0</ArticleId></ArticleIdList></Reference><Reference><Citation>Serizawa T, Higuchi Y, Yamamoto M, Matsunaga S, Nagano O, Sato Y, Aoyagi K, Yomo S, Koiso T, Hasegawa T, Nakazaki K, Moriki A, Kondoh T, Nagatomo Y, Okamoto H, Kohda Y, Kawai H, Shidoh S, Shibazaki T, Onoue S, Kenai H, Inoue A, Mori H (2018) Comparison of treatment results between 3- and 2-stage Gamma Knife radiosurgery for large brain metastases: a retrospective multi-institutional study. J Neurosurg 131:227&#x2013;237. https://doi.org/10.3171/2018.4.Jns172596</Citation><ArticleIdList><ArticleId IdType="doi">10.3171/2018.4.Jns172596</ArticleId></ArticleIdList></Reference><Reference><Citation>P&#xe9;rez-Ram&#xed;rez &#xda;, Arana E, Moratal D (2016) Brain metastases detection on MR by means of three-dimensional tumor-appearance template matching. J Magn Reson Imaging 44:642&#x2013;652. https://doi.org/10.1002/jmri.25207</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmri.25207</ArticleId></ArticleIdList></Reference><Reference><Citation>Farjam R, Parmar HA, Noll DC, Tsien CI, Cao Y (2012) An approach for computer-aided detection of brain metastases in post-Gd T1-W MRI. Magn Reson Imaging 30:824&#x2013;836. https://doi.org/10.1016/j.mri.2012.02.024</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.mri.2012.02.024</ArticleId></ArticleIdList></Reference><Reference><Citation>Ambrosini RD, Wang P, O&#x2019;Dell WG (2010) Computer-aided detection of metastatic brain tumors using automated three-dimensional template matching. J Magn Reson Imaging 31:85&#x2013;93. https://doi.org/10.1002/jmri.22009</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmri.22009</ArticleId></ArticleIdList></Reference><Reference><Citation>Zheng Q, Yang L, Zeng B, Li J, Guo K, Liang Y, Liao G (2021) Artificial intelligence performance in detecting tumor metastasis from medical radiology imaging: a systematic review and meta-analysis. EClinicalMedicine 31:100669. https://doi.org/10.1016/j.eclinm.2020.100669</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.eclinm.2020.100669</ArticleId></ArticleIdList></Reference><Reference><Citation>Cho SJ, Sunwoo L, Baik SH, Bae YJ, Choi BS, Kim JH (2021) Brain metastasis detection using machine learning: a systematic review and meta-analysis. Neuro Oncol 23:214&#x2013;225. https://doi.org/10.1093/neuonc/noaa232</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/neuonc/noaa232</ArticleId></ArticleIdList></Reference><Reference><Citation>Lu SL, Xiao FR, Cheng JC, Yang WC, Cheng YH, Chang YC, Lin JY, Liang CH, Lu JT, Chen YF, Hsu FM (2021) Randomized multi-reader evaluation of automated detection and segmentation of brain tumors in stereotactic radiosurgery with deep neural networks. Neuro Oncol 23:1560&#x2013;1568. https://doi.org/10.1093/neuonc/noab071</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/neuonc/noab071</ArticleId></ArticleIdList></Reference><Reference><Citation>Guoa WY, Liub KC, Suna YC, Wub WL, Wua HM, Tub E (2020) Device-agnostic AI model for brain metastasis: deep active learning over a nationwide population-based medical image database. Digital Healthcare 2020 at Taipei Veterans General Hospital. Taipei, Taiwan, p 26</Citation></Reference><Reference><Citation>Charron O, Lallement A, Jarnet D, Noblet V, Clavier JB, Meyer P (2018) Automatic detection and segmentation of brain metastases on multimodal MR images with a deep convolutional neural network. Comput Biol Med 95:43&#x2013;54. https://doi.org/10.1016/j.compbiomed.2018.02.004</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2018.02.004</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang M, Young GS, Chen H, Li J, Qin L, McFaline-Figueroa JR, Reardon DA, Cao X, Wu X, Xu X (2020) Deep-learning detection of cancer metastases to the brain on MRI. J Magn Reson Imaging 52:1227&#x2013;1236. https://doi.org/10.1002/jmri.27129</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmri.27129</ArticleId></ArticleIdList></Reference><Reference><Citation>Yin S, Luo X, Yang Y, Shao Y, Ma L, Lin C, Yang Q, Wang D, Luo Y, Mai Z, Fan W, Zheng D, Li J, Cheng F, Zhang Y, Zhong X, Shen F, Shao G, Wu J, Sun Y, Luo H, Li C, Gao Y, Shen D, Zhang R, Xie C (2022) Development and validation of a deep-learning model for detecting brain metastases on 3D post-contrast MRI: a multi-center multi-reader evaluation study. Neuro Oncol 24:1559&#x2013;1570. https://doi.org/10.1093/neuonc/noac025</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/neuonc/noac025</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhou Z, Sanders JW, Johnson JM, Gule-Monroe M, Chen M, Briere TM, Wang Y, Son JB, Pagel MD, Ma J, Li J (2020) MetNet: computer-aided segmentation of brain metastases in post-contrast T1-weighted magnetic resonance imaging. Radiother Oncol 153:189&#x2013;196. https://doi.org/10.1016/j.radonc.2020.09.016</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.radonc.2020.09.016</ArticleId></ArticleIdList></Reference><Reference><Citation>Peng SJ, Lee CC, Wu HM, Lin CJ, Shiau CY, Guo WY, Pan DH, Liu KD, Chung WY, Yang HC (2019) Fully automated tissue segmentation of the prescription isodose region delineated through the Gamma knife plan for cerebral arteriovenous malformation (AVM) using fuzzy C-means (FCM) clustering. Neuroimage Clin 21:101608. https://doi.org/10.1016/j.nicl.2018.11.018</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.nicl.2018.11.018</ArticleId></ArticleIdList></Reference><Reference><Citation>Lee CC, Yang HC, Lin CJ, Chen CJ, Wu HM, Shiau CY, Guo WY, Hung-Chi Pan D, Liu KD, Chung WY, Peng SJ (2019) Intervening nidal brain parenchyma and risk of radiation-induced changes after radiosurgery for brain arteriovenous malformation: a study using an unsupervised machine learning algorithm. World Neurosurg 125:e132&#x2013;e138. https://doi.org/10.1016/j.wneu.2018.12.220</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.wneu.2018.12.220</ArticleId></ArticleIdList></Reference><Reference><Citation>Simon AB, Hurt B, Karunamuni R, Kim GY, Moiseenko V, Olson S, Farid N, Hsiao A, Hattangadi-Gluth JA (2022) Automated segmentation of multiparametric magnetic resonance images for cerebral AVM radiosurgery planning: a deep learning approach. Sci Rep 12:786. https://doi.org/10.1038/s41598-021-04466-3</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-021-04466-3</ArticleId></ArticleIdList></Reference><Reference><Citation>Shapey J, Wang G, Dorent R, Dimitriadis A, Li W, Paddick I, Kitchen N, Bisdas S, Saeed SR, Ourselin S, Bradford R, Vercauteren T (2019) An artificial intelligence framework for automatic segmentation and volumetry of vestibular schwannomas from contrast-enhanced T1-weighted and high-resolution T2-weighted MRI. J Neurosurg 134:1&#x2013;9. https://doi.org/10.3171/2019.9.Jns191949</Citation><ArticleIdList><ArticleId IdType="doi">10.3171/2019.9.Jns191949</ArticleId></ArticleIdList></Reference><Reference><Citation>Lee WK, Wu CC, Lee CC, Lu CF, Yang HC, Huang TH, Lin CY, Chung WY, Wang PS, Wu HM, Guo WY, Wu YT (2020) Combining analysis of multi-parametric MR images into a convolutional neural network: precise target delineation for vestibular schwannoma treatment planning. Artif Intell Med 107:101911. https://doi.org/10.1016/j.artmed.2020.101911</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.artmed.2020.101911</ArticleId></ArticleIdList></Reference><Reference><Citation>Lee CC, Lee WK, Wu CC, Lu CF, Yang HC, Chen YW, Chung WY, Hu YS, Wu HM, Wu YT, Guo WY (2021) Applying artificial intelligence to longitudinal imaging analysis of vestibular schwannoma following radiosurgery. Sci Rep 11:3106. https://doi.org/10.1038/s41598-021-82665-8</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-021-82665-8</ArticleId></ArticleIdList></Reference><Reference><Citation>Cassinelli Petersen G, Bousabarah K, Verma T, von Reppert M, Jekel L, Gordem A, Jang B, Merkaj S, Abi Fadel S, Owens R, Omuro A, Chiang V, Ikuta I, Lin M, Aboian MS (2022) Real-time PACS-integrated longitudinal brain metastasis tracking tool provides comprehensive assessment of treatment response to radiosurgery. Neuro-Oncol Adv 22:505&#x2013;514. https://doi.org/10.1093/noajnl/vdac116</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/noajnl/vdac116</ArticleId></ArticleIdList></Reference><Reference><Citation>Xue J, Wang B, Ming Y, Liu X, Jiang Z, Wang C, Liu X, Chen L, Qu J, Xu S, Tang X, Mao Y, Liu Y, Li D (2020) Deep learning-based detection and segmentation-assisted management of brain metastases. Neuro Oncol 22:505&#x2013;514. https://doi.org/10.1093/neuonc/noz234</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/neuonc/noz234</ArticleId></ArticleIdList></Reference><Reference><Citation>Gr&#xf8;vik E, Yi D, Iv M, Tong E, Rubin D, Zaharchuk G (2020) Deep learning enables automatic detection and segmentation of brain metastases on multisequence MRI. J Magn Reson Imaging 51:175&#x2013;182. https://doi.org/10.1002/jmri.26766</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmri.26766</ArticleId></ArticleIdList></Reference><Reference><Citation>Hu S-Y, Weng W-H, Lu S-L, Cheng Y-H, Xiao F, Hsu F-M, Lu J-T Multimodal volume-aware detection and segmentation for brain metastases radiosurgery. In: Workshop on Artificial Intelligence in Radiation Therapy. Springer, pp 61&#x2013;69</Citation></Reference><Reference><Citation>Bousabarah K, Ruge M, Brand JS, Hoevels M, Rue&#xdf; D, Borggrefe J, Gro&#xdf;e Hokamp N, Visser-Vandewalle V, Maintz D, Treuer H, Kocher M (2020) Deep convolutional neural networks for automated segmentation of brain metastases trained on clinical data. Radiat Oncol 15:87. https://doi.org/10.1186/s13014-020-01514-6</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s13014-020-01514-6</ArticleId></ArticleIdList></Reference><Reference><Citation>Dikici E, Ryu JL, Demirer M, Bigelow M, White RD, Slone W, Erdal BS, Prevedello LM (2020) Automated brain metastases detection framework for T1-weighted contrast-enhanced 3D MRI. IEEE J Biomed Health Inform 24:2883&#x2013;2893. https://doi.org/10.1109/jbhi.2020.2982103</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/jbhi.2020.2982103</ArticleId></ArticleIdList></Reference><Reference><Citation>Cao Y, Vassantachart A, Ye JC, Yu C, Ruan D, Sheng K, Lao Y, Shen ZL, Balik S, Bian S, Zada G, Shiu A, Chang EL, Yang W (2021) Automatic detection and segmentation of multiple brain metastases on magnetic resonance image using asymmetric UNet architecture. Phys Med Biol 66:015003. https://doi.org/10.1088/1361-6560/abca53</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/1361-6560/abca53</ArticleId></ArticleIdList></Reference><Reference><Citation>Rudie JD, Weiss DA, Colby JB, Rauschecker AM, Laguna B, Braunstein S, Sugrue LP, Hess CP, Villanueva-Meyer JE (2021) Three-dimensional U-net convolutional neural network for detection and segmentation of intracranial metastases. Radiol Artif Intell 3:e200204. https://doi.org/10.1148/ryai.2021200204</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/ryai.2021200204</ArticleId></ArticleIdList></Reference><Reference><Citation>Pennig L, Shahzad R, Caldeira L, Lennartz S, Thiele F, Goertz L, Zopfs D, Mei&#xdf;ner AK, F&#xfc;rtjes G, Perkuhn M, Kabbasch C, Grau S, Borggrefe J, Laukamp KR (2021) Automated detection and segmentation of brain metastases in malignant melanoma: evaluation of a dedicated deep learning model. AJNR Am J Neuroradiol 42:655&#x2013;662. https://doi.org/10.3174/ajnr.A6982</Citation><ArticleIdList><ArticleId IdType="doi">10.3174/ajnr.A6982</ArticleId></ArticleIdList></Reference><Reference><Citation>Hsu DG, Ballangrud &#xc5;, Shamseddine A, Deasy JO, Veeraraghavan H, Cervino L, Beal K, Aristophanous M (2021) Automatic segmentation of brain metastases using T1 magnetic resonance and computed tomography images. Phys Med Biol 66:175014</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/1361-6560/ac1835</ArticleId></ArticleIdList></Reference><Reference><Citation>Yoo Y, Ceccaldi P, Liu S, Re TJ, Cao Y, Balter JM, Gibson E (2021) Evaluating deep learning methods in detecting and segmenting different sizes of brain metastases on 3D post-contrast T1-weighted images. J Med Imaging (Bellingham) 8:037001. https://doi.org/10.1117/1.Jmi.8.3.037001</Citation><ArticleIdList><ArticleId IdType="doi">10.1117/1.Jmi.8.3.037001</ArticleId></ArticleIdList></Reference><Reference><Citation>Chartrand G, Emiliani RD, Pawlowski SA, Markel DA, Bahig H, Cengarle-Samak A, Rajakesari S, Lavoie J, Ducharme S, Roberge D (2022) Automated detection of brain metastases on T1-weighted MRI using a convolutional neural network: impact of volume aware loss and sampling strategy. J Magn Resonan Imaging 56:1885. https://doi.org/10.1002/jmri.28274</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmri.28274</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang T, Lei Y, Tian S, Jiang X, Zhou J, Liu T, Dresser S, Curran WJ, Shu HK, Yang X (2019) Learning-based automatic segmentation of arteriovenous malformations on contrast CT images in brain stereotactic radiosurgery. Med Phys 46:3133&#x2013;3141. https://doi.org/10.1002/mp.13560</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mp.13560</ArticleId></ArticleIdList></Reference><Reference><Citation>Huang CY, Peng SJ, Wu HM, Yang HC, Chen CJ, Wang MC, Hu YS, Chen YW, Lin CJ, Guo WY, Pan DH, Chung WY, Lee CC (2021) Quantification of tumor response of cystic vestibular schwannoma to Gamma Knife radiosurgery by using artificial intelligence. J Neurosurg 2021:1&#x2013;9. https://doi.org/10.3171/2021.4.Jns203700</Citation><ArticleIdList><ArticleId IdType="doi">10.3171/2021.4.Jns203700</ArticleId></ArticleIdList></Reference><Reference><Citation>Lambin P, Leijenaar RTH, Deist TM, Peerlings J, de Jong EEC, van Timmeren J, Sanduleanu S, Larue R, Even AJG, Jochems A, van Wijk Y, Woodruff H, van Soest J, Lustberg T, Roelofs E, van Elmpt W, Dekker A, Mottaghy FM, Wildberger JE, Walsh S (2017) Radiomics: the bridge between medical imaging and personalized medicine. Nat Rev Clin Oncol 14:749&#x2013;762. https://doi.org/10.1038/nrclinonc.2017.141</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nrclinonc.2017.141</ArticleId></ArticleIdList></Reference><Reference><Citation>Huang CY, Lee CC, Yang HC, Lin CJ, Wu HM, Chung WY, Shiau CY, Guo WY, Pan DH, Peng SJ (2020) Radiomics as prognostic factor in brain metastases treated with Gamma Knife radiosurgery. J Neurooncol 146:439&#x2013;449. https://doi.org/10.1007/s11060-019-03343-4</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11060-019-03343-4</ArticleId></ArticleIdList></Reference><Reference><Citation>Gao D, Meng X, Jin H, Liu A, Sun S (2022) Assessment of gamma knife radiosurgery for unruptured cerebral arterioveneus malformations based on multi-parameter radiomics of MRI. Magn Reson Imaging 92:251&#x2013;259. https://doi.org/10.1016/j.mri.2022.07.008</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.mri.2022.07.008</ArticleId></ArticleIdList></Reference><Reference><Citation>Meng X, Gao D, He H, Sun S, Liu A, Jin H, Li Y (2022) A machine learning model predicts the outcome of SRS for residual arteriovenous malformations after partial embolization: a real-world clinical obstacle. World Neurosurg 163:e73&#x2013;e82. https://doi.org/10.1016/j.wneu.2022.03.007</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.wneu.2022.03.007</ArticleId></ArticleIdList></Reference><Reference><Citation>Yang HC, Wu CC, Lee CC, Huang HE, Lee WK, Chung WY, Wu HM, Guo WY, Wu YT, Lu CF (2021) Prediction of pseudoprogression and long-term outcome of vestibular schwannoma after Gamma Knife radiosurgery based on preradiosurgical MR radiomics. Radiother Oncol 155:123&#x2013;130. https://doi.org/10.1016/j.radonc.2020.10.041</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.radonc.2020.10.041</ArticleId></ArticleIdList></Reference><Reference><Citation>Langenhuizen P, Sebregts SHP, Zinger S, Leenstra S, Verheul JB, de With PHN (2020) Prediction of transient tumor enlargement using MRI tumor texture after radiosurgery on vestibular schwannoma. Med Phys 47:1692&#x2013;1701. https://doi.org/10.1002/mp.14042</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mp.14042</ArticleId></ArticleIdList></Reference><Reference><Citation>Langenhuizen P, Zinger S, Leenstra S, Kunst HPM, Mulder JJS, Hanssens PEJ, de With PHN, Verheul JB (2020) Radiomics-based prediction of long-term treatment response of vestibular schwannomas following stereotactic radiosurgery. Otol Neurotol 41:e1321&#x2013;e1327. https://doi.org/10.1097/mao.0000000000002886</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/mao.0000000000002886</ArticleId></ArticleIdList></Reference><Reference><Citation>Mouraviev A, Detsky J, Sahgal A, Ruschin M, Lee YK, Karam I, Heyn C, Stanisz GJ, Martel AL (2020) Use of radiomics for the prediction of local control of brain metastases after stereotactic radiosurgery. Neuro Oncol 22:797&#x2013;805. https://doi.org/10.1093/neuonc/noaa007</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/neuonc/noaa007</ArticleId></ArticleIdList></Reference><Reference><Citation>Liao CY, Lee CC, Yang HC, Chen CJ, Chung WY, Wu HM, Guo WY, Liu RS, Lu CF (2021) Enhancement of radiosurgical treatment outcome prediction using MRI radiomics in patients with non-small cell lung cancer brain metastases. Cancers (Basel) 13:4030. https://doi.org/10.3390/cancers13164030</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/cancers13164030</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang H, Xue J, Qu T, Bernstein K, Chen T, Barbee D, Silverman JS, Kondziolka D (2021) Predicting local failure of brain metastases after stereotactic radiosurgery with radiomics on planning MR images and dose maps. Med Phys 48:5522&#x2013;5530. https://doi.org/10.1002/mp.15110</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mp.15110</ArticleId></ArticleIdList></Reference><Reference><Citation>Mulford K, Chen C, Dusenbery K, Yuan J, Hunt MA, Chen CC, Sperduto P, Watanabe Y, Wilke C (2021) A radiomics-based model for predicting local control of resected brain metastases receiving adjuvant SRS. Clin Transl Radiat Oncol 29:27&#x2013;32. https://doi.org/10.1016/j.ctro.2021.05.001</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ctro.2021.05.001</ArticleId></ArticleIdList></Reference><Reference><Citation>Dohm AE, Nickles TM, Miller CE, Bowers HJ, Miga MI, Attia A, Chan MD, Weis JA (2021) Clinical assessment of a biophysical model for distinguishing tumor progression from radiation necrosis. Med Phys 48:3852&#x2013;3859. https://doi.org/10.1002/mp.14999</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mp.14999</ArticleId></ArticleIdList></Reference><Reference><Citation>Peng L, Parekh V, Huang P, Lin DD, Sheikh K, Baker B, Kirschbaum T, Silvestri F, Son J, Robinson A, Huang E, Ames H, Grimm J, Chen L, Shen C, Soike M, McTyre E, Redmond K, Lim M, Lee J, Jacobs MA, Kleinberg L (2018) Distinguishing true progression from radionecrosis after stereotactic radiation therapy for brain metastases with machine learning and radiomics. Int J Radiat Oncol Biol Phys 102:1236&#x2013;1243. https://doi.org/10.1016/j.ijrobp.2018.05.041</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ijrobp.2018.05.041</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen X, Parekh VS, Peng L, Chan MD, Redmond KJ, Soike M, McTyre E, Lin D, Jacobs MA, Kleinberg LR (2021) Multiparametric radiomic tissue signature and machine learning for distinguishing radiation necrosis from tumor progression after stereotactic radiosurgery. Neurooncol Adv 3:vdab150. https://doi.org/10.1093/noajnl/vdab150</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/noajnl/vdab150</ArticleId></ArticleIdList></Reference><Reference><Citation>Lu CF, Lee CC, Wu HM, Yang HC, Chen MC, Lin CJ, Guo WY, Chung WY (2020) Prediction of hemorrhage free survival after gamma knife radiosurgery based on preradiosurgical MR radiomics in cavernous malformation. In: The 28th annual meeting &amp; exhibition of ISMRM</Citation></Reference><Reference><Citation>Meng X, Gao D, Jin H, Wang K, Bao E, Liu A, Li Y, Sun S (2021) Factors affecting volume reduction velocity for arteriovenous malformations after treatment with dose-stage stereotactic radiosurgery. Front Oncol 11:769533. https://doi.org/10.3389/fonc.2021.769533</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fonc.2021.769533</ArticleId></ArticleIdList></Reference><Reference><Citation>Hsu CY, Xiao F, Liu KL, Chen TL, Lee YC, Wang W (2020) Radiomic analysis of magnetic resonance imaging predicts brain metastases velocity and clinical outcome after upfront radiosurgery. Neurooncol Adv 2:vdaa100. https://doi.org/10.1093/noajnl/vdaa100</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/noajnl/vdaa100</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36635192</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>12</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1879-291X</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>10</Day></PubDate></JournalIssue><Title>Ultrasound in medicine &amp; biology</Title><ISOAbbreviation>Ultrasound Med Biol</ISOAbbreviation></Journal><ArticleTitle>Ultrasound Signal Processing: From Models to Deep Learning.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">S0301-5629(22)00632-9</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1016/j.ultrasmedbio.2022.11.003</ELocationID><Abstract><AbstractText>Medical ultrasound imaging relies heavily on high-quality signal processing to provide reliable and interpretable image reconstructions. Conventionally, reconstruction algorithms have been derived from physical principles. These algorithms rely on assumptions and approximations of the underlying measurement model, limiting image quality in settings where these assumptions break down. Conversely, more sophisticated solutions based on statistical modeling or careful parameter tuning or derived from increased model complexity can be sensitive to different environments. Recently, deep learning-based methods, which are optimized in a data-driven fashion, have gained popularity. These model-agnostic techniques often rely on generic model structures and require vast training data to converge to a robust solution. A relatively new paradigm combines the power of the two: leveraging data-driven deep learning and exploiting domain knowledge. These model-based solutions yield high robustness and require fewer parameters and training data than conventional neural networks. In this work we provide an overview of these techniques from the recent literature and discuss a wide variety of ultrasound applications. We aim to inspire the reader to perform further research in this area and to address the opportunities within the field of ultrasound signal processing. We conclude with a future perspective on model-based deep learning techniques for medical ultrasound.</AbstractText><CopyrightInformation>Copyright &#xa9; 2022 The Authors. Published by Elsevier Inc. All rights reserved.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Luijten</LastName><ForeName>Ben</ForeName><Initials>B</Initials><AffiliationInfo><Affiliation>Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, The Netherlands. Electronic address: w.m.b.luijten@tue.nl.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Chennakeshava</LastName><ForeName>Nishith</ForeName><Initials>N</Initials><AffiliationInfo><Affiliation>Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, The Netherlands.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Eldar</LastName><ForeName>Yonina C</ForeName><Initials>YC</Initials><AffiliationInfo><Affiliation>Faculty of Math and Computer Science, Weizmann Institute of Science, Rehovot, Israel.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Mischi</LastName><ForeName>Massimo</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, The Netherlands.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>van Sloun</LastName><ForeName>Ruud J G</ForeName><Initials>RJG</Initials><AffiliationInfo><Affiliation>Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, The Netherlands; Philips Research, Eindhoven, The Netherlands.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType><PublicationType UI="D016454">Review</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>10</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>Ultrasound Med Biol</MedlineTA><NlmUniqueID>0410553</NlmUniqueID><ISSNLinking>0301-5629</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Probabilistic modeling</Keyword><Keyword MajorTopicYN="N">Ultrasound</Keyword></KeywordList><CoiStatement>Conflict of interest disclosure The authors declare that they have no conflicts of interest with respect to this work.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>3</Month><Day>10</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>11</Month><Day>2</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>11</Month><Day>5</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>22</Hour><Minute>3</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36635192</ArticleId><ArticleId IdType="doi">10.1016/j.ultrasmedbio.2022.11.003</ArticleId><ArticleId IdType="pii">S0301-5629(22)00632-9</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36634294</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>12</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1527-7755</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>12</Day></PubDate></JournalIssue><Title>Journal of clinical oncology : official journal of the American Society of Clinical Oncology</Title><ISOAbbreviation>J Clin Oncol</ISOAbbreviation></Journal><ArticleTitle>Sybil: A Validated Deep Learning Model to Predict Future Lung Cancer Risk From a Single Low-Dose Chest Computed Tomography.</ArticleTitle><Pagination><StartPage>JCO2201345</StartPage><MedlinePgn>JCO2201345</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1200/JCO.22.01345</ELocationID><Abstract><AbstractText Label="PURPOSE" NlmCategory="OBJECTIVE">Low-dose computed tomography (LDCT) for lung cancer screening is effective, although most eligible people are not being screened. Tools that provide personalized future cancer risk assessment could focus approaches toward those most likely to benefit. We hypothesized that a deep learning model assessing the entire volumetric LDCT data could be built to predict individual risk without requiring additional demographic or clinical data.</AbstractText><AbstractText Label="METHODS" NlmCategory="METHODS">We developed a model called Sybil using LDCTs from the National Lung Screening Trial (NLST). Sybil requires only one LDCT and does not require clinical data or radiologist annotations; it can run in real time in the background on a radiology reading station. Sybil was validated on three independent data sets: a heldout set of 6,282 LDCTs from NLST participants, 8,821 LDCTs from Massachusetts General Hospital (MGH), and 12,280 LDCTs from Chang Gung Memorial Hospital (CGMH, which included people with a range of smoking history including nonsmokers).</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">Sybil achieved area under the receiver-operator curves for lung cancer prediction at 1 year of 0.92 (95% CI, 0.88 to 0.95) on NLST, 0.86 (95% CI, 0.82 to 0.90) on MGH, and 0.94 (95% CI, 0.91 to 1.00) on CGMH external validation sets. Concordance indices over 6 years were 0.75 (95% CI, 0.72 to 0.78), 0.81 (95% CI, 0.77 to 0.85), and 0.80 (95% CI, 0.75 to 0.86) for NLST, MGH, and CGMH, respectively.</AbstractText><AbstractText Label="CONCLUSION" NlmCategory="CONCLUSIONS">Sybil can accurately predict an individual's future lung cancer risk from a single LDCT scan to further enable personalized screening. Future study is required to understand Sybil's clinical applications. Our model and annotations are publicly available.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Mikhael</LastName><ForeName>Peter G</ForeName><Initials>PG</Initials><Identifier Source="ORCID">0000-0002-6030-1636</Identifier><AffiliationInfo><Affiliation>Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Jameel Clinic, Massachusetts Institute of Technology, Cambridge, MA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wohlwend</LastName><ForeName>Jeremy</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Jameel Clinic, Massachusetts Institute of Technology, Cambridge, MA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yala</LastName><ForeName>Adam</ForeName><Initials>A</Initials><Identifier Source="ORCID">0000-0001-9576-2590</Identifier><AffiliationInfo><Affiliation>Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Jameel Clinic, Massachusetts Institute of Technology, Cambridge, MA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Karstens</LastName><ForeName>Ludvig</ForeName><Initials>L</Initials><Identifier Source="ORCID">0000-0003-4287-6185</Identifier><AffiliationInfo><Affiliation>Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Jameel Clinic, Massachusetts Institute of Technology, Cambridge, MA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Xiang</LastName><ForeName>Justin</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Jameel Clinic, Massachusetts Institute of Technology, Cambridge, MA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Takigami</LastName><ForeName>Angelo K</ForeName><Initials>AK</Initials><Identifier Source="ORCID">0000-0002-2605-1362</Identifier><AffiliationInfo><Affiliation>Harvard Medical School, Boston, MA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, Massachusetts General Hospital, Boston, MA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Bourgouin</LastName><ForeName>Patrick P</ForeName><Initials>PP</Initials><Identifier Source="ORCID">0000-0002-8187-3983</Identifier><AffiliationInfo><Affiliation>Harvard Medical School, Boston, MA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, Massachusetts General Hospital, Boston, MA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Chan</LastName><ForeName>PuiYee</ForeName><Initials>P</Initials><Identifier Source="ORCID">0000-0002-9133-9506</Identifier><AffiliationInfo><Affiliation>Department of Medicine, Massachusetts General Hospital, Boston, MA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Mrah</LastName><ForeName>Sofiane</ForeName><Initials>S</Initials><Identifier Source="ORCID">0000-0001-5253-2090</Identifier><AffiliationInfo><Affiliation>Department of Radiology, Massachusetts General Hospital, Boston, MA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Amayri</LastName><ForeName>Wael</ForeName><Initials>W</Initials><AffiliationInfo><Affiliation>Department of Radiology, Massachusetts General Hospital, Boston, MA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Juan</LastName><ForeName>Yu-Hsiang</ForeName><Initials>YH</Initials><AffiliationInfo><Affiliation>Chang Gung University, Taoyuan, Taiwan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Medical Imaging and Intervention, Chang Gung Memorial Hospital, Taoyuan, Taiwan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yang</LastName><ForeName>Cheng-Ta</ForeName><Initials>CT</Initials><AffiliationInfo><Affiliation>Chang Gung University, Taoyuan, Taiwan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Thoracic Medicine, Chang Gung Memorial Hospital, Taoyuan, Taiwan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wan</LastName><ForeName>Yung-Liang</ForeName><Initials>YL</Initials><Identifier Source="ORCID">0000-0002-3039-996X</Identifier><AffiliationInfo><Affiliation>Chang Gung University, Taoyuan, Taiwan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Medical Imaging and Intervention, Chang Gung Memorial Hospital, Taoyuan, Taiwan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lin</LastName><ForeName>Gigin</ForeName><Initials>G</Initials><Identifier Source="ORCID">0000-0001-7246-1058</Identifier><AffiliationInfo><Affiliation>Chang Gung University, Taoyuan, Taiwan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Medical Imaging and Intervention, Chang Gung Memorial Hospital, Taoyuan, Taiwan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Sequist</LastName><ForeName>Lecia V</ForeName><Initials>LV</Initials><Identifier Source="ORCID">0000-0002-8965-6991</Identifier><AffiliationInfo><Affiliation>Harvard Medical School, Boston, MA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Medicine, Massachusetts General Hospital, Boston, MA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Fintelmann</LastName><ForeName>Florian J</ForeName><Initials>FJ</Initials><Identifier Source="ORCID">0000-0002-0119-3903</Identifier><AffiliationInfo><Affiliation>Harvard Medical School, Boston, MA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, Massachusetts General Hospital, Boston, MA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Barzilay</LastName><ForeName>Regina</ForeName><Initials>R</Initials><AffiliationInfo><Affiliation>Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Jameel Clinic, Massachusetts Institute of Technology, Cambridge, MA.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>12</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>J Clin Oncol</MedlineTA><NlmUniqueID>8309333</NlmUniqueID><ISSNLinking>0732-183X</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>16</Hour><Minute>2</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36634294</ArticleId><ArticleId IdType="doi">10.1200/JCO.22.01345</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36633792</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>12</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1867-1462</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>12</Day></PubDate></JournalIssue><Title>Interdisciplinary sciences, computational life sciences</Title><ISOAbbreviation>Interdiscip Sci</ISOAbbreviation></Journal><ArticleTitle>Review of Progress in Diagnostic Studies of Autism Spectrum Disorder Using Neuroimaging.</ArticleTitle><ELocationID EIdType="doi" ValidYN="Y">10.1007/s12539-022-00548-6</ELocationID><Abstract><AbstractText>This review article summarizes the recent advances in the diagnostic studies of autism spectrum disorders (ASDs) considering some of the most influential research articles from the last two decades. ASD is a heterogeneous neurodevelopmental disorder characterized by abnormalities in social interaction, communication, and behavioral patterns as well as some unique strengths and differences. The current diagnosis systems are based on autism diagnostic observation schedule (ADOS) or autism diagnostic interview-revised (ADI-R), but biological markers are also important for an effective diagnosis of ASDs. The amalgamation of neuroimaging techniques, such as structural and functional magnetic resonance imaging (sMRI and fMRI), with machine-learning and deep-learning approaches helps throw new light on typical biological markers of ASDs at the early stage of life. To assess the performance of a deep neural network, we develop a light-weighted CNN model for ASD classification. The overall accuracy, precision, and F1-score of the proposed model are 99.92%, 99.93% and 99.92%, respectively. All the neuroimaging studies we have reviewed can be divided into 3 categories, viz. thickness, volume and functional connectivity-based studies. We conclude with a discussion of the major findings of considered studies and promising directions for future research in this field.</AbstractText><CopyrightInformation>&#xa9; 2023. International Association of Scientists in the Interdisciplinary Areas.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Kaur</LastName><ForeName>Palwinder</ForeName><Initials>P</Initials><Identifier Source="ORCID">0000-0002-0800-6058</Identifier><AffiliationInfo><Affiliation>Department of Computer Science and Technology, Central University of Punjab, Bathinda, Punjab, 151001, India.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kaur</LastName><ForeName>Amandeep</ForeName><Initials>A</Initials><Identifier Source="ORCID">0000-0003-4844-9299</Identifier><AffiliationInfo><Affiliation>Department of Computer Science and Technology, Central University of Punjab, Bathinda, Punjab, 151001, India. amandeep.kaur@cup.edu.in.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>12</Day></ArticleDate></Article><MedlineJournalInfo><Country>Germany</Country><MedlineTA>Interdiscip Sci</MedlineTA><NlmUniqueID>101515919</NlmUniqueID><ISSNLinking>1867-1462</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Autism spectrum disorders</Keyword><Keyword MajorTopicYN="N">Functional magnetic resonance imaging</Keyword><Keyword MajorTopicYN="N">Machine-learning</Keyword><Keyword MajorTopicYN="N">Neurodevelopmental disorder</Keyword><Keyword MajorTopicYN="N">Structural magnetic resonance imaging</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>4</Month><Day>22</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>27</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>27</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>11</Hour><Minute>20</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36633792</ArticleId><ArticleId IdType="doi">10.1007/s12539-022-00548-6</ArticleId><ArticleId IdType="pii">10.1007/s12539-022-00548-6</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Patra S, Kar SK (2021) Autism spectrum disorder in India: a scoping review. Int Rev Psychiatry 33(1&#x2013;2):81&#x2013;112. https://doi.org/10.1080/09540261.2020.1761136</Citation><ArticleIdList><ArticleId IdType="doi">10.1080/09540261.2020.1761136</ArticleId></ArticleIdList></Reference><Reference><Citation>Geschwind DH, Levitt P (2007) Autism spectrum disorders: developmental disconnection syndromes. Curr Opin Neurobiol 17(1):103&#x2013;111. https://doi.org/10.1016/j.conb.2007.01.009</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.conb.2007.01.009</ArticleId></ArticleIdList></Reference><Reference><Citation>Elsabbagh M, Divan G, Koh Y-J, Kim YS, Kauchali S, Marc&#xed;n C et al (2012) Global prevalence of autism and other pervasive developmental disorders. Autism Res 5(3):160&#x2013;179. https://doi.org/10.1002/aur.239</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/aur.239</ArticleId></ArticleIdList></Reference><Reference><Citation>Chauhan A, Sahu JK, Jaiswal N, Kumar K, Agarwal A, Kaur J et al (2019) Prevalence of autism spectrum disorder in Indian children: a systematic review and meta-analysis. Neurol India 67(1):100. https://doi.org/10.4103/0028-3886.253970</Citation><ArticleIdList><ArticleId IdType="doi">10.4103/0028-3886.253970</ArticleId></ArticleIdList></Reference><Reference><Citation>Kim YS, Leventhal BL, Koh Y-J, Fombonne E, Laska E, Lim E-C et al (2011) Prevalence of autism spectrum disorders in a total population sample. Am J Psychiatry 168(9):904&#x2013;912. https://doi.org/10.1176/appi.ajp.2011.10101532</Citation><ArticleIdList><ArticleId IdType="doi">10.1176/appi.ajp.2011.10101532</ArticleId></ArticleIdList></Reference><Reference><Citation>Saemundsen E, Ludvigsson P, Rafnsson V (2007) Autism spectrum disorders in children with a history of infantile spasms: a population-based study. J Child Neurol 22(9):1102&#x2013;1107. https://doi.org/10.1177/0883073807306251</Citation><ArticleIdList><ArticleId IdType="doi">10.1177/0883073807306251</ArticleId></ArticleIdList></Reference><Reference><Citation>Schipul SE, Keller TA, Just MA (2011) Inter-regional brain communication and its disturbance in autism. Front Syst Neurosci 5:10. https://doi.org/10.3389/fnsys.2011.00010</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fnsys.2011.00010</ArticleId></ArticleIdList></Reference><Reference><Citation>Mueller S, Schuff N, Weiner M (2006) Evaluation of treatment effects in Alzheimer&#x2019;s and other neurodegenerative diseases by MRI and MRS. NMR Biomed 19(6):655&#x2013;668. https://doi.org/10.1002/nbm.1062</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/nbm.1062</ArticleId></ArticleIdList></Reference><Reference><Citation>Suk H-I, Wee C-Y, Lee S-W, Shen D (2016) State-space model with deep learning for functional dynamics estimation in resting-state fmri. Neuroimage 129:292&#x2013;307. https://doi.org/10.1016/j.neuroimage.2016.01.005</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2016.01.005</ArticleId></ArticleIdList></Reference><Reference><Citation>Plis SM, Hjelm DR, Salakhutdinov R, Allen EA, Bockholt HJ, Long JD, Calhoun VD (2014) Deep learning for neuroimaging: a validation study. Front Neurosci 8:229. https://doi.org/10.3389/fnins.2014.00229</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fnins.2014.00229</ArticleId></ArticleIdList></Reference><Reference><Citation>Heinsfeld AS, Franco AR, Craddock RC, Buchweitz A, Meneguzzi F (2018) Identification of autism spectrum disorder using deep learning and the abide dataset. Neuroimage 17:16&#x2013;23. https://doi.org/10.1016/j.nicl.2017.08.017</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.nicl.2017.08.017</ArticleId></ArticleIdList></Reference><Reference><Citation>Anagnostou E, Taylor MJ (2011) Review of neuroimaging in autism spectrum disorders: what have we learned and where we go from here. Mol Autism 2(1):4. https://doi.org/10.1186/2040-2392-2-4</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/2040-2392-2-4</ArticleId></ArticleIdList></Reference><Reference><Citation>Hardan AY, Muddasani S, Vemulapalli M, Keshavan MS, Minshew NJ (2006) An mri study of increased cortical thickness in autism. Am J Psychiatry 163(7):1290&#x2013;1292. https://doi.org/10.1176/ajp.2006.163.7.1290</Citation><ArticleIdList><ArticleId IdType="doi">10.1176/ajp.2006.163.7.1290</ArticleId></ArticleIdList></Reference><Reference><Citation>Hardan AY, Libove RA, Keshavan MS, Melhem NM, Minshew NJ (2009) A preliminary longitudinal magnetic resonance imaging study of brain volume and cortical thickness in autism. Biol Psychiat 66(4):320&#x2013;326. https://doi.org/10.1016/j.biopsych.2009.04.024</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.biopsych.2009.04.024</ArticleId></ArticleIdList></Reference><Reference><Citation>Sowell ER, Thompson PM, Leonard CM, Welcome SE, Kan E, Toga AW (2004) Longitudinal mapping of cortical thickness and brain growth in normal children. J Neurosci 24(38):8223&#x2013;8231. https://doi.org/10.1523/JNEUROSCI.1798-04.2004</Citation><ArticleIdList><ArticleId IdType="doi">10.1523/JNEUROSCI.1798-04.2004</ArticleId></ArticleIdList></Reference><Reference><Citation>Hadjikhani N, Joseph RM, Snyder J, Tager-Flusberg H (2006) Anatomical differences in the mirror neuron system and social cognition network in autism. Cereb Cortex 16(9):1276&#x2013;1282. https://doi.org/10.1093/cercor/bhj069</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/cercor/bhj069</ArticleId></ArticleIdList></Reference><Reference><Citation>Hyde KL, Samson F, Evans AC, Mottron L (2010) Neuroanatomical differences in brain areas implicated in perceptual and other core features of autism revealed by cortical thickness analysis and voxel-based morphometry. Hum Brain Mapp 31(4):556&#x2013;566. https://doi.org/10.1002/hbm.20887</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/hbm.20887</ArticleId></ArticleIdList></Reference><Reference><Citation>Jiao Y, Chen R, Ke X, Chu K, Lu Z, Herskovits EH (2010) Predictive models of autism spectrum disorder based on brain regional cortical thickness. Neuroimage 50(2):589&#x2013;599. https://doi.org/10.1016/j.neuroimage.2009.12.047</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2009.12.047</ArticleId></ArticleIdList></Reference><Reference><Citation>Hazlett HC, Poe MD, Gerig G, Styner M, Chappell C, Smith RG, Piven J (2011) Early brain overgrowth in autism associated with an increase in cortical surface area before age 2 years. Arch Gen Psychiatry 68(5):467&#x2013;476. https://doi.org/10.1001/archgenpsychiatry.2011.39</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/archgenpsychiatry.2011.39</ArticleId></ArticleIdList></Reference><Reference><Citation>Hazlett HC, Gu H, Munsell BC, Kim SH, Styner M, Wolff JJ et al (2017) Early brain development in infants at high risk for autism spectrum disorder. Nature 542(7641):348&#x2013;351. https://doi.org/10.1038/nature21369</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nature21369</ArticleId></ArticleIdList></Reference><Reference><Citation>Nunes AS, Vakorin VA, Kozhemiako N, Peatfield N, Ribary U, Doesburg SM (2020) Atypical age-related changes in cortical thickness in autism spectrum disorder. Sci Rep 10(1):1&#x2013;15. https://doi.org/10.1038/s41598-020-67507-3</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-020-67507-3</ArticleId></ArticleIdList></Reference><Reference><Citation>Squarcina L, Nosari G, Marin R, Castellani U, Bellani M, Bonivento C, Brambilla P (2021) Automatic classification of autism spectrum disorder in children using cortical thickness and support vector machine. Brain Behav 11(8):e2238. https://doi.org/10.1002/brb3.2238</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/brb3.2238</ArticleId></ArticleIdList></Reference><Reference><Citation>Kim JI, Bang S, Yang JJ, Kwon H, Jang S, Roh S, Kim BN (2022) Classification of preschoolers with low-functioning autism spectrum disorder using multimodal MRI data. J Autism Develop Disorders. https://doi.org/10.1007/s10803-021-05368-z</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10803-021-05368-z</ArticleId></ArticleIdList></Reference><Reference><Citation>Piven J, Arndt S, Bailey J, Andreasen N (1996) Regional brain enlargement in autism: a magnetic resonance imaging study. J Am Acad Child Adolesc Psychiatry 35(4):530&#x2013;536. https://doi.org/10.1097/00004583-199604000-00020</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/00004583-199604000-00020</ArticleId></ArticleIdList></Reference><Reference><Citation>Piven J, Bailey J, Ranson BJ, Arndt S (1997) An mri study of the corpus callosum in autism. Am J Psychiatry 154(8):1051&#x2013;1056. https://doi.org/10.1176/ajp.154.8.1051</Citation><ArticleIdList><ArticleId IdType="doi">10.1176/ajp.154.8.1051</ArticleId></ArticleIdList></Reference><Reference><Citation>Courchesne E, Karns C, Davis H, Ziccardi R, Carper R, Tigue Z et al (2001) Unusual brain growth patterns in early life in patients with autistic disorder: an mri study. Neurology 57(2):245&#x2013;254. https://doi.org/10.1212/wnl.57.2.245</Citation><ArticleIdList><ArticleId IdType="doi">10.1212/wnl.57.2.245</ArticleId></ArticleIdList></Reference><Reference><Citation>Herbert M, Ziegler D, Deutsch C, O&#x2019;brien L, Lange N, Bakardjiev A et al (2003) Dissociations of cerebral cortex, subcortical and cerebral white matter volumes in autistic boys. Brain 126(5):1182&#x2013;1192. https://doi.org/10.1093/brain/awg110</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/brain/awg110</ArticleId></ArticleIdList></Reference><Reference><Citation>Herbert MR, Ziegler DA, Makris N, Filipek PA, Kemper TL, Normandin JJ, Cavi- ness Jr VS (2004) Localization of white matter volume increase in autism and developmental language disorder. Annal Neurol 55(4):530&#x2013;540.&#xa0; https://doi.org/10.1002/ana.20032</Citation></Reference><Reference><Citation>Mosconi MW, Cody-Hazlett H, Poe MD, Gerig G, Gimpel-Smith R, Piven J (2009) Longitudinal study of amygdala volume and joint attention in 2-to 4-year-old children with autism. Arch Gen Psychiatry 66(5):509&#x2013;516. https://doi.org/10.1001/archgenpsychiatry.2009.19</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/archgenpsychiatry.2009.19</ArticleId></ArticleIdList></Reference><Reference><Citation>Schumann CM, Bloss CS, Barnes CC, Wideman GM, Carper RA, Akshoomoff N et al (2010) Longitudinal magnetic resonance imaging study of cortical development through early childhood in autism. J Neurosci 30(12):4419&#x2013;4427. https://doi.org/10.1523/JNEUROSCI.5714-09.2010</Citation><ArticleIdList><ArticleId IdType="doi">10.1523/JNEUROSCI.5714-09.2010</ArticleId></ArticleIdList></Reference><Reference><Citation>Frazier TW, Keshavan MS, Minshew NJ, Hardan AY (2012) A two-year longitudinal mri study of the corpus callosum in autism. J Autism Dev Disord 42(11):2312&#x2013;2322. https://doi.org/10.1007/s10803-012-1478-z</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10803-012-1478-z</ArticleId></ArticleIdList></Reference><Reference><Citation>Li G, Chen M-H, Li G, Wu D, Lian C, Sun Q, Wang L (2019) A longitudinal mri study of amygdala and hippocampal subfields for infants with risk of autism. In: International workshop on graph learning in medical imaging, pp. 164&#x2013;171.&#xa0; https://doi.org/10.1007/978-3-030-35817-4_20</Citation></Reference><Reference><Citation>Gao J, Chen M, Li Y, Gao Y, Li Y, Cai S, Wang J (2021) Multisite autism spectrum disorder classification using convolutional neural network classifier and individual morphological brain networks. Front Neurosci 14:1473. https://doi.org/10.3389/fnins.2020.629630</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fnins.2020.629630</ArticleId></ArticleIdList></Reference><Reference><Citation>Friston KJ (2011) Functional and effective connectivity: a review. Brain connectivity 1(1):13&#x2013;36. https://doi.org/10.1089/brain.2011.0008</Citation><ArticleIdList><ArticleId IdType="doi">10.1089/brain.2011.0008</ArticleId></ArticleIdList></Reference><Reference><Citation>Just MA, Cherkassky VL, Keller TA, Minshew NJ (2004) Cortical activation and synchronization during sentence comprehension in high-functioning autism: evidence of under- connectivity. Brain 127(8):1811&#x2013;1821. https://doi.org/10.1093/brain/awh199</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/brain/awh199</ArticleId></ArticleIdList></Reference><Reference><Citation>Koshino H, Carpenter PA, Minshew NJ, Cherkassky VL, Keller TA, Just MA (2005) Functional connectivity in an fmri working memory task in high-functioning autism. Neuroim Age 24(3):810&#x2013;821. https://doi.org/10.1016/j.neuroimage.2004.09.028</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2004.09.028</ArticleId></ArticleIdList></Reference><Reference><Citation>Just MA, Cherkassky VL, Keller TA, Kana RK, Minshew NJ (2007) Functional and anatomical cortical underconnectivity in autism: evidence from an fmri study of an executive function task and corpus callosum morphometry. Cereb Cortex 17(4):951&#x2013;961. https://doi.org/10.1093/cercor/bhl006</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/cercor/bhl006</ArticleId></ArticleIdList></Reference><Reference><Citation>Travers BG, Adluru N, Ennis C, Tromp DP, Destiche D, Doran S, Alexander AL (2012) Diffusion tensor imaging in autism spectrum disorder: a review. Autism Res 5(5):289&#x2013;313. https://doi.org/10.1002/aur.1243</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/aur.1243</ArticleId></ArticleIdList></Reference><Reference><Citation>Supekar K, Uddin LQ, Khouzam A, Phillips J, Gaillard WD, Kenworthy LE, Menon V (2013) Brain hyperconnectivity in children with autism and its links to social deficits. Cell Rep 5(3):738&#x2013;747. https://doi.org/10.1016/j.celrep.2013.10.001</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.celrep.2013.10.001</ArticleId></ArticleIdList></Reference><Reference><Citation>Nielsen JA, Zielinski BA, Fletcher PT, Alexander AL, Lange N, Bigler ED, Ander-son JS (2013) Multisite functional connectivity mri classification of autism: Abide results. Front Human Neurosci 7:599. https://doi.org/10.3389/fnhum.2013.00599</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fnhum.2013.00599</ArticleId></ArticleIdList></Reference><Reference><Citation>Tyszka JM, Kennedy DP, Paul LK, Adolphs R (2014) Largely typical patterns of resting- state functional connectivity in high-functioning adults with autism. Cereb Cortex 24(7):1894&#x2013;1905. https://doi.org/10.1093/cercor/bht040</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/cercor/bht040</ArticleId></ArticleIdList></Reference><Reference><Citation>Moseley R, Ypma R, Holt R, Floris D, Chura L, Spencer MD, Rubinov M (2015) Whole- brain functional hypoconnectivity as an endophenotype of autism in adolescents. Neuroimage 9:140&#x2013;152. https://doi.org/10.1016/j.nicl.2015.07.015</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.nicl.2015.07.015</ArticleId></ArticleIdList></Reference><Reference><Citation>Guo X, Dominick KC, Minai AA, Li H, Erickson CA, Lu LJ (2017) Diagnosing autism spectrum disorder from brain resting-state functional connectivity patterns using a deep neural network with a novel feature selection method. Front Neurosci 11:460. https://doi.org/10.3389/fnins.2017.00460</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fnins.2017.00460</ArticleId></ArticleIdList></Reference><Reference><Citation>Fishman I, Linke AC, Hau J, Carper RA, M&#xfc;ller R-A (2018) Atypical functional connectivity of amygdala related to reduced symptom severity in children with autism. J Am Acad Child Adolesc Psychiatry 57(10):764&#x2013;774. https://doi.org/10.1016/j.jaac.2018.06.015</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jaac.2018.06.015</ArticleId></ArticleIdList></Reference><Reference><Citation>Odriozola P, Dajani DR, Burrows CA, Gabard-Durnam LJ, Goodman E, Baez AC, Gee DG (2019) Atypical frontoamygdala functional connectivity in youth with autism. Develop Cognitive Neurosci 37:100603. https://doi.org/10.1016/j.dcn.2018.12.001</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.dcn.2018.12.001</ArticleId></ArticleIdList></Reference><Reference><Citation>Friston KJ, Holmes AP, Worsley KJ, Poline J-P, Frith CD, Frackowiak RS (1994) Statistical parametric maps in functional imaging: a general linear approach. Hum Brain Mapp 2(4):189&#x2013;210. https://doi.org/10.1002/hbm.460020402</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/hbm.460020402</ArticleId></ArticleIdList></Reference><Reference><Citation>Supekar K, Musen M, Menon V (2009) Development of large-scale functional brain networks in children. PLoS Biol 7(7):e1000157. https://doi.org/10.1371/journal.pbio.1000157</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pbio.1000157</ArticleId></ArticleIdList></Reference><Reference><Citation>Sun JW, Fan R, Wang Q, Wang QQ, Jia XZ, Ma HB (2021) Identify abnormal functional connectivity of resting state networks in Autism spectrum disorder and apply to machine learning-based classification. Brain Res 1757:147299. https://doi.org/10.1016/j.brainres.2021.147299</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.brainres.2021.147299</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang J, Feng F, Han T, Gong X, Duan F (2022) Detection of autism spectrum disorder using fMRI functional connectivity with feature selection and deep learning. Cognitive Comput. https://doi.org/10.1007/s12559-021-09981-z</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s12559-021-09981-z</ArticleId></ArticleIdList></Reference><Reference><Citation>Yang X, Zhang N, Schrader P (2022) A study of brain networks for autism spectrum disorder classification using resting-state functional connectivity. Mach Learn Appl 8:100290. https://doi.org/10.1016/j.mlwa.2022.100290</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.mlwa.2022.100290</ArticleId></ArticleIdList></Reference><Reference><Citation>Di Martino A, Yan CG, Li Q, Denio E, Castellanos FX, Alaerts K, Milham MP (2014) The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism. Mol Psychiatry 19(6):659&#x2013;667. https://doi.org/10.1038/mp.2013.78</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/mp.2013.78</ArticleId></ArticleIdList></Reference><Reference><Citation>Hossin M, Sulaiman MN (2015) A review on evaluation metrics for data classification evaluations. Int J Data Mining Knowledge Manag Process 5(2):1. https://doi.org/10.5121/ijdkp.2015.5201</Citation><ArticleIdList><ArticleId IdType="doi">10.5121/ijdkp.2015.5201</ArticleId></ArticleIdList></Reference><Reference><Citation>Aghdam MA, Sharifi A, Pedram MM (2018) Combination of rs-fmri and smri data to discriminate autism spectrum disorders in young children using deep belief network. J Digit Imaging 31(6):895&#x2013;903. https://doi.org/10.1007/s10278-018-0093-8</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10278-018-0093-8</ArticleId></ArticleIdList></Reference><Reference><Citation>Dekhil O, Ali M, El-Nakieb Y, Shalaby A, Soliman A, Switala A et al (2019) A personalized autism diagnosis cad system using a fusion of structural mri and resting-state functional mri data. Front Psychiatry. https://doi.org/10.3389/fpsyt.2019.00392</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fpsyt.2019.00392</ArticleId></ArticleIdList></Reference><Reference><Citation>Suzuki K, Armato SG III, Li F, Sone S, Doi K (2003) Massive training artificial neural network (mtann) for reduction of false positives in computerized detection of lung nodules in low-dose computed tomography. Med Phys 30(7):1602&#x2013;1617. https://doi.org/10.1118/1.1580485</Citation><ArticleIdList><ArticleId IdType="doi">10.1118/1.1580485</ArticleId></ArticleIdList></Reference><Reference><Citation>Shen W, Zhou M, Yang F, Yu D, Dong D, Yang C, Tian J (2017) Multi-crop convolutional neural networks for lung nodule malignancy suspiciousness classification. Pattern Recogn 61:663&#x2013;673. https://doi.org/10.1016/j.patcog.2016.05.029</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.patcog.2016.05.029</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang S-H, Zhang Y, Cheng X, Zhang X, Zhang Y-D (2021) Psspnn: Patchshuffle stochastic pooling neural network for an explainable diagnosis of covid-19 with multiple-way data augmentation. Comput Math Methods Med. https://doi.org/10.1155/2021/6633755</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2021/6633755</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36633614</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>12</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1619-7089</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>12</Day></PubDate></JournalIssue><Title>European journal of nuclear medicine and molecular imaging</Title><ISOAbbreviation>Eur J Nucl Med Mol Imaging</ISOAbbreviation></Journal><ArticleTitle>Low-count whole-body PET/MRI restoration: an evaluation of dose reduction spectrum and five state-of-the-art artificial intelligence models.</ArticleTitle><ELocationID EIdType="doi" ValidYN="Y">10.1007/s00259-022-06097-w</ELocationID><Abstract><AbstractText Label="PURPOSE" NlmCategory="OBJECTIVE">To provide a holistic and complete comparison of the five most advanced AI models in the augmentation of low-dose <sup>18</sup>F-FDG PET data over the entire dose reduction spectrum.</AbstractText><AbstractText Label="METHODS" NlmCategory="METHODS">In this multicenter study, five AI models were investigated for restoring low-count whole-body PET/MRI, covering convolutional benchmarks - U-Net, enhanced deep super-resolution network (EDSR), generative adversarial network (GAN) - and the most cutting-edge image reconstruction transformer models in computer vision to date - Swin transformer image restoration network (SwinIR) and EDSR-ViT (vision transformer). The models were evaluated against six groups of count levels representing the simulated 75%, 50%, 25%, 12.5%, 6.25%, and 1% (extremely ultra-low-count) of the clinical standard 3&#xa0;MBq/kg <sup>18</sup>F-FDG dose. The comparisons were performed upon two independent cohorts - (1) a primary cohort from Stanford University and (2) a cross-continental external validation cohort from T&#xfc;bingen University - in order to ensure the findings are generalizable. A total of 476 original count and simulated low-count whole-body PET/MRI scans were incorporated into this analysis.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">For low-count PET restoration on the primary cohort, the mean structural similarity index (SSIM) scores for dose 6.25% were 0.898 (95% CI, 0.887-0.910) for EDSR, 0.893 (0.881-0.905) for EDSR-ViT, 0.873 (0.859-0.887) for GAN, 0.885 (0.873-0.898) for U-Net, and 0.910 (0.900-0.920) for SwinIR. In continuation, SwinIR and U-Net's performances were also discreetly evaluated at each simulated radiotracer dose levels. Using the primary Stanford cohort, the mean diagnostic image quality (DIQ; 5-point Likert scale) scores of SwinIR restoration were 5 (SD, 0) for dose 75%, 4.50 (0.535) for dose 50%, 3.75 (0.463) for dose 25%, 3.25 (0.463) for dose 12.5%, 4 (0.926) for dose 6.25%, and 2.5 (0.534) for dose 1%.</AbstractText><AbstractText Label="CONCLUSION" NlmCategory="CONCLUSIONS">Compared to low-count PET images, with near-to or nondiagnostic images at higher dose reduction levels (up to 6.25%), both SwinIR and U-Net significantly improve the diagnostic quality of PET images. A radiotracer dose reduction to 1% of the current clinical standard radiotracer dose is out of scope for current AI techniques.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Wang</LastName><ForeName>Yan-Ran Joyce</ForeName><Initials>YJ</Initials><AffiliationInfo><Affiliation>Department of Radiology, School of Medicine, Stanford University, 725 Welch Road, Stanford, CA, 94304, USA. wangyanran100@gmail.com.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Biomedical Data Science, Stanford University, Stanford, CA, 94304, USA. wangyanran100@gmail.com.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wang</LastName><ForeName>Pengcheng</ForeName><Initials>P</Initials><AffiliationInfo><Affiliation>Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Adams</LastName><ForeName>Lisa Christine</ForeName><Initials>LC</Initials><AffiliationInfo><Affiliation>Department of Radiology, School of Medicine, Stanford University, 725 Welch Road, Stanford, CA, 94304, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Sheybani</LastName><ForeName>Natasha Diba</ForeName><Initials>ND</Initials><AffiliationInfo><Affiliation>Department of Biomedical Data Science, Stanford University, Stanford, CA, 94304, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Qu</LastName><ForeName>Liangqiong</ForeName><Initials>L</Initials><AffiliationInfo><Affiliation>Department of Biomedical Data Science, Stanford University, Stanford, CA, 94304, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Sarrami</LastName><ForeName>Amir Hossein</ForeName><Initials>AH</Initials><AffiliationInfo><Affiliation>Department of Radiology, School of Medicine, Stanford University, 725 Welch Road, Stanford, CA, 94304, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Theruvath</LastName><ForeName>Ashok Joseph</ForeName><Initials>AJ</Initials><AffiliationInfo><Affiliation>Department of Radiology, School of Medicine, Stanford University, 725 Welch Road, Stanford, CA, 94304, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Gatidis</LastName><ForeName>Sergios</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Department of Diagnostic and Interventional Radiology, University Hospital Tuebingen, Tuebingen, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Ho</LastName><ForeName>Tina</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>Department of Radiology, School of Medicine, Stanford University, 725 Welch Road, Stanford, CA, 94304, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhou</LastName><ForeName>Quan</ForeName><Initials>Q</Initials><AffiliationInfo><Affiliation>Department of Radiology, School of Medicine, Stanford University, 725 Welch Road, Stanford, CA, 94304, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Pribnow</LastName><ForeName>Allison</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Department of Pediatrics, Pediatric Oncology, Lucile Packard Children's Hospital, Stanford University, Stanford, CA, 94304, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Thakor</LastName><ForeName>Avnesh S</ForeName><Initials>AS</Initials><AffiliationInfo><Affiliation>Department of Pediatrics, Pediatric Oncology, Lucile Packard Children's Hospital, Stanford University, Stanford, CA, 94304, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Rubin</LastName><ForeName>Daniel</ForeName><Initials>D</Initials><AffiliationInfo><Affiliation>Department of Biomedical Data Science, Stanford University, Stanford, CA, 94304, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Pediatrics, Pediatric Oncology, Lucile Packard Children's Hospital, Stanford University, Stanford, CA, 94304, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Daldrup-Link</LastName><ForeName>Heike E</ForeName><Initials>HE</Initials><AffiliationInfo><Affiliation>Department of Radiology, School of Medicine, Stanford University, 725 Welch Road, Stanford, CA, 94304, USA. heiked@stanford.edu.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Pediatrics, Pediatric Oncology, Lucile Packard Children's Hospital, Stanford University, Stanford, CA, 94304, USA. heiked@stanford.edu.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><GrantList CompleteYN="Y"><Grant><GrantID>R01CA269231</GrantID><Acronym>CA</Acronym><Agency>NCI NIH HHS</Agency><Country>United States</Country></Grant></GrantList><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>12</Day></ArticleDate></Article><MedlineJournalInfo><Country>Germany</Country><MedlineTA>Eur J Nucl Med Mol Imaging</MedlineTA><NlmUniqueID>101140988</NlmUniqueID><ISSNLinking>1619-7070</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">CNN</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">PET restoration</Keyword><Keyword MajorTopicYN="N">Transformer model</Keyword><Keyword MajorTopicYN="N">Whole-body PET imaging</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>7</Month><Day>26</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>24</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>11</Hour><Minute>13</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36633614</ArticleId><ArticleId IdType="doi">10.1007/s00259-022-06097-w</ArticleId><ArticleId IdType="pii">10.1007/s00259-022-06097-w</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Chaudhari AS, Mittra E, Davidzon GA, Gulaka P, Gandhi H, Brown A, et al. Low-count whole-body PET with deep learning in a multicenter and externally validated study. NPJ Digit Med. 2021;4:1&#x2013;11.</Citation></Reference><Reference><Citation>Baum SH, Fr&#xfc;hwald M, Rahbar K, Wessling J, Schober O, Weckesser M. Contribution of PET/CT to prediction of outcome in children and young adults with rhabdomyosarcoma. J Nucl Med. 2011;52:1535&#x2013;40.</Citation><ArticleIdList><ArticleId IdType="doi">10.2967/jnumed.110.082511</ArticleId></ArticleIdList></Reference><Reference><Citation>Kleis M, Daldrup-Link H, Matthay K, Goldsby R, Lu Y, Schuster T, et al. Diagnostic value of PET/CT for the staging and restaging of pediatric tumors. Eur J Nucl Med Mol Imaging. 2009;36:23&#x2013;36.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00259-008-0911-1</ArticleId></ArticleIdList></Reference><Reference><Citation>Baratto L, Hawk KE, Qi J, Gatidis S, Kiru L, Daldrup-Link HE. PET/MRI improves management of children with cancer. J Nucl Med. 2021;62:1334&#x2013;40.</Citation><ArticleIdList><ArticleId IdType="doi">10.2967/jnumed.120.259747</ArticleId></ArticleIdList></Reference><Reference><Citation>Huang B, Law MW-M, Khong P-L. Whole-body PET/CT scanning: estimation of radiation dose and cancer risk. Radiology. 2009;251:166&#x2013;74.</Citation></Reference><Reference><Citation>Meulepas JM, Ronckers CM, Smets AM, Nievelstein RA, Gradowska P, Lee C, et al. Radiation exposure from pediatric CT scans and subsequent cancer risk in the Netherlands. JNCI J Natl Cancer Institute. 2019;111:256&#x2013;63.</Citation></Reference><Reference><Citation>Brenner DJ, Doll R, Goodhead DT, Hall EJ, Land CE, Little JB, et al. Cancer risks attributable to low doses of ionizing radiation: assessing what we really know. Proc Natl Acad Sci. 2003;100:13761&#x2013;6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1073/pnas.2235592100</ArticleId></ArticleIdList></Reference><Reference><Citation>Townsend D. Physical principles and technology of clinical PET imaging. Ann Acad Med Singap. 2004;33:133&#x2013;45.</Citation></Reference><Reference><Citation>Wang T, Lei Y, Fu Y, Curran WJ, Liu T, Nye JA, et al. Machine learning in quantitative PET: a review of attenuation correction and low-count image reconstruction methods. Phys Med. 2020;76:294&#x2013;306.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejmp.2020.07.028</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang G, Ye JC, Mueller K, Fessler JA. Image reconstruction is a new frontier of machine learning. IEEE Trans Med Imaging. 2018;37:1289&#x2013;96.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2018.2833635</ArticleId></ArticleIdList></Reference><Reference><Citation>Reader AJ, Corda G, Mehranian A, da Costa-Luis C, Ellis S, Schnabel JA. Deep learning for PET image reconstruction. IEEE Trans Radiat Plasma Med Sci. 2020;5:1&#x2013;25.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TRPMS.2020.3014786</ArticleId></ArticleIdList></Reference><Reference><Citation>Raj A, Bresler Y, Li B. Improving robustness of deep-learning-based image reconstruction. International Conference on Machine Learning. 2020:7932&#x2013;42.</Citation></Reference><Reference><Citation>H&#xe4;ggstr&#xf6;m I, Schmidtlein CR, Campanella G, Fuchs TJ. DeepPET: a deep encoder&#x2013;decoder network for directly solving the PET image reconstruction inverse problem. Med Image Anal. 2019;54:253&#x2013;62.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2019.03.013</ArticleId></ArticleIdList></Reference><Reference><Citation>Gong K, Catana C, Qi J, Li Q. PET image reconstruction using deep image prior. IEEE Trans Med Imaging. 2018;38:1655&#x2013;65.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2018.2888491</ArticleId></ArticleIdList></Reference><Reference><Citation>Feng Q, Liu H. Rethinking PET image reconstruction: ultra-low-dose, sinogram and deep learning. International Conference on Medical Image Computing and Computer-Assisted Intervention: Springer.&#xa0;2020:783&#x2013;92.</Citation></Reference><Reference><Citation>Theruvath AJ, Siedek F, Yerneni K, Muehe AM, Spunt SL, Pribnow A, et al. Validation of deep learning&#x2013;based augmentation for reduced 18F-FDG dose for PET/MRI in children and young adults with lymphoma. Radiology: Artificial Intelligence. 2021;3:e200232.</Citation></Reference><Reference><Citation>Liang J, Cao J, Sun G, Zhang K, Gool LV, Timofte R. SwinIR: image restoration using Swin transformer. IEEE/CVF International Conference on Computer Vision Workshops (ICCVW). 2021:1833&#x2013;44.</Citation></Reference><Reference><Citation>Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, et al. An image is worth 16x16 words: transformers for image recognition at scale. The International Conference on Learning Representations (ICLR). 2021.</Citation></Reference><Reference><Citation>Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, et al. Swin transformer: Hierarchical vision transformer using shifted windows. Proceedings of the IEEE/CVF International Conference on Computer Vision.&#xa0;2021:10012&#x2013;22.</Citation></Reference><Reference><Citation>Khan S, Naseer M, Hayat M, Zamir SW, Khan FS, Shah M. Transformers in vision: A survey. ACM computing surveys (CSUR). 2022;54:1&#x2013;41.</Citation></Reference><Reference><Citation>Whiteley W, Luk WK, Gregor J. DirectPET: full-size neural network PET reconstruction from sinogram data. J Med Imaging. 2020;7: 032503.</Citation><ArticleIdList><ArticleId IdType="doi">10.1117/1.JMI.7.3.032503</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang Y-RJ, Baratto L, Hawk KE, Theruvath AJ, Pribnow A, Thakor AS, et al. Artificial intelligence enables whole-body positron emission tomography scans with minimal radiation exposure. Eur J Nucl Med Mol Imaging. 2021:1&#x2013;11.</Citation></Reference><Reference><Citation>Schramm G, Rigie D, Vahle T, Rezaei A, Van Laere K, Shepherd T, et al. Approximating anatomically-guided PET reconstruction in image space using a convolutional neural network. Neuroimage. 2021;224: 117399.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2020.117399</ArticleId></ArticleIdList></Reference><Reference><Citation>Ronneberger O, Fischer P, Brox T. U-net: convolutional networks for biomedical image segmentation. International Conference on Medical image computing and computer-assisted intervention. 2015:234&#x2013;41.</Citation></Reference><Reference><Citation>Ouyang J, Chen KT, Gong E, Pauly J, Zaharchuk G. Ultra-low-dose PET reconstruction using generative adversarial network with feature matching and task-specific perceptual loss. Med Phys. 2019;46:3555&#x2013;64.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mp.13626</ArticleId></ArticleIdList></Reference><Reference><Citation>Sekine T, Delso G, Zeimpekis KG, de Galiza BF, Ter Voert EE, Huellner M, et al. Reduction of 18F-FDG dose in clinical PET/MR imaging by using silicon photomultiplier detectors. Radiology. 2018;286:249&#x2013;59.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2017162305</ArticleId></ArticleIdList></Reference><Reference><Citation>Esser P, Rombach R, Ommer B. Taming transformers for high-resolution image synthesis. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.&#xa0;2021:2873&#x2013;83.</Citation></Reference><Reference><Citation>Zheng S, Lu J, Zhao H, Zhu X, Luo Z, Wang Y, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.&#xa0;2021;6881&#x2013;90.</Citation></Reference><Reference><Citation>Lim B, Son S, Kim H, Nah S, Mu Lee K. Enhanced deep residual networks for single image super-resolution. Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2017:136&#x2013;44.</Citation></Reference><Reference><Citation>He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. Proceedings of the IEEE conference on computer vision and pattern recognition. 2016:770&#x2013;8.</Citation></Reference><Reference><Citation>Ledig C, Theis L, Husz&#xe1;r F, Caballero J, Cunningham A, Acosta A, et al. Photo-realistic single image super-resolution using a generative adversarial network. Proceedings of the IEEE conference on computer vision and pattern recognition. 2017:4681&#x2013;90.</Citation></Reference><Reference><Citation>Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, et al. Generative adversarial nets. Advances in neural information processing systems. 2014:27.</Citation></Reference><Reference><Citation>Lucas A, Iliadis M, Molina R, Katsaggelos AK. Using deep neural networks for inverse problems in imaging: beyond analytical methods. IEEE Signal Process Mag. 2018;35:20&#x2013;36.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/MSP.2017.2760358</ArticleId></ArticleIdList></Reference><Reference><Citation>Islam J, Zhang Y. GAN-based synthetic brain PET image generation. Brain Inform. 2020;7:1&#x2013;12.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s40708-020-00104-2</ArticleId></ArticleIdList></Reference><Reference><Citation>Wolf T, Chaumond J, Debut L, Sanh V, Delangue C, Moi A, et al. Transformers: state-of-the-art natural language processing. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2020:38&#x2013;45.</Citation></Reference><Reference><Citation>Tang C, Zhao Y, Wang G, Luo C, Xie W, Zeng W. Sparse MLP for image recognition: Is self-attention really necessary? Proceedings of the AAAI Conference on Artificial Intelligence.&#xa0;2022:2344&#x2013;51.</Citation></Reference><Reference><Citation>Carion N, Massa F, Synnaeve G, Usunier N, Kirillov A, Zagoruyko S. End-to-end object detection with transformers. European conference on computer vision. 2020:213&#x2013;29.</Citation></Reference><Reference><Citation>Touvron H, Cord M, Douze M, Massa F, Sablayrolles A, J&#xe9;gou H. Training data-efficient image transformers &amp; distillation through attention. International Conference on Machine Learning. 2021:10347&#x2013;57.</Citation></Reference><Reference><Citation>Zhang Y, Liu H, Hu Q. Transfuse: Fusing transformers and cnns for medical image segmentation. International Conference on Medical Image Computing and Computer-Assisted Intervention: Springer; 2021. p. 14-24.</Citation></Reference><Reference><Citation>Preetha CJ, Meredig H, Brugnara G, Mahmutoglu MA, Foltyn M, Isensee F, et al. Deep-learning-based synthesis of post-contrast T1-weighted MRI for tumour response assessment in neuro-oncology: a multicentre, retrospective cohort study. Lancet Digit Health. 2021;3:e784&#x2013;94.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S2589-7500(21)00205-3</ArticleId></ArticleIdList></Reference><Reference><Citation>Hore A, Ziou D. Image quality metrics: PSNR vs. SSIM. 20th international conference on pattern recognition. 2010:2366&#x2013;9.</Citation></Reference><Reference><Citation>Sheikh HR, Bovik AC. Image information and visual quality. IEEE Trans Image Process. 2006;15:430&#x2013;44.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TIP.2005.859378</ArticleId></ArticleIdList></Reference><Reference><Citation>Thie JA. Understanding the standardized uptake value, its methods, and implications for usage. J Nucl Med. 2004;45:1431&#x2013;4.</Citation></Reference><Reference><Citation>Fletcher J, Kinahan P. PET/CT standardized uptake values (SUVs) in clinical practice and assessing response to therapy. NIH Public Access. 2010;31:496&#x2013;505.</Citation></Reference><Reference><Citation>Yang Q, Tan K-H, Ahuja N. Real-time O (1) bilateral filtering. 2009 IEEE Conference on Computer Vision and Pattern Recognition. 2009:557-64</Citation></Reference><Reference><Citation>Paris S, Durand F. A fast approximation of the bilateral filter using a signal processing approach. European conference on computer vision. 2006:568&#x2013;80.</Citation></Reference><Reference><Citation>Luo Y, Wang Y, Zu C, Zhan B, Wu X, Zhou J, et al. 3D Transformer-GAN for high-quality PET reconstruction. International conference on medical image computing and computer-assisted intervention: Springer; 2021. p. 276&#x2013;85.</Citation></Reference><Reference><Citation>Hu R, Liu H. TransEM: residual Swin-transformer based regularized PET image reconstruction. arXiv preprint arXiv:220504204. 2022.</Citation></Reference><Reference><Citation>Wang Y-RJ, Baratto L, Hawk KE, Theruvath AJ, Pribnow A, Thakor AS, et al. Artificial intelligence enables whole-body positron emission tomography scans with minimal radiation exposure. Eur J Nucl Med Mol Imaging. 2021;48:2771&#x2013;81.</Citation></Reference><Reference><Citation>MD Dipl-math SG, Seith F, Sch&#xe4;fer JF, Christian la Foug&#xe8;re M, Nikolaou K, Schwenzer NF. Towards tracer dose reduction in PET studies: simulation of dose reduction by retrospective randomized undersampling of list-mode data. Hell J Nucl Med. 2016;19:15&#x2013;8.</Citation></Reference><Reference><Citation>Brenner DJ, Hall EJ. Computed tomography&#x2014;an increasing source of radiation exposure. N Engl J Med. 2007;357:2277&#x2013;84.</Citation><ArticleIdList><ArticleId IdType="doi">10.1056/NEJMra072149</ArticleId></ArticleIdList></Reference><Reference><Citation>Chawla SC, Federman N, Zhang D, Nagata K, Nuthakki S, McNitt-Gray M, et al. Estimated cumulative radiation dose from PET/CT in children with malignancies: a 5-year retrospective review. Pediatr Radiol. 2010;40:681&#x2013;6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00247-009-1434-z</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36633187</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>12</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1520-6882</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>12</Day></PubDate></JournalIssue><Title>Analytical chemistry</Title><ISOAbbreviation>Anal Chem</ISOAbbreviation></Journal><ArticleTitle>Divide and Conquer: A Flexible Deep Learning Strategy for Exploring Metabolic Heterogeneity from Mass Spectrometry Imaging Data.</ArticleTitle><ELocationID EIdType="doi" ValidYN="Y">10.1021/acs.analchem.2c04045</ELocationID><Abstract><AbstractText>Research on metabolic heterogeneity provides an important basis for the study of the molecular mechanism of a disease and personalized treatment. The screening of metabolism-related sub-regions that affect disease development is essential for the more focused exploration on disease progress aberrant phenotypes, even carcinogenesis and metastasis. The mass spectrometry imaging (MSI) technique has distinct advantages to reveal the heterogeneity of an organism based on <i>in situ</i> molecular profiles. The challenge of heterogeneous analysis has been to perform an objective identification among biological tissues with different characteristics. By introducing the divide-and-conquer strategy to architecture design and application, we establish here a flexible unsupervised deep learning model, called divide-and-conquer (dc)-DeepMSI, for metabolic heterogeneity analysis from MSI data without prior knowledge of histology. dc-DeepMSI can be used to identify either spatially contiguous regions of interest (ROIs) or spatially sporadic ROIs by designing two specific modes, spat-contig and spat-spor. Comparison results on fetus mouse data demonstrate that the dc-DeepMSI outperforms state-of-the-art MSI segmentation methods. We demonstrate that the novel learning strategy successfully obtained sub-regions that are statistically linked to the invasion status and molecular phenotypes of breast cancer as well as organizing principles during developmental phase.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Guo</LastName><ForeName>Lei</ForeName><Initials>L</Initials><Identifier Source="ORCID">0000-0002-8032-8748</Identifier><AffiliationInfo><Affiliation>Department of Electronic Science, National Institute for Data Science in Health and Medicine, Xiamen University, Xiamen, Fujian 361005, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Dong</LastName><ForeName>Jiyang</ForeName><Initials>J</Initials><Identifier Source="ORCID">0000-0002-1064-6548</Identifier><AffiliationInfo><Affiliation>Department of Electronic Science, National Institute for Data Science in Health and Medicine, Xiamen University, Xiamen, Fujian 361005, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Xu</LastName><ForeName>Xiangnan</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>School of Mathematics and Statistics, The University of Sydney, Sydney, NSW 2006, Australia.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wu</LastName><ForeName>Zhichao</ForeName><Initials>Z</Initials><AffiliationInfo><Affiliation>School of Artificial Intelligence, Beijing Normal University, Beijing 100875, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Yinbin</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Department of Oncology, The Second Affiliated Hospital of Medical College, Xi'an Jiaotong University, Xi'an, Shaanxi 710004, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wang</LastName><ForeName>Yongwei</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Bruker Scientific Technology Co., Ltd., Beijing 100086, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Li</LastName><ForeName>Pengfei</ForeName><Initials>P</Initials><AffiliationInfo><Affiliation>Bruker Scientific Technology Co., Ltd., Beijing 100086, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Tang</LastName><ForeName>Zhi</ForeName><Initials>Z</Initials><AffiliationInfo><Affiliation>School of Public Health, Dongguan Key Laboratory of Environmental Medicine, Institute of Environmental Health, Guangdong Medical University, Dongguan, Guangdong 523808, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhao</LastName><ForeName>Chao</ForeName><Initials>C</Initials><Identifier Source="ORCID">0000-0002-1765-6423</Identifier><AffiliationInfo><Affiliation>Bionic Sensing and Intelligence Center, Institute of Biomedical and Health Engineering, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong 518055, China.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>State Key Laboratory of Environmental and Biological Analysis, Department of Chemistry, Hong Kong Baptist University, Hong Kong SAR 999077, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Cai</LastName><ForeName>Zongwei</ForeName><Initials>Z</Initials><Identifier Source="ORCID">0000-0002-8724-7684</Identifier><AffiliationInfo><Affiliation>State Key Laboratory of Environmental and Biological Analysis, Department of Chemistry, Hong Kong Baptist University, Hong Kong SAR 999077, China.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>12</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>Anal Chem</MedlineTA><NlmUniqueID>0370536</NlmUniqueID><ISSNLinking>0003-2700</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>6</Hour><Minute>32</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36633187</ArticleId><ArticleId IdType="doi">10.1021/acs.analchem.2c04045</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36633186</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>12</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">2473-4209</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>12</Day></PubDate></JournalIssue><Title>Medical physics</Title><ISOAbbreviation>Med Phys</ISOAbbreviation></Journal><ArticleTitle>Automatic detection of pulmonary embolism in computed tomography pulmonary angiography using scaled-YOLOv4.</ArticleTitle><ELocationID EIdType="doi" ValidYN="Y">10.1002/mp.16218</ELocationID><Abstract><AbstractText Label="BACKGROUND" NlmCategory="BACKGROUND">Pulmonary embolism (PE) is a common but fatal clinical condition and the gold standard of diagnosis is computed tomography pulmonary angiography (CTPA). Prompt diagnosis and rapid treatment can dramatically reduce mortality in patients. However, the diagnosis of PE is often delayed and missed.</AbstractText><AbstractText Label="METHODS" NlmCategory="METHODS">In this study, we identified a deep learning model Scaled-YOLOv4 that enables end-to-end automated detection of PE to help solve these problems. A total of 307 CTPA data (Tianjin 142 cases, Linyi 133 cases and FUMPE 32 cases) were included in this study. The Tianjin dataset was divided ten times in the ratio of training set: validation set: test set = 7:2:1 for model tuning, and both the Linyi and FUMPE datasets were used as independent external test sets to evaluate the generalization of the model.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">Scaled-YOLOv4 was able to process one patient in average 3.55 seconds [95% CI: 3.51-3.59 seconds]. It also achieved an average precision (AP) of 83.04 [95% CI: 79.36-86.72] for PE detection on the Tianjin test set, and 75.86 [95% CI: 75.48-76.24] and 72.74 [95% CI: 72.10-73.38] on Linyi and FUMPE, respectively.</AbstractText><AbstractText Label="CONCLUSIONS" NlmCategory="CONCLUSIONS">This deep learning algorithm helps detect PE in realtime, providing radiologists with aided diagnostic evidence without increasing their workload, and can effectively reduce the probability of delayed patient diagnosis. This article is protected by copyright. All rights reserved.</AbstractText><CopyrightInformation>This article is protected by copyright. All rights reserved.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Xu</LastName><ForeName>Haijun</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>School of Medical Imaging, Tianjin Medical University, Tianjin, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Li</LastName><ForeName>Huiyao</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>Department of MR, Beijing Shijitan Hospital, Capital Medical University, Beijing, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Xu</LastName><ForeName>Qifei</ForeName><Initials>Q</Initials><AffiliationInfo><Affiliation>Department of Radiology, Linyi people's Hospital, Linyi, Shandong, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Zewei</ForeName><Initials>Z</Initials><AffiliationInfo><Affiliation>Department of Nuclear Medicine, National Cancer Center/National Clinical Research Center for Cancer/Cancer Hospital, Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wang</LastName><ForeName>Ping</ForeName><Initials>P</Initials><AffiliationInfo><Affiliation>School of Medical Imaging, Tianjin Medical University, Tianjin, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Li</LastName><ForeName>Dong</ForeName><Initials>D</Initials><AffiliationInfo><Affiliation>Department of Radiology, Tianjin Medical University General Hospital, Tianjin, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Guo</LastName><ForeName>Li</ForeName><Initials>L</Initials><AffiliationInfo><Affiliation>School of Medical Imaging, Tianjin Medical University, Tianjin, China.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>12</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>Med Phys</MedlineTA><NlmUniqueID>0425746</NlmUniqueID><ISSNLinking>0094-2405</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Computed tomography(CT)</Keyword><Keyword MajorTopicYN="N">Computer-aided detection</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">YOLO</Keyword><Keyword MajorTopicYN="N">pulmonary embolism</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>11</Month><Day>10</Day></PubMedPubDate><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>7</Month><Day>24</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>24</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>6</Hour><Minute>32</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36633186</ArticleId><ArticleId IdType="doi">10.1002/mp.16218</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36631872</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>13</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>13</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">1472-6831</ISSN><JournalIssue CitedMedium="Internet"><Volume>23</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>11</Day></PubDate></JournalIssue><Title>BMC oral health</Title><ISOAbbreviation>BMC Oral Health</ISOAbbreviation></Journal><ArticleTitle>Machine learning in 3D auto-filling alveolar cleft of CT images to assess the influence of alveolar bone grafting on the development of maxilla.</ArticleTitle><Pagination><StartPage>16</StartPage><MedlinePgn>16</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">16</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1186/s12903-023-02706-8</ELocationID><Abstract><AbstractText Label="BACKGROUND" NlmCategory="BACKGROUND">Machine learning based auto-segmentation of 3D images has been developed rapidly in recent years. However, the application of this new method in the research of patients with unilateral cleft lip and palate (UCLP) is very limited. In this study, a machine learning algorithm utilizing 3D U-net was used to automatically segment the maxilla, fill the cleft and evaluate the alveolar bone graft in UCLP patients. Cleft related factors and the surgery impact on the development of maxilla were analyzed.</AbstractText><AbstractText Label="METHODS" NlmCategory="METHODS">Preoperative and postoperative computed tomography images of 32 patients (64 images) were obtained. The deep-learning-based protocol was used to segment the maxilla and defect, followed by manual refinement. Paired t-tests and Mann-Whitney tests were performed to reveal the changes of the maxilla after surgery. Two-factor, two-level analysis for repeated measurement was used to examine the different trends of growth on the cleft and non-cleft sides of the maxilla. Pearson and Spearman correlations were used to explore the relationship between the defect and the changes of the maxillary cleft side.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">One-year after the alveolar bone grafting surgery, different growth amount was found on the cleft and non-cleft sides of maxilla. The maxillary length (from 34.64&#x2009;&#xb1;&#x2009;2.48 to 35.67&#x2009;&#xb1;&#x2009;2.45&#xa0;mm) and the alveolar length (from 36.58&#x2009;&#xb1;&#x2009;3.21 to 37.63&#x2009;&#xb1;&#x2009;2.94&#xa0;mm) increased significantly only on the cleft side while the maxillary anterior width (from 11.61&#x2009;&#xb1;&#x2009;1.61 to 12.01&#x2009;&#xb1;&#x2009;1.41&#xa0;mm) and posterior width (from 29.63&#x2009;&#xb1;&#x2009;2.25 to 30.74&#x2009;&#xb1;&#x2009;2.63&#xa0;mm) increased significantly only on the non-cleft side after surgery. Morphology of the cleft was found to be related to the pre-surgical maxillary dimension on the cleft side, while its correlation with the change of the maxilla after surgery was low or not statistically significant.</AbstractText><AbstractText Label="CONCLUSION" NlmCategory="CONCLUSIONS">The auto-segmentation of the maxilla and the cleft could be performed very efficiently and accurately with the machine learning method. Asymmetric growth was found on the cleft and non-cleft sides of the maxilla after alveolar bone graft in UCLP patients. The morphology of the cleft mainly contributed to the pre-operation variance of the maxilla but had little impact on the maxilla growth after surgery.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Xin</ForeName><Initials>X</Initials><Identifier Source="ORCID">0000-0001-5825-7229</Identifier><AffiliationInfo><Affiliation>Department of Orthodontics, Peking University School and Hospital of Stomatology &amp; National Center of Stomatology &amp; National Clinical Research Center for Oral Diseases &amp; National Engineering Research Center of Oral Biomaterials and Digital Medical Devices, Beijing, People's Republic of China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Qin</LastName><ForeName>Niu</ForeName><Initials>N</Initials><Identifier Source="ORCID">0000-0003-0311-1120</Identifier><AffiliationInfo><Affiliation>Department of Orthodontics, Peking University School and Hospital of Stomatology &amp; National Center of Stomatology &amp; National Clinical Research Center for Oral Diseases &amp; National Engineering Research Center of Oral Biomaterials and Digital Medical Devices, Beijing, People's Republic of China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhou</LastName><ForeName>Zhibo</ForeName><Initials>Z</Initials><Identifier Source="ORCID">0000-0001-5476-9921</Identifier><AffiliationInfo><Affiliation>Department of Oral and Maxillofacial Surgery, Peking University School and Hospital of Stomatology &amp; National Center of Stomatology &amp; National Clinical Research Center for Oral Diseases &amp; National Engineering Research Center of Oral Biomaterials and Digital Medical Devices, Beijing, People's Republic of China. zzbooo@126.com.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Chen</LastName><ForeName>Si</ForeName><Initials>S</Initials><Identifier Source="ORCID">0000-0003-4195-6351</Identifier><AffiliationInfo><Affiliation>Department of Orthodontics, Peking University School and Hospital of Stomatology &amp; National Center of Stomatology &amp; National Clinical Research Center for Oral Diseases &amp; National Engineering Research Center of Oral Biomaterials and Digital Medical Devices, Beijing, People's Republic of China. elisa02@163.com.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>11</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>BMC Oral Health</MedlineTA><NlmUniqueID>101088684</NlmUniqueID><ISSNLinking>1472-6831</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D002972" MajorTopicYN="Y">Cleft Palate</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName><QualifierName UI="Q000601" MajorTopicYN="N">surgery</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D008437" MajorTopicYN="N">Maxilla</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName><QualifierName UI="Q000601" MajorTopicYN="N">surgery</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D002971" MajorTopicYN="Y">Cleft Lip</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName><QualifierName UI="Q000601" MajorTopicYN="N">surgery</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D064728" MajorTopicYN="Y">Alveolar Bone Grafting</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D014057" MajorTopicYN="N">Tomography, X-Ray Computed</DescriptorName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Alveolar bone grafting</Keyword><Keyword MajorTopicYN="N">Alveolar cleft</Keyword><Keyword MajorTopicYN="N">Auto-filling alveolar cleft</Keyword><Keyword MajorTopicYN="N">CT</Keyword><Keyword MajorTopicYN="N">Machine learning</Keyword><Keyword MajorTopicYN="N">Unilateral cleft lip and palate</Keyword></KeywordList><CoiStatement>The authors declare that they have no competing interests.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>9</Month><Day>25</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2023</Year><Month>1</Month><Day>2</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>23</Hour><Minute>39</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36631872</ArticleId><ArticleId IdType="doi">10.1186/s12903-023-02706-8</ArticleId><ArticleId IdType="pii">10.1186/s12903-023-02706-8</ArticleId><ArticleId IdType="pmc">PMC9835292</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Group IW Prevalence at birth of cleft lip with or without cleft palate: data from the International perinatal database of typical oral clefts (IPDTOC) Cleft Palate Craniofac J. 2011;48(1):66&#x2013;81. doi: 10.1597/09-217.</Citation><ArticleIdList><ArticleId IdType="doi">10.1597/09-217</ArticleId><ArticleId IdType="pubmed">20507242</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhu Y, Miao H, Zeng Q, Li B, Wang D, Yu X, Wu H, Chen Y, Guo P, Liu F. Prevalence of cleft lip and/or cleft palate in Guangdong province, China, 2015&#x2013;2018: a spatio-temporal descriptive analysis. BMJ Open. 2021;11(8):e046430. doi: 10.1136/bmjopen-2020-046430.</Citation><ArticleIdList><ArticleId IdType="doi">10.1136/bmjopen-2020-046430</ArticleId><ArticleId IdType="pmc">PMC8330564</ArticleId><ArticleId IdType="pubmed">34341041</ArticleId></ArticleIdList></Reference><Reference><Citation>Li L, Yu HT, Wang XD, Zhou F, Wang F, Wang CF. Analysis of birth defect rate trend of cleft lip and palate in Shanghai from 2007 to 2016. Zhonghua Kou Qiang Yi Xue Za Zhi. 2018;53(5):301&#x2013;306.</Citation><ArticleIdList><ArticleId IdType="pubmed">29972986</ArticleId></ArticleIdList></Reference><Reference><Citation>McCrary H, Skirko JR. Bone grafting of alveolar clefts. Oral Maxillofac Surg Clin North Am. 2021;33(2):231&#x2013;238. doi: 10.1016/j.coms.2021.01.007.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.coms.2021.01.007</ArticleId><ArticleId IdType="pubmed">33663951</ArticleId></ArticleIdList></Reference><Reference><Citation>Shaheen E, Danneels M, Doucet K, Dormaar T, Verdonck A, Cadenas de Llano-Perula M, Willems G, Politis C, Jacobs R. Validation of a 3D methodology for the evaluation and follow-up of secondary alveolar bone grafting in unilateral cleft lip and palate patients. Orthod Craniofac Res. 2022;25(3):377&#x2013;383. doi: 10.1111/ocr.12546.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/ocr.12546</ArticleId><ArticleId IdType="pubmed">34817927</ArticleId></ArticleIdList></Reference><Reference><Citation>Santiago PE, Schuster LA, Levy-Bercowski D. Management of the alveolar cleft. Clin Plast Surg. 2014;41(2):219&#x2013;232. doi: 10.1016/j.cps.2014.01.001.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cps.2014.01.001</ArticleId><ArticleId IdType="pubmed">24607190</ArticleId></ArticleIdList></Reference><Reference><Citation>Brudnicki A, Sawicka E, Brudnicka R, Fudalej P. Effects of different timing of alveolar bone graft on craniofacial morphology in unilateral cleft lip and palate. Cleft Palate-craniofac J Off Publ Am Cleft Palate-Craniofac Assoc. 2020;57(1):105&#x2013;113. doi: 10.1177/1055665619866363.</Citation><ArticleIdList><ArticleId IdType="doi">10.1177/1055665619866363</ArticleId><ArticleId IdType="pubmed">31370693</ArticleId></ArticleIdList></Reference><Reference><Citation>Brudnicki A, Sawicka E, Fudalej P. Maxillofacial morphology in post-pubertal patients with unilateral cleft lip and palate following early vs. late secondary alveolar bone grafting. J Cranio-maxillo-fac Surg Off Publ Eur Assoc Cranio-Maxillo-Fac Surg. 2021;49(9):809&#x2013;814. doi: 10.1016/j.jcms.2021.04.012.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jcms.2021.04.012</ArticleId><ArticleId IdType="pubmed">33965325</ArticleId></ArticleIdList></Reference><Reference><Citation>Oberoi S, Chigurupati R, Gill P, Hoffman WY, Vargervik K. Volumetric assessment of secondary alveolar bone grafting using cone beam computed tomography. Cleft Palate Craniofac J. 2009;46(5):503&#x2013;511. doi: 10.1597/08-153.1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1597/08-153.1</ArticleId><ArticleId IdType="pubmed">19929098</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang X, Pastewait M, Wu TH, Lian C, Tejera B, Lee YT, Lin FC, Wang L, Shen D, Li S, et al. 3D morphometric quantification of maxillae and defects for patients with unilateral cleft palate via deep learning-based CBCT image auto-segmentation. Orthod Craniofac Res. 2021;24(Suppl 2):108&#x2013;116. doi: 10.1111/ocr.12482.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/ocr.12482</ArticleId><ArticleId IdType="pmc">PMC8435046</ArticleId><ArticleId IdType="pubmed">33711187</ArticleId></ArticleIdList></Reference><Reference><Citation>Suri S, Utreja A, Khandelwal N, Mago SK. Craniofacial computerized tomography analysis of the midface of patients with repaired complete unilateral cleft lip and palate. Am J Orthod Dentofac Orthop. 2008;134(3):418&#x2013;429. doi: 10.1016/j.ajodo.2006.09.065.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ajodo.2006.09.065</ArticleId><ArticleId IdType="pubmed">18774088</ArticleId></ArticleIdList></Reference><Reference><Citation>Li H, Yang Y, Chen Y, Wu Y, Zhang Y, Wu D, Liang Y. Three-dimensional reconstruction of maxillae using spiral computed tomography and its application in postoperative adult patients with unilateral complete cleft lip and palate. J Oral Maxillofac Surg. 2011;69(12):e549&#x2013;557. doi: 10.1016/j.joms.2011.07.024.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.joms.2011.07.024</ArticleId><ArticleId IdType="pubmed">21982692</ArticleId></ArticleIdList></Reference><Reference><Citation>Brudnicki A, Regulski P, Sawicka E, Fudalej P. Alveolar volume following different timings of secondary bone grafting in patients with unilateral cleft lip and palate. A pilot study. J Clin Med. 2021;10(16):3524. doi: 10.3390/jcm10163524.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/jcm10163524</ArticleId><ArticleId IdType="pmc">PMC8396845</ArticleId><ArticleId IdType="pubmed">34441820</ArticleId></ArticleIdList></Reference><Reference><Citation>Kasaven C, Ivekovic S, McIntyre G, Gillgrass T, Thomson D, Menhinick A, Mossey P. Validation of the volumetric measurement of a simulated maxillary alveolar bone defect using cone-beam computed tomography. Cleft Palate-Craniofac J Off Publ Am Cleft Palate-Craniofac Assoc. 2013;50(6):e115&#x2013;120. doi: 10.1597/12-161.</Citation><ArticleIdList><ArticleId IdType="doi">10.1597/12-161</ArticleId><ArticleId IdType="pubmed">23157577</ArticleId></ArticleIdList></Reference><Reference><Citation>Kang NH. Current methods for the treatment of alveolar cleft. Arch Plast Surg. 2017;44(3):188&#x2013;193. doi: 10.5999/aps.2017.44.3.188.</Citation><ArticleIdList><ArticleId IdType="doi">10.5999/aps.2017.44.3.188</ArticleId><ArticleId IdType="pmc">PMC5447527</ArticleId><ArticleId IdType="pubmed">28573092</ArticleId></ArticleIdList></Reference><Reference><Citation>Feng B, Jiang M, Xu X, Li J. A new method of volumetric assessment of alveolar bone grafting for cleft patients using cone beam computed tomography. Oral Surg Oral Med Oral Pathol Oral Radiol. 2017;124(2):e171&#x2013;e182. doi: 10.1016/j.oooo.2017.04.003.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.oooo.2017.04.003</ArticleId><ArticleId IdType="pubmed">28606827</ArticleId></ArticleIdList></Reference><Reference><Citation>Lee J, Jun S, Cho Y, Lee H, Kim G, Seo J, Kim N. Deep learning in medical imaging: general overview. Korean J Radiol. 2017;18(4):570&#x2013;584. doi: 10.3348/kjr.2017.18.4.570.</Citation><ArticleIdList><ArticleId IdType="doi">10.3348/kjr.2017.18.4.570</ArticleId><ArticleId IdType="pmc">PMC5447633</ArticleId><ArticleId IdType="pubmed">28670152</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen S, Wang L, Li G, Wu TH, Diachina S, Tejera B, Kwon JJ, Lin FC, Lee YT, Xu T, et al. Machine learning in orthodontics: Introducing a 3D auto-segmentation and auto-landmark finder of CBCT images to assess maxillary constriction in unilateral impacted canine patients. Angle Orthod. 2020;90(1):77&#x2013;84. doi: 10.2319/012919-59.1.</Citation><ArticleIdList><ArticleId IdType="doi">10.2319/012919-59.1</ArticleId><ArticleId IdType="pmc">PMC8087054</ArticleId><ArticleId IdType="pubmed">31403836</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang Y, Pei Y, Chen S, Guo Y, Ma G, Xu T, Zha H, IEEE. Volumetric registration-based cleft volume estimation of alveolar cleft grafting procedures. In: IEEE 17th international symposium on biomedical imaging (ISBI). Iowa, IA; 2020. p. 99&#x2013;103.</Citation></Reference><Reference><Citation>Zhang Y, Pei Y, Guo Y, Chen S, Xu T, Zha H. Cleft volume estimation and maxilla completion using cascaded deep neural networks. In: Machine learning in medical imaging. 2020. p. 332&#x2013;341.</Citation></Reference><Reference><Citation>Yushkevich PA, Piven J, Hazlett HC, Smith RG, Ho S, Gee JC, Gerig G. User-guided 3D active contour segmentation of anatomical structures: significantly improved efficiency and reliability. Neuroimage. 2006;31(3):1116&#x2013;1128. doi: 10.1016/j.neuroimage.2006.01.015.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2006.01.015</ArticleId><ArticleId IdType="pubmed">16545965</ArticleId></ArticleIdList></Reference><Reference><Citation>Agarwal R, Parihar A, Mandhani PA, Chandra R. Three-dimensional computed tomographic analysis of the maxilla in unilateral cleft lip and palate: implications for rhinoplasty. J Craniofac Surg. 2012;23(5):1338&#x2013;1342. doi: 10.1097/SCS.0b013e31826466d8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/SCS.0b013e31826466d8</ArticleId><ArticleId IdType="pubmed">22948621</ArticleId></ArticleIdList></Reference><Reference><Citation>Berkowitz S, Berkowitz S, Berkowitz S. The effect of clefting of the lip and palate and the palatal arch form. In: Berkowitz S, editor. Cleft lip and palate. Berlin Heidelberg: Springer; 2013. pp. 61&#x2013;85.</Citation></Reference><Reference><Citation>Aduss H, Pruzansky S. The nasal cavity in complete unilateral cleft lip and palate. Arch Otolaryngol. 1967;85(1):53&#x2013;61. doi: 10.1001/archotol.1967.00760040055011.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/archotol.1967.00760040055011</ArticleId><ArticleId IdType="pubmed">6016251</ArticleId></ArticleIdList></Reference><Reference><Citation>Ozawa T, Omura S, Fukuyama E, Matsui Y, Torikai K, Fujita K. Factors influencing secondary alveolar bone grafting in cleft lip and palate patients: prospective analysis using CT image analyzer. Cleft Palate Craniofac J. 2007;44(3):286&#x2013;291. doi: 10.1597/06-054.</Citation><ArticleIdList><ArticleId IdType="doi">10.1597/06-054</ArticleId><ArticleId IdType="pubmed">17477757</ArticleId></ArticleIdList></Reference><Reference><Citation>Linderup BW, Cattaneo PM, Jensen J, Kuseler A. Mandibular symphyseal bone graft for reconstruction of alveolar cleft defects: volumetric assessment with cone beam computed tomography 1-year postsurgery. Cleft Palate Craniofac J. 2016;53(1):64&#x2013;72. doi: 10.1597/14-143.</Citation><ArticleIdList><ArticleId IdType="doi">10.1597/14-143</ArticleId><ArticleId IdType="pubmed">25489772</ArticleId></ArticleIdList></Reference><Reference><Citation>Honma K, Kobayashi T, Nakajima T, Hayasi T. Computed tomographic evaluation of bone formation after secondary bone grafting of alveolar clefts. J Oral Maxillofac Surg. 1999;57(10):1209&#x2013;1213. doi: 10.1016/S0278-2391(99)90488-3.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0278-2391(99)90488-3</ArticleId><ArticleId IdType="pubmed">10513867</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36631859</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>13</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>13</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">1471-2393</ISSN><JournalIssue CitedMedium="Internet"><Volume>23</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>11</Day></PubDate></JournalIssue><Title>BMC pregnancy and childbirth</Title><ISOAbbreviation>BMC Pregnancy Childbirth</ISOAbbreviation></Journal><ArticleTitle>Learning deep architectures for the interpretation of first-trimester fetal echocardiography (LIFE) - a study protocol for developing an automated intelligent decision support system for early fetal echocardiography.</ArticleTitle><Pagination><StartPage>20</StartPage><MedlinePgn>20</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">20</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1186/s12884-022-05204-x</ELocationID><Abstract><AbstractText Label="BACKGROUND" NlmCategory="BACKGROUND">Congenital Heart Disease represents the most frequent fetal malformation. The lack of prenatal identification of congenital heart defects can have adverse consequences for the neonate, while a correct prenatal diagnosis of specific cardiac anomalies improves neonatal care neurologic and surgery outcomes. Sonographers perform prenatal diagnosis manually during the first or second-trimester scan, but the reported detection rates are low. This project's primary objective is to develop an Intelligent Decision Support System that uses two-dimensional video files of cardiac sweeps obtained during the standard first-trimester fetal echocardiography (FE) to signal the presence/absence of previously learned key features.</AbstractText><AbstractText Label="METHODS" NlmCategory="METHODS">The cross-sectional study will be divided into a training part of the machine learning approaches and the testing phase on previously unseen frames and eventually on actual video scans. Pregnant women in their 12-13&#x2009;+&#x2009;6&#x2009;weeks of gestation admitted for routine first-trimester anomaly scan will be consecutively included in a two-year study, depending on the availability of the experienced sonographers in early fetal cardiac imaging involved in this research. The Data Science / IT department (DSIT) will process the key planes identified by the sonographers in the two- dimensional heart cine loop sweeps: four-chamber view, left and right ventricular outflow tracts, three vessels, and trachea view. The frames will be grouped into the classes representing the plane views, and then different state-of-the- art deep-learning (DL) pre-trained algorithms will be tested on the data set. The sonographers will validate all the intermediary findings at the frame level and the meaningfulness of the video labeling.</AbstractText><AbstractText Label="DISCUSSION" NlmCategory="CONCLUSIONS">FE is feasible and efficient during the first trimester. Still, the continuous training process is impaired by the lack of specialists or their limited availability. Therefore, in our study design, the sonographer benefits from a second opinion provided by the developed software, which may be very helpful, especially if a more experienced colleague is unavailable. In addition, the software may be implemented on the ultrasound device so that the process could take place during the live examination.</AbstractText><AbstractText Label="TRIAL REGISTRATION" NlmCategory="BACKGROUND">The study is registered under the name "Learning deep architectures for the Interpretation of Fetal Echocardiography (LIFE)", project number 408PED/2020, project code PN-III-P2-2.1-PED-2019.</AbstractText><AbstractText Label="TRIAL REGISTRATION" NlmCategory="BACKGROUND">ClinicalTrials.gov , unique identifying number NCT05090306, date of registration 30.10.2020.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Anda</LastName><ForeName>Ungureanu</ForeName><Initials>U</Initials><AffiliationInfo><Affiliation>Department of Paediatric Cardiology, University Emergency County Hospital Craiova, Tabaci, no.1, 200642, Craiova, Romania.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Obstetrics and Gynecology, University of Medicine and Pharmacy Craiova, Petru Rares, no. 2, 200412, Craiova, Romania.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Obstetrics and Gynecology, University Emergency County Hospital Craiova, Romania Tabaci, no.1, 200642, Craiova, Romania.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>MEDGIN / GINECHO Clinic, 1 Mai, no. 29, 200333, Craiova, Romania.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Andreea-Sorina</LastName><ForeName>Marcu</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Obstetrics and Gynecology, University of Medicine and Pharmacy Craiova, Petru Rares, no. 2, 200412, Craiova, Romania.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Laurentiu</LastName><ForeName>Patru Ciprian</ForeName><Initials>PC</Initials><AffiliationInfo><Affiliation>Department of Obstetrics and Gynecology, University of Medicine and Pharmacy Craiova, Petru Rares, no. 2, 200412, Craiova, Romania. patru_ciprian@yahoo.com.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Obstetrics and Gynecology, University Emergency County Hospital Craiova, Romania Tabaci, no.1, 200642, Craiova, Romania. patru_ciprian@yahoo.com.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>MEDGIN / GINECHO Clinic, 1 Mai, no. 29, 200333, Craiova, Romania. patru_ciprian@yahoo.com.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Dan</LastName><ForeName>Ruican</ForeName><Initials>R</Initials><AffiliationInfo><Affiliation>Department of Obstetrics and Gynecology, University of Medicine and Pharmacy Craiova, Petru Rares, no. 2, 200412, Craiova, Romania.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Obstetrics and Gynecology, University Emergency County Hospital Craiova, Romania Tabaci, no.1, 200642, Craiova, Romania.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>MEDGIN / GINECHO Clinic, 1 Mai, no. 29, 200333, Craiova, Romania.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Rodica</LastName><ForeName>Nagy</ForeName><Initials>N</Initials><AffiliationInfo><Affiliation>Department of Obstetrics and Gynecology, University of Medicine and Pharmacy Craiova, Petru Rares, no. 2, 200412, Craiova, Romania.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Obstetrics and Gynecology, University Emergency County Hospital Craiova, Romania Tabaci, no.1, 200642, Craiova, Romania.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>MEDGIN / GINECHO Clinic, 1 Mai, no. 29, 200333, Craiova, Romania.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Ruxandra</LastName><ForeName>Stoean</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Romanian Institute of Science and Technology, Virgil Fulicea, no. 3, 400022, Cluj Napoca, Romania.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Computer Science, University of Craiova, A.I. Cuza, 13, 200585, Craiova, Romania.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Catalin</LastName><ForeName>Stoean</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Romanian Institute of Science and Technology, Virgil Fulicea, no. 3, 400022, Cluj Napoca, Romania.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Computer Science, University of Craiova, A.I. Cuza, 13, 200585, Craiova, Romania.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Gabriel</LastName><ForeName>Iliescu Dominic</ForeName><Initials>ID</Initials><AffiliationInfo><Affiliation>Department of Obstetrics and Gynecology, University of Medicine and Pharmacy Craiova, Petru Rares, no. 2, 200412, Craiova, Romania.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Obstetrics and Gynecology, University Emergency County Hospital Craiova, Romania Tabaci, no.1, 200642, Craiova, Romania.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>MEDGIN / GINECHO Clinic, 1 Mai, no. 29, 200333, Craiova, Romania.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><DataBankList CompleteYN="Y"><DataBank><DataBankName>ClinicalTrials.gov</DataBankName><AccessionNumberList><AccessionNumber>NCT05090306</AccessionNumber></AccessionNumberList></DataBank></DataBankList><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>11</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>BMC Pregnancy Childbirth</MedlineTA><NlmUniqueID>100967799</NlmUniqueID><ISSNLinking>1471-2393</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D007231" MajorTopicYN="N">Infant, Newborn</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D011247" MajorTopicYN="N">Pregnancy</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D005260" MajorTopicYN="N">Female</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D011261" MajorTopicYN="N">Pregnancy Trimester, First</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D003430" MajorTopicYN="N">Cross-Sectional Studies</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D016216" MajorTopicYN="Y">Ultrasonography, Prenatal</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D006330" MajorTopicYN="Y">Heart Defects, Congenital</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D004452" MajorTopicYN="N">Echocardiography</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D005318" MajorTopicYN="N">Fetal Heart</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Artificial intelligence</Keyword><Keyword MajorTopicYN="N">Decision support systems</Keyword><Keyword MajorTopicYN="N">Echocardiography</Keyword><Keyword MajorTopicYN="N">Fetal cardiology</Keyword></KeywordList><CoiStatement>The authors have no financial connections with companies that may be interested in the submitted work and no non- financial interests that may be relevant to the article.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>4</Month><Day>7</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>11</Month><Day>9</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>23</Hour><Minute>38</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36631859</ArticleId><ArticleId IdType="pmc">PMC9832772</ArticleId><ArticleId IdType="doi">10.1186/s12884-022-05204-x</ArticleId><ArticleId IdType="pii">10.1186/s12884-022-05204-x</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Hoffman JI. Incidence of congenital heart disease: II. Prenatal incidence. Pediatr Cardiol. 1995;16(4):155&#x2013;165. doi: 10.1007/BF00794186.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/BF00794186</ArticleId><ArticleId IdType="pubmed">7567659</ArticleId></ArticleIdList></Reference><Reference><Citation>Crispi F, Gratac&#xf3;s E. Fetal cardiac function: technical considerations and potential research and clinical applications. Fetal Diagn Ther. 2012;32(1&#x2013;2):47&#x2013;64. doi: 10.1159/000338003.</Citation><ArticleIdList><ArticleId IdType="doi">10.1159/000338003</ArticleId><ArticleId IdType="pubmed">22614129</ArticleId></ArticleIdList></Reference><Reference><Citation>Gardiner HM. Advances in fetal echocardiography. Semin Fetal Neonatal Med. 2018;23(2):112&#x2013;118. doi: 10.1016/j.siny.2017.11.006.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.siny.2017.11.006</ArticleId><ArticleId IdType="pubmed">29221765</ArticleId></ArticleIdList></Reference><Reference><Citation>Tibballs J, Cantwell-Bartl A. Outcomes of management decisions by parents for their infants with hypoplastic left heart syndrome born with and without a prenatal diagnosis. J Paediatr Child Health. 2008;44(6):321&#x2013;324. doi: 10.1111/j.1440-1754.2007.01265.x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/j.1440-1754.2007.01265.x</ArticleId><ArticleId IdType="pubmed">18194197</ArticleId></ArticleIdList></Reference><Reference><Citation>Franklin O, Burch M, Manning N, et al. Prenatal diagnosis of coarctation of the aorta improves survival and reduces morbidity. Heart. 2002;87(1):67&#x2013;69. doi: 10.1136/heart.87.1.67.</Citation><ArticleIdList><ArticleId IdType="doi">10.1136/heart.87.1.67</ArticleId><ArticleId IdType="pmc">PMC1766965</ArticleId><ArticleId IdType="pubmed">11751670</ArticleId></ArticleIdList></Reference><Reference><Citation>Acherman RJ, Evans WN, Luna CF, et al. Prenatal detection of congenital heart disease in southern Nevada: the need for universal fetal cardiac evaluation. J Ultrasound Med. 2007;26(12):1715&#x2013;1719. doi: 10.7863/jum.2007.26.12.1715.</Citation><ArticleIdList><ArticleId IdType="doi">10.7863/jum.2007.26.12.1715</ArticleId><ArticleId IdType="pubmed">18029923</ArticleId></ArticleIdList></Reference><Reference><Citation>Schultz AH, Localio AR, Clark BJ, et al. Epidemiologic features of the presentation of critical congenital heart disease: implications for screening. Pediatrics. 2008;121(4):751&#x2013;757. doi: 10.1542/peds.2007-0421.</Citation><ArticleIdList><ArticleId IdType="doi">10.1542/peds.2007-0421</ArticleId><ArticleId IdType="pubmed">18381540</ArticleId></ArticleIdList></Reference><Reference><Citation>Brown KL, Ridout DA, Hoskote A, et al. Delayed diagnosis of congenital heart disease worsens preoperative condition and outcome of surgery in neonates. Heart. 2006;92(9):1298&#x2013;1302. doi: 10.1136/hrt.2005.078097.</Citation><ArticleIdList><ArticleId IdType="doi">10.1136/hrt.2005.078097</ArticleId><ArticleId IdType="pmc">PMC1861169</ArticleId><ArticleId IdType="pubmed">16449514</ArticleId></ArticleIdList></Reference><Reference><Citation>Tworetzky W, McElhinney DB, Reddy VM, et al. Improved surgical outcome after fetal diagnosis of hypoplastic left heart syndrome. Circulation. 2001;103(9):1269&#x2013;1273. doi: 10.1161/01.CIR.103.9.1269.</Citation><ArticleIdList><ArticleId IdType="doi">10.1161/01.CIR.103.9.1269</ArticleId><ArticleId IdType="pubmed">11238272</ArticleId></ArticleIdList></Reference><Reference><Citation>Verheijen PM, Lisowski LA, Stoutenbeek P, et al. Lactacidosis in the neonate is minimized by prenatal detection of congenital heart disease. Ultrasound Obstet Gynecol. 2002;19:552&#x2013;555. doi: 10.1046/j.1469-0705.2002.00714.x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1046/j.1469-0705.2002.00714.x</ArticleId><ArticleId IdType="pubmed">12047532</ArticleId></ArticleIdList></Reference><Reference><Citation>Kumar RK, Newburger JW, Gauvreau K, et al. Comparison of outcome when hypoplastic left heart syndrome and transposition of the great arteries are diagnosed prenatally versus when diagnosis of these two conditions is made only postnatally. Am J Cardiol. 1999;83(12):1649&#x2013;1653. doi: 10.1016/S0002-9149(99)00172-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0002-9149(99)00172-1</ArticleId><ArticleId IdType="pubmed">10392870</ArticleId></ArticleIdList></Reference><Reference><Citation>Mahle WT, Clancy RR, McGaurn SP, et al. Impact of prenatal diagnosis on survival and early neurologic morbidity in neonates with the hypoplastic left heart syndrome. Pediatrics. 2001;107(6):1277&#x2013;1282. doi: 10.1542/peds.107.6.1277.</Citation><ArticleIdList><ArticleId IdType="doi">10.1542/peds.107.6.1277</ArticleId><ArticleId IdType="pubmed">11389243</ArticleId></ArticleIdList></Reference><Reference><Citation>Bonnet D, Coltri A, Butera G, et al. Detection of transposition of the great arteries in fetuses reduces neonatal morbidity and mortality. Circulation. 1999;99(7):916&#x2013;918. doi: 10.1161/01.CIR.99.7.916.</Citation><ArticleIdList><ArticleId IdType="doi">10.1161/01.CIR.99.7.916</ArticleId><ArticleId IdType="pubmed">10027815</ArticleId></ArticleIdList></Reference><Reference><Citation>Khoshnood B, De Vigan C, Vodovar V, et al. Trends in prenatal diagnosis, pregnancy termination, and perinatal mortality of newborns with congenital heart disease in France, 1983-2000: a population-based evaluation. Pediatrics. 2005;115(1):95&#x2013;101. doi: 10.1542/peds.2004-0516.</Citation><ArticleIdList><ArticleId IdType="doi">10.1542/peds.2004-0516</ArticleId><ArticleId IdType="pubmed">15629987</ArticleId></ArticleIdList></Reference><Reference><Citation>Bensemlali M, Stirnemann J, Le Bidois J, et al. Discordances between prenatal and postnatal diagnoses of congenital heart diseases and impact on care strategies. J Am Coll Cardiol. 2016;68(9):921&#x2013;930. doi: 10.1016/j.jacc.2016.05.087.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jacc.2016.05.087</ArticleId><ArticleId IdType="pubmed">27561766</ArticleId></ArticleIdList></Reference><Reference><Citation>Mozumdar N, Rowland J, Pan S, et al. Diagnostic accuracy of fetal echocardiography in congenital heart disease. J Am Soc Echocardiogr. 2020;33(11):1384&#x2013;1390. doi: 10.1016/j.echo.2020.06.017.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.echo.2020.06.017</ArticleId><ArticleId IdType="pubmed">32828627</ArticleId></ArticleIdList></Reference><Reference><Citation>Abu-Rustum RS, Ziade MF, Abu-Rustum SE. Learning curve and factors influencing the feasibility of performing fetal echocardiography at the time of the first-trimester scan. J Ultrasound Med. 2011;30(5):695&#x2013;700. doi: 10.7863/jum.2011.30.5.695.</Citation><ArticleIdList><ArticleId IdType="doi">10.7863/jum.2011.30.5.695</ArticleId><ArticleId IdType="pubmed">21527618</ArticleId></ArticleIdList></Reference><Reference><Citation>Nemescu D, Onofriescu M. Factors affecting the feasibility of routine first-trimester fetal echocardiography. J Ultrasound Med. 2015;34(1):161&#x2013;166. doi: 10.7863/ultra.34.1.161.</Citation><ArticleIdList><ArticleId IdType="doi">10.7863/ultra.34.1.161</ArticleId><ArticleId IdType="pubmed">25542952</ArticleId></ArticleIdList></Reference><Reference><Citation>DeVore GR, Medearis AL, Bear MB, et al. Fetal echocardiography: factors that influence imaging of the fetal heart during the second trimester of pregnancy. J Ultrasound Med. 1993;12(11):659&#x2013;663. doi: 10.7863/jum.1993.12.11.659.</Citation><ArticleIdList><ArticleId IdType="doi">10.7863/jum.1993.12.11.659</ArticleId><ArticleId IdType="pubmed">8264018</ArticleId></ArticleIdList></Reference><Reference><Citation>Paladini D. Sonography in obese and overweight pregnant women: clinical, medicolegal and technical issues. Ultrasound Obstet Gynecol. 2009;33(6):720&#x2013;729. doi: 10.1002/uog.6393.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/uog.6393</ArticleId><ArticleId IdType="pubmed">19479683</ArticleId></ArticleIdList></Reference><Reference><Citation>Tegnander E, Eik-Nes SH. The examiner's ultrasound experience significantly impacts the detection rate of congenital heart defects at the second-trimester fetal examination. Ultrasound Obstet Gynecol. 2006;28(1):8&#x2013;14. doi: 10.1002/uog.2804.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/uog.2804</ArticleId><ArticleId IdType="pubmed">16736449</ArticleId></ArticleIdList></Reference><Reference><Citation>Hunter S, Heads A, Wyllie J, et al. Prenatal diagnosis of congenital heart disease in the northern region of England: benefits of a training program for obstetric ultrasonographers. Heart. 2000;84(3):294&#x2013;298. doi: 10.1136/heart.84.3.294.</Citation><ArticleIdList><ArticleId IdType="doi">10.1136/heart.84.3.294</ArticleId><ArticleId IdType="pmc">PMC1760944</ArticleId><ArticleId IdType="pubmed">10956294</ArticleId></ArticleIdList></Reference><Reference><Citation>International Society of Ultrasound in Obstetrics and Gynecology Guidelines Cardiac screening examination of the fetus: guidelines for performing the "basic" and "extended basic" cardiac scans. Ultrasound Obstet Gynecol. 2006;27:107&#x2013;113. doi: 10.1002/uog.2677.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/uog.2677</ArticleId><ArticleId IdType="pubmed">16374757</ArticleId></ArticleIdList></Reference><Reference><Citation>Iliescu DG. Echocardiography. USA: SM Group; 2016.</Citation></Reference><Reference><Citation>Bennasar M, Martinez JM, Olivella A, del Rio M, Gomez O, Figueras F, Puerto B, Gratacos E. Feasibility and accuracy of fetal echocardiography using four-dimensional spatiotemporal image correlation technology before 16 weeks gestation. Ultrasound Obstet Gynecol. 2009;33:645&#x2013;651. doi: 10.1002/uog.6374.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/uog.6374</ArticleId><ArticleId IdType="pubmed">19479815</ArticleId></ArticleIdList></Reference><Reference><Citation>Vi&#xf1;als F, Mandujano L, Vargas G, Giuliano A. Prenatal diagnosis of congenital heart disease using four-dimensional Spatio-temporal image correlation (STIC) telemedicine via an internet link a pilot study. Ultrasound Obstet Gynecol. 2005;25:25&#x2013;31. doi: 10.1002/uog.1796.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/uog.1796</ArticleId><ArticleId IdType="pubmed">15593355</ArticleId></ArticleIdList></Reference><Reference><Citation>Bennasar M, Mart&#xed;nez JM, G&#xf3;mez O, Figueras F, Olivella A, Puerto B, Gratac&#xf3;s E. Intra- and interobserver repeatability of fetal cardiac examination using four-dimensional spatiotemporal image correlation in each trimester of pregnancy. Ultrasound Obstet Gynecol. 2010;35:318&#x2013;323. doi: 10.1002/uog.7570.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/uog.7570</ArticleId><ArticleId IdType="pubmed">20127758</ArticleId></ArticleIdList></Reference><Reference><Citation>Turan S, Turan OM, Ty-Torredes K, Harman CR, Baschat AA. Standardization of the first-trimester fetal cardiac examination using spatiotemporal image correlation with tomographic ultrasound and color Doppler imaging. Ultrasound Obstet Gynecol. 2009;33:652&#x2013;656. doi: 10.1002/uog.6372.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/uog.6372</ArticleId><ArticleId IdType="pubmed">19405042</ArticleId></ArticleIdList></Reference><Reference><Citation>Tudorache S, Cara M, Iliescu DG, Novac L, Cernea N. First trimester two- and four-dimensional cardiac scan: intra-and interobserver agreement, comparison between methods and benefits of color Doppler technique. Ultrasound Obstet Gynecol. 2013;42(6):659&#x2013;668. doi: 10.1002/uog.12459.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/uog.12459</ArticleId><ArticleId IdType="pubmed">23494803</ArticleId></ArticleIdList></Reference><Reference><Citation>Iliescu D, Tudorache S, Comanescu A, Antsaklis P, Cotarcea S, Novac L, Cernea N, Antsaklis A. Improved detection rate of structural abnormalities in the first trimester using an extended examination protocol. Ultrasound Obstet Gynecol. 2013;42(3):300&#x2013;309. doi: 10.1002/uog.12489.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/uog.12489</ArticleId><ArticleId IdType="pubmed">23595897</ArticleId></ArticleIdList></Reference><Reference><Citation>Belciug S, Gorunescu F. Intelligent decision support systems&#x2014;a journey to smarter healthcare. Switzerland: Springer Nature; 2020.</Citation></Reference><Reference><Citation>Garcia-Canadilla P, Sanchez-Martinez S, Crispi F, et al. Machine learning in fetal cardiology: what to expect. Fetal Diagn Ther. 2020;47(5):363&#x2013;372. doi: 10.1159/000505021.</Citation><ArticleIdList><ArticleId IdType="doi">10.1159/000505021</ArticleId><ArticleId IdType="pubmed">31910421</ArticleId></ArticleIdList></Reference><Reference><Citation>Matsuoka R, Komatsu M, Sakai A, et al. A novel deep learning-based system for fetal cardiac screening. Ultrasound Obstet Gynecol. 2019;56(1):177&#x2013;178. doi: 10.1002/uog.20945.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/uog.20945</ArticleId></ArticleIdList></Reference><Reference><Citation>Komatsu R, Matsuoka R, Arakaki T, et al. Novel AI-guided ultrasound screening system for fetal heart can demonstrate findings in timeline diagram. Ultrasound Obstet Gynecol. 2019;54(1):134. doi: 10.1002/uog.20796.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/uog.20796</ArticleId></ArticleIdList></Reference><Reference><Citation>Arnaout R, Curran L, Chinn E, et al. Deep-learning models improve community-level diagnosis for common congenital heart disease lesions. 2019.</Citation></Reference><Reference><Citation>Stoean R, Iliescu D, Stoean C, Ilie V, Patru C, Hotoleanu M, Nagy R, Ruican R, Trocan R, Marcu A, Atencia M, Joya G. Deep learning for the detection of frames of interest in fetal heart assessment from first trimester ultrasound, 16th international work-conference on artificial neural networks, June, 16th&#x2013;18th. 2021.</Citation></Reference><Reference><Citation>Carvalho JS, Allan LD, Chaoui R, et al. ISUOG practice guidelines (updated): sonographic screening examination of the fetal heart. Ultrasound Obstet Gynecol. 2013;41:348&#x2013;359. doi: 10.1002/uog.12403.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/uog.12403</ArticleId><ArticleId IdType="pubmed">23460196</ArticleId></ArticleIdList></Reference><Reference><Citation>Becker R, Wegner RD. Detailed screening for fetal anomalies and cardiac defects at the 11&#x2013;13-week scan. Ultrasound Obstet Gynecol. 2006;27:613&#x2013;618. doi: 10.1002/uog.2709.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/uog.2709</ArticleId><ArticleId IdType="pubmed">16570262</ArticleId></ArticleIdList></Reference><Reference><Citation>Lombardi CM, Bellotti M, Fesslova V, Cappellini A. Fetal echocardiography at the time of the nuchal translucency scan. Ultrasound Obstet Gynecol. 2007;29:249&#x2013;57.&#xa0;</Citation><ArticleIdList><ArticleId IdType="pubmed">17318942</ArticleId></ArticleIdList></Reference><Reference><Citation>Persico N, Moratalla J, Lombardi CM, Zidere V, Allan L, Nicolaides KH. Fetal echocardiography at 11&#x2013;13 weeks by transabdominal high-frequency ultrasound. Ultrasound Obstet Gynecol. 2011;37:296&#x2013;301. doi: 10.1002/uog.8934.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/uog.8934</ArticleId><ArticleId IdType="pubmed">21229572</ArticleId></ArticleIdList></Reference><Reference><Citation>Selvaraju RR, Cogswell M, Das A, et al. Grad-CAM: visual explanations from deep networks via gradient-based localization, 2017 IEEE International Conference on Computer Vision (ICCV) 2017. pp. 618&#x2013;626.</Citation></Reference><Reference><Citation>Ter Haar G, Duck FA, Shaw A, et al. The safe use of ultrasound in medical diagnosis. UK: The British Institute of Radiology; 2012.</Citation></Reference><Reference><Citation>Bioeffects and Safety Committee. Salvesen K, Lees C, Abramowicz J, et al. ISUOG-WFUMB statement on the non- medical use of ultrasound, 2011. Ultrasound Obstet Gynecol. 2011;38(5):608. doi: 10.1002/uog.10107.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/uog.10107</ArticleId><ArticleId IdType="pubmed">22028045</ArticleId></ArticleIdList></Reference><Reference><Citation>Campbell S, Platt L. The publishing of papers on first-trimester Doppler. Ultrasound Obstet Gynecol. 1999;14:159&#x2013;160. doi: 10.1046/j.1469-0705.1999.14030159.x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1046/j.1469-0705.1999.14030159.x</ArticleId><ArticleId IdType="pubmed">10550872</ArticleId></ArticleIdList></Reference><Reference><Citation>Salvesen K, Lees C, Abramowicz J, Brezinka C, Ter Haar G, Mar&#x161;&#xe1;l K, on behalf of Board of International Society of 'Ultrasound in obstetrics and gynecology (ISUOG) ISUOG statement on the safe use of Doppler in the 11 to 13 + 6- week fetal ultrasound examination. Ultrasound Obstet Gynecol. 2011;37:628. doi: 10.1002/uog.9026.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/uog.9026</ArticleId><ArticleId IdType="pubmed">21618313</ArticleId></ArticleIdList></Reference><Reference><Citation>Salvesen KA, Lees C, Abramowicz J, Brezinka C, Ter Haar G, Mar&#x161;&#xe1;l K. Safe use of Doppler ultrasound during the 11 to 13 + 6-week scan: is it possible? Ultrasound Obstet Gynecol. 2011;37:625&#x2013;628. doi: 10.1002/uog.9025.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/uog.9025</ArticleId><ArticleId IdType="pubmed">21618312</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36631349</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>11</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1878-4046</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>09</Day></PubDate></JournalIssue><Title>Academic radiology</Title><ISOAbbreviation>Acad Radiol</ISOAbbreviation></Journal><ArticleTitle>Deep Learning-based Automatic Diagnosis of Breast Cancer on MRI Using Mask R-CNN for Detection Followed by ResNet50 for Classification.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">S1076-6332(22)00695-X</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1016/j.acra.2022.12.038</ELocationID><Abstract><AbstractText Label="RATIONALE AND OBJECTIVES" NlmCategory="OBJECTIVE">Diagnosis of breast cancer on MRI requires, first, the identification of suspicious lesions; second, the characterization to give a diagnostic impression. We implemented Mask Reginal-Convolutional Neural Network (R-CNN) to detect abnormal lesions, followed by ResNet50 to estimate the malignancy probability.</AbstractText><AbstractText Label="MATERIALS AND METHODS" NlmCategory="METHODS">Two datasets were used. The first set had 176 cases, 103 cancer, and 73 benign. The second set had 84 cases, 53 cancer, and 31 benign. For detection, the pre-contrast image and the subtraction images of left and right breasts were used as inputs, so the symmetry could be considered. The detected suspicious area was characterized by ResNet50, using three DCE parametric maps as inputs. The results obtained using slice-based analyses were combined to give a lesion-based diagnosis.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">In the first dataset, 101 of 103 cancers were detected by Mask R-CNN as suspicious, and 99 of 101 were correctly classified by ResNet50 as cancer, with a sensitivity of 99/103&#xa0;=&#xa0;96%. 48 of 73 benign lesions and 131 normal areas were identified as suspicious. Following classification by ResNet50, only 16 benign and 16 normal areas remained as malignant. The second dataset was used for independent testing. The sensitivity was 43/53&#xa0;=&#xa0;81%. Of the total of 121 identified non-cancerous lesions, only 6 of 31 benign lesions and 22 normal tissues were classified as malignant.</AbstractText><AbstractText Label="CONCLUSION" NlmCategory="CONCLUSIONS">ResNet50 could eliminate approximately 80% of false positives detected by Mask R-CNN. Combining Mask R-CNN and ResNet50 has the potential to develop a fully-automatic computer-aided diagnostic system for breast cancer on MRI.</AbstractText><CopyrightInformation>Copyright &#xa9; 2022 The Association of University Radiologists. Published by Elsevier Inc. All rights reserved.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Yang</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Department of Radiological Sciences, University of California, Irvine, California; Department of Radiation Oncology, Rutgers-Cancer Institute of New Jersey, Robert Wood Johnson Medical School, New Brunswick, New Jersey.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Liu</LastName><ForeName>Yan-Lin</ForeName><Initials>YL</Initials><AffiliationInfo><Affiliation>Department of Radiological Sciences, University of California, Irvine, California.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Nie</LastName><ForeName>Ke</ForeName><Initials>K</Initials><AffiliationInfo><Affiliation>Department of Radiation Oncology, Rutgers-Cancer Institute of New Jersey, Robert Wood Johnson Medical School, New Brunswick, New Jersey.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhou</LastName><ForeName>Jiejie</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Department of Radiology, First Affiliated Hospital of Wenzhou Medical University, Wenzhou, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Chen</LastName><ForeName>Zhongwei</ForeName><Initials>Z</Initials><AffiliationInfo><Affiliation>Department of Radiology, First Affiliated Hospital of Wenzhou Medical University, Wenzhou, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Chen</LastName><ForeName>Jeon-Hor</ForeName><Initials>JH</Initials><AffiliationInfo><Affiliation>Department of Radiological Sciences, University of California, Irvine, California; Department of Radiology, E-Da Hospital and I-Shou University, Kaohsiung, Taiwan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wang</LastName><ForeName>Xiao</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>Department of Radiation Oncology, Rutgers-Cancer Institute of New Jersey, Robert Wood Johnson Medical School, New Brunswick, New Jersey.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kim</LastName><ForeName>Bomi</ForeName><Initials>B</Initials><AffiliationInfo><Affiliation>Department of Radiological Sciences, University of California, Irvine, California; Department of Breast Radiology, Ilsan Hospital, Goyang, South Korea.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Parajuli</LastName><ForeName>Ritesh</ForeName><Initials>R</Initials><AffiliationInfo><Affiliation>Department of Medicine, University of California, Irvine, United States.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Mehta</LastName><ForeName>Rita S</ForeName><Initials>RS</Initials><AffiliationInfo><Affiliation>Department of Medicine, University of California, Irvine, United States.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wang</LastName><ForeName>Meihao</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Radiology, First Affiliated Hospital of Wenzhou Medical University, Wenzhou, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Su</LastName><ForeName>Min-Ying</ForeName><Initials>MY</Initials><AffiliationInfo><Affiliation>Department of Radiological Sciences, University of California, Irvine, California; Department of Medical Imaging and Radiological Sciences, Kaohsiung Medical University, Kaohsiung, Taiwan. Electronic address: msu@uci.edu.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>09</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>Acad Radiol</MedlineTA><NlmUniqueID>9440159</NlmUniqueID><ISSNLinking>1076-6332</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Breast MRI</Keyword><Keyword MajorTopicYN="N">Computer-Aided Diagnosis (CAD)</Keyword><Keyword MajorTopicYN="N">Deep Learning</Keyword><Keyword MajorTopicYN="N">Mask Reginal-Convolutional Neural Network (R-CNN)</Keyword><Keyword MajorTopicYN="N">ResNet50</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>10</Month><Day>3</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>10</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>23</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>22</Hour><Minute>1</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36631349</ArticleId><ArticleId IdType="doi">10.1016/j.acra.2022.12.038</ArticleId><ArticleId IdType="pii">S1076-6332(22)00695-X</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36629989</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>11</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1618-727X</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>11</Day></PubDate></JournalIssue><Title>Journal of digital imaging</Title><ISOAbbreviation>J Digit Imaging</ISOAbbreviation></Journal><ArticleTitle>Using Deep Learning to Predict Treatment Response in Patients with Hepatocellular Carcinoma Treated with Y90 Radiation Segmentectomy.</ArticleTitle><ELocationID EIdType="doi" ValidYN="Y">10.1007/s10278-022-00762-0</ELocationID><Abstract><AbstractText>Treatment of hepatocellular carcinoma (HCC) with Y90 radioembolization segmentectomy (Y90-RE) demonstrates a tumor dose-response threshold, where dose estimates are highly dependent on accurate SPECT/CT acquisition, registration, and reconstruction. Any error can result in distorted absorbed dose distributions and inaccurate estimates of treatment success. This study improves upon the voxel-based dosimetry model, one of&#xa0;the most accurate methods available clinically, by using a deep convolutional network ensemble to account for the spatially variable uptake of Y90 within a treated lesion. A retrospective analysis was conducted in patients with HCC who received Y90-RE at a single institution. Seventy-seven patients with 103 lesions met the inclusion criteria: three or fewer tumors, pre- and post treatment MRI, and no prior Y90-RE. Lesions were labeled as complete (n&#x2009;=&#x2009;57) or incomplete response (n&#x2009;=&#x2009;46) based on 3-month post treatment MRI and divided by medical record number into a 20% hold-out test set and 80% training set with 5-fold cross-validation. Slice-wise predictions were made from an average ensemble of models and thresholds from the highest accuracy epochs across all five folds. Lesion predictions were made by thresholding all slice predictions through the lesion. When compared to the voxel-based dosimetry model, our model had a higher F1-score (0.72 vs. 0.2), higher accuracy (0.65 vs. 0.60), and higher sensitivity (1.0 vs. 0.11) at predicting complete treatment response. This algorithm has the potential to identify patients with treatment failure who may benefit from earlier follow-up or additional treatment.</AbstractText><CopyrightInformation>&#xa9; 2022. The Author(s) under exclusive licence to Society for Imaging Informatics in Medicine.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Wagstaff</LastName><ForeName>William V</ForeName><Initials>WV</Initials><Identifier Source="ORCID">0000-0002-7071-2973</Identifier><AffiliationInfo><Affiliation>Department of Radiology and Imaging Sciences, Emory University School of Medicine, Atlanta, GA, USA. william.wagstaff@emory.edu.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Villalobos</LastName><ForeName>Alexander</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Division of Interventional Radiology and Image-Guided Medicine, Department of Radiology and Imaging Sciences, Emory University School of Medicine, Atlanta, GA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Gichoya</LastName><ForeName>Judy</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Division of Interventional Radiology and Image-Guided Medicine, Department of Radiology and Imaging Sciences, Emory University School of Medicine, Atlanta, GA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kokabi</LastName><ForeName>Nima</ForeName><Initials>N</Initials><AffiliationInfo><Affiliation>Division of Interventional Radiology and Image-Guided Medicine, Department of Radiology and Imaging Sciences, Emory University School of Medicine, Atlanta, GA, USA.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>11</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>J Digit Imaging</MedlineTA><NlmUniqueID>9100529</NlmUniqueID><ISSNLinking>0897-1889</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Dosimetry</Keyword><Keyword MajorTopicYN="N">Hepatocellular carcinoma</Keyword><Keyword MajorTopicYN="N">Interventional radiology</Keyword><Keyword MajorTopicYN="N">Voxel-based dosimetry</Keyword><Keyword MajorTopicYN="N">Y90 radioembolization</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2021</Year><Month>12</Month><Day>2</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>15</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>6</Month><Day>27</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>11</Hour><Minute>18</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36629989</ArticleId><ArticleId IdType="doi">10.1007/s10278-022-00762-0</ArticleId><ArticleId IdType="pii">10.1007/s10278-022-00762-0</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Golabi P, Fazel S, Otgonsuren M, Sayiner M, Locklear CT, Younossi ZM. Mortality assessment of patients with hepatocellular carcinoma according to underlying disease and treatment modalities. Medicine. 2017;96(9):e5904.&#xa0; https://doi.org/10.1097/MD.0000000000005904</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/MD.0000000000005904</ArticleId></ArticleIdList></Reference><Reference><Citation>Padia SA, Lewandowski RJ, Johnson GE, et al. Radioembolization of hepatic malignancies: background, quality improvement guidelines, and future directions. J Vasc Interv Radiol. 2017;28(1):1-15.&#xa0; https://doi.org/10.1016/j.jvir.2016.09.024</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jvir.2016.09.024</ArticleId></ArticleIdList></Reference><Reference><Citation>Salem R, Lewandowski RJ, Mulcahy MF, et al. Radioembolization for hepatocellular carcinoma using Yttrium-90 microspheres: a comprehensive report of long-term outcomes. Gastroenterology. 2010;138(1):52-64. https://doi.org/10.1053/j.gastro.2009.09.006</Citation><ArticleIdList><ArticleId IdType="doi">10.1053/j.gastro.2009.09.006</ArticleId></ArticleIdList></Reference><Reference><Citation>Kim SP, Cohalan C, Kopek N, Enger SA. A guide to 90Y radioembolization and its dosimetry. Phys Med. 2019;68:132-145. https://doi.org/10.1016/j.ejmp.2019.09.236</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejmp.2019.09.236</ArticleId></ArticleIdList></Reference><Reference><Citation>Kunnen B, van der Velden S, Bastiaannet R, Lam MGEH, Viergever MA, de Jong HWAM. Radioembolization lung shunt estimation based on a 90 Y pretreatment procedure: A phantom study. Med Phys. 2018;45(10):4744-4753. https://doi.org/10.1002/mp.13168</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mp.13168</ArticleId></ArticleIdList></Reference><Reference><Citation>Chiesa C, Mira M, Maccauro M, et al. Radioembolization of hepatocarcinoma with (90)Y glass microspheres: development of an individualized treatment planning strategy based on dosimetry and radiobiology. Eur J Nucl Med Mol Imaging. 2015;42(11):1718-1738. https://doi.org/10.1007/s00259-015-3068-8</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00259-015-3068-8</ArticleId></ArticleIdList></Reference><Reference><Citation>Malhotra A, Liu DM, Talenfeld AD. Radiation segmentectomy and radiation lobectomy: A practical review of techniques. Tech Vasc Interv Radiol. 2019;22(2):49-57. https://doi.org/10.1053/j.tvir.2019.02.003</Citation><ArticleIdList><ArticleId IdType="doi">10.1053/j.tvir.2019.02.003</ArticleId></ArticleIdList></Reference><Reference><Citation>Villalobos A, Cheng B, Wagstaff W, et al. Tumor-to-Normal Ratio Relationship between Planning Technetium-99 Macroaggregated Albumin and Posttherapy Yttrium-90 Bremsstrahlung SPECT/CT. J Vasc Interv Radiol. February 2021. https://doi.org/10.1016/j.jvir.2020.12.023</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jvir.2020.12.023</ArticleId></ArticleIdList></Reference><Reference><Citation>Chansanti O, Jahangiri Y, Matsui Y, et al. Tumor Dose Response in Yttrium-90 Resin Microsphere Embolization for Neuroendocrine Liver Metastases: A Tumor-Specific Analysis with Dose Estimation Using SPECT-CT. J Vasc Interv Radiol. 2017;28(11):1528-1535. https://doi.org/10.1016/j.jvir.2017.07.008</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jvir.2017.07.008</ArticleId></ArticleIdList></Reference><Reference><Citation>Eaton BR, Kim HS, Schreibmann E, et al. Quantitative dosimetry for yttrium-90 radionuclide therapy: tumor dose predicts fluorodeoxyglucose positron emission tomography response in hepatic metastatic melanoma. J Vasc Interv Radiol. 2014;25(2):288-295. https://doi.org/10.1016/j.jvir.2013.08.021</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jvir.2013.08.021</ArticleId></ArticleIdList></Reference><Reference><Citation>Kao Y-H, Steinberg JD, Tay Y-S, et al. Post-radioembolization yttrium-90 PET/CT - part 2: dose-response and tumor predictive dosimetry for resin microspheres. EJNMMI Res. 2013;3(1):57. https://doi.org/10.1186/2191-219X-3-57</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/2191-219X-3-57</ArticleId></ArticleIdList></Reference><Reference><Citation>Thamboo TP, Tan K-B, Wang S-C, Salto-Tellez M. Extra-hepatic embolisation of Y-90 microspheres from selective internal radiation therapy (SIRT) of the liver. Pathology. 2003;35(4):351-353. https://doi.org/10.1080/0031302031000152892</Citation><ArticleIdList><ArticleId IdType="doi">10.1080/0031302031000152892</ArticleId></ArticleIdList></Reference><Reference><Citation>Xiao Y, Roncali E, Hobbs R, et al. Toward Individualized Voxel-Level Dosimetry for Radiopharmaceutical Therapy. Int J Radiat Oncol Biol Phys. 2021;109(4):902-904. https://doi.org/10.1016/j.ijrobp.2020.08.026</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ijrobp.2020.08.026</ArticleId></ArticleIdList></Reference><Reference><Citation>Kafrouni M, Allimant C, Fourcade M, et al. Retrospective Voxel-Based Dosimetry for Assessing the Ability of the Body-Surface-Area Model to Predict Delivered Dose and Radioembolization Outcome. J Nucl Med. 2018;59(8):1289-1295. https://doi.org/10.2967/jnumed.117.202937</Citation><ArticleIdList><ArticleId IdType="doi">10.2967/jnumed.117.202937</ArticleId></ArticleIdList></Reference><Reference><Citation>Cheng B, Villalobos A, Sethi I, et al. Determination of Tumor Dose Response Thresholds in Patients with Chemorefractory Intrahepatic Cholangiocarcinoma Treated with Resin and Glass-based Y90 Radioembolization. Cardiovasc Intervent Radiol. April 2021. https://doi.org/10.1007/s00270-021-02834-0</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00270-021-02834-0</ArticleId></ArticleIdList></Reference><Reference><Citation>Garin E, Tselikas L, Guiu B, et al. Personalised versus standard dosimetry approach of selective internal radiation therapy in patients with locally advanced hepatocellular carcinoma (DOSISPHERE-01): a randomised, multicentre, open-label phase 2 trial. Lancet Gastroenterol Hepatol. 2021;6(1):17-29. https://doi.org/10.1016/S2468-1253(20)30290-9</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S2468-1253(20)30290-9</ArticleId></ArticleIdList></Reference><Reference><Citation>Tran-Gia J, Salas-Ramirez M, Lassmann M. What You See Is Not What You Get: On the Accuracy of Voxel-Based Dosimetry in Molecular Radiotherapy. J Nucl Med. 2020;61(8):1178-1186. https://doi.org/10.2967/jnumed.119.231480</Citation><ArticleIdList><ArticleId IdType="doi">10.2967/jnumed.119.231480</ArticleId></ArticleIdList></Reference><Reference><Citation>Kathiravelu P, Sharma A, Sharma P. Understanding Scanner Utilization With Real-Time DICOM Metadata Extraction. IEEE Access. 2021;9:10621-10633. https://doi.org/10.1109/ACCESS.2021.3050467</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2021.3050467</ArticleId></ArticleIdList></Reference><Reference><Citation>Kathiravelu P, Sharma P, Sharma A, et al. A DICOM Framework for Machine Learning and Processing Pipelines Against Real-time Radiology Images. J Digit Imaging. 2021;34(4):1005-1013. https://doi.org/10.1007/s10278-021-00491-w</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10278-021-00491-w</ArticleId></ArticleIdList></Reference><Reference><Citation>Braat AJAT, Smits MLJ, Braat MNGJA, et al. <sup>90</sup>Y hepatic radioembolization: an update on current practice and recent developments. J Nucl Med. 2015;56(7):1079&#x2013;1087. https://doi.org/10.2967/jnumed.115.157446</Citation></Reference><Reference><Citation>Lencioni R, Montal R, Torres F, et al. Objective response by mRECIST as a predictor and potential surrogate end-point of overall survival in advanced HCC. J Hepatol. 2017;66(6):1166-1172. https://doi.org/10.1016/j.jhep.2017.01.012</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jhep.2017.01.012</ArticleId></ArticleIdList></Reference><Reference><Citation>Llovet JM, Lencioni R. mRECIST for HCC: Performance and novel refinements. J Hepatol. 2020;72(2):288-306. https://doi.org/10.1016/j.jhep.2019.09.026</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jhep.2019.09.026</ArticleId></ArticleIdList></Reference><Reference><Citation>Yushkevich PA, Pluta J, Wang H, Wisse LEM, Das S, Wolk D. IC-P-174: Fast Automatic Segmentation of Hippocampal Subfields and Medial Temporal Lobe Subregions In 3 Tesla and 7 Tesla T2-Weighted MRI. Alzheimers Dement. 2016;12:P126-P127. https://doi.org/10.1016/j.jalz.2016.06.205</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jalz.2016.06.205</ArticleId></ArticleIdList></Reference><Reference><Citation>Yushkevich PA, Piven J, Hazlett HC, et al. User-guided 3D active contour segmentation of anatomical structures: significantly improved efficiency and reliability. Neuroimage. 2006;31(3):1116-1128. https://doi.org/10.1016/j.neuroimage.2006.01.015</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2006.01.015</ArticleId></ArticleIdList></Reference><Reference><Citation>Li X, Wang W, Hu X, Yang J. Selective Kernel Networks. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE; 2019:510&#x2013;519. https://doi.org/10.1109/CVPR.2019.00060</Citation></Reference><Reference><Citation>Villalobos A, Soliman MM, Majdalany BS, et al. Yttrium-90 Radioembolization Dosimetry: What Trainees Need to Know. Semin Intervent Radiol. 2020;37(5):543-554. https://doi.org/10.1055/s-0040-1720954</Citation><ArticleIdList><ArticleId IdType="doi">10.1055/s-0040-1720954</ArticleId></ArticleIdList></Reference><Reference><Citation>Gabr A, Riaz A, Johnson GE, et al. Correlation of Y90-absorbed radiation dose to pathological necrosis in hepatocellular carcinoma: confirmatory multicenter analysis in 45 explants. Eur J Nucl Med Mol Imaging. 2021;48(2):580-583. https://doi.org/10.1007/s00259-020-04976-8</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00259-020-04976-8</ArticleId></ArticleIdList></Reference><Reference><Citation>Mikell JK, Mahvash A, Siman W, Baladandayuthapani V, Mourtada F, Kappadath SC. Selective Internal Radiation Therapy With Yttrium-90 Glass Microspheres: Biases and Uncertainties in Absorbed Dose Calculations Between Clinical Dosimetry Models. Int J Radiat Oncol Biol Phys. 2016;96(4):888-896. https://doi.org/10.1016/j.ijrobp.2016.07.021</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ijrobp.2016.07.021</ArticleId></ArticleIdList></Reference><Reference><Citation>Sarwar A, Kudla A, Weinstein JL, et al. Yttrium-90 radioembolization using MIRD dosimetry with resin microspheres. Eur Radiol. 2021;31(3):1316-1324. https://doi.org/10.1007/s00330-020-07231-8</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-020-07231-8</ArticleId></ArticleIdList></Reference><Reference><Citation>Stephens RW, Tredwell GD, Knox KJ, et al. 99mTc-radiolabeled composites enabling in vivo imaging of arterial dispersal and retention of microspheres in the vascular network of rabbit lungs, liver, and liver tumors. Int J Nanomedicine. 2019;14:889-900. https://doi.org/10.2147/IJN.S187153</Citation><ArticleIdList><ArticleId IdType="doi">10.2147/IJN.S187153</ArticleId></ArticleIdList></Reference><Reference><Citation>Kennedy AS, Nutting C, Coldwell D, Gaiser J, Drachenberg C. Pathologic response and microdosimetry of (90)Y microspheres in man: review of four explanted whole livers. Int J Radiat Oncol Biol Phys. 2004;60(5):1552-1563. https://doi.org/10.1016/j.ijrobp.2004.09.004</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ijrobp.2004.09.004</ArticleId></ArticleIdList></Reference><Reference><Citation>Chiesa C, Bardi&#xe8;s M, Zaidi H. Voxel-based dosimetry is superior to mean absorbed dose approach for establishing dose-effect relationship in targeted radionuclide therapy. Med Phys. 2019;46(12):5403-5406. https://doi.org/10.1002/mp.13851</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mp.13851</ArticleId></ArticleIdList></Reference><Reference><Citation>Kao YH, Hock Tan AE, Burgmans MC, et al. Image-guided personalized predictive dosimetry by artery-specific SPECT/CT partition modeling for safe and effective 90Y radioembolization. J Nucl Med. 2012;53(4):559-566. https://doi.org/10.2967/jnumed.111.097469</Citation><ArticleIdList><ArticleId IdType="doi">10.2967/jnumed.111.097469</ArticleId></ArticleIdList></Reference><Reference><Citation>Garin E, Rolland Y, Laffont S, Edeline J. Clinical impact of (99m)Tc-MAA SPECT/CT-based dosimetry in the radioembolization of liver malignancies with (90)Y-loaded microspheres. Eur J Nucl Med Mol Imaging. 2016;43(3):559-575. https://doi.org/10.1007/s00259-015-3157-8</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00259-015-3157-8</ArticleId></ArticleIdList></Reference><Reference><Citation>Kao YH, Tan EH, Ng CE, Goh SW. Yttrium-90 time-of-flight PET/CT is superior to Bremsstrahlung SPECT/CT for postradioembolization imaging of microsphere biodistribution. Clin Nucl Med. 2011;36(12):e186-7. https://doi.org/10.1097/RLU.0b013e31821c9a11</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/RLU.0b013e31821c9a11</ArticleId></ArticleIdList></Reference><Reference><Citation>Tafti BA, Padia SA. Dosimetry of Y-90 Microspheres Utilizing Tc-99m SPECT and Y-90 PET. Semin Nucl Med. 2019;49(3):211-217. https://doi.org/10.1053/j.semnuclmed.2019.01.005</Citation><ArticleIdList><ArticleId IdType="doi">10.1053/j.semnuclmed.2019.01.005</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36629980</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>13</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Print">1869-4101</ISSN><JournalIssue CitedMedium="Print"><Volume>14</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>11</Day></PubDate></JournalIssue><Title>Insights into imaging</Title><ISOAbbreviation>Insights Imaging</ISOAbbreviation></Journal><ArticleTitle>Deep learning and radiomic feature-based blending ensemble classifier for malignancy risk prediction in cystic renal lesions.</ArticleTitle><Pagination><StartPage>6</StartPage><MedlinePgn>6</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">6</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1186/s13244-022-01349-7</ELocationID><Abstract><AbstractText Label="BACKGROUND" NlmCategory="BACKGROUND">The rising prevalence of cystic renal lesions (CRLs) detected by computed tomography necessitates better identification of the malignant cystic renal neoplasms since a significant majority of CRLs are benign renal cysts. Using arterial phase CT scans combined with pathology diagnosis results, a fusion feature-based blending ensemble machine learning model was created to identify malignant renal neoplasms from cystic renal lesions (CRLs). Histopathology results were adopted as diagnosis standard. Pretrained 3D-ResNet50 network was selected for non-handcrafted features extraction and pyradiomics toolbox was selected for handcrafted features extraction. Tenfold cross validated least absolute shrinkage and selection operator regression methods were selected to identify the most discriminative candidate features in the development cohort. Feature's reproducibility was evaluated by intra-class correlation coefficients and inter-class correlation coefficients. Pearson correlation coefficients for normal distribution and Spearman's rank correlation coefficients for non-normal distribution were utilized to remove redundant features. After that, a blending ensemble machine learning model were developed in training cohort. Area under the receiver operator characteristic curve (AUC), accuracy score (ACC), and decision curve analysis (DCA) were employed to evaluate the performance of the final model in testing cohort.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">The fusion feature-based machine learning algorithm demonstrated excellent diagnostic performance in external validation dataset (AUC&#x2009;=&#x2009;0.934, ACC&#x2009;=&#x2009;0.905). Net benefits presented by DCA are higher than Bosniak-2019 version classification for stratifying patients with CRL to the appropriate surgery procedure.</AbstractText><AbstractText Label="CONCLUSIONS" NlmCategory="CONCLUSIONS">Fusion feature-based classifier accurately distinguished malignant and benign CRLs which outperformed the Bosniak-2019 version classification and illustrated improved clinical decision-making utility.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y" EqualContrib="Y"><LastName>He</LastName><ForeName>Quan-Hao</ForeName><Initials>QH</Initials><AffiliationInfo><Affiliation>Department of Urology, The First Affiliated Hospital of Chongqing Medical University, Chongqing, 400016, People's Republic of China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y" EqualContrib="Y"><LastName>Feng</LastName><ForeName>Jia-Jun</ForeName><Initials>JJ</Initials><AffiliationInfo><Affiliation>Department of Medical Imaging, Guangzhou First People's Hospital, School of Medicine, South China University of Technology, Guangzhou, 51000, People's Republic of China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lv</LastName><ForeName>Fa-Jin</ForeName><Initials>FJ</Initials><AffiliationInfo><Affiliation>Department of Radiology, The First Affiliated Hospital of Chongqing Medical University, Chongqing, 400016, People's Republic of China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Jiang</LastName><ForeName>Qing</ForeName><Initials>Q</Initials><AffiliationInfo><Affiliation>Department of Urology, The Second Affiliated Hospital of Chongqing Medical University, Chongqing, 400010, People's Republic of China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Xiao</LastName><ForeName>Ming-Zhao</ForeName><Initials>MZ</Initials><AffiliationInfo><Affiliation>Department of Urology, The First Affiliated Hospital of Chongqing Medical University, Chongqing, 400016, People's Republic of China. xmz.2004@163.com.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>11</Day></ArticleDate></Article><MedlineJournalInfo><Country>Germany</Country><MedlineTA>Insights Imaging</MedlineTA><NlmUniqueID>101532453</NlmUniqueID><ISSNLinking>1869-4101</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Bosniak-2019 classification</Keyword><Keyword MajorTopicYN="N">Cystic renal lesions</Keyword><Keyword MajorTopicYN="N">Machine learning</Keyword><Keyword MajorTopicYN="N">Radiomics</Keyword></KeywordList><CoiStatement>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>9</Month><Day>27</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>4</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>11</Hour><Minute>18</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36629980</ArticleId><ArticleId IdType="doi">10.1186/s13244-022-01349-7</ArticleId><ArticleId IdType="pii">10.1186/s13244-022-01349-7</ArticleId><ArticleId IdType="pmc">PMC9834471</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Moch H, Cubilla AL, Humphrey PA, Reuter VE, Ulbright TM. The 2016 WHO classification of tumours of the urinary system and male genital organs-part A: renal, penile, and testicular tumours. Eur Urol. 2016;70:93&#x2013;105. doi: 10.1016/j.eururo.2016.02.029.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.eururo.2016.02.029</ArticleId><ArticleId IdType="pubmed">26935559</ArticleId></ArticleIdList></Reference><Reference><Citation>Hu EM, Zhang A, Silverman SG, Pedrosa I, Wang ZJ, Smith AD, Chandarana H, Doshi A, Shinagare AB, Remer EM. Multi-institutional analysis of CT and MRI reports evaluating indeterminate renal masses: comparison to a national survey investigating desired report elements. Abdom Radiol (NY) 2018;43:3493&#x2013;3502. doi: 10.1007/s00261-018-1609-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00261-018-1609-x</ArticleId><ArticleId IdType="pubmed">29666953</ArticleId></ArticleIdList></Reference><Reference><Citation>Smith AD, Allen BC, Sanyal R, Carson JD, Zhang H, Williams JH, Collins C, Griswold M, Zhang X. Outcomes and complications related to the management of Bosniak cystic renal lesions. AJR Am J Roentgenol. 2015;204:W550&#x2013;556. doi: 10.2214/AJR.14.13149.</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/AJR.14.13149</ArticleId><ArticleId IdType="pubmed">25905961</ArticleId></ArticleIdList></Reference><Reference><Citation>Agnello F, Albano D, Micci G, Di Buono G, Agrusa A, Salvaggio G, Pardo S, Sparacia G, Bartolotta TV, Midiri M, Lagalla R, Galia M. CT and MR imaging of cystic renal lesions. Insights Imaging. 2020;11:5. doi: 10.1186/s13244-019-0826-3.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s13244-019-0826-3</ArticleId><ArticleId IdType="pmc">PMC6942066</ArticleId><ArticleId IdType="pubmed">31900669</ArticleId></ArticleIdList></Reference><Reference><Citation>Yang B, Qiu C, Wan S, Liu J, Li Q, Mai Z, Zeng T, Liu Y, He W, Zeng G. Long-term follow-up study of the malignant transformation potential of the simple renal cysts. Transl Androl Urol. 2020;9:684&#x2013;689. doi: 10.21037/tau.2020.03.29.</Citation><ArticleIdList><ArticleId IdType="doi">10.21037/tau.2020.03.29</ArticleId><ArticleId IdType="pmc">PMC7215042</ArticleId><ArticleId IdType="pubmed">32420175</ArticleId></ArticleIdList></Reference><Reference><Citation>Soputro NA, Kapoor J, Zargar H, Dias BH. Malignant ascites following radical nephrectomy for cystic renal cell carcinoma. BMJ Case Rep. 2021;14:e243103. doi: 10.1136/bcr-2021-243103.</Citation><ArticleIdList><ArticleId IdType="doi">10.1136/bcr-2021-243103</ArticleId><ArticleId IdType="pmc">PMC8278893</ArticleId><ArticleId IdType="pubmed">34257120</ArticleId></ArticleIdList></Reference><Reference><Citation>Park MY, Park KJ, Kim MH, Kim JK. Bosniak classification of cystic renal masses version 2019: comparison with version 2005 for class distribution, diagnostic performance, and interreader agreement using CT and MRI. AJR Am J Roentgenol. 2021;217:1367&#x2013;1376. doi: 10.2214/AJR.21.25796.</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/AJR.21.25796</ArticleId><ArticleId IdType="pubmed">34076460</ArticleId></ArticleIdList></Reference><Reference><Citation>Shampain KL, Shankar PR, Troost JP, Galantowicz ML, Pampati RA, Schoenheit TR, Shlensky DA, Barkmeier D, Curci NE, Kaza RK, Khalatbari S, Davenport MS. Interrater agreement of Bosniak classification version 2019 and version 2005 for cystic renal masses at CT and MRI. Radiology. 2022;302:357&#x2013;366. doi: 10.1148/radiol.2021210853.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2021210853</ArticleId><ArticleId IdType="pmc">PMC8805658</ArticleId><ArticleId IdType="pubmed">34726535</ArticleId></ArticleIdList></Reference><Reference><Citation>Dana J, Gauvin S, Zhang M, Lotero J, Cassim C, Artho G, Bhatnagar SR, Tanguay S, Reinhold C. CT-based Bosniak classification of cystic renal lesions: is version 2019 an improvement on version 2005? Eur Radiol. 2022;23:1&#x2013;10.</Citation><ArticleIdList><ArticleId IdType="pubmed">36048207</ArticleId></ArticleIdList></Reference><Reference><Citation>Schoots IG, Zaccai K, Hunink MG, Verhagen P. Bosniak classification for complex renal cysts reevaluated: a systematic review. J Urol. 2017;198:12&#x2013;21. doi: 10.1016/j.juro.2016.09.160.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.juro.2016.09.160</ArticleId><ArticleId IdType="pubmed">28286071</ArticleId></ArticleIdList></Reference><Reference><Citation>Yan JH, Chan J, Osman H, Munir J, Alrasheed S, Flood TA, Schieda N. Bosniak Classification version 2019: validation and comparison to original classification in pathologically confirmed cystic masses. Eur Radiol. 2021;31:9579&#x2013;9587. doi: 10.1007/s00330-021-08006-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-021-08006-5</ArticleId><ArticleId IdType="pubmed">34019130</ArticleId></ArticleIdList></Reference><Reference><Citation>Spiesecke P, Reinhold T, Wehrenberg Y, Werner S, Maxeiner A, Busch J, Fischer T, Hamm B, Lerchbaumer MH. Cost-effectiveness analysis of multiple imaging modalities in diagnosis and follow-up of intermediate complex cystic renal lesions. BJU Int. 2021;128:575&#x2013;585. doi: 10.1111/bju.15353.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/bju.15353</ArticleId><ArticleId IdType="pubmed">33528886</ArticleId></ArticleIdList></Reference><Reference><Citation>Corrias G, Micheletti G, Barberini L, Suri JS, Saba L. Texture analysis imaging &#x201c;what a clinical radiologist needs to know&#x201d;. Eur J Radiol. 2022;146:110055. doi: 10.1016/j.ejrad.2021.110055.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejrad.2021.110055</ArticleId><ArticleId IdType="pubmed">34902669</ArticleId></ArticleIdList></Reference><Reference><Citation>Xv Y, Lv F, Guo H, Zhou X, Tan H, Xiao M, Zheng Y. Machine learning-based CT radiomics approach for predicting WHO/ISUP nuclear grade of clear cell renal cell carcinoma: an exploratory and comparative study. Insights Imaging. 2021;12:170. doi: 10.1186/s13244-021-01107-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s13244-021-01107-1</ArticleId><ArticleId IdType="pmc">PMC8605949</ArticleId><ArticleId IdType="pubmed">34800179</ArticleId></ArticleIdList></Reference><Reference><Citation>Hu Y, Xie C, Yang H, Ho JWK, Wen J, Han L, Lam KO, Wong IYH, Law SYK, Chiu KWH, Vardhanabhuti V, Fu J. Computed tomography-based deep-learning prediction of neoadjuvant chemoradiotherapy treatment response in esophageal squamous cell carcinoma. Radiother Oncol J Eur Soc Ther Radiol Oncol. 2021;154:6&#x2013;13. doi: 10.1016/j.radonc.2020.09.014.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.radonc.2020.09.014</ArticleId><ArticleId IdType="pubmed">32941954</ArticleId></ArticleIdList></Reference><Reference><Citation>Orlhac F, Frouin F, Nioche C, Ayache N, Buvat I. Validation of a method to compensate multicenter effects affecting CT radiomics. Radiology. 2019;291:53&#x2013;59. doi: 10.1148/radiol.2019182023.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2019182023</ArticleId><ArticleId IdType="pubmed">30694160</ArticleId></ArticleIdList></Reference><Reference><Citation>Ligero M, Jordi-Ollero O, Bernatowicz K, Garcia-Ruiz A, Delgado-Mu&#xf1;oz E, Leiva D, Mast R, Suarez C, Sala-Llonch R, Calvo N, Escobar M, Navarro-Martin A, Villacampa G, Dienstmann R, Perez-Lopez R. Minimizing acquisition-related radiomics variability by image resampling and batch effect correction to allow for large-scale data analysis. Eur Radiol. 2021;31:1460&#x2013;1470. doi: 10.1007/s00330-020-07174-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-020-07174-0</ArticleId><ArticleId IdType="pmc">PMC7880962</ArticleId><ArticleId IdType="pubmed">32909055</ArticleId></ArticleIdList></Reference><Reference><Citation>Pleil JD, Wallace MAG, Stiegel MA, Funk WE. Human biomarker interpretation: the importance of intra-class correlation coefficients (ICC) and their calculations based on mixed models, ANOVA, and variance estimates. J Toxicol Environ Health B Crit Rev. 2018;21:161&#x2013;180. doi: 10.1080/10937404.2018.1490128.</Citation><ArticleIdList><ArticleId IdType="doi">10.1080/10937404.2018.1490128</ArticleId><ArticleId IdType="pmc">PMC6704467</ArticleId><ArticleId IdType="pubmed">30067478</ArticleId></ArticleIdList></Reference><Reference><Citation>Zwanenburg A, Valli&#xe8;res M, Abdalah MA, Aerts H, Andrearczyk V, Apte A, Ashrafinia S, Bakas S, Beukinga RJ, Boellaard R, Bogowicz M, Boldrini L, Buvat I, Cook GJR, Davatzikos C, Depeursinge A, Desseroit MC, Dinapoli N, Dinh CV, Echegaray S, El Naqa I, Fedorov AY, Gatta R, Gillies RJ, Goh V, G&#xf6;tz M, Guckenberger M, Ha SM, Hatt M, Isensee F, Lambin P, Leger S, Leijenaar RTH, Lenkowicz J, Lippert F, Losneg&#xe5;rd A, Maier-Hein KH, Morin O, M&#xfc;ller H, Napel S, Nioche C, Orlhac F, Pati S, Pfaehler EAG, Rahmim A, Rao AUK, Scherer J, Siddique MM, Sijtsema NM, Socarras Fernandez J, Spezi E, Steenbakkers R, Tanadini-Lang S, Thorwarth D, Troost EGC, Upadhaya T, Valentini V, van Dijk LV, van Griethuysen J, van Velden FHP, Whybra P, Richter C, L&#xf6;ck S. The image biomarker standardization initiative: standardized quantitative radiomics for high-throughput image-based phenotyping. Radiology. 2020;295:328&#x2013;338. doi: 10.1148/radiol.2020191145.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2020191145</ArticleId><ArticleId IdType="pmc">PMC7193906</ArticleId><ArticleId IdType="pubmed">32154773</ArticleId></ArticleIdList></Reference><Reference><Citation>Lambin P, Leijenaar RTH, Deist TM, Peerlings J, de Jong EEC, van Timmeren J, Sanduleanu S, Larue RTHM, Even AJG, Jochems A, van Wijk Y, Woodruff H, van Soest J, Lustberg T, Roelofs E, van Elmpt W, Dekker A, Mottaghy FM, Wildberger JE, Walsh S. Radiomics: the bridge between medical imaging and personalized medicine. Nat Rev Clin Oncol. 2017;14:749&#x2013;762. doi: 10.1038/nrclinonc.2017.141.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nrclinonc.2017.141</ArticleId><ArticleId IdType="pubmed">28975929</ArticleId></ArticleIdList></Reference><Reference><Citation>Perez-Ortiz M, Gutierrez PA, Tino P, Hervas-Martinez C. Oversampling the minority class in the feature space. IEEE Trans Neural Netw Learn Syst. 2016;27:1947&#x2013;1961. doi: 10.1109/TNNLS.2015.2461436.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TNNLS.2015.2461436</ArticleId><ArticleId IdType="pubmed">26316222</ArticleId></ArticleIdList></Reference><Reference><Citation>Vasquez MM, Hu C, Roe DJ, Chen Z, Halonen M, Guerra S. Least absolute shrinkage and selection operator type methods for the identification of serum biomarkers of overweight and obesity: simulation and application. BMC Med Res Methodol. 2016;16:154. doi: 10.1186/s12874-016-0254-8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s12874-016-0254-8</ArticleId><ArticleId IdType="pmc">PMC5109787</ArticleId><ArticleId IdType="pubmed">27842498</ArticleId></ArticleIdList></Reference><Reference><Citation>Campbell SC, Clark PE, Chang SS, Karam JA, Souter L, Uzzo RG. Renal mass and localized renal cancer: evaluation, management, and follow-up: AUA guideline: part I. J Urol. 2021;206:199&#x2013;208. doi: 10.1097/JU.0000000000001911.</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/JU.0000000000001911</ArticleId><ArticleId IdType="pubmed">34115547</ArticleId></ArticleIdList></Reference><Reference><Citation>Campbell SC, Uzzo RG, Karam JA, Chang SS, Clark PE, Souter L. Renal mass and localized renal cancer: evaluation, management, and follow-up: AUA guideline: part II. J Urol. 2021;206:209&#x2013;218. doi: 10.1097/JU.0000000000001912.</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/JU.0000000000001912</ArticleId><ArticleId IdType="pubmed">34115531</ArticleId></ArticleIdList></Reference><Reference><Citation>Boissier R, Ouzaid I, Nouhaud FX, Khene Z, Dariane C, Chkir S, Chelly S, Giwerc A, Allenet C, Lefrancq JB, Gimel P, Bodin T, Rioux-Leclercq N, Correas JM, Albiges L, Hetet JF, Bigot P, Bernhard JC, Long JA, Mejean A, Bensalah K. Long-term oncological outcomes of cystic renal cell carcinoma according to the Bosniak classification. Int Urol Nephrol. 2019;51:951&#x2013;958. doi: 10.1007/s11255-019-02085-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11255-019-02085-6</ArticleId><ArticleId IdType="pubmed">30977021</ArticleId></ArticleIdList></Reference><Reference><Citation>Huang Z, Wang H, Ji Z. Giant polycystic papillary renal cell carcinoma: a case report and literature review. Front Oncol. 2022;12:876217. doi: 10.3389/fonc.2022.876217.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fonc.2022.876217</ArticleId><ArticleId IdType="pmc">PMC9134105</ArticleId><ArticleId IdType="pubmed">35646650</ArticleId></ArticleIdList></Reference><Reference><Citation>Xv Y, Lv F, Guo H, Liu Z, Luo D, Liu J, Gou X, He W, Xiao M, Zheng Y. A CT-based radiomics nomogram integrated with clinic-radiological features for preoperatively predicting WHO/ISUP grade of clear cell renal cell carcinoma. Front Oncol. 2021;11:712554. doi: 10.3389/fonc.2021.712554.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fonc.2021.712554</ArticleId><ArticleId IdType="pmc">PMC8677659</ArticleId><ArticleId IdType="pubmed">34926241</ArticleId></ArticleIdList></Reference><Reference><Citation>Pacheco EO, Torres US, Alves AMA, Bekhor D, D&#x2019;Ippolito G. Bosniak classification of cystic renal masses version 2019 does not increase the interobserver agreement or the proportion of masses categorized into lower Bosniak classes for non-subspecialized readers on CT or MR. Eur J Radiol. 2020;131:109270. doi: 10.1016/j.ejrad.2020.109270.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejrad.2020.109270</ArticleId><ArticleId IdType="pubmed">32947091</ArticleId></ArticleIdList></Reference><Reference><Citation>McGrath TA, Bai X, Kamaya A et al (2022) Proportion of malignancy in Bosniak classification of cystic renal masses version 2019 (v2019) classes: systematic review and meta-analysis. Eur Radiol</Citation><ArticleIdList><ArticleId IdType="pubmed">35999371</ArticleId></ArticleIdList></Reference><Reference><Citation>Gillingham N, Chandarana H, Kamath A, Shaish H, Hindman N. Bosniak IIF and III renal cysts: can apparent diffusion coefficient-derived texture features discriminate between malignant and benign IIF and III cysts? J Comput Assist Tomogr. 2019;43:485&#x2013;492. doi: 10.1097/RCT.0000000000000851.</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/RCT.0000000000000851</ArticleId><ArticleId IdType="pubmed">30801565</ArticleId></ArticleIdList></Reference><Reference><Citation>Lee Y, Kim N, Cho KS, Kang SH, Kim DY, Jung YY, Kim JK. Bayesian classifier for predicting malignant renal cysts on MDCT: early clinical experience. AJR Am J Roentgenol. 2009;193:W106&#x2013;111. doi: 10.2214/AJR.08.1858.</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/AJR.08.1858</ArticleId><ArticleId IdType="pubmed">19620410</ArticleId></ArticleIdList></Reference><Reference><Citation>Miskin N, Qin L, Matalon SA, Tirumani SH, Alessandrino F, Silverman SG, Shinagare AB. Stratification of cystic renal masses into benign and potentially malignant: applying machine learning to the bosniak classification. Abdom Radiol (NY) 2021;46:311&#x2013;318. doi: 10.1007/s00261-020-02629-w.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00261-020-02629-w</ArticleId><ArticleId IdType="pubmed">32613401</ArticleId></ArticleIdList></Reference><Reference><Citation>Li Y, Dai C, Bian T, Zhou J, Xiang Z, He M, Huang J, Zhu Y, Hu X, Jiang S, Guo J, Wang H. Development and prospective validation of a novel weighted quantitative scoring system aimed at predicting the pathological features of cystic renal masses. Eur Radiol. 2019;29:1809&#x2013;1819. doi: 10.1007/s00330-018-5722-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-018-5722-6</ArticleId><ArticleId IdType="pubmed">30311030</ArticleId></ArticleIdList></Reference><Reference><Citation>Dana J, Lefebvre TL, Savadjiev P, Bodard S, Gauvin S, Bhatnagar SR, Forghani R, H&#xe9;l&#xe9;non O, Reinhold C. Malignancy risk stratification of cystic renal lesions based on a contrast-enhanced CT-based machine learning model and a clinical decision algorithm. Eur Radiol. 2022;32:4116&#x2013;4127. doi: 10.1007/s00330-021-08449-w.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-021-08449-w</ArticleId><ArticleId IdType="pubmed">35066631</ArticleId></ArticleIdList></Reference><Reference><Citation>Kim Y, Tao C, Kim H, Oh GY, Ko J, Bae KT. A deep learning approach for automated segmentation of kidneys and exophytic cysts in individuals with autosomal dominant polycystic kidney disease. J Am Soc Nephrol. 2022;33:1581&#x2013;1589. doi: 10.1681/ASN.2021111400.</Citation><ArticleIdList><ArticleId IdType="doi">10.1681/ASN.2021111400</ArticleId><ArticleId IdType="pmc">PMC9342631</ArticleId><ArticleId IdType="pubmed">35768178</ArticleId></ArticleIdList></Reference><Reference><Citation>Han K, Wang Y, Chen H et al (2022) A survey on vision transformer. IEEE Trans Pattern Anal Mach Intell</Citation><ArticleIdList><ArticleId IdType="pubmed">35180075</ArticleId></ArticleIdList></Reference><Reference><Citation>Gao R, Zhao S, Aishanjiang K, Cai H, Wei T, Zhang Y, Liu Z, Zhou J, Han B, Wang J, Ding H, Liu Y, Xu X, Yu Z, Gu J. Deep learning for differential diagnosis of malignant hepatic tumors based on multi-phase contrast-enhanced CT and clinical data. J Hematol Oncol. 2021;14:154. doi: 10.1186/s13045-021-01167-2.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s13045-021-01167-2</ArticleId><ArticleId IdType="pmc">PMC8474892</ArticleId><ArticleId IdType="pubmed">34565412</ArticleId></ArticleIdList></Reference><Reference><Citation>Ferreira AM, Reis RB, Kajiwara PP, Silva GEB, Elias J, Muglia VF. MRI evaluation of complex renal cysts using the Bosniak classification: a comparison to CT. Abdom Radiol (NY) 2016;41:2011&#x2013;2019. doi: 10.1007/s00261-016-0797-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00261-016-0797-5</ArticleId><ArticleId IdType="pubmed">27271286</ArticleId></ArticleIdList></Reference><Reference><Citation>Krishna S, Schieda N, Pedrosa I, Hindman N, Baroni RH, Silverman SG, Davenport MS. Update on MRI of cystic renal masses including Bosniak version 2019. J Magn Reson Imaging. 2021;54:341&#x2013;356. doi: 10.1002/jmri.27364.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmri.27364</ArticleId><ArticleId IdType="pmc">PMC8017011</ArticleId><ArticleId IdType="pubmed">33009722</ArticleId></ArticleIdList></Reference><Reference><Citation>Davenport MS, Hu EM, Smith AD, Chandarana H, Hafez K, Palapattu GS, Stuart Wolf J, Silverman SG. Reporting standards for the imaging-based diagnosis of renal masses on CT and MRI: a national survey of academic abdominal radiologists and urologists. Abdom Radiol (NY) 2017;42:1229&#x2013;1240. doi: 10.1007/s00261-016-0962-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00261-016-0962-x</ArticleId><ArticleId IdType="pubmed">27878338</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36628795</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>12</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>12</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1873-2860</ISSN><JournalIssue CitedMedium="Internet"><Volume>135</Volume><PubDate><Year>2023</Year><Month>Jan</Month></PubDate></JournalIssue><Title>Artificial intelligence in medicine</Title><ISOAbbreviation>Artif Intell Med</ISOAbbreviation></Journal><ArticleTitle>Clipped DeepControl: Deep neural network two-dimensional pulse design with an amplitude constraint layer.</ArticleTitle><Pagination><StartPage>102460</StartPage><MedlinePgn>102460</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1016/j.artmed.2022.102460</ELocationID><ELocationID EIdType="pii" ValidYN="Y">S0933-3657(22)00212-3</ELocationID><Abstract><AbstractText>Advanced radio-frequency pulse design used in magnetic resonance imaging has recently been demonstrated with deep learning of (convolutional) neural networks and reinforcement learning. For two-dimensionally selective radio-frequency pulses, the (convolutional) neural network pulse prediction time (a few milliseconds) was in comparison more than three orders of magnitude faster than the conventional optimal control computation. The network pulses were from the supervised training capable of compensating scan-subject dependent inhomogeneities of B<sub>0</sub> and B<sub>1</sub><sup>+</sup> fields. Unfortunately, the network presented with a small percentage of pulse amplitude overshoots in the test subset, despite the optimal control pulses used in training were fully constrained. Here, we have extended the convolutional neural network with a custom-made clipping layer that completely eliminates the risk of pulse amplitude overshoots, while preserving the ability to compensate for the inhomogeneous field conditions.</AbstractText><CopyrightInformation>Copyright &#xa9; 2022 The Author(s). Published by Elsevier B.V. All rights reserved.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Vinding</LastName><ForeName>Mads Sloth</ForeName><Initials>MS</Initials><AffiliationInfo><Affiliation>Center of Functionally Integrative Neuroscience (CFIN), Department of Clinical Medicine, Faculty of Health, Aarhus University, Denmark. Electronic address: msv@cfin.au.dk.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lund</LastName><ForeName>Torben Ellegaard</ForeName><Initials>TE</Initials><AffiliationInfo><Affiliation>Center of Functionally Integrative Neuroscience (CFIN), Department of Clinical Medicine, Faculty of Health, Aarhus University, Denmark.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>11</Month><Day>24</Day></ArticleDate></Article><MedlineJournalInfo><Country>Netherlands</Country><MedlineTA>Artif Intell Med</MedlineTA><NlmUniqueID>8915031</NlmUniqueID><ISSNLinking>0933-3657</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D019047" MajorTopicYN="N">Phantoms, Imaging</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D016571" MajorTopicYN="Y">Neural Networks, Computer</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D006339" MajorTopicYN="N">Heart Rate</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D008279" MajorTopicYN="Y">Magnetic Resonance Imaging</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D011846" MajorTopicYN="N">Radio Waves</DescriptorName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">2D RF</Keyword><Keyword MajorTopicYN="N">Clipping</Keyword><Keyword MajorTopicYN="N">DeepControl</Keyword><Keyword MajorTopicYN="N">MRI</Keyword><Keyword MajorTopicYN="N">Optimal control</Keyword></KeywordList><CoiStatement>Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>1</Month><Day>21</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>11</Month><Day>18</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>11</Month><Day>18</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>4</Hour><Minute>27</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>ppublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36628795</ArticleId><ArticleId IdType="doi">10.1016/j.artmed.2022.102460</ArticleId><ArticleId IdType="pii">S0933-3657(22)00212-3</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36628790</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>12</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>12</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1873-2860</ISSN><JournalIssue CitedMedium="Internet"><Volume>135</Volume><PubDate><Year>2023</Year><Month>Jan</Month></PubDate></JournalIssue><Title>Artificial intelligence in medicine</Title><ISOAbbreviation>Artif Intell Med</ISOAbbreviation></Journal><ArticleTitle>DeepGA for automatically estimating fetal gestational age through ultrasound imaging.</ArticleTitle><Pagination><StartPage>102453</StartPage><MedlinePgn>102453</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1016/j.artmed.2022.102453</ELocationID><ELocationID EIdType="pii" ValidYN="Y">S0933-3657(22)00205-6</ELocationID><Abstract><AbstractText>Accurate estimation of gestational age (GA) is vital for identifying fetal abnormalities. Conventionally, GA is estimated by measuring the morphology of the cranium, abdomen, and femur manually and inputting them into the classic Hadlock formula to assess fetal growth. However, this procedure incurs considerable overhead and suffers from bias caused by the operators, yielding suboptimal estimations. To address this challenge, we develop an automatic DeepGA model to achieve fully automatic GA prediction in an end-to-end manner. Our model uses a deep segmentation model (DeepSeg) to accurately identify and segment three critical tissues, including the cranium, abdomen, and femur, in which their morphology is automatically extracted. After that, we are able to directly estimate the GA via a deep regression model (DeepReg). We evaluate DeepGA on a large dataset, including 10,413 ultrasound images from 7113 subjects. It achieves superior performance over the traditional measurement approach, with a mean absolute estimation error (MAE) of 5&#xa0;days. Our DeepGA model is a novel automatic solution on the basis of artificial intelligence learning that can help radiologists improve the performance of GA estimation in various clinical scenarios, thereby enhancing the efficiency of prenatal examinations.</AbstractText><CopyrightInformation>Copyright &#xa9; 2022 Elsevier B.V. All rights reserved.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Dan</LastName><ForeName>Tingting</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>School of Computer Science and Engineering, South China University of Technology, Guangzhou, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Chen</LastName><ForeName>Xijie</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>School of Computer Science and Engineering, South China University of Technology, Guangzhou, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>He</LastName><ForeName>Miao</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>The First Affiliated Hospital of Sun Yat-sen University, Guangzhou, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Guo</LastName><ForeName>Hongmei</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>Dongguan City Maternal and Child Health Hospital, Dongguan, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>He</LastName><ForeName>Xiaoqin</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>Women and Children's Hospital, School of Medicine, Xiamen University, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Chen</LastName><ForeName>Jiazhou</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>School of Computer Science and Engineering, South China University of Technology, Guangzhou, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Xian</LastName><ForeName>Jianbo</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Guangzhou Aiyunji Information Technology Co., Ltd., Guangzhou, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Hu</LastName><ForeName>Yu</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>School of Computer Science and Engineering, South China University of Technology, Guangzhou, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Bin</ForeName><Initials>B</Initials><AffiliationInfo><Affiliation>School of Computer Science and Engineering, South China University of Technology, Guangzhou, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wang</LastName><ForeName>Nan</ForeName><Initials>N</Initials><AffiliationInfo><Affiliation>Guangzhou Aiyunji Information Technology Co., Ltd., Guangzhou, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Xie</LastName><ForeName>Hongning</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>The First Affiliated Hospital of Sun Yat-sen University, Guangzhou, China. Electronic address: xiehn@mail.sysu.edu.cn.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Cai</LastName><ForeName>Hongmin</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>School of Computer Science and Engineering, South China University of Technology, Guangzhou, China. Electronic address: hmcai@scut.edu.cn.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>11</Month><Day>17</Day></ArticleDate></Article><MedlineJournalInfo><Country>Netherlands</Country><MedlineTA>Artif Intell Med</MedlineTA><NlmUniqueID>8915031</NlmUniqueID><ISSNLinking>0933-3657</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D011247" MajorTopicYN="N">Pregnancy</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D005260" MajorTopicYN="N">Female</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D005865" MajorTopicYN="N">Gestational Age</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D001185" MajorTopicYN="Y">Artificial Intelligence</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D016216" MajorTopicYN="Y">Ultrasonography, Prenatal</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D006257" MajorTopicYN="N">Head</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D014463" MajorTopicYN="N">Ultrasonography</DescriptorName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Biometric measurement</Keyword><Keyword MajorTopicYN="N">Computer-aided diagnosis</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Fetal gestational age</Keyword><Keyword MajorTopicYN="N">Ultrasound</Keyword></KeywordList><CoiStatement>Conflict of interest We declare that we have no financial and personal relationships with other people or organizations that can inappropriately influence our work, there is no professional or other personal interest of any nature or kind in any product, service and/or company that could be construed as influencing the position presented in, or the review of, the manuscript entitled, &#x201c;DeepGA for automatically estimating fetal gestational age through ultrasound imaging&#x201d;.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2021</Year><Month>11</Month><Day>16</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>8</Month><Day>2</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>11</Month><Day>13</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>4</Hour><Minute>27</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>ppublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36628790</ArticleId><ArticleId IdType="doi">10.1016/j.artmed.2022.102453</ArticleId><ArticleId IdType="pii">S0933-3657(22)00205-6</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36628786</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>12</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>12</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1873-2860</ISSN><JournalIssue CitedMedium="Internet"><Volume>135</Volume><PubDate><Year>2023</Year><Month>Jan</Month></PubDate></JournalIssue><Title>Artificial intelligence in medicine</Title><ISOAbbreviation>Artif Intell Med</ISOAbbreviation></Journal><ArticleTitle>A hand motion capture method based on infrared thermography for measuring fine motor skills in biomedicine.</ArticleTitle><Pagination><StartPage>102474</StartPage><MedlinePgn>102474</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1016/j.artmed.2022.102474</ELocationID><ELocationID EIdType="pii" ValidYN="Y">S0933-3657(22)00226-3</ELocationID><Abstract><AbstractText>Many biomedical applications require fine motor skill assessments; however, real-time and contactless fine motor skill assessments are not typically implemented. In this study, we followed the 2D-to-3D pipeline principle and proposed a transformer-based spatial-temporal network to accurately regress 3D hand joint locations by inputting infrared thermal video for eliminating need of multiple cameras or RGB-D devices. We also developed a dataset composed of infrared thermal videos and ground truth annotations for training. The label represents a set of 3D joint locations from infrared optical trackers, which is considered the gold standard for clinical applications. To demonstrate their potential, the proposed method was used to measure the finger motion angle, and we investigated its accuracy by comparing the proposal with the Azure Kinect system and Leap Motion system. On the proposed dataset, the proposed method achieved a 3D hand pose mean error of less than 14&#xa0;mm and outperforms the other deep learning methods. When the error thresholds were larger than approximately 35&#xa0;mm, our method first to achieved excellent performance&#xa0;(&gt;80%) in terms of the fraction of good frames. For the finger motion angle calculation task, the proposed and commercial systems had comparable inter-system reliability (ICC<sub>2,1</sub> ranging from 0.81 to 0.83) and excellent validity&#xa0;(Pearson's r-values ranging from 0.82 to 0.86). We believe that the proposed approaches can capture hand motion and measure finger motion angles and can be used in different biomedicine scenarios as an effective evaluation tool for fine motor skills.</AbstractText><CopyrightInformation>Copyright &#xa9; 2022. Published by Elsevier B.V.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Zhu</LastName><ForeName>Yean</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Department of Rehabilitation Medicine, Jiangxi Provincial People's Hospital, Nanchang, 330013, China. Electronic address: yeanzhu@cqu.edu.cn.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Guo</LastName><ForeName>Chonglun</ForeName><Initials>C</Initials><AffiliationInfo><Affiliation>Department of Rehabilitation Medicine, Suichuan County People's Hospital, Jian, 343900, China; Epilepsy Center, Suichuan County People's Hospital, Jian, 343900, China. Electronic address: chonglunguo@ncu.edu.cn.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>16</Day></ArticleDate></Article><MedlineJournalInfo><Country>Netherlands</Country><MedlineTA>Artif Intell Med</MedlineTA><NlmUniqueID>8915031</NlmUniqueID><ISSNLinking>0933-3657</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D009048" MajorTopicYN="Y">Motor Skills</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000092682" MajorTopicYN="Y">Motion Capture</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D015203" MajorTopicYN="N">Reproducibility of Results</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D013817" MajorTopicYN="N">Thermography</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D006225" MajorTopicYN="N">Hand</DescriptorName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Computer vision</Keyword><Keyword MajorTopicYN="N">Fine motor skills</Keyword><Keyword MajorTopicYN="N">Infrared thermal image</Keyword><Keyword MajorTopicYN="N">Pose estimation</Keyword><Keyword MajorTopicYN="N">Transformer</Keyword></KeywordList><CoiStatement>Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2021</Year><Month>11</Month><Day>19</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>11</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>11</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>4</Hour><Minute>27</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>ppublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36628786</ArticleId><ArticleId IdType="doi">10.1016/j.artmed.2022.102474</ArticleId><ArticleId IdType="pii">S0933-3657(22)00226-3</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36627518</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1618-727X</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>10</Day></PubDate></JournalIssue><Title>Journal of digital imaging</Title><ISOAbbreviation>J Digit Imaging</ISOAbbreviation></Journal><ArticleTitle>Automated Bone Tumor Segmentation and Classification as Benign or Malignant Using Computed Tomographic Imaging.</ArticleTitle><ELocationID EIdType="doi" ValidYN="Y">10.1007/s10278-022-00771-z</ELocationID><Abstract><AbstractText>The purpose of this study was to pair computed tomography (CT) imaging and machine learning for automated bone tumor segmentation and classification to aid clinicians in determining the need for biopsy. In this retrospective study (March 2005-October 2020), a dataset of 84 femur CT scans (50 females and 34 males, 20&#xa0;years and older) with definitive histologic confirmation of bone lesion (71% malignant) were leveraged to perform automated tumor segmentation and classification. Our method involves a deep learning architecture that receives a DICOM slice and predicts (i) a segmentation mask over the estimated tumor region, and (ii) a corresponding class as benign or malignant. Class prediction for each case is then determined via majority voting. Statistical analysis was conducted via fivefold cross validation, with results reported as averages along with 95% confidence intervals. Despite the imbalance between benign and malignant cases in our dataset, our approach attains similar classification performances in specificity (75%) and sensitivity (79%). Average segmentation performance attains 56% Dice score and reaches up to 80% for an image slice in each scan. The proposed approach establishes the first steps in developing an automated deep learning method on bone tumor segmentation and classification from CT imaging. Our approach attains comparable quantitative performance to existing deep learning models using other imaging modalities, including X-ray. Moreover, visual analysis of bone tumor segmentation indicates that our model is capable of learning typical tumor characteristics and provides a promising direction in aiding the clinical decision process for biopsy.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s) under exclusive licence to Society for Imaging Informatics in Medicine.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Yildiz Potter</LastName><ForeName>Ilkay</ForeName><Initials>I</Initials><Identifier Source="ORCID">0000-0002-2827-7672</Identifier><AffiliationInfo><Affiliation>BioSensics LLC, 57 Chapel Street, Newton, MA, 02458, USA. ilkay.yildiz@biosensics.com.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yeritsyan</LastName><ForeName>Diana</ForeName><Initials>D</Initials><AffiliationInfo><Affiliation>Beth Israel Deaconess Medical Center (BIDMC), Harvard Medical School, 330 Brookline Avenue, Boston, MA, 02215, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Mahar</LastName><ForeName>Sarah</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Beth Israel Deaconess Medical Center (BIDMC), Harvard Medical School, 330 Brookline Avenue, Boston, MA, 02215, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wu</LastName><ForeName>Jim</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Beth Israel Deaconess Medical Center (BIDMC), Harvard Medical School, 330 Brookline Avenue, Boston, MA, 02215, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Nazarian</LastName><ForeName>Ara</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Beth Israel Deaconess Medical Center (BIDMC), Harvard Medical School, 330 Brookline Avenue, Boston, MA, 02215, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Vaziri</LastName><ForeName>Aidin</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>BioSensics LLC, 57 Chapel Street, Newton, MA, 02458, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Vaziri</LastName><ForeName>Ashkan</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>BioSensics LLC, 57 Chapel Street, Newton, MA, 02458, USA.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>10</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>J Digit Imaging</MedlineTA><NlmUniqueID>9100529</NlmUniqueID><ISSNLinking>0897-1889</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Bone tumor</Keyword><Keyword MajorTopicYN="N">Classification</Keyword><Keyword MajorTopicYN="N">Computed tomography</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Segmentation</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>11</Month><Day>14</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>27</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>23</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>23</Hour><Minute>28</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36627518</ArticleId><ArticleId IdType="doi">10.1007/s10278-022-00771-z</ArticleId><ArticleId IdType="pii">10.1007/s10278-022-00771-z</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Mundy GR. Metastasis to bone: causes, consequences and therapeutic opportunities. Nature Reviews Cancer. 2002 Aug;2(8):584&#x2013;93.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nrc867</ArticleId></ArticleIdList></Reference><Reference><Citation>Coleman RE. Clinical features of metastatic bone disease and risk of skeletal morbidity. Clinical Cancer Research. 2006 Oct 15;12(20):6243s&#x2013;9s.</Citation><ArticleIdList><ArticleId IdType="doi">10.1158/1078-0432.CCR-06-0931</ArticleId></ArticleIdList></Reference><Reference><Citation>Pockett RD, Castellano D, Mcewan P, Oglesby A, Barber BL, Chung K. The hospital burden of disease associated with bone metastases and skeletal-related events in patients with breast cancer, lung cancer, or prostate cancer in Spain. European Journal of Cancer Care. 2009 Aug 26;19(6):755&#x2013;60.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/j.1365-2354.2009.01135.x</ArticleId></ArticleIdList></Reference><Reference><Citation>Cancer Incidence and Survival among Children and Adolescents: United States SEER Program 1975&#x2013;1995 [Internet]. Test accounts; 1999. Available from: https://doi.org/10.1037/e407432005-001</Citation></Reference><Reference><Citation>Siegel RL, Miller KD, Jemal A. Cancer statistics, 2019. CA Cancer J Clin 2019;69(1):7&#x2013;34.</Citation><ArticleIdList><ArticleId IdType="doi">10.3322/caac.21551</ArticleId></ArticleIdList></Reference><Reference><Citation>Lodwick GS. A probabilistic approach to the diagnosis of bone tumors. Radiol Clin North Am. 1965 Dec;3(3):487&#x2013;97.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0033-8389(22)02844-5</ArticleId></ArticleIdList></Reference><Reference><Citation>Resnick D, Kransdorf MJ. Tumors and tumor-like lesions of bone: radiographic principles. Bone and Joint Imaging. 2005;1109&#x2013;19.</Citation></Reference><Reference><Citation>Lietman SA, Joyce MJ. Bone sarcomas: overview of management, with a focus on surgical treatment considerations. Cleveland Clinic Journal of Medicine. 2010 Feb 23;77(Suppl_1):S8&#x2013;12.</Citation></Reference><Reference><Citation>Vanel D, Rimondi E, Vanel M, Gambarotti M, Alberghini M. Solitary bone lesions: which ones to worry about? Cancer Imaging. 2012;12(2):409&#x2013;13.</Citation><ArticleIdList><ArticleId IdType="doi">10.1102/1470-7330.2012.9049</ArticleId></ArticleIdList></Reference><Reference><Citation>Pommersheim WJ, Chew FS. Imaging, diagnosis, and staging of bone tumors: a primer. Seminars in Roentgenology. 2004 Jul;39(3):361&#x2013;72.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ro.2004.06.013</ArticleId></ArticleIdList></Reference><Reference><Citation>Ofluoglu O, Boriani S, Gasbarrini A, De Iure F, Donthineni R. Diagnosis and planning in the management of musculoskeletal tumors: surgical perspective. Seminars in Interventional Radiology. 2010 May 18;27(02):185&#x2013;90.</Citation><ArticleIdList><ArticleId IdType="doi">10.1055/s-0030-1253516</ArticleId></ArticleIdList></Reference><Reference><Citation>Hwang S, Lefkowitz RA, Landa J, Zheng J, Moskowitz CS, Maybody M, et al. Percutaneous CT-guided bone biopsy: diagnosis of malignancy in lesions with initially indeterminate biopsy results and CT features associated with diagnostic or indeterminate results. American Journal of Roentgenology. 2011 Dec;197(6):1417&#x2013;25.</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/AJR.11.6820</ArticleId></ArticleIdList></Reference><Reference><Citation>Zamora T, Urrutia J, Schweitzer D, Amenabar PP, Botello E. Do orthopedic oncologists agree on the diagnosis and treatment of cartilage tumors of the appendicular skeleton? Clinical Orthopedics and Related Research&#xae;. 2017 Feb 15;475(9):2176&#x2013;86.</Citation></Reference><Reference><Citation>Rozenberg A, Kenneally BE, Abraham JA, Strogus K, Roedl JB, Morrison WB, et al. Clinical impact of second-opinion musculoskeletal subspecialty interpretations during a multidisciplinary orthopedic oncology conference. Journal of the American College of Radiology. 2017 Jul;14(7):931&#x2013;6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jacr.2017.01.006</ArticleId></ArticleIdList></Reference><Reference><Citation>Rozenberg A, Kenneally BE, Abraham JA, Strogus K, Roedl JB, Morrison WB, et al. Second opinions in orthopedic oncology imaging: can fellowship training reduce clinically significant discrepancies? Skeletal Radiology. 2018 Jul 12;48(1):143&#x2013;7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00256-018-3024-3</ArticleId></ArticleIdList></Reference><Reference><Citation>Caracciolo JT, Temple HT, Letson GD, Kransdorf MJ. A modified Lodwick-Madewell grading system for the evaluation of lytic bone lesions. AJR Am J Roentgenol 2016;207(1):150&#x2013;156.</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/AJR.15.14368</ArticleId></ArticleIdList></Reference><Reference><Citation>Mintz DN, Hwang S. Bone tumor imaging, then and now: review article. HSS J 2014;10(3):230&#x2013;239.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11420-014-9403-y</ArticleId></ArticleIdList></Reference><Reference><Citation>Redondo A, Bagu&#xe9; S, Bernabeu D, et al. Malignant bone tumors (other than Ewing&#x2019;s): clinical practice guidelines for diagnosis, treatment and follow-up by Spanish Group for Research on Sarcomas (GEIS). Cancer Chemother Pharmacol 2017;80(6):1113&#x2013;1131.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00280-017-3436-0</ArticleId></ArticleIdList></Reference><Reference><Citation>Yushkevich PA, Gao Y, Gerig G. ITK-SNAP: an interactive tool for semi-automatic segmentation of multi-modality biomedical images. In Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) 2016 (pp. 3342&#x2013;3345).</Citation></Reference><Reference><Citation>Cheng B, Misra I, Schwing AG, Kirillov A, Girdhar R. Masked-attention Mask Transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2022 (pp. 1290&#x2013;1299).</Citation></Reference><Reference><Citation>Zhang Y, Yang Q. An overview of multi-task learning. Natl Sci Rev. 2018;5(1):30-43.</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/nsr/nwx105</ArticleId></ArticleIdList></Reference><Reference><Citation>Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Lin S, Guo B. Swin transformer: hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision 2021 (pp. 10012&#x2013;10022).</Citation></Reference><Reference><Citation>Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser &#x141;, Polosukhin I. Attention is all you need. Advances in Neural Information Processing Systems. 2017;30.</Citation></Reference><Reference><Citation>Bengio, Yoshua, Aaron Courville, and Pascal Vincent. Representation learning: a review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence 35, no. 8 (2013): 1798-1828.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TPAMI.2013.50</ArticleId></ArticleIdList></Reference><Reference><Citation>Lin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll&#xe1;r, and C. Lawrence Zitnick. Microsoft coco: common objects in context. In European Conference on Computer Vision, pp. 740&#x2013;755. Springer, 2014.</Citation></Reference><Reference><Citation>&#xc7;i&#xe7;ek &#xd6;, Abdulkadir A, Lienkamp SS, Brox T, Ronneberger O. 3D U-Net: learning dense volumetric segmentation from sparse annotation. In International Conference on Medical Image Computing and Computer-Assisted Intervention 2016 Oct 17 (pp. 424&#x2013;432). Springer, Cham.</Citation></Reference><Reference><Citation>Kingma, Diederik P., and Jimmy Ba. Adam: a method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).</Citation></Reference><Reference><Citation>Krogh, Anders, and John Hertz. A simple weight decay can improve generalization. Advances in Neural Information Processing Systems 4 (1991).</Citation></Reference><Reference><Citation>Pedregosa F, Varoquaux G, Gramfort A, et al. Scikit-learn: machine learning in Python. The Journal of machine Learning research 2011;12:2825-2830.</Citation></Reference><Reference><Citation>He, K., Gkioxari, G., Doll&#xe1;r, P. and Girshick, R., 2017. Mask r-cnn. In Proceedings of the IEEE International Conference on Computer Vision (pp. 2961&#x2013;2969).</Citation></Reference><Reference><Citation>F. N. Iandola, M. W. Moskewicz, S. Karayev, R. B. Girshick, T. Darrell, and K. Keutzer, DenseNet: implementing efficient ConvNet descriptor pyramids technical report, 2014.</Citation></Reference><Reference><Citation>N. -H. Ho, H. -J. Yang, S. -H. Kim, S. T. Jung and S. -D. Joo, Regenerative semi-supervised bidirectional W-network-based knee bone tumor classification on radiographs guided by three-region bone segmentation, in IEEE Access, vol. 7, pp. 154277-154289, 2019, doi: https://doi.org/10.1109/ACCESS.2019.2949125 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2019.2949125</ArticleId></ArticleIdList></Reference><Reference><Citation>Bandyopadhyay O, Biswas A, Bhattacharya BB. Bone-cancer assessment and destruction pattern analysis in long-bone X-ray image. J Digit Imaging. 2019 Apr;32(2):300-313. doi: https://doi.org/10.1007/s10278-018-0145-0 . PMID: 30367308</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10278-018-0145-0</ArticleId></ArticleIdList></Reference><Reference><Citation>Xu, R., Kido, S., Suga, K. et al. Texture analysis on 18F-FDG PET/CT images to differentiate malignant and benign bone and soft-tissue lesions. Ann Nucl Med 28, 926&#x2013;935 (2014). https://doi.org/ https://doi.org/10.1007/s12149-014-0895-9</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s12149-014-0895-9</ArticleId></ArticleIdList></Reference><Reference><Citation>N. Moreau et al., Deep learning approaches for bone and bone lesion segmentation on 18FDG PET/CT imaging in the context of metastatic breast cancer, 42nd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC), 2020, pp. 1532&#x2013;1535, doi: https://doi.org/10.1109/EMBC44109.2020.9175904 .</Citation></Reference><Reference><Citation>Hossain E, Hossain MF, Rahaman MA. An approach for the detection and classification of tumor cells from bone MRI using wavelet transform and KNN classifier. In International Conference on Innovation in Engineering and Technology (ICIET) 2018 Dec 27 (pp. 1&#x2013;6). IEEE.</Citation></Reference><Reference><Citation>Wu J, Guo Y, Gou F, Dai Z. A medical assistant segmentation method for MRI images of osteosarcoma based on DecoupleSegNet. International Journal of Intelligent Systems. 2022 Jul 6.</Citation></Reference><Reference><Citation>Eweje FR, Bao B, Wu J, Dalal D, Liao WH, He Y, Luo Y, Lu S, Zhang P, Peng X, Sebro R. Deep learning for classification of bone lesions on routine MRI. EBioMedicine. 2021 Jun 1;68:103402.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ebiom.2021.103402</ArticleId></ArticleIdList></Reference><Reference><Citation>Liu X, Han C, Cui Y, Xie T, Zhang X, Wang X. Detection and segmentation of pelvic bones metastases in MRI images for patients with prostate cancer based on deep learning. Frontiers in Oncology. 2021 Nov 29:4984.</Citation></Reference><Reference><Citation>Georgeanu VA, M&#x103;muleanu M, Ghiea S, Seli&#x219;teanu D. Malignant bone tumors diagnosis using magnetic resonance imaging based on deep learning algorithms. Medicina. 2022 May 4;58(5):636.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/medicina58050636</ArticleId></ArticleIdList></Reference><Reference><Citation>Costelloe CM, Madewell JE. Radiography in the initial diagnosis of primary bone tumors. American Journal of Roentgenology. 2013 Jan;200(1):3-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/AJR.12.8488</ArticleId></ArticleIdList></Reference><Reference><Citation>Catal Reis H, Bayram B, Seker DZ. Bone tumor segmentation by gradient operators from CT images. Selcuk International Scientific Conference on Applied Sciences (The Sel&#xe7;uk ISCAS 2016) 2016 Sep (Vol. 27, p. 30).</Citation></Reference><Reference><Citation>Sun W, Liu S, Guo J, Liu S, Hao D, Hou F, Wang H, Xu W. A CT-based radiomics nomogram for distinguishing between benign and malignant bone tumors. Cancer imaging. 2021 Dec;21(1):1-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s40644-021-00387-6</ArticleId></ArticleIdList></Reference><Reference><Citation>Do N-T, Jung S-T, Yang H-J, Kim S-H. Multi-level Seg-U-Net model with global and patch-based X-ray images for knee bone tumor detection. Diagnostics. 2021; 11(4):691. https://doi.org/ https://doi.org/10.3390/diagnostics11040691</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/diagnostics11040691</ArticleId></ArticleIdList></Reference><Reference><Citation>von Schacky CE, Wilhelm NJ, Sch&#xe4;fer VS, Leonhardt Y, Gassert FG, Foreman SC, Gassert FT, Jung M, Jungmann PM, Russe MF, Mogler C. Multitask deep learning for segmentation and classification of primary bone tumors on radiographs. Radiology. 2021 Nov;301(2):398-406.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2021204531</ArticleId></ArticleIdList></Reference><Reference><Citation>He Y, Pan I, Bao B, et al. Deep learning-based classification of primary bone tumors on radiographs: a preliminary study. EBioMedicine 2020;62:103121.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ebiom.2020.103121</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36627354</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>12</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>13</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">2045-2322</ISSN><JournalIssue CitedMedium="Internet"><Volume>13</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>10</Day></PubDate></JournalIssue><Title>Scientific reports</Title><ISOAbbreviation>Sci Rep</ISOAbbreviation></Journal><ArticleTitle>Optical force estimation for interactions between tool and soft tissues.</ArticleTitle><Pagination><StartPage>506</StartPage><MedlinePgn>506</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">506</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1038/s41598-022-27036-7</ELocationID><Abstract><AbstractText>Robotic assistance in minimally invasive surgery offers numerous advantages for both patient and surgeon. However, the lack of force feedback in robotic surgery is a major limitation, and accurately estimating tool-tissue interaction forces remains a challenge. Image-based force estimation offers a promising solution without the need to integrate sensors into surgical tools. In this indirect approach, interaction forces are derived from the observed deformation, with learning-based methods improving accuracy and real-time capability. However, the relationship between deformation and force is determined by the stiffness of the tissue. Consequently, both deformation and local tissue properties must be observed for an approach applicable to heterogeneous tissue. In this work, we use optical coherence tomography, which can combine the detection of tissue deformation with shear wave elastography in a single modality. We present a multi-input deep learning network for processing of local elasticity estimates and volumetric image data. Our results demonstrate that accounting for elastic properties is critical for accurate image-based force estimation across different tissue types and properties. Joint processing of local elasticity information yields the best performance throughout our phantom study. Furthermore, we test our approach on soft tissue samples that were not present during training and show that generalization to other tissue properties is possible.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y" EqualContrib="Y"><LastName>Neidhardt</LastName><ForeName>Maximilian</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Institute of Medical Technology and Intelligent Systems, Hamburg University of Technology, Am Schwarzenberg-Campus 3, Hamburg, 21073, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y" EqualContrib="Y"><LastName>Mieling</LastName><ForeName>Robin</ForeName><Initials>R</Initials><AffiliationInfo><Affiliation>Institute of Medical Technology and Intelligent Systems, Hamburg University of Technology, Am Schwarzenberg-Campus 3, Hamburg, 21073, Germany. Robin.Mieling@tuhh.de.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Bengs</LastName><ForeName>Marcel</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Institute of Medical Technology and Intelligent Systems, Hamburg University of Technology, Am Schwarzenberg-Campus 3, Hamburg, 21073, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Schlaefer</LastName><ForeName>Alexander</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Institute of Medical Technology and Intelligent Systems, Hamburg University of Technology, Am Schwarzenberg-Campus 3, Hamburg, 21073, Germany.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>10</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>Sci Rep</MedlineTA><NlmUniqueID>101563288</NlmUniqueID><ISSNLinking>2045-2322</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D055595" MajorTopicYN="N">Mechanical Phenomena</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D065287" MajorTopicYN="Y">Robotic Surgical Procedures</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D004548" MajorTopicYN="N">Elasticity</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D012371" MajorTopicYN="Y">Robotics</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D019047" MajorTopicYN="N">Phantoms, Imaging</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D054459" MajorTopicYN="Y">Elasticity Imaging Techniques</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D041623" MajorTopicYN="N">Tomography, Optical Coherence</DescriptorName></MeshHeading></MeshHeadingList><CoiStatement>The authors declare no competing interests.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>7</Month><Day>25</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>23</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>23</Hour><Minute>18</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36627354</ArticleId><ArticleId IdType="pmc">PMC9831996</ArticleId><ArticleId IdType="doi">10.1038/s41598-022-27036-7</ArticleId><ArticleId IdType="pii">10.1038/s41598-022-27036-7</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Ghezzi TL, Corleta OC. 30 years of robotic surgery. World J. Surg. 2016;40:2550&#x2013;2557. doi: 10.1007/s00268-016-3543-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00268-016-3543-9</ArticleId><ArticleId IdType="pubmed">27177648</ArticleId></ArticleIdList></Reference><Reference><Citation>Armijo PR, Pagkratis S, Boilesen E, Tanner T, Oleynikov D. Growth in robotic-assisted procedures is from conversion of laparoscopic procedures and not from open surgeons&#x2019; conversion: A study of trends and costs. Surg. Endosc. 2018;32:2106&#x2013;2113. doi: 10.1007/s00464-017-5908-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00464-017-5908-z</ArticleId><ArticleId IdType="pubmed">29067582</ArticleId></ArticleIdList></Reference><Reference><Citation>Diana M, Marescaux J. Robotic surgery. J. Br. Surg. 2015;102:e15&#x2013;e28. doi: 10.1002/bjs.9711.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/bjs.9711</ArticleId><ArticleId IdType="pubmed">25627128</ArticleId></ArticleIdList></Reference><Reference><Citation>Wee IJY, Kuo L-J, Ngu JC-Y. A systematic review of the true benefit of robotic surgery: Ergonomics. The Int. J. Med. Robot. Comput. Assist. Surg. 2020;16:e2113. doi: 10.1002/rcs.2113.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/rcs.2113</ArticleId><ArticleId IdType="pubmed">32304167</ArticleId></ArticleIdList></Reference><Reference><Citation>Aviles-Rivero AI, et al. Sensory substitution for force feedback recovery. ACM Trans. Appl. Percept. 2018;15:1&#x2013;19. doi: 10.1145/3176642.</Citation><ArticleIdList><ArticleId IdType="doi">10.1145/3176642</ArticleId></ArticleIdList></Reference><Reference><Citation>Overtoom EM, Horeman T, Jansen F-W, Dankelman J, Schreuder HWR. Haptic feedback, force feedback, and force-sensing in simulation training for laparoscopy: A systematic overview. J. Surg. Educ. 2019;76:242&#x2013;261. doi: 10.1016/j.jsurg.2018.06.008.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jsurg.2018.06.008</ArticleId><ArticleId IdType="pubmed">30082239</ArticleId></ArticleIdList></Reference><Reference><Citation>Golahmadi AK, Khan DZ, Mylonas GP, Marcus HJ. Tool-tissue forces in surgery: A systematic review. Ann. Med. Surg. 2021;65:102268. doi: 10.1016/j.amsu.2021.102268.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.amsu.2021.102268</ArticleId><ArticleId IdType="pmc">PMC8058906</ArticleId><ArticleId IdType="pubmed">33898035</ArticleId></ArticleIdList></Reference><Reference><Citation>Lim S-C, Lee H-K, Park J. Role of combined tactile and kinesthetic feedback in minimally invasive surgery. The Int. J. Med. Robot. Comput. Assist. Surg. 2015;11:360&#x2013;374. doi: 10.1002/rcs.1625.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/rcs.1625</ArticleId><ArticleId IdType="pubmed">25328100</ArticleId></ArticleIdList></Reference><Reference><Citation>Marb&#xe1;n, A., Casals, A., Fern&#xe1;ndez, J. &amp; Amat, J. Haptic feedback in surgical robotics: Still a challenge. In ROBOT2013: First Iberian Robotics Conference, 245&#x2013;253 (Springer, 2014).</Citation></Reference><Reference><Citation>Simaan N, Yasin RM, Wang L. Medical technologies and challenges of robot-assisted minimally invasive intervention and diagnostics. Annu. Rev. Control Robot. Autonom. Syst. 2018;1:465&#x2013;490. doi: 10.1146/annurev-control-060117-104956.</Citation><ArticleIdList><ArticleId IdType="doi">10.1146/annurev-control-060117-104956</ArticleId></ArticleIdList></Reference><Reference><Citation>Okamura, A. M., Verner, L. N., Reiley, C. E. &amp; Mahvash, M. Haptics for robot-assisted minimally invasive surgery. In Robotics Research, (eds Siciliano, B. et al.) vol. 66 of Springer Tracts in Advanced Robotics, 361&#x2013;372, 10.1007/978-3-642-14743-2_30 (Springer Berlin Heidelberg, Berlin, Heidelberg, 2011).</Citation></Reference><Reference><Citation>Amirabdollahian F, et al. Prevalence of haptic feedback in robot-mediated surgery: A systematic review of literature. J. Robot. Surg. 2018;12:11&#x2013;25. doi: 10.1007/s11701-017-0763-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11701-017-0763-4</ArticleId><ArticleId IdType="pubmed">29196867</ArticleId></ArticleIdList></Reference><Reference><Citation>Culmer, P., Alazmani, A., Mushtaq, F., Cross, W. &amp; Jayne, D. 15 - haptics in surgical robots. In Handbook of robotic and image-guided surgery, (eds Abedin-Nasab, M. H.) 239&#x2013;263, 10.1016/B978-0-12-814245-5.00015-3 (Elsevier, Amsterdam, Netherlands, 2020).</Citation></Reference><Reference><Citation>Yang C, Xie Y, Liu S, Sun D. Force modeling, identification, and feedback control of robot-assisted needle insertion: A survey of the literature. Sensors (Basel, Switzerland) 2018 doi: 10.3390/s18020561.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s18020561</ArticleId><ArticleId IdType="pmc">PMC5855056</ArticleId><ArticleId IdType="pubmed">29439539</ArticleId></ArticleIdList></Reference><Reference><Citation>Nazari AA, Janabi-Sharifi F, Zareinia K. Image-based force estimation in medical applications: A review. IEEE Sens. J. 2021;21:8805&#x2013;8830. doi: 10.1109/JSEN.2021.3052755.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/JSEN.2021.3052755</ArticleId></ArticleIdList></Reference><Reference><Citation>Berkelman, P. J., Whitcomb, L. L., Taylor, R. H. &amp; Jensen, P. A miniature instrument tip force sensor for robot/human cooperative microsurgical manipulation with enhanced force feedback. In International Conference on Medical Image Computing and Computer-Assisted Intervention, 897&#x2013;906 (Springer, 2000).</Citation></Reference><Reference><Citation>Sang H, et al. External force estimation and implementation in robotically assisted minimally invasive surgery. The Int. J. Med. Robot. Comput. Assist. Surg. 2017;13:e1824. doi: 10.1002/rcs.1824.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/rcs.1824</ArticleId><ArticleId IdType="pubmed">28466997</ArticleId></ArticleIdList></Reference><Reference><Citation>Chua, Z., Jarc, A. M. &amp; Okamura, A. M. Toward force estimation in robot-assisted surgery using deep learning with vision and robot state. In 2021 IEEE International Conference on Robotics and Automation (ICRA), 12335-12341. (IEEE, 2021).</Citation></Reference><Reference><Citation>Sande JA, et al. Ultrasound shear wave elastography and liver fibrosis: A prospective multicenter study. World J. Hepatol. 2017;9:38. doi: 10.4254/wjh.v9.i1.38.</Citation><ArticleIdList><ArticleId IdType="doi">10.4254/wjh.v9.i1.38</ArticleId><ArticleId IdType="pmc">PMC5220270</ArticleId><ArticleId IdType="pubmed">28105257</ArticleId></ArticleIdList></Reference><Reference><Citation>Yang Y-P, et al. Qualitative and quantitative analysis with a novel shear wave speed imaging for differential diagnosis of breast lesions. Sci. Rep. 2017;7:1&#x2013;11.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5244419</ArticleId><ArticleId IdType="pubmed">28102328</ArticleId></ArticleIdList></Reference><Reference><Citation>Miller K, Chinzei K, Orssengo G, Bednarz P. Mechanical properties of brain tissue in-vivo: Experiment and computer simulation. J. Biomech. 2000;33:1369&#x2013;1376. doi: 10.1016/S0021-9290(00)00120-2.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0021-9290(00)00120-2</ArticleId><ArticleId IdType="pubmed">10940395</ArticleId></ArticleIdList></Reference><Reference><Citation>Haouchine N, Kuang W, Cotin S, Yip M. Vision-based force feedback estimation for robot-assisted surgery using instrument-constrained biomechanical three-dimensional maps. IEEE Robot. Autom. Lett. 2018;3:2160&#x2013;2165. doi: 10.1109/LRA.2018.2810948.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/LRA.2018.2810948</ArticleId></ArticleIdList></Reference><Reference><Citation>Giannarou S, et al. Vision-based deformation recovery for intraoperative force estimation of tool-tissue interaction for neurosurgery. Int. J. Comput. Assist. Radiol. Surg. 2016;11:929&#x2013;936. doi: 10.1007/s11548-016-1361-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11548-016-1361-z</ArticleId><ArticleId IdType="pmc">PMC4893380</ArticleId><ArticleId IdType="pubmed">27008473</ArticleId></ArticleIdList></Reference><Reference><Citation>Aviles, A. I., Marban, A., Sobrevilla, P., Fernandez, J. &amp; Casals, A. A recurrent neural network approach for 3d vision-based force estimation. In 2014 4th International Conference on Image Processing Theory, Tools and Applications (IPTA), 1&#x2013;6, 10.1109/IPTA.2014.7001941 (IEEE, 2014).</Citation></Reference><Reference><Citation>Marban A, Srinivasan V, Samek W, Fern&#xe1;ndez J, Casals A. A recurrent convolutional neural network approach for sensorless force estimation in robotic surgery. Biomed. Signal Process. Control. 2019;50:134&#x2013;150. doi: 10.1016/j.bspc.2019.01.011.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bspc.2019.01.011</ArticleId></ArticleIdList></Reference><Reference><Citation>Behrendt F, Gessert N, Schlaefer A. Generalization of spatio-temporal deep learning for vision-based force estimation. Curr. Direct. Biomed. Eng. 2020 doi: 10.1515/cdbme-2020-0024.</Citation><ArticleIdList><ArticleId IdType="doi">10.1515/cdbme-2020-0024</ArticleId></ArticleIdList></Reference><Reference><Citation>Aviles AI, Alsaleh SM, Hahn JK, Casals A. Towards retrieving force feedback in robotic-assisted surgery: A supervised neuro-recurrent-vision approach. IEEE Trans. Haptics. 2017;10:431&#x2013;443. doi: 10.1109/TOH.2016.2640289.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TOH.2016.2640289</ArticleId><ArticleId IdType="pubmed">28113330</ArticleId></ArticleIdList></Reference><Reference><Citation>Gessert N, Schl&#xfc;ter M, Schlaefer A. A deep learning approach for pose estimation from volumetric oct data. Med. Image Anal. 2018;46:162&#x2013;179. doi: 10.1016/j.media.2018.03.002.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2018.03.002</ArticleId><ArticleId IdType="pubmed">29550582</ArticleId></ArticleIdList></Reference><Reference><Citation>Gessert N, Bengs M, Schl&#xfc;ter M, Schlaefer A. Deep learning with 4d spatio-temporal data representations for oct-based force estimation. Med. Image Anal. 2020;64:101730. doi: 10.1016/j.media.2020.101730.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2020.101730</ArticleId><ArticleId IdType="pubmed">32492583</ArticleId></ArticleIdList></Reference><Reference><Citation>Neidhardt M, et al. Force estimation from 4d oct data in a human tumor xenograft mouse model. Curr. Directi. Biomed. Eng. 2020;6:20200022. doi: 10.1515/cdbme-2020-0022.</Citation><ArticleIdList><ArticleId IdType="doi">10.1515/cdbme-2020-0022</ArticleId></ArticleIdList></Reference><Reference><Citation>Qiu Y, et al. Quantitative optical coherence elastography based on fiber-optic probe for in situ measurement of tissue mechanical properties. Biomed. Opt. Express. 2016;7:688&#x2013;700. doi: 10.1364/BOE.7.000688.</Citation><ArticleIdList><ArticleId IdType="doi">10.1364/BOE.7.000688</ArticleId><ArticleId IdType="pmc">PMC4771481</ArticleId><ArticleId IdType="pubmed">26977372</ArticleId></ArticleIdList></Reference><Reference><Citation>Mieling R, Sprenger J, Latus S, Bargsten L, Schlaefer A. A novel optical needle probe for deep learning-based tissue elasticity characterization. Curr. Direct. Biomed. Eng. 2021;7:21&#x2013;25. doi: 10.1515/cdbme-2021-1005.</Citation><ArticleIdList><ArticleId IdType="doi">10.1515/cdbme-2021-1005</ArticleId></ArticleIdList></Reference><Reference><Citation>Neidhardt M, et al. 4d deep learning for real-time volumetric optical coherence elastography. Int. J. Comput. Assist. Radiol. Surg. 2021;16:23&#x2013;27. doi: 10.1007/s11548-020-02261-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11548-020-02261-5</ArticleId><ArticleId IdType="pmc">PMC7822782</ArticleId><ArticleId IdType="pubmed">32997312</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang S, et al. A focused air-pulse system for optical-coherence-tomography-based measurements of tissue elasticity. Laser Phys. Lett. 2013;10:075605. doi: 10.1088/1612-2011/10/7/075605.</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/1612-2011/10/7/075605</ArticleId><ArticleId IdType="pmc">PMC5969524</ArticleId><ArticleId IdType="pubmed">29805349</ArticleId></ArticleIdList></Reference><Reference><Citation>Kijanka P, Urban MW. Local phase velocity based imaging: A new technique used for ultrasound shear wave elastography. IEEE Trans. Med. Imaging. 2018;38:894&#x2013;908. doi: 10.1109/TMI.2018.2874545.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2018.2874545</ArticleId><ArticleId IdType="pmc">PMC6467061</ArticleId><ArticleId IdType="pubmed">30296217</ArticleId></ArticleIdList></Reference><Reference><Citation>Maksuti E, et al. Arterial stiffness estimation by shear wave elastography: Validation in phantoms with mechanical testing. Ultrasound Med. Biol. 2016;42:308&#x2013;321. doi: 10.1016/j.ultrasmedbio.2015.08.012.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ultrasmedbio.2015.08.012</ArticleId><ArticleId IdType="pubmed">26454623</ArticleId></ArticleIdList></Reference><Reference><Citation>Beuve S, Kritly L, Call&#xe9; S, Remenieras J-P. Diffuse shear wave spectroscopy for soft tissue viscoelastic characterization. Ultrasonics. 2021;110:106239. doi: 10.1016/j.ultras.2020.106239.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ultras.2020.106239</ArticleId><ArticleId IdType="pubmed">32942089</ArticleId></ArticleIdList></Reference><Reference><Citation>Kennedy CW, Desai JP. A vision-based approach for estimating contact forces: Applications to robot-assisted surgery. Appl. Bionics Biomech. 2005;2:53&#x2013;60. doi: 10.1155/2005/436897.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2005/436897</ArticleId></ArticleIdList></Reference><Reference><Citation>Huang, G., Liu, Z., Van Der Maaten, L. &amp; Weinberger, K. Q. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition 4700&#x2013;4708 (2017).</Citation></Reference><Reference><Citation>Nair, V. &amp; Hinton, G. E. Rectified linear units improve restricted boltzmann machines. In Icml (2010).</Citation></Reference><Reference><Citation>Ioffe, S. &amp; Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, 448&#x2013;456 (PMLR, 2015).</Citation></Reference><Reference><Citation>Smith, L.&#xa0;N. &amp; Topin, N. Super-convergence: Very fast training of neural networks using large learning rates. In Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications, vol. 11006, 1100612 (International Society for Optics and Photonics, 2019).</Citation></Reference><Reference><Citation>Kingma, D.&#xa0;P. &amp; Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).</Citation></Reference><Reference><Citation>He, K., Zhang, X., Ren, S. &amp; Sun, J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, 1026&#x2013;1034 (2015).</Citation></Reference><Reference><Citation>Crameri F, Shephard GE, Heron PJ. The misuse of colour in science communication. Nat. Commun. 2020;11:1&#x2013;10. doi: 10.1038/s41467-020-19160-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41467-020-19160-7</ArticleId><ArticleId IdType="pmc">PMC7595127</ArticleId><ArticleId IdType="pubmed">33116149</ArticleId></ArticleIdList></Reference><Reference><Citation>Ouyang Q, et al. Bio-inspired haptic feedback for artificial palpation in robotic surgery. IEEE Trans. Biomed. Eng. 2021;68:3184&#x2013;3193. doi: 10.1109/TBME.2021.3076094.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TBME.2021.3076094</ArticleId><ArticleId IdType="pmc">PMC8486347</ArticleId><ArticleId IdType="pubmed">33905321</ArticleId></ArticleIdList></Reference><Reference><Citation>Wagner, C. R., Howe, R. D. &amp; Stylopoulos, N. The role of force feedback in surgery: Analysis of blunt dissection. In Haptic Interfaces for Virtual Environment and Teleoperator Systems, International Symposium on, 73 (Citeseer, 2002).</Citation></Reference><Reference><Citation>Haouchine N, Kuang W, Cotin S, Yip M. Vision-based force feedback estimation for robot-assisted surgery using instrument-constrained biomechanical three-dimensional maps. IEEE Robot. Autom. Lett. 2018;3:2160&#x2013;2165. doi: 10.1109/LRA.2018.2810948.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/LRA.2018.2810948</ArticleId></ArticleIdList></Reference><Reference><Citation>Berry MF, et al. Mesenchymal stem cell injection after myocardial infarction improves myocardial compliance. Am. J. Physiol.-Heart Circ. Physiol. 2006;290:H2196&#x2013;H2203. doi: 10.1152/ajpheart.01017.2005.</Citation><ArticleIdList><ArticleId IdType="doi">10.1152/ajpheart.01017.2005</ArticleId><ArticleId IdType="pubmed">16473959</ArticleId></ArticleIdList></Reference><Reference><Citation>Neidhardt, M. et al. Ultrasound shear wave elasticity imaging with spatio-temporal deep learning. IEEE Trans. Biomed. Eng.69(11), 3356-3364 (2022).</Citation><ArticleIdList><ArticleId IdType="pubmed">35439123</ArticleId></ArticleIdList></Reference><Reference><Citation>Marban, A., Srinivasan, V., Samek, W., Fern&#xe1;ndez, J. &amp; Casals, A. A recurrent convolutional neural network approach for sensorless force estimation in robotic surgery. Biomedical Signal Processing and Control50, 134-150. (2019).</Citation></Reference><Reference><Citation>Aviles, A. I., Alsaleh, S., Sobrevilla, P. &amp; Casals, A. Sensorless force estimation using a neuro-vision-based approach for robotic-assisted surgery. In 2015 7th International IEEE/EMBS Conference on Neural Engineering (NER), 86&#x2013;89, 10.1109/NER.2015.7146566 (2015).</Citation></Reference><Reference><Citation>Yengul SS, Barbone PE, Madore B. Dispersion in tissue-mimicking gels measured with shear wave elastography and torsional vibration rheometry. Ultrasound Med. Biol. 2019;45:586&#x2013;604. doi: 10.1016/j.ultrasmedbio.2018.07.002.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ultrasmedbio.2018.07.002</ArticleId><ArticleId IdType="pmc">PMC6325023</ArticleId><ArticleId IdType="pubmed">30473175</ArticleId></ArticleIdList></Reference><Reference><Citation>Rus G, Faris IH, Torres J, Callejas A, Melchor J. Why are viscosity and nonlinearity bound to make an impact in clinical elastographic diagnosis? Sensors. 2020;20:2379. doi: 10.3390/s20082379.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s20082379</ArticleId><ArticleId IdType="pmc">PMC7219338</ArticleId><ArticleId IdType="pubmed">32331295</ArticleId></ArticleIdList></Reference><Reference><Citation>Yuting L, et al. Microscale characterization of prostate biopsies tissues using optical coherence elastography and second harmonic generation imaging. Lab. Invest. 2018;98:380&#x2013;390. doi: 10.1038/labinvest.2017.132.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/labinvest.2017.132</ArticleId><ArticleId IdType="pmc">PMC5842892</ArticleId><ArticleId IdType="pubmed">29251735</ArticleId></ArticleIdList></Reference><Reference><Citation>Li C, et al. Detection and characterisation of biopsy tissue using quantitative optical coherence elastography (oce) in men with suspected prostate cancer. Cancer Lett. 2015;357:121&#x2013;128. doi: 10.1016/j.canlet.2014.11.021.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.canlet.2014.11.021</ArticleId><ArticleId IdType="pubmed">25444932</ArticleId></ArticleIdList></Reference><Reference><Citation>Patel RV, Atashzar SF, Tavakoli M. Haptic feedback and force-based teleoperation in surgical robotics. Proc. IEEE. 2022;110:1012&#x2013;1027. doi: 10.1109/JPROC.2022.3180052.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/JPROC.2022.3180052</ArticleId></ArticleIdList></Reference><Reference><Citation>Mieling, R. et al. Proximity-based haptic feedback for collaborative robotic needle insertion. In International Conference on Human Haptic Sensing and Touch Enabled Computer Applications, 301&#x2013;309 (Springer, 2022).</Citation></Reference><Reference><Citation>Aggravi M, Estima DA, Krupa A, Misra S, Pacchierotti C. Haptic teleoperation of flexible needles combining 3d ultrasound guidance and needle tip force feedback. IEEE Robot. Autom. Lett. 2021;6:4859&#x2013;4866. doi: 10.1109/LRA.2021.3068635.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/LRA.2021.3068635</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" VersionID="2" Owner="NLM" IndexingMethod="Automated"><PMID Version="2">36627339</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>12</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>13</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">2045-2322</ISSN><JournalIssue CitedMedium="Internet"><Volume>13</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>10</Day></PubDate></JournalIssue><Title>Scientific reports</Title><ISOAbbreviation>Sci Rep</ISOAbbreviation></Journal><ArticleTitle>Coronavirus covid-19 detection by means of explainable deep learning.</ArticleTitle><Pagination><StartPage>462</StartPage><MedlinePgn>462</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">462</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1038/s41598-023-27697-y</ELocationID><Abstract><AbstractText>The coronavirus is caused by the infection of the SARS-CoV-2 virus: it represents a complex and new condition, considering that until the end of December 2019 this virus was totally unknown to the international scientific community. The clinical management of patients with the coronavirus disease has undergone an evolution over the months, thanks to the increasing knowledge of the virus, symptoms and efficacy of the various therapies. Currently, however, there is no specific therapy for SARS-CoV-2 virus, know also as Coronavirus disease 19, and treatment is based on the symptoms of the patient taking into account the overall clinical picture. Furthermore, the test to identify whether a patient is affected by the virus is generally performed on sputum and the result is generally available within a few hours or days. Researches previously found that the biomedical imaging analysis is able to show signs of pneumonia. For this reason in this paper, with the aim of providing a fully automatic and faster diagnosis, we design and implement a method adopting deep learning for the novel coronavirus disease detection, starting from computed tomography medical images. The proposed approach is aimed to detect whether a computed tomography medical images is related to an healthy patient, to a patient with a pulmonary disease or to a patient affected with Coronavirus disease 19. In case the patient is marked by the proposed method as affected by the Coronavirus disease 19, the areas symptomatic of the Coronavirus disease 19 infection are automatically highlighted in the computed tomography medical images. We perform an experimental analysis to empirically demonstrate the effectiveness of the proposed approach, by considering medical images belonging from different institutions, with an average time for Coronavirus disease 19 detection of approximately 8.9 s and an accuracy equal to 0.95.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Mercaldo</LastName><ForeName>Francesco</ForeName><Initials>F</Initials><AffiliationInfo><Affiliation>Department of Medicine and Health Sciences "Vincenzo Tiberio", University of Molise, Campobasso, Italy. francesco.mercaldo@unimol.it.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Belfiore</LastName><ForeName>Maria Paola</ForeName><Initials>MP</Initials><AffiliationInfo><Affiliation>Department of Precision Medicine, University of Campania "Luigi Vanvitelli", Naples, Italy.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Reginelli</LastName><ForeName>Alfonso</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Department of Precision Medicine, University of Campania "Luigi Vanvitelli", Naples, Italy.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Brunese</LastName><ForeName>Luca</ForeName><Initials>L</Initials><AffiliationInfo><Affiliation>Department of Medicine and Health Sciences "Vincenzo Tiberio", University of Molise, Campobasso, Italy.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Santone</LastName><ForeName>Antonella</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Department of Medicine and Health Sciences "Vincenzo Tiberio", University of Molise, Campobasso, Italy.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>10</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>Sci Rep</MedlineTA><NlmUniqueID>101563288</NlmUniqueID><ISSNLinking>2045-2322</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000086382" MajorTopicYN="Y">COVID-19</DescriptorName><QualifierName UI="Q000175" MajorTopicYN="N">diagnosis</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D000086402" MajorTopicYN="N">SARS-CoV-2</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D011014" MajorTopicYN="Y">Pneumonia</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D008171" MajorTopicYN="Y">Lung Diseases</DescriptorName></MeshHeading></MeshHeadingList><CoiStatement>All authors confirm that there are not potential conflicts of interest include employment, consultancies, stock ownership, honoraria, paid expert testimony, patent applications/registrations, and grants or other funding.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>2</Month><Day>1</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2023</Year><Month>1</Month><Day>5</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>23</Hour><Minute>18</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36627339</ArticleId><ArticleId IdType="pmc">PMC9830129</ArticleId><ArticleId IdType="doi">10.1038/s41598-023-27697-y</ArticleId><ArticleId IdType="pii">10.1038/s41598-023-27697-y</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Struyf T, et al. Signs and symptoms to determine if a patient presenting in primary care or hospital outpatient settings has COVID-19. Cochrane Database Syst. Rev. 2022;5:5&#x2013;11.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC9121352</ArticleId><ArticleId IdType="pubmed">35593186</ArticleId></ArticleIdList></Reference><Reference><Citation>Brunese L, Martinelli F, Mercaldo F, Santone A. Machine learning for coronavirus COVID-19 detection from chest x-rays. Procedia Comput. Sci. 2020;176:2212&#x2013;2221. doi: 10.1016/j.procs.2020.09.258.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.procs.2020.09.258</ArticleId><ArticleId IdType="pmc">PMC7531990</ArticleId><ArticleId IdType="pubmed">33042308</ArticleId></ArticleIdList></Reference><Reference><Citation>Jeyanathan M, Afkhami S, Smaill F, Miller MS, Lichty BD, Xing Z. Immunological considerations for COVID-19 vaccine strategies. Nat. Rev. Immunol. 2020;20:1&#x2013;18. doi: 10.1038/s41577-020-00434-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41577-020-00434-6</ArticleId><ArticleId IdType="pmc">PMC7472682</ArticleId><ArticleId IdType="pubmed">32887954</ArticleId></ArticleIdList></Reference><Reference><Citation>Brunese L, Mercaldo F, Reginelli A, Santone A. Explainable deep learning for pulmonary disease and coronavirus COVID-19 detection from x-rays. Comput. Methods Programs Biomed. 2020;196:105608. doi: 10.1016/j.cmpb.2020.105608.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cmpb.2020.105608</ArticleId><ArticleId IdType="pmc">PMC7831868</ArticleId><ArticleId IdType="pubmed">32599338</ArticleId></ArticleIdList></Reference><Reference><Citation>Le TT, Andreadakis Z, Kumar A, Roman RG, Tollefsen S, Saville M, Mayhew S. The COVID-19 vaccine development landscape. Nat. Rev. Drug Discov. 2020;19(5):305&#x2013;306. doi: 10.1038/d41573-020-00073-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/d41573-020-00073-5</ArticleId><ArticleId IdType="pubmed">32273591</ArticleId></ArticleIdList></Reference><Reference><Citation>Li Q. Early transmission dynamics in Wuhan, China, of novel coronavirus-infected pneumonia. New Engl. J. Med. 2020;382:1199&#x2013;1207. doi: 10.1056/NEJMoa2001316.</Citation><ArticleIdList><ArticleId IdType="doi">10.1056/NEJMoa2001316</ArticleId><ArticleId IdType="pmc">PMC7121484</ArticleId><ArticleId IdType="pubmed">31995857</ArticleId></ArticleIdList></Reference><Reference><Citation>Gu J, Han B, Wang J. COVID-19: Gastrointestinal manifestations and potential fecal-oral transmission. Gastroenterology. 2020;158(6):1518&#x2013;1519. doi: 10.1053/j.gastro.2020.02.054.</Citation><ArticleIdList><ArticleId IdType="doi">10.1053/j.gastro.2020.02.054</ArticleId><ArticleId IdType="pmc">PMC7130192</ArticleId><ArticleId IdType="pubmed">32142785</ArticleId></ArticleIdList></Reference><Reference><Citation>Roques L, Klein EK, Papaix J, Sar A, Soubeyrand S. Using early data to estimate the actual infection fatality ratio from COVID-19 in France. Biology. 2020;9(5):97. doi: 10.3390/biology9050097.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/biology9050097</ArticleId><ArticleId IdType="pmc">PMC7284549</ArticleId><ArticleId IdType="pubmed">32397286</ArticleId></ArticleIdList></Reference><Reference><Citation>Covid, T.C., Team, R Severe outcomes among patients with coronavirus disease 2019 (COVID-19)-United States. MMWR Morb. Mortal. Wkly. Rep. 2020;69(12):343&#x2013;346.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7725513</ArticleId><ArticleId IdType="pubmed">32214079</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang Y, Wang Y, Chen Y, Qin Q. Unique epidemiological and clinical features of the emerging 2019 novel coronavirus pneumonia (COVID-19) implicate special control measures. J. Med. Virol. 2020;92(6):568&#x2013;576. doi: 10.1002/jmv.25748.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmv.25748</ArticleId><ArticleId IdType="pmc">PMC7228347</ArticleId><ArticleId IdType="pubmed">32134116</ArticleId></ArticleIdList></Reference><Reference><Citation>Holmes KV. SARS-associated coronavirus. N. Engl. J. Med. 2003;348(20):1948&#x2013;1951. doi: 10.1056/NEJMp030078.</Citation><ArticleIdList><ArticleId IdType="doi">10.1056/NEJMp030078</ArticleId><ArticleId IdType="pubmed">12748314</ArticleId></ArticleIdList></Reference><Reference><Citation>van der Hoek L, Pyrc K, Jebbink MF, Vermeulen-Oost W, Berkhout RJ, Wolthers KC, Wertheim-van Dillen PM, Kaandorp J, Spaargaren J, Berkhout B. Identification of a new human coronavirus. Nat. Med. 2004;10(4):368&#x2013;373. doi: 10.1038/nm1024.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nm1024</ArticleId><ArticleId IdType="pmc">PMC7095789</ArticleId><ArticleId IdType="pubmed">15034574</ArticleId></ArticleIdList></Reference><Reference><Citation>Abroug F, Slim A, Ouanes-Besbes L, Kacem M-AH, Dachraoui F, Ouanes I, Lu X, Tao Y, Paden C, Caidi H, et al. Family cluster of middle east respiratory syndrome coronavirus infections, Tunisia, 2013. Emerg. Infect. Dis. 2014;20(9):1527. doi: 10.3201/eid2009.140378.</Citation><ArticleIdList><ArticleId IdType="doi">10.3201/eid2009.140378</ArticleId><ArticleId IdType="pmc">PMC4178422</ArticleId><ArticleId IdType="pubmed">25148113</ArticleId></ArticleIdList></Reference><Reference><Citation>Jung S-M, Akhmetzhanov AR, Hayashi K, Linton NM, Yang Y, Yuan B, Kobayashi T, Kinoshita R, Nishiura H. Real-time estimation of the risk of death from novel coronavirus (COVID-19) infection: Inference using exported cases. J. Clin. Med. 2020;9(2):523. doi: 10.3390/jcm9020523.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/jcm9020523</ArticleId><ArticleId IdType="pmc">PMC7074479</ArticleId><ArticleId IdType="pubmed">32075152</ArticleId></ArticleIdList></Reference><Reference><Citation>Livingston E, Bucher K. Coronavirus disease 2019 (COVID-19) in Italy. Jama. 2020;323:1335. doi: 10.1001/jama.2020.4344.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jama.2020.4344</ArticleId><ArticleId IdType="pubmed">32181795</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang C, Horby PW, Hayden FG, Gao GF. A novel coronavirus outbreak of global health concern. Lancet. 2020;395(10223):470&#x2013;473. doi: 10.1016/S0140-6736(20)30185-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0140-6736(20)30185-9</ArticleId><ArticleId IdType="pmc">PMC7135038</ArticleId><ArticleId IdType="pubmed">31986257</ArticleId></ArticleIdList></Reference><Reference><Citation>Huang C, Wang Y, Li X, Ren L, Zhao J, Hu Y, Zhang L, Fan G, Xu J, Gu X, et al. Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China. Lancet. 2020;395(10223):497&#x2013;506. doi: 10.1016/S0140-6736(20)30183-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0140-6736(20)30183-5</ArticleId><ArticleId IdType="pmc">PMC7159299</ArticleId><ArticleId IdType="pubmed">31986264</ArticleId></ArticleIdList></Reference><Reference><Citation>Li L, et al. Artificial intelligence distinguishes COVID-19 from community acquired pneumonia on chest CT. Radiology. 2020;296:200905. doi: 10.1148/radiol.2020200905.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2020200905</ArticleId><ArticleId IdType="pubmed">0</ArticleId></ArticleIdList></Reference><Reference><Citation>Long C, Xu H, Shen Q, Zhang X, Fan B, Wang C, Zeng B, Li Z, Li X, Li H, et al. Diagnosis of the coronavirus disease (COVID-19): RRT-PCR or CT? Eur. J. Radiol. 2020;126:108961. doi: 10.1016/j.ejrad.2020.108961.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejrad.2020.108961</ArticleId><ArticleId IdType="pmc">PMC7102545</ArticleId><ArticleId IdType="pubmed">32229322</ArticleId></ArticleIdList></Reference><Reference><Citation>Sellers SA, Dover KL, Bailey AG, Cheves A, Eason AB, Popowitch EB, Miller MB, Wohl DA, Dittmer DP, Fischer WA. Burden of respiratory viral infection in persons with human immunodeficiency virus. Influenza Other Respir. Viruses. 2020;14:465&#x2013;469. doi: 10.1111/irv.12734.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/irv.12734</ArticleId><ArticleId IdType="pmc">PMC7298306</ArticleId><ArticleId IdType="pubmed">32153113</ArticleId></ArticleIdList></Reference><Reference><Citation>Ai T, et al. Correlation of chest CT and RT-PCR testing in coronavirus disease 2019 (COVID-19) in China: A report of 1014 cases. Radiology. 2019;2020:200642.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7233399</ArticleId><ArticleId IdType="pubmed">32101510</ArticleId></ArticleIdList></Reference><Reference><Citation>Fang Y, Zhang H, Xie J, Lin M, Ying L, Pang P, Ji W. Sensitivity of chest CT for COVID-19: Comparison to RT-PCR. Radiology. 2020;296:200432. doi: 10.1148/radiol.2020200432.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2020200432</ArticleId><ArticleId IdType="pmc">PMC7233365</ArticleId><ArticleId IdType="pubmed">32073353</ArticleId></ArticleIdList></Reference><Reference><Citation>Alakwaa W, Nassef M, Badr A. Lung cancer detection and classification with 3D convolutional neural network (3D-CNN) Lung Cancer. 2017;8(8):409&#x2013;417.</Citation></Reference><Reference><Citation>Bhatia S, Sinha Y, Goel L. Soft Computing for Problem Solving. Springer; 2019. Lung cancer detection: A deep learning approach; pp. 699&#x2013;705.</Citation></Reference><Reference><Citation>Brunese, L., Mercaldo, F., Reginelli, A., &amp; Santone, A. Neural networks for lung cancer detection through radiomic features. In 2019 International Joint Conference on Neural Networks (IJCNN), 1&#x2013;10 (IEEE, 2019).</Citation></Reference><Reference><Citation>Bulten W, Pinckaers H, van Boven H, Vink R, de Bel T, van Ginneken B, van der Laak J, Hulsbergen-van de Kaa C, Litjens G. Automated deep-learning system for Gleason grading of prostate cancer using biopsies: A diagnostic study. Lancet Oncol. 2020;21:233&#x2013;241. doi: 10.1016/S1470-2045(19)30739-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S1470-2045(19)30739-9</ArticleId><ArticleId IdType="pubmed">31926805</ArticleId></ArticleIdList></Reference><Reference><Citation>Puderbach M, Eichinger M, Haeselbarth J, Ley S, Kopp-Schneider A, Tuengerthal S, Schmaehl A, Fink C, Plathow C, Wiebel M, et al. Assessment of morphological MRI for pulmonary changes in cystic fibrosis (CF) patients: Comparison to thin-section CT and chest X-ray. Investig. Radiol. 2007;42(10):715&#x2013;724. doi: 10.1097/RLI.0b013e318074fd81.</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/RLI.0b013e318074fd81</ArticleId><ArticleId IdType="pubmed">17984769</ArticleId></ArticleIdList></Reference><Reference><Citation>Rohde M, Nielsen AL, Johansen J, S&#xf8;rensen JA, Nguyen N, Diaz A, Nielsen MK, Asmussen JT, Christiansen JM, Gerke O, et al. Head-to-head comparison of chest x-ray/head and neck MRI, chest CT/head and neck MRI, and 18F-FDG PET/CT for detection of distant metastases and synchronous cancer in oral, pharyngeal, and laryngeal cancer. J. Nucl. Med. 2017;58(12):1919&#x2013;1924. doi: 10.2967/jnumed.117.189704.</Citation><ArticleIdList><ArticleId IdType="doi">10.2967/jnumed.117.189704</ArticleId><ArticleId IdType="pubmed">28572489</ArticleId></ArticleIdList></Reference><Reference><Citation>Schaefer O, Langer M. Detection of recurrent rectal cancer with CT, MRI and PET/CT. Eur. Radiol. 2007;17(8):2044&#x2013;2054. doi: 10.1007/s00330-007-0613-2.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-007-0613-2</ArticleId><ArticleId IdType="pubmed">17404742</ArticleId></ArticleIdList></Reference><Reference><Citation>Khan S, Rahmani H, Shah SAA, Bennamoun M. A guide to convolutional neural networks for computer vision. Synth. Lect. Comput. Vis. 2018;8(1):1&#x2013;207. doi: 10.1007/978-3-031-01821-3.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-031-01821-3</ArticleId></ArticleIdList></Reference><Reference><Citation>LeCun Y, Bottou L, Bengio Y, Haffner P. Gradient-based learning applied to document recognition. Proc. IEEE. 1998;86(11):2278&#x2013;2324. doi: 10.1109/5.726791.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/5.726791</ArticleId></ArticleIdList></Reference><Reference><Citation>de&#xa0;Lima&#xa0;Hedayioglu, F., Coimbra, M. T., &amp; da&#xa0;Silva&#xa0;Mattos, S. A survey of audio processing algorithms for digital stethoscopes. In HEALTHINF 425&#x2013;429, (2009).</Citation></Reference><Reference><Citation>Simonyan, K. &amp; Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, (2014).</Citation></Reference><Reference><Citation>Selvaraju, R. R. et al. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE International Conference on Computer Vision 618&#x2013;626, (2017).</Citation></Reference><Reference><Citation>Chen J, Wu L, Zhang J, et al. Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomography. Sci Rep. 2020;10:19196. doi: 10.1038/s41598-020-76282-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-020-76282-0</ArticleId><ArticleId IdType="pmc">PMC7645624</ArticleId><ArticleId IdType="pubmed">33154542</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang S, Kang B, Ma J, Zeng X, Xiao M, Guo J, Cai M, Yang J, Li Y, Meng X, et al. A deep learning algorithm using CT images to screen for corona virus disease (COVID-19) Eur. Radiol. 2021;31(8):6096&#x2013;6104. doi: 10.1007/s00330-021-07715-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-021-07715-1</ArticleId><ArticleId IdType="pmc">PMC7904034</ArticleId><ArticleId IdType="pubmed">33629156</ArticleId></ArticleIdList></Reference><Reference><Citation>Xu X, Jiang X, Ma C, Du P, Li X, Lv S, Yu L, Ni Q, Chen Y, Su J, et al. A deep learning system to screen novel coronavirus disease 2019 pneumonia. Engineering. 2020;6(10):1122&#x2013;1129. doi: 10.1016/j.eng.2020.04.010.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.eng.2020.04.010</ArticleId><ArticleId IdType="pmc">PMC7320702</ArticleId><ArticleId IdType="pubmed">32837749</ArticleId></ArticleIdList></Reference><Reference><Citation>Beck BR, Shin B, Choi Y, Park S, Kang K, et al. Predicting commercially available antiviral drugs that may act on the novel coronavirus (2019-nCoV), Wuhan, China through a drug-target interaction deep learning model. bioRxiv. 2020;18:784&#x2013;790.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7118541</ArticleId><ArticleId IdType="pubmed">32280433</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang, H. et al. Deep learning based drug screening for novel coronavirus 2019-nCoV. (2020).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7266118</ArticleId><ArticleId IdType="pubmed">32488835</ArticleId></ArticleIdList></Reference><Reference><Citation>Apostolopoulos ID, Mpesiana TA, et al. Covid-19: Automatic detection from x-ray images utilizing transfer learning with convolutional neural networks. Phys. Eng. Sci. Med. 2020;43:1. doi: 10.1007/s13246-020-00865-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s13246-020-00865-4</ArticleId><ArticleId IdType="pmc">PMC7118364</ArticleId><ArticleId IdType="pubmed">32524445</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang, X. et al. A weakly-supervised framework for COVID-19 classification and lesion localization from chest CT. In IEEE Transactions on Medical Imaging, 39(8), 2615&#x2013;2625 10.1109/TMI.2020.2995965 (2020).</Citation><ArticleIdList><ArticleId IdType="pubmed">33156775</ArticleId></ArticleIdList></Reference><Reference><Citation>Narin, A., Kaya, C., &amp; Pamuk, Z. Automatic detection of coronavirus disease (COVID-19) using x-ray images and deep convolutional neural networks. arXiv preprint arXiv:2003.10849, (2020).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8106971</ArticleId><ArticleId IdType="pubmed">33994847</ArticleId></ArticleIdList></Reference><Reference><Citation>Song Y, Zheng S, Li L, Zhang X, Zhang X, Huang Z, Chen J, Wang R, Zhao H, Chong Y, et al. Deep learning enables accurate diagnosis of novel coronavirus (COVID-19) with CT images. IEEE/ACM Trans. Comput. Biol. Bioinform. 2021;18(6):2775&#x2013;2780. doi: 10.1109/TCBB.2021.3065361.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TCBB.2021.3065361</ArticleId><ArticleId IdType="pmc">PMC8851430</ArticleId><ArticleId IdType="pubmed">33705321</ArticleId></ArticleIdList></Reference><Reference><Citation>Ozturk T, et al. Automated detection of COVID-19 cases using deep neural networks with x-ray images. Comput. Biol. Med. 2020;121:103792. doi: 10.1016/j.compbiomed.2020.103792.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2020.103792</ArticleId><ArticleId IdType="pmc">PMC7187882</ArticleId><ArticleId IdType="pubmed">32568675</ArticleId></ArticleIdList></Reference><Reference><Citation>Ardakani AA, Kanafi AR, Acharya UR, Khadem N, Mohammadi A. Application of deep learning technique to manage COVID-19 in routine clinical practice using CT images: Results of 10 convolutional neural networks. Comput. Biol. Med. 2020;121:103795. doi: 10.1016/j.compbiomed.2020.103795.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2020.103795</ArticleId><ArticleId IdType="pmc">PMC7190523</ArticleId><ArticleId IdType="pubmed">32568676</ArticleId></ArticleIdList></Reference><Reference><Citation>Glowacz A, Glowacz Z. Recognition of images of finger skin with application of histogram, image filtration and K-NN classifier. Biocybern. Biomed. Eng. 2016;36(1):95&#x2013;101. doi: 10.1016/j.bbe.2015.12.005.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bbe.2015.12.005</ArticleId></ArticleIdList></Reference><Reference><Citation>Piekarski M, Jaworek-Korjakowska J, Wawrzyniak AI, Gorgon M. Convolutional neural network architecture for beam instabilities identification in synchrotron radiation systems as an anomaly detection problem. Measurement. 2020;165:108116. doi: 10.1016/j.measurement.2020.108116.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.measurement.2020.108116</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang L, Lin ZQ, Wong A. Covid-net: A tailored deep convolutional neural network design for detection of covid-19 cases from chest x-ray images. Sci. Rep. 2020;10(1):1&#x2013;12.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7658227</ArticleId><ArticleId IdType="pubmed">33177550</ArticleId></ArticleIdList></Reference><Reference><Citation>Sethy, P. K. &amp; Behera, S. K. Detection of coronavirus disease (COVID-19) based on deep features. (2020).</Citation></Reference><Reference><Citation>Hemdan, E. E.-D., Shouman, M. A., &amp; Karar, M. E. Covidx-net: A framework of deep learning classifiers to diagnose COVID-19 in x-ray images. arXiv preprint arXiv:2003.11055, (2020).</Citation></Reference><Reference><Citation>Butt C, Gill J, Chun D, Babu BA. Deep learning system to screen coronavirus disease, pneumonia. Appl. Intell. 2019;2020:1.</Citation></Reference><Reference><Citation>Pel&#xe1;ez E, Serrano R, Murillo G, C&#xe1;rdenas W. A comparison of deep learning models for detecting covid-19 in chest x-ray images. Ifac-papersonline. 2021;54(15):358&#x2013;363. doi: 10.1016/j.ifacol.2021.10.282.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ifacol.2021.10.282</ArticleId></ArticleIdList></Reference><Reference><Citation>Iadarola G, Martinelli F, Mercaldo F, Santone A. Towards an interpretable deep learning model for mobile malware detection and family identification. Comput. Security. 2021;105:102198. doi: 10.1016/j.cose.2021.102198.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cose.2021.102198</ArticleId></ArticleIdList></Reference><Reference><Citation>Brunese L, Mercaldo F, Reginelli A, Santone A. Prostate Gleason score detection and cancer treatment through real-time formal verification. IEEE Access. 2019;7:186236&#x2013;186246. doi: 10.1109/ACCESS.2019.2961754.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2019.2961754</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36627326</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>12</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>13</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">2045-2322</ISSN><JournalIssue CitedMedium="Internet"><Volume>13</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>10</Day></PubDate></JournalIssue><Title>Scientific reports</Title><ISOAbbreviation>Sci Rep</ISOAbbreviation></Journal><ArticleTitle>Tongue crack recognition using segmentation based deep learning.</ArticleTitle><Pagination><StartPage>511</StartPage><MedlinePgn>511</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">511</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1038/s41598-022-27210-x</ELocationID><Abstract><AbstractText>Tongue cracks refer to fissures with different depth and shapes on the tongue's surface, which can characterize the pathological characteristics of spleen and stomach. Tongue cracks are of great significance to the objective study of tongue diagnosis. However, tongue cracks are small and complex, existing methods are difficult to extract them effectively. In order to achieve more accurate extraction and identification of tongue crack, this paper proposes to apply a deep learning network based on image segmentation (Segmentation-Based Deep-Learning, SBDL) to extract and identify tongue crack. In addition, we have studied the quantitative description of tongue crack features. Firstly, the pre-processed tongue crack samples were amplified by using adding salt and pepper noise, changing the contrast and horizontal mirroring; secondly, the annotation tool Crack-Tongue was used to label tongue crack; thirdly, the tongue crack extraction model was trained by using SBDL; fourthly,&#xa0;the cracks on the tongue surface were detected and located by the segmentation network, and then the output and features of the segmentation network were put into the decision network for the classification of crack tongue images; finally, the tongue crack segmentation and identification results were quantitatively evaluated. The experimental results showed that the tongue crack extraction and recognition results based on SBDL were better than Mask Region-based Convolutional Neural Network (Mask R-CNN), DeeplabV3+, U-Net, UNet++&#x2009;and Semantic Segmentation with Adversarial Learning (SegAN). This method effectively solved the inaccurate tongue crack extraction caused by the tongue crack's color being close to the surrounding tongue coating's color. This method can achieve better tongue crack extraction and recognition results on a small tongue crack data set and provides a new idea for tongue crack recognition, which is of practical value for tongue diagnosis objectification.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Yan</LastName><ForeName>Jianjun</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Shanghai Key Laboratory of Intelligent Sensing and Detection Technology, East China University of Science and Technology, 130 Meilong Road, Shanghai, 200237, China. jjyan@ecust.edu.cn.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Cai</LastName><ForeName>Jinxing</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Shanghai Key Laboratory of Intelligent Sensing and Detection Technology, East China University of Science and Technology, 130 Meilong Road, Shanghai, 200237, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Xu</LastName><ForeName>Zi</ForeName><Initials>Z</Initials><AffiliationInfo><Affiliation>Shanghai Key Laboratory of Intelligent Sensing and Detection Technology, East China University of Science and Technology, 130 Meilong Road, Shanghai, 200237, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Guo</LastName><ForeName>Rui</ForeName><Initials>R</Initials><AffiliationInfo><Affiliation>Comprehensive Laboratory of Four Diagnostic Methods, Shanghai University of Traditional Chinese Medicine, Shanghai, 201203, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhou</LastName><ForeName>Wei</ForeName><Initials>W</Initials><AffiliationInfo><Affiliation>Shanghai Key Laboratory of Intelligent Sensing and Detection Technology, East China University of Science and Technology, 130 Meilong Road, Shanghai, 200237, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yan</LastName><ForeName>Haixia</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>Comprehensive Laboratory of Four Diagnostic Methods, Shanghai University of Traditional Chinese Medicine, Shanghai, 201203, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Xu</LastName><ForeName>Zhaoxia</ForeName><Initials>Z</Initials><AffiliationInfo><Affiliation>Comprehensive Laboratory of Four Diagnostic Methods, Shanghai University of Traditional Chinese Medicine, Shanghai, 201203, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wang</LastName><ForeName>Yiqin</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Comprehensive Laboratory of Four Diagnostic Methods, Shanghai University of Traditional Chinese Medicine, Shanghai, 201203, China.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>10</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>Sci Rep</MedlineTA><NlmUniqueID>101563288</NlmUniqueID><ISSNLinking>2045-2322</ISSNLinking></MedlineJournalInfo><ChemicalList><Chemical><RegistryNumber>0</RegistryNumber><NameOfSubstance UI="D005079">Excipients</NameOfSubstance></Chemical></ChemicalList><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D014059" MajorTopicYN="N">Tongue</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D005079" MajorTopicYN="N">Excipients</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D005502" MajorTopicYN="N">Food</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D016571" MajorTopicYN="N">Neural Networks, Computer</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D007091" MajorTopicYN="N">Image Processing, Computer-Assisted</DescriptorName></MeshHeading></MeshHeadingList><CoiStatement>The authors declare no competing interests.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>8</Month><Day>28</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>28</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>23</Hour><Minute>17</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>13</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36627326</ArticleId><ArticleId IdType="pmc">PMC9832139</ArticleId><ArticleId IdType="doi">10.1038/s41598-022-27210-x</ArticleId><ArticleId IdType="pii">10.1038/s41598-022-27210-x</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Zhang B, Wang X, You J, et al. Tongue color analysis for medical application[J] Evid.-based Complement. Altern. Med. 2013;2013:264742. doi: 10.1155/2013/264742.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2013/264742</ArticleId><ArticleId IdType="pmc">PMC3659485</ArticleId><ArticleId IdType="pubmed">23737824</ArticleId></ArticleIdList></Reference><Reference><Citation>Li Q, Liu Z. Tongue color analysis and discrimination based on hyperspectral images. Comput. Med. Imaging Graph. 2009;33(3):217&#x2013;221. doi: 10.1016/j.compmedimag.2008.12.004.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compmedimag.2008.12.004</ArticleId><ArticleId IdType="pubmed">19157779</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang Y. Diagnostics of Traditional Chinese Medicine. High Education Press; 2006.</Citation></Reference><Reference><Citation>Kirschbaum B. Atlas of Chinese Tongue Diagnosis. Eastland Press; 2000.</Citation></Reference><Reference><Citation>Rhee S-Y. Tongue area segmentation and tongue cracks detection using CNN[J] J. Korean Inst. Intell. Syst. 2020;30(4):258&#x2013;264. doi: 10.5391/JKIIS.2020.30.4.258.</Citation><ArticleIdList><ArticleId IdType="doi">10.5391/JKIIS.2020.30.4.258</ArticleId></ArticleIdList></Reference><Reference><Citation>Liu, L., Zhang, D. Extracting tongue cracks using the wide line detector[C]. In 1st International Conference on Medical Biometrics. Hong Kong: ICMB, 2008: 49&#x2013;56. 10.1007/978-3-540-77413-6_7 (2008).</Citation></Reference><Reference><Citation>Li, X., Shao, Q., Yao, Q. Cracked tongue recognition using statistic feature[C]. In IEEE International Conference on Bioinformatics &amp; Biomedicine. Sao Paulo: IEEE Computer Society, 2014: 72&#x2013;73. 10.1109/BIBM.2014.6999328 (2014).</Citation></Reference><Reference><Citation>Cao W, Liu Q, He Z. Review of pavement defect detection methods[J] IEEE Access. 2020;8:14531&#x2013;14544. doi: 10.1109/ACCESS.2020.2966881.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2020.2966881</ArticleId></ArticleIdList></Reference><Reference><Citation>Ni B, Liu Z, Cai X, Nappi M, Wan S. Segmentation of ultrasound image sequences by combing a novel deep siamese network with a deformable contour model. Neural Comput. Appl. 2022 doi: 10.1007/s00521-022-07054-2.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00521-022-07054-2</ArticleId></ArticleIdList></Reference><Reference><Citation>Ding S, Wang H, Lu H, Nappi M, Wan S. Two path gland segmentation algorithm of colon pathological image based on local semantic guidance. IEEE J. Biomed. Health Inform. 2022;2022:1&#x2013;8. doi: 10.1109/JBHI.2022.3207874.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/JBHI.2022.3207874</ArticleId><ArticleId IdType="pubmed">36126032</ArticleId></ArticleIdList></Reference><Reference><Citation>Weng H, Li L, Lei H, Luo Z, Li C, Li S. A weakly supervised tooth-mark and crack detection method in tongue image[J] Concurr. Comput. Pract. Expert. 2021;33:e6262. doi: 10.1002/cpe.6262.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/cpe.6262</ArticleId></ArticleIdList></Reference><Reference><Citation>Peng, J., Li, X., Yang, D., et al. Automatic tongue crack extraction for real-time diagnosis[C]. In 2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pp. 694-699. 10.1109/BIBM49941.2020.9313383 (2020).</Citation></Reference><Reference><Citation>Xue, Y., Li, X., Cui, Q., Wang, L., Wu, P. Cracked tongue recognition based on deep features and multiple-instance SVM[C]. Advances in Multimedia Information Processing&#x2013;PCM 2018. Lecture Notes in Computer Science, vol.11165. 10.1007/978-3-030-00767-6_59 (Springer, Cham, 2018)</Citation></Reference><Reference><Citation>Tabernik D, &#x160;ela S, Skvar&#x10d; J, et al. Segmentation-based deep-learning approach for surface-defect detection[J] J. Intell. Manuf. 2020;31(3):759&#x2013;776. doi: 10.1007/s10845-019-01476-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10845-019-01476-x</ArticleId></ArticleIdList></Reference><Reference><Citation>Charmouti B, Junoh AK, Abdurrazzaq A, et al. A new denoising method for removing salt &amp; pepper noise from image[J] Multimed. Tools Appl. 2022;81:3981&#x2013;3993. doi: 10.1007/s11042-021-11615-3.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11042-021-11615-3</ArticleId></ArticleIdList></Reference><Reference><Citation>He, K., Gkioxari, G., Doll&#xe1;r, P. and Girshick, R. Mask R-CNN[C]. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2980-2988. 10.1109/ICCV.2017.322 (2017).</Citation></Reference><Reference><Citation>Ronneberger, O., Fischer, P., Brox, T. U-Net: Convolutional networks for biomedical image segmentation[C]. Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015. Lecture Notes in Computer Science, vol. 9351. 10.1007/978-3-319-24574-4_28 (Springer, Cham, 2015).</Citation></Reference><Reference><Citation>Chen, L., Zhu, Y., Papandreou, G., Schroff, F., Adam, H. Encoder-decoder with atrous separable convolution for semantic image segmentation[C]. Computer Vision-ECCV 2018. Lecture Notes in Computer Science, vol. 11211. 10.1007/978-3-030-01234-2_49 (Springer, Cham, 2018).</Citation></Reference><Reference><Citation>Zhou Z, Md M, Tajbakhsh N, Liang J. UNet plus plus: Redesigning skip connections to exploit multiscale features in image segmentation [J] IEEE Transact. Med. Imaging. 2020;39(6):1856&#x2013;1867. doi: 10.1109/TMI.2019.2959609.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2019.2959609</ArticleId><ArticleId IdType="pmc">PMC7357299</ArticleId><ArticleId IdType="pubmed">31841402</ArticleId></ArticleIdList></Reference><Reference><Citation>Xue Y, Xu T, Zhang H, et al. SegAN: Adversarial network with multi-scale L1 loss for medical image segmentation[J] Neuroinform. 2018;16:383&#x2013;392. doi: 10.1007/s12021-018-9377-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s12021-018-9377-x</ArticleId><ArticleId IdType="pubmed">29725916</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36627175</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1468-2079</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>10</Day></PubDate></JournalIssue><Title>The British journal of ophthalmology</Title><ISOAbbreviation>Br J Ophthalmol</ISOAbbreviation></Journal><ArticleTitle>AI-based clinical assessment of optic nerve head robustness superseding biomechanical testing.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">bjo-2022-322374</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1136/bjo-2022-322374</ELocationID><Abstract><AbstractText Label="BACKGROUND/AIMS" NlmCategory="OBJECTIVE">To use artificial intelligence (AI) to: (1) exploit biomechanical knowledge of the optic nerve head (ONH) from a relatively large population; (2) assess ONH robustness (ie, sensitivity of the ONH to changes in intraocular pressure (IOP)) from a single optical coherence tomography (OCT) volume scan of the ONH without the need for biomechanical testing and (3) identify what critical three-dimensional (3D) structural features dictate ONH robustness.</AbstractText><AbstractText Label="METHODS" NlmCategory="METHODS">316 subjects had their ONHs imaged with OCT before and after acute IOP elevation through ophthalmo-dynamometry. IOP-induced lamina cribrosa (LC) deformations were then mapped in 3D and used to classify ONHs. Those with an average effective LC strain superior to 4% were considered fragile, while those with a strain inferior to 4% robust. Learning from these data, we compared three AI algorithms to predict ONH robustness strictly from a baseline (undeformed) OCT volume: (1) a random forest classifier; (2) an autoencoder and (3) a dynamic graph convolutional neural network (DGCNN). The latter algorithm also allowed us to identify what critical 3D structural features make a given ONH robust.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">All three methods were able to predict ONH robustness from a single OCT volume scan alone and without the need to perform biomechanical testing. The DGCNN (area under the curve (AUC): 0.76&#xb1;0.08) outperformed the autoencoder (AUC: 0.72&#xb1;0.09) and the random forest classifier (AUC: 0.69&#xb1;0.05). Interestingly, to assess ONH robustness, the DGCNN mainly used information from the scleral canal and the LC insertion sites.</AbstractText><AbstractText Label="CONCLUSIONS" NlmCategory="CONCLUSIONS">We propose an AI-driven approach that can assess the robustness of a given ONH solely from a single OCT volume scan of the ONH, and without the need to perform biomechanical testing. Longitudinal studies should establish whether ONH robustness could help us identify fast visual field loss progressors.</AbstractText><AbstractText Label="PRECIS" NlmCategory="CONCLUSIONS">Using geometric deep learning, we can assess optic nerve head robustness (ie, sensitivity to a change in IOP) from a standard OCT scan that might help to identify fast visual field loss progressors.</AbstractText><CopyrightInformation>&#xa9; Author(s) (or their employer(s)) 2023. No commercial re-use. See rights and permissions. Published by BMJ.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Braeu</LastName><ForeName>Fabian A</ForeName><Initials>FA</Initials><AffiliationInfo><Affiliation>Yong Loo Lin School of Medicine, National University of Singapore, Singapore.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Singapore-MIT Alliance for Research and Technology, Singapore.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Ophthalmic Engineering &amp; Innovation Laboratory, Singapore Eye Research Institute, Singapore.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Chuangsuwanich</LastName><ForeName>Thanadet</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>Yong Loo Lin School of Medicine, National University of Singapore, Singapore.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Ophthalmic Engineering &amp; Innovation Laboratory, Singapore Eye Research Institute, Singapore.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Tun</LastName><ForeName>Tin A</ForeName><Initials>TA</Initials><AffiliationInfo><Affiliation>Singapore Eye Research Institute, Singapore.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Singapore National Eye Centre, Singapore.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Perera</LastName><ForeName>Shamira</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Singapore Eye Research Institute, Singapore.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Singapore National Eye Centre, Singapore.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Husain</LastName><ForeName>Rahat</ForeName><Initials>R</Initials><AffiliationInfo><Affiliation>Singapore Eye Research Institute, Singapore.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Singapore National Eye Centre, Singapore.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Thiery</LastName><ForeName>Alexandre H</ForeName><Initials>AH</Initials><AffiliationInfo><Affiliation>Statistics and Applied Probability, National University of Singapore, Singapore.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Aung</LastName><ForeName>Tin</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>Yong Loo Lin School of Medicine, National University of Singapore, Singapore.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Singapore Eye Research Institute, Singapore.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Singapore National Eye Centre, Singapore.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Duke-NUS Graduate Medical School, Singapore.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Barbastathis</LastName><ForeName>George</ForeName><Initials>G</Initials><AffiliationInfo><Affiliation>Singapore-MIT Alliance for Research and Technology, Singapore.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Mechanical Engineering, Massachusetts Institute of Technology, Cambridge, Massachusetts, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Girard</LastName><ForeName>Micha&#xeb;l J A</ForeName><Initials>MJA</Initials><Identifier Source="ORCID">0000-0003-4408-5918</Identifier><AffiliationInfo><Affiliation>Ophthalmic Engineering &amp; Innovation Laboratory, Singapore Eye Research Institute, Singapore mgirard@ophthalmic.engineering.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Duke-NUS Graduate Medical School, Singapore.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Institute for Molecular and Clinical Ophthalmology, Basel, Switzerland.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>10</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>Br J Ophthalmol</MedlineTA><NlmUniqueID>0421041</NlmUniqueID><ISSNLinking>0007-1161</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">glaucoma</Keyword><Keyword MajorTopicYN="N">imaging</Keyword><Keyword MajorTopicYN="N">intraocular pressure</Keyword><Keyword MajorTopicYN="N">optic nerve</Keyword></KeywordList><CoiStatement>Competing interests: MJAG and AHT are the co-founders of the AI start-up company Abyss Processing.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>8</Month><Day>9</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>22</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>21</Hour><Minute>3</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36627175</ArticleId><ArticleId IdType="doi">10.1136/bjo-2022-322374</ArticleId><ArticleId IdType="pii">bjo-2022-322374</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36625882</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1432-1084</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>10</Day></PubDate></JournalIssue><Title>European radiology</Title><ISOAbbreviation>Eur Radiol</ISOAbbreviation></Journal><ArticleTitle>Automated lung cancer assessment on 18F-PET/CT using Retina U-Net and anatomical region segmentation.</ArticleTitle><ELocationID EIdType="doi" ValidYN="Y">10.1007/s00330-022-09332-y</ELocationID><Abstract><AbstractText Label="OBJECTIVES" NlmCategory="OBJECTIVE">To develop and test a Retina U-Net algorithm for the detection of primary lung tumors and associated metastases of all stages on FDG-PET/CT.</AbstractText><AbstractText Label="METHODS" NlmCategory="METHODS">A data set consisting of 364 FDG-PET/CTs of patients with histologically confirmed lung cancer was used for algorithm development and internal testing. The data&#xa0;set comprised tumors of all stages. All lung tumors (T), lymphatic metastases (N), and distant metastases (M) were manually segmented as 3D volumes using whole-body PET/CT series. The data&#xa0;set was split into a training (n = 216), validation (n = 74), and internal test data&#xa0;set (n = 74). Detection performance for all lesion types at multiple classifier thresholds was evaluated and false-positive-findings-per-case (FP/c) calculated. Next, detected lesions were assigned to categories T, N, or M using an automated anatomical region segmentation. Furthermore, reasons for FPs were visually assessed and analyzed. Finally, performance was tested on 20 PET/CTs from another institution.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">Sensitivity for T lesions was 86.2% (95% CI: 77.2-92.7) at a FP/c of 2.0 on the internal test set. The anatomical correlate to most FPs was the physiological activity of bone marrow (16.8%). TNM categorization based on the anatomical region approach was correct in 94.3% of lesions. Performance on the external test set confirmed the good performance of the algorithm (overall detection rate = 88.8% (95% CI: 82.5-93.5%) and FP/c = 2.7).</AbstractText><AbstractText Label="CONCLUSIONS" NlmCategory="CONCLUSIONS">Retina U-Nets are a valuable tool for tumor detection tasks on PET/CT and can form the backbone of reading assistance tools in this field. FPs have anatomical correlates that can lead the way to further algorithm improvements. The code is publicly available.</AbstractText><AbstractText Label="KEY POINTS" NlmCategory="CONCLUSIONS">&#x2022; Detection of malignant lesions in PET/CT with Retina U-Net is feasible. &#x2022; All false-positive findings had anatomical correlates, physiological bone marrow activity being the most prevalent. &#x2022; Retina U-Nets can build the backbone for tools assisting imaging professionals in lung tumor staging.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Weikert</LastName><ForeName>T</ForeName><Initials>T</Initials><Identifier Source="ORCID">0000-0001-9274-053X</Identifier><AffiliationInfo><Affiliation>Department of Radiology, University Hospital Basel, University of Basel, Petersgraben 4, 4031, Basel, Switzerland. thomas.weikert@usb.ch.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Jaeger</LastName><ForeName>P F</ForeName><Initials>PF</Initials><AffiliationInfo><Affiliation>Division of Medical Image Computing, German Cancer Research Center, Im Neuenheimer Feld 223, 69120, Heidelberg, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yang</LastName><ForeName>S</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Department of Radiology, University Hospital Basel, University of Basel, Petersgraben 4, 4031, Basel, Switzerland.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Baumgartner</LastName><ForeName>M</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Division of Medical Image Computing, German Cancer Research Center, Im Neuenheimer Feld 223, 69120, Heidelberg, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Breit</LastName><ForeName>H C</ForeName><Initials>HC</Initials><AffiliationInfo><Affiliation>Department of Radiology, University Hospital Basel, University of Basel, Petersgraben 4, 4031, Basel, Switzerland.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Winkel</LastName><ForeName>D J</ForeName><Initials>DJ</Initials><AffiliationInfo><Affiliation>Department of Radiology, University Hospital Basel, University of Basel, Petersgraben 4, 4031, Basel, Switzerland.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Sommer</LastName><ForeName>G</ForeName><Initials>G</Initials><AffiliationInfo><Affiliation>Institute of Radiology and Nuclear Medicine, Hirslanden Klinik St. Anna, St. Anna-Strasse 32, 6006, Lucerne, Switzerland.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Stieltjes</LastName><ForeName>B</ForeName><Initials>B</Initials><AffiliationInfo><Affiliation>Department of Radiology, University Hospital Basel, University of Basel, Petersgraben 4, 4031, Basel, Switzerland.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Thaiss</LastName><ForeName>W</ForeName><Initials>W</Initials><AffiliationInfo><Affiliation>Department of Nuclear Medicine, University Hospital Ulm, Albert-Einstein-Allee 23, 89081, Ulm, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Bremerich</LastName><ForeName>J</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Department of Radiology, University Hospital Basel, University of Basel, Petersgraben 4, 4031, Basel, Switzerland.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Maier-Hein</LastName><ForeName>K H</ForeName><Initials>KH</Initials><AffiliationInfo><Affiliation>Division of Medical Image Computing, German Cancer Research Center, Im Neuenheimer Feld 223, 69120, Heidelberg, Germany.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiation Oncology, Pattern Analysis and Learning Group, Heidelberg University Hospital, Im Neuenheimer Feld 400, 69120, Heidelberg, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Sauter</LastName><ForeName>A W</ForeName><Initials>AW</Initials><AffiliationInfo><Affiliation>Department of Radiology, University Hospital Basel, University of Basel, Petersgraben 4, 4031, Basel, Switzerland.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>10</Day></ArticleDate></Article><MedlineJournalInfo><Country>Germany</Country><MedlineTA>Eur Radiol</MedlineTA><NlmUniqueID>9114774</NlmUniqueID><ISSNLinking>0938-7994</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Artificial intelligence</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Lung neoplasm</Keyword><Keyword MajorTopicYN="N">Neoplasm staging</Keyword><Keyword MajorTopicYN="N">PET/CT</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>4</Month><Day>22</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>10</Month><Day>17</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>8</Month><Day>13</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>11</Hour><Minute>14</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36625882</ArticleId><ArticleId IdType="doi">10.1007/s00330-022-09332-y</ArticleId><ArticleId IdType="pii">10.1007/s00330-022-09332-y</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>De Wever W, Ceyssens S, Mortelmans L et al (2007) Additional value of PET-CT in the staging of lung cancer: comparison with CT alone, PET alone and visual correlation of PET and CT. Eur Radiol 17:23&#x2013;32</Citation></Reference><Reference><Citation>Shim SS, Lee KS, Kim B-T et al (2005) Non&#x2013;Small cell lung cancer: prospective comparison of integrated FDG PET/CT and CT alone for preoperative staging. Radiology 236:1011&#x2013;1019</Citation></Reference><Reference><Citation>Kandathil A, Kay FU, Butt YM, Wachsmann JW, Subramaniam RM (2018) Role of FDG PET/CT in the eighth edition of TNM staging of non&#x2013;small cell lung cancer. Radiographics 38:2134&#x2013;2149</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/rg.2018180060</ArticleId></ArticleIdList></Reference><Reference><Citation>Heineman DJ, Daniels JM, Schreurs WH (2017) Clinical staging of NSCLC: current evidence and implications for adjuvant chemotherapy. Ther Adv Med Oncol 9:599&#x2013;609</Citation><ArticleIdList><ArticleId IdType="doi">10.1177/1758834017722746</ArticleId></ArticleIdList></Reference><Reference><Citation>Karantanis D, Kalkanis D, Czernin J et al (2014) Perceived misinterpretation rates in oncologic 18F-FDG PET/CT studies: a survey of referring physicians. J Nucl Med 55:1925&#x2013;1929</Citation></Reference><Reference><Citation>Woodard GA, Jones KD, Jablons DM (2016) Lung cancer staging and prognosis. Springer, Cham, pp 47&#x2013;75</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-319-40389-2_3</ArticleId></ArticleIdList></Reference><Reference><Citation>Yang P, Allen MS, Aubry MC et al (2005) Clinical features of 5,628 primary lung cancer patients. Chest 128:452&#x2013;462</Citation></Reference><Reference><Citation>Masood A, Sheng B, Li P, Hou X, Wei X, Qin J, Feng D (2018) Computer-assisted decision support system in pulmonary cancer detection and stage classification on CT images. J Biomed Inform 79:117&#x2013;128</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jbi.2018.01.005</ArticleId></ArticleIdList></Reference><Reference><Citation>Ardila D, Kiraly AP, Bharadwaj S et al (2019) End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography. Nat Med 25:954&#x2013;961</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41591-019-0447-x</ArticleId></ArticleIdList></Reference><Reference><Citation>Weikert T, Akinci D&#x2019;Antonoli T, Bremerich J, Stieltjes B, Sommer G, Sauter AW (2019) Evaluation of an AI-powered lung nodule algorithm for detection and 3D segmentation of primary lung tumors. Contrast Media Mol Imaging 2019:1&#x2013;10</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2019/1517208</ArticleId></ArticleIdList></Reference><Reference><Citation>Teramoto A, Fujita H, Takahashi K, Yamamuro O, Tamaki T, Nishio M, Kobayashi T (2014) Hybrid method for the detection of pulmonary nodules using positron emission tomography/computed tomography: a preliminary study. Int J Comput Assist Radiol Surg 9:59&#x2013;69</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11548-013-0910-y</ArticleId></ArticleIdList></Reference><Reference><Citation>Teramoto A, Fujita H, Yamamuro O, Tamaki T (2016) Automated detection of pulmonary nodules in PET/CT images: ensemble false-positive reduction using a convolutional neural network technique. Med Phys 43:2821&#x2013;2827</Citation><ArticleIdList><ArticleId IdType="doi">10.1118/1.4948498</ArticleId></ArticleIdList></Reference><Reference><Citation>Schwyzer M, Ferraro DA, Muehlematter UJ et al (2018) Automated detection of lung cancer at ultralow dose PET/CT by deep neural networks &#x2013; initial results. Lung Cancer 126:170&#x2013;173</Citation></Reference><Reference><Citation>Zhao J, Ji G, Qiang Y, Han X, Pei B, Shi Z (2015) A new method of detecting pulmonary nodules with PET/CT Based on an improved watershed algorithm. PLoS One 10:e0123694</Citation></Reference><Reference><Citation>Jaeger PF, Kohl SAA, Bickelhaupt S, Isensee F, Kuder TA, Schlemmer H-P, Maier-Hein KH Retina U-Net: embarrassingly simple exploitation of segmentation supervision for medical object detection. arXiv:1811.08661</Citation></Reference><Reference><Citation>Goldstraw P, Chansky K, Crowley J et al (2016) The IASLC lung cancer staging project: proposals for revision of the TNM stage groupings in the forthcoming (eighth) edition of the TNM classification for lung cancer. J Thorac Oncol 11:39&#x2013;51</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jtho.2015.09.009</ArticleId></ArticleIdList></Reference><Reference><Citation>Hofmanninger J GitHub - JoHof/lungmask: Automated lung segmentation in CT. https://github.com/JoHof/lungmask . Accessed 04 Jan 2023</Citation></Reference><Reference><Citation>Pedregosa F, Varoquaux G, Gramfort A et al (2011) Scikit-learn: machine learning in python. J Mach Learn Res 12:2825&#x2013;2830</Citation></Reference><Reference><Citation>Kirienko M, Sollini M, Silvestri G et al (2018) Convolutional neural networks promising in lung cancer T-parameter assessment on baseline FDG-PET/CT. Contrast Media Mol Imaging 2018:1&#x2013;6</Citation></Reference><Reference><Citation>Borrelli P, Ly J, Kaboteh R, Ul&#xe9;n J, Enqvist O, Tr&#xe4;g&#xe5;rdh E, Edenbrandt L (2021) AI-based detection of lung lesions in [18 F]FDG PET-CT from lung cancer patients. EJNMMI Phys. https://doi.org/10.1186/S40658-021-00376-5</Citation></Reference><Reference><Citation>Paidpally V, Mercier G, Shah BA, Senthamizhchelvan S, Subramaniam RM (2014) Interreader agreement and variability of FDG PET volumetric parameters in human solid tumors. AJR Am J Roentgenol 202:406&#x2013;412</Citation></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36625762</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1879-1123</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>10</Day></PubDate></JournalIssue><Title>Journal of the American Society for Mass Spectrometry</Title><ISOAbbreviation>J Am Soc Mass Spectrom</ISOAbbreviation></Journal><ArticleTitle>Deep Learning on Multimodal Chemical and Whole Slide Imaging Data for Predicting Prostate Cancer Directly from Tissue Images.</ArticleTitle><ELocationID EIdType="doi" ValidYN="Y">10.1021/jasms.2c00254</ELocationID><Abstract><AbstractText>Prostate cancer is one of the most common cancers globally and is the second most common cancer in the male population in the US. Here we develop a study based on correlating the hematoxylin and eosin (H&amp;E)-stained biopsy data with MALDI mass-spectrometric imaging data of the corresponding tissue to determine the cancerous regions and their unique chemical signatures and variations of the predicted regions with original pathological annotations. We obtain features from high-resolution optical micrographs of whole slide H&amp;E stained data through deep learning and spatially register them with mass spectrometry imaging (MSI) data to correlate the chemical signature with the tissue anatomy of the data. We then use the learned correlation to predict prostate cancer from observed H&amp;E images using trained coregistered MSI data. This multimodal approach can predict cancerous regions with &#x223c;80% accuracy, which indicates a correlation between optical H&amp;E features and chemical information found in MSI. We show that such paired multimodal data can be used for training feature extraction networks on H&amp;E data which bypasses the need to acquire expensive MSI data and eliminates the need for manual annotation saving valuable time. Two chemical biomarkers were also found to be predicting the ground truth cancerous regions. This study shows promise in generating improved patient treatment trajectories by predicting prostate cancer directly from readily available H&amp;E-stained biopsy images aided by coregistered MSI data.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Haque</LastName><ForeName>Md Inzamam Ul</ForeName><Initials>MIU</Initials><Identifier Source="ORCID">0000-0003-3945-1792</Identifier><AffiliationInfo><Affiliation>The Bredesen Center, University of Tennessee, Knoxville, Tennessee 37996, United States.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Mukherjee</LastName><ForeName>Debangshu</ForeName><Initials>D</Initials><Identifier Source="ORCID">0000-0003-0437-9807</Identifier><AffiliationInfo><Affiliation>Computational Sciences and Engineering Division, Oak Ridge National Laboratory, Oak Ridge, Tennessee 37830, United States.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Stopka</LastName><ForeName>Sylwia A</ForeName><Initials>SA</Initials><Identifier Source="ORCID">0000-0003-3761-6899</Identifier><AffiliationInfo><Affiliation>Department of Neurosurgery, Brigham and Women's Hospital, Harvard Medical School, Boston, Massachusetts 02115, United States.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, Brigham and Women's Hospital, Harvard Medical School, Boston, Massachusetts 02115, United States.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Agar</LastName><ForeName>Nathalie Y R</ForeName><Initials>NYR</Initials><Identifier Source="ORCID">0000-0003-3149-3146</Identifier><AffiliationInfo><Affiliation>Department of Neurosurgery, Brigham and Women's Hospital, Harvard Medical School, Boston, Massachusetts 02115, United States.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, Brigham and Women's Hospital, Harvard Medical School, Boston, Massachusetts 02115, United States.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Cancer Biology, Dana-Farber Cancer Institute, Boston, Massachusetts 02115, United States.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Hinkle</LastName><ForeName>Jacob</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Computational Sciences and Engineering Division, Oak Ridge National Laboratory, Oak Ridge, Tennessee 37830, United States.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Ovchinnikova</LastName><ForeName>Olga S</ForeName><Initials>OS</Initials><Identifier Source="ORCID">0000-0001-8935-2309</Identifier><AffiliationInfo><Affiliation>The Bredesen Center, University of Tennessee, Knoxville, Tennessee 37996, United States.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>10</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>J Am Soc Mass Spectrom</MedlineTA><NlmUniqueID>9010412</NlmUniqueID><ISSNLinking>1044-0305</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>10</Hour><Minute>23</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36625762</ArticleId><ArticleId IdType="doi">10.1021/jasms.2c00254</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36624404</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>11</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>13</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">1471-2342</ISSN><JournalIssue CitedMedium="Internet"><Volume>23</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>09</Day></PubDate></JournalIssue><Title>BMC medical imaging</Title><ISOAbbreviation>BMC Med Imaging</ISOAbbreviation></Journal><ArticleTitle>Acceleration of knee magnetic resonance imaging using a combination of compressed sensing and commercially available deep learning reconstruction: a preliminary study.</ArticleTitle><Pagination><StartPage>5</StartPage><MedlinePgn>5</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">5</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1186/s12880-023-00962-2</ELocationID><Abstract><AbstractText Label="PURPOSE" NlmCategory="OBJECTIVE">To evaluate whether deep learning reconstruction (DLR) accelerates the acquisition of 1.5-T magnetic resonance imaging (MRI) knee data without image deterioration.</AbstractText><AbstractText Label="MATERIALS AND METHODS" NlmCategory="METHODS">Twenty-one healthy volunteers underwent MRI of the right knee on a 1.5-T MRI scanner. Proton-density-weighted images with one or four numbers of signal averages (NSAs) were obtained via compressed sensing, and DLR was applied to the images with 1 NSA to obtain 1NSA-DLR images. The 1NSA-DLR and 4NSA images were compared objectively (by deriving the signal-to-noise ratios of the lateral and the medial menisci and the contrast-to-noise ratios of the lateral and the medial menisci and articular cartilages) and subjectively (in terms of the visibility of the anterior cruciate ligament, the medial collateral ligament, the medial and lateral menisci, and bone) and in terms of image noise, artifacts, and overall diagnostic acceptability. The paired t-test and Wilcoxon signed-rank test were used for statistical analyses.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">The 1NSA-DLR images were obtained within 100&#xa0;s. The signal-to-noise ratios (lateral: 3.27&#x2009;&#xb1;&#x2009;0.30 vs. 1.90&#x2009;&#xb1;&#x2009;0.13, medial: 2.71&#x2009;&#xb1;&#x2009;0.24 vs. 1.80&#x2009;&#xb1;&#x2009;0.15, both p&#x2009;&lt;&#x2009;0.001) and contrast-to-noise ratios (lateral: 2.61&#x2009;&#xb1;&#x2009;0.51 vs. 2.18&#x2009;&#xb1;&#x2009;0.58, medial 2.19&#x2009;&#xb1;&#x2009;0.32 vs. 1.97&#x2009;&#xb1;&#x2009;0.36, both p&#x2009;&lt;&#x2009;0.001) were significantly higher for 1NSA-DLR than 4NSA images. Subjectively, all anatomical structures (except bone) were significantly clearer on the 1NSA-DLR than on the 4NSA images. Also, in the former images, the noise was lower, and the overall diagnostic acceptability was higher.</AbstractText><AbstractText Label="CONCLUSION" NlmCategory="CONCLUSIONS">Compared with the 4NSA images, the 1NSA-DLR images exhibited less noise, higher overall image quality, and allowed more precise visualization of the menisci and ligaments.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Akai</LastName><ForeName>Hiroyuki</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>Department of Radiology, Institute of Medical Science, University of Tokyo, 4-6-1 Shirokanedai, Minato-ku, Tokyo, 108-8639, Japan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, International University of Health and Welfare Narita Hospital, 852 Hatakeda, Narita, Chiba, 286-0124, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yasaka</LastName><ForeName>Koichiro</ForeName><Initials>K</Initials><AffiliationInfo><Affiliation>Department of Radiology, International University of Health and Welfare Narita Hospital, 852 Hatakeda, Narita, Chiba, 286-0124, Japan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, Graduate School of Medicine, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8655, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Sugawara</LastName><ForeName>Haruto</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>Department of Radiology, Institute of Medical Science, University of Tokyo, 4-6-1 Shirokanedai, Minato-ku, Tokyo, 108-8639, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Tajima</LastName><ForeName>Taku</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>Department of Radiology, International University of Health and Welfare Narita Hospital, 852 Hatakeda, Narita, Chiba, 286-0124, Japan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, International University of Health and Welfare Mita Hospital, 1-4-3 Mita, Minato-ku, Tokyo, 108-8329, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kamitani</LastName><ForeName>Masaru</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Radiology, Institute of Medical Science, University of Tokyo, 4-6-1 Shirokanedai, Minato-ku, Tokyo, 108-8639, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Furuta</LastName><ForeName>Toshihiro</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>Department of Radiology, Institute of Medical Science, University of Tokyo, 4-6-1 Shirokanedai, Minato-ku, Tokyo, 108-8639, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Akahane</LastName><ForeName>Masaaki</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Radiology, International University of Health and Welfare Narita Hospital, 852 Hatakeda, Narita, Chiba, 286-0124, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yoshioka</LastName><ForeName>Naoki</ForeName><Initials>N</Initials><AffiliationInfo><Affiliation>Department of Radiology, International University of Health and Welfare Narita Hospital, 852 Hatakeda, Narita, Chiba, 286-0124, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Ohtomo</LastName><ForeName>Kuni</ForeName><Initials>K</Initials><AffiliationInfo><Affiliation>International University of Health and Welfare, 2600-1 Kiakanemaru, Ohtawara, Tochigi, 324-8501, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Abe</LastName><ForeName>Osamu</ForeName><Initials>O</Initials><AffiliationInfo><Affiliation>Department of Radiology, Graduate School of Medicine, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8655, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kiryu</LastName><ForeName>Shigeru</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Department of Radiology, International University of Health and Welfare Narita Hospital, 852 Hatakeda, Narita, Chiba, 286-0124, Japan. kiryu-tky@umin.ac.jp.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>09</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>BMC Med Imaging</MedlineTA><NlmUniqueID>100968553</NlmUniqueID><ISSNLinking>1471-2342</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D007719" MajorTopicYN="N">Knee Joint</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D008279" MajorTopicYN="N">Magnetic Resonance Imaging</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D059629" MajorTopicYN="N">Signal-To-Noise Ratio</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000054" MajorTopicYN="N">Acceleration</DescriptorName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Artificial intelligence</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Knee</Keyword><Keyword MajorTopicYN="N">Magnetic resonance imaging</Keyword></KeywordList><CoiStatement>S.K. disclosed grants from Canon Medical Systems. All other authors declare that they have no competing interests.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>5</Month><Day>12</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2023</Year><Month>1</Month><Day>4</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>23</Hour><Minute>29</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36624404</ArticleId><ArticleId IdType="pmc">PMC9827641</ArticleId><ArticleId IdType="doi">10.1186/s12880-023-00962-2</ArticleId><ArticleId IdType="pii">10.1186/s12880-023-00962-2</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Petron DJ, Greis PE, Aoki SK, et al. Use of knee magnetic resonance imaging by primary care physicians in patients aged 40 years and older. Sports Health. 2010;2:385&#x2013;90. doi: 10.1177/1941738110377420.</Citation><ArticleIdList><ArticleId IdType="doi">10.1177/1941738110377420</ArticleId><ArticleId IdType="pmc">PMC3445052</ArticleId><ArticleId IdType="pubmed">23015964</ArticleId></ArticleIdList></Reference><Reference><Citation>Pai DR, Strouse PJ. MRI of the pediatric knee. Am J Roentgenol. 2011;196:1019&#x2013;27. doi: 10.2214/AJR.10.6117.</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/AJR.10.6117</ArticleId><ArticleId IdType="pubmed">21512066</ArticleId></ArticleIdList></Reference><Reference><Citation>Walczak BE, McCulloch PC, Kang RW, Zelazny A, Tedeschi F, Cole BJ. Abnormal findings on knee magnetic resonance imaging in asymptomatic NBA players. J Knee Surg. 2008;21:27&#x2013;33. doi: 10.1055/s-0030-1247788.</Citation><ArticleIdList><ArticleId IdType="doi">10.1055/s-0030-1247788</ArticleId><ArticleId IdType="pubmed">18300668</ArticleId></ArticleIdList></Reference><Reference><Citation>Chien A, Weaver JS, Kinne E, Omar I. Magnetic resonance imaging of the knee. Pol J Radiol. 2020;85:e509&#x2013;31. doi: 10.5114/pjr.2020.99415.</Citation><ArticleIdList><ArticleId IdType="doi">10.5114/pjr.2020.99415</ArticleId><ArticleId IdType="pmc">PMC7571514</ArticleId><ArticleId IdType="pubmed">33101555</ArticleId></ArticleIdList></Reference><Reference><Citation>Yusuf E, Kortekaas MC, Watt I, Huizinga TW, Kloppenburg M. Do knee abnormalities visualised on MRI explain knee pain in knee osteoarthritis? A systematic review. Ann Rheum Dis. 2011;70:60&#x2013;7. doi: 10.1136/ard.2010.131904.</Citation><ArticleIdList><ArticleId IdType="doi">10.1136/ard.2010.131904</ArticleId><ArticleId IdType="pubmed">20829200</ArticleId></ArticleIdList></Reference><Reference><Citation>Kwok WE, Zhong J, You Z, Seo G, Totterman SM. A four-element phased array coil for high resolution and parallel MR imaging of the knee. Magn Reson Imaging. 2003;21:961&#x2013;7. doi: 10.1016/S0730-725X(03)00202-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0730-725X(03)00202-9</ArticleId><ArticleId IdType="pubmed">14684197</ArticleId></ArticleIdList></Reference><Reference><Citation>Zuo J, Li X, Banerjee S, Han E, Majumdar S. Parallel imaging of knee cartilage at 3 Tesla. J Magn Reson Imaging. 2007;26:1001&#x2013;9. doi: 10.1002/jmri.21122.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmri.21122</ArticleId><ArticleId IdType="pubmed">17896394</ArticleId></ArticleIdList></Reference><Reference><Citation>Deshmane A, Gulani V, Griswold MA, Seiberlich N. Parallel MR imaging. J Magn Reson Imaging. 2012;36:55&#x2013;72. doi: 10.1002/jmri.23639.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmri.23639</ArticleId><ArticleId IdType="pmc">PMC4459721</ArticleId><ArticleId IdType="pubmed">22696125</ArticleId></ArticleIdList></Reference><Reference><Citation>Fritz J, Fritz B, Thawait GG, Meyer H, Gilson WD, Raithel E. Three-dimensional CAIPIRINHA SPACE TSE for 5-minute high-resolution MRI of the knee. Invest Radiol. 2016;51:609&#x2013;17. doi: 10.1097/RLI.0000000000000287.</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/RLI.0000000000000287</ArticleId><ArticleId IdType="pubmed">27187045</ArticleId></ArticleIdList></Reference><Reference><Citation>Kijowski R, Rosas H, Samsonov A, King K, Peters R, Liu F. Knee imaging: Rapid three-dimensional fast spin-echo using compressed sensing. J Magn Reson Imaging. 2017;45:1712&#x2013;22. doi: 10.1002/jmri.25507.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmri.25507</ArticleId><ArticleId IdType="pmc">PMC5388597</ArticleId><ArticleId IdType="pubmed">27726244</ArticleId></ArticleIdList></Reference><Reference><Citation>Lustig M, Donoho D, Pauly JM, Sparse MRI. The application of compressed sensing for rapid MR imaging. Magn Reson Med. 2007;58:1182&#x2013;95. doi: 10.1002/mrm.21391.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.21391</ArticleId><ArticleId IdType="pubmed">17969013</ArticleId></ArticleIdList></Reference><Reference><Citation>Barth M, Breuer F, Koopmans PJ, Norris DG, Poser BA. Simultaneous multislice (SMS) imaging techniques. Magn Reson Med. 2016;75:63&#x2013;81. doi: 10.1002/mrm.25897.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.25897</ArticleId><ArticleId IdType="pmc">PMC4915494</ArticleId><ArticleId IdType="pubmed">26308571</ArticleId></ArticleIdList></Reference><Reference><Citation>Del Grande F, Rashidi A, Luna R, et al. Five-minute five-sequence knee MRI using combined simultaneous multislice and parallel imaging acceleration: comparison with 10-Minute parallel imaging knee MRI. Radiology. 2021;299:635&#x2013;46. doi: 10.1148/radiol.2021203655.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2021203655</ArticleId><ArticleId IdType="pubmed">33825510</ArticleId></ArticleIdList></Reference><Reference><Citation>Yasaka K, Akai H, Kunimatsu A, Kiryu S, Abe O. Deep learning with convolutional neural network in radiology. Jpn J Radiol. 2018;36:257&#x2013;72. doi: 10.1007/s11604-018-0726-3.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11604-018-0726-3</ArticleId><ArticleId IdType="pubmed">29498017</ArticleId></ArticleIdList></Reference><Reference><Citation>Weston AD, Korfiatis P, Kline TL, et al. Automated abdominal segmentation of CT scans for body composition analysis using deep learning. Radiology. 2019;290:669&#x2013;79. doi: 10.1148/radiol.2018181432.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2018181432</ArticleId><ArticleId IdType="pubmed">30526356</ArticleId></ArticleIdList></Reference><Reference><Citation>Trebeschi S, van Griethuysen JJM, Lambregts DMJ, et al. Deep learning for fully-automated localization and segmentation of rectal Cancer on multiparametric MR. Sci Rep. 2017;7:5301. doi: 10.1038/s41598-017-05728-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-017-05728-9</ArticleId><ArticleId IdType="pmc">PMC5509680</ArticleId><ArticleId IdType="pubmed">28706185</ArticleId></ArticleIdList></Reference><Reference><Citation>Yasaka K, Akai H, Abe O, Kiryu S. Deep learning with convolutional neural network for differentiation of liver masses at dynamic contrast-enhanced CT: a preliminary study. Radiology. 2018;286:887&#x2013;96. doi: 10.1148/radiol.2017170706.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2017170706</ArticleId><ArticleId IdType="pubmed">29059036</ArticleId></ArticleIdList></Reference><Reference><Citation>Kiryu S, Yasaka K, Akai H, et al. Deep learning to differentiate parkinsonian disorders separately using single midsagittal MR imaging: a proof of concept study. Eur Radiol. 2019;29:6891&#x2013;9. doi: 10.1007/s00330-019-06327-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-019-06327-0</ArticleId><ArticleId IdType="pubmed">31264017</ArticleId></ArticleIdList></Reference><Reference><Citation>Kidoh M, Shinoda K, Kitajima M, et al. Deep learning based noise reduction for brain MR Imaging: tests on phantoms and healthy volunteers. Magn Reson Med Sci. 2020;19:195&#x2013;206. doi: 10.2463/mrms.mp.2019-0018.</Citation><ArticleIdList><ArticleId IdType="doi">10.2463/mrms.mp.2019-0018</ArticleId><ArticleId IdType="pmc">PMC7553817</ArticleId><ArticleId IdType="pubmed">31484849</ArticleId></ArticleIdList></Reference><Reference><Citation>Herrmann J, Koerzdoerfer G, Nickel D, et al. Feasibility and implementation of a deep learning MR reconstruction for TSE sequences in musculoskeletal imaging. Diagnostics (Basel) 2021;11:1484. doi: 10.3390/diagnostics11081484.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/diagnostics11081484</ArticleId><ArticleId IdType="pmc">PMC8394583</ArticleId><ArticleId IdType="pubmed">34441418</ArticleId></ArticleIdList></Reference><Reference><Citation>Naganawa S, Nakamichi R, Ichikawa K, et al. MR imaging of endolymphatic Hydrops: utility of iHYDROPS-Mi2 combined with deep learning reconstruction denoising. Magn Reson Med Sci. 2021;20:272&#x2013;9. doi: 10.2463/mrms.mp.2020-0082.</Citation><ArticleIdList><ArticleId IdType="doi">10.2463/mrms.mp.2020-0082</ArticleId><ArticleId IdType="pmc">PMC8424026</ArticleId><ArticleId IdType="pubmed">32830173</ArticleId></ArticleIdList></Reference><Reference><Citation>Wong S, Steinbach L, Zhao J, Stehling C, Ma CB, Link TM. Comparative study of imaging at 3.0 T versus 1.5 T of the knee. Skeletal Radiol. 2009;38:761&#x2013;9. doi: 10.1007/s00256-009-0683-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00256-009-0683-0</ArticleId><ArticleId IdType="pmc">PMC2704948</ArticleId><ArticleId IdType="pubmed">19350234</ArticleId></ArticleIdList></Reference><Reference><Citation>Helito CP, Helito PV, Costa HP, et al. MRI evaluation of the anterolateral ligament of the knee: assessment in routine 1.5-T scans. Skeletal Radiol. 2014;43:1421&#x2013;7. doi: 10.1007/s00256-014-1966-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00256-014-1966-7</ArticleId><ArticleId IdType="pubmed">25085699</ArticleId></ArticleIdList></Reference><Reference><Citation>Yasaka K, Tanishima T, Ohtake Y, et al. Deep learning reconstruction for 1.5 T cervical spine MRI: effect on interobserver agreement in the evaluation of degenerative changes. Eur Radiol. 2022;32:6118&#x2013;25. doi: 10.1007/s00330-022-08729-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-022-08729-z</ArticleId><ArticleId IdType="pubmed">35348861</ArticleId></ArticleIdList></Reference><Reference><Citation>Tajima T, Akai H, Sugawara H, et al. Feasibility of accelerated whole-body diffusion-weighted imaging using a deep learning-based noise-reduction technique in patients with prostate cancer. Magn Reson Imaging. 2022;92:169&#x2013;79. doi: 10.1016/j.mri.2022.06.014.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.mri.2022.06.014</ArticleId><ArticleId IdType="pubmed">35772583</ArticleId></ArticleIdList></Reference><Reference><Citation>Altahawi FF, Blount KJ, Morley NP, Raithel E, Omar IM. Comparing an accelerated 3D fast spin-echo sequence (CS-SPACE) for knee 3-T magnetic resonance imaging with traditional 3D fast spin-echo (SPACE) and routine 2D sequences. Skeletal Radiol. 2017;46:7&#x2013;15. doi: 10.1007/s00256-016-2490-8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00256-016-2490-8</ArticleId><ArticleId IdType="pubmed">27744578</ArticleId></ArticleIdList></Reference><Reference><Citation>Recht MP, Zbontar J, Sodickson DK, et al. Using Deep Learning to accelerate knee MRI at 3 T: results of an interchangeability study. Am J Roentgenol. 2020;215:1421&#x2013;9. doi: 10.2214/AJR.20.23313.</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/AJR.20.23313</ArticleId><ArticleId IdType="pmc">PMC8209682</ArticleId><ArticleId IdType="pubmed">32755163</ArticleId></ArticleIdList></Reference><Reference><Citation>Lefevre N, Naouri JF, Herman S, Gerometta A, Klouche S, Bohu Y. A current review of the meniscus imaging: proposition of a useful tool for its radiologic analysis. Radiol Res Pract. 2016;2016:8329296.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4766355</ArticleId><ArticleId IdType="pubmed">27057352</ArticleId></ArticleIdList></Reference><Reference><Citation>Tajima T, Akai H, Sugawara H, et al. Breath-hold 3D magnetic resonance cholangiopancreatography at 1.5T using a deep learning-based noise-reduction approach: comparison with the conventional respiratory-triggered technique. Eur J Radiol. 2021;144:109994. doi: 10.1016/j.ejrad.2021.109994.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejrad.2021.109994</ArticleId><ArticleId IdType="pubmed">34627106</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Curated"><PMID Version="1">36624184</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>12</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>12</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">2045-2322</ISSN><JournalIssue CitedMedium="Internet"><Volume>13</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>09</Day></PubDate></JournalIssue><Title>Scientific reports</Title><ISOAbbreviation>Sci Rep</ISOAbbreviation></Journal><ArticleTitle>3D reconstruction of proximal femoral fracture from biplanar radiographs with fractural representative learning.</ArticleTitle><Pagination><StartPage>455</StartPage><MedlinePgn>455</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">455</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1038/s41598-023-27607-2</ELocationID><Abstract><AbstractText>A femoral fracture is a severe injury occurring in traumatic and pathologic causes. Diagnosis and Preoperative planning are indispensable procedures relying on preoperative radiographs such as X-ray and CT images. Nevertheless, CT imaging has a higher cost, radiation dose, and longer acquisition time than X-ray imaging. Thus, the fracture 3D reconstruction from X-ray images had been needed and remains a challenging problem, as well as a lack of dataset. This paper proposes a 3D proximal femoral fracture reconstruction from biplanar radiographs to improve the 3D visualization of bone fragments during preoperative planning. A novel Fracture Reconstruction Network (FracReconNet) is proposed to retrieve the femoral bone shape with fracture details, including the 3D Reconstruction Network (3DReconNet), novel Auxiliary class (AC), and Fractural augmentation (FA). The 3D reconstruction network applies a deep learning-based, fully Convolutional Network with Feature Pyramid Network architecture. Specifically, the auxiliary class is proposed, which refers to fracture representation. It encourages network learning to reconstruct the fracture. Since the samples are scarce to acquire, the fractural augmentation is invented to enlarge the fracture training samples and improve reconstruction accuracy. The evaluation of FracReconNet achieved a mIoU of 0.851 and mASSD of 0.906&#xa0;mm. The proposed FracReconNet's results show fracture detail similar to the real fracture, while the 3DReconNet cannot offer.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Buttongkum</LastName><ForeName>Danupong</ForeName><Initials>D</Initials><AffiliationInfo><Affiliation>Center of Excellence for Prosthetic and Orthopedic Implant, Chulalongkorn University, Bangkok, 10330, Thailand.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Biomedical Engineering Research Center, Faculty of Engineering, Chulalongkorn University, Bangkok, 10330, Thailand.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Tangpornprasert</LastName><ForeName>Pairat</ForeName><Initials>P</Initials><AffiliationInfo><Affiliation>Center of Excellence for Prosthetic and Orthopedic Implant, Chulalongkorn University, Bangkok, 10330, Thailand. pairat.t@chula.ac.th.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Mechanical Engineering, Faculty of Engineering, Chulalongkorn University, Bangkok, 10330, Thailand. pairat.t@chula.ac.th.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Biomedical Engineering Research Center, Faculty of Engineering, Chulalongkorn University, Bangkok, 10330, Thailand. pairat.t@chula.ac.th.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Virulsri</LastName><ForeName>Chanyaphan</ForeName><Initials>C</Initials><AffiliationInfo><Affiliation>Center of Excellence for Prosthetic and Orthopedic Implant, Chulalongkorn University, Bangkok, 10330, Thailand.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Mechanical Engineering, Faculty of Engineering, Chulalongkorn University, Bangkok, 10330, Thailand.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Biomedical Engineering Research Center, Faculty of Engineering, Chulalongkorn University, Bangkok, 10330, Thailand.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Numkarunarunrote</LastName><ForeName>Numphung</ForeName><Initials>N</Initials><AffiliationInfo><Affiliation>Department of Radiology, Faculty of Medicine, Chulalongkorn University, Bangkok, 10330, Thailand.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Amarase</LastName><ForeName>Chavarin</ForeName><Initials>C</Initials><AffiliationInfo><Affiliation>Hip Fracture Research Unit, Department of Orthopaedics, Faculty of Medicine, Chulalongkorn University, Bangkok, 10330, Thailand.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kobchaisawat</LastName><ForeName>Thananop</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>Perceptual Intelligent Computing Lab, Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Bangkok, 10330, Thailand.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Chalidabhongse</LastName><ForeName>Thanarat</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>Perceptual Intelligent Computing Lab, Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Bangkok, 10330, Thailand.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Applied Digital Technology in Medicine Research Group, Chulalongkorn University, Bangkok, 10330, Thailand.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>09</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>Sci Rep</MedlineTA><NlmUniqueID>101563288</NlmUniqueID><ISSNLinking>2045-2322</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D005264" MajorTopicYN="Y">Femoral Fractures</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName><QualifierName UI="Q000601" MajorTopicYN="N">surgery</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D021621" MajorTopicYN="Y">Imaging, Three-Dimensional</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D000092526" MajorTopicYN="Y">Proximal Femoral Fractures</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D011859" MajorTopicYN="N">Radiography</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D014057" MajorTopicYN="N">Tomography, X-Ray Computed</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading></MeshHeadingList><CoiStatement>The authors declare no competing interests.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>5</Month><Day>24</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2023</Year><Month>1</Month><Day>4</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>23</Hour><Minute>20</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36624184</ArticleId><ArticleId IdType="pmc">PMC9829664</ArticleId><ArticleId IdType="doi">10.1038/s41598-023-27607-2</ArticleId><ArticleId IdType="pii">10.1038/s41598-023-27607-2</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>B&#xe4;cker HC, et al. Epidemiology of proximal femoral fractures. J. Clin. Orthopaed. Trauma. 2021;12:161&#x2013;165. doi: 10.1016/j.jcot.2020.07.001.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jcot.2020.07.001</ArticleId><ArticleId IdType="pmc">PMC7920330</ArticleId><ArticleId IdType="pubmed">33716441</ArticleId></ArticleIdList></Reference><Reference><Citation>Dietmar Krappinger BW, Dammerer D, Thaler M, Schwendinger P, Lindtner RA. Risk factors for nonunion after intramedullary nailing of subtrochanteric femoral fractures. Arch. Orthopaed. Trauma Surg. 2019;139:769&#x2013;777. doi: 10.1007/s00402-019-03131-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00402-019-03131-9</ArticleId><ArticleId IdType="pmc">PMC6514068</ArticleId><ArticleId IdType="pubmed">30729990</ArticleId></ArticleIdList></Reference><Reference><Citation>Amin S, Achenbach SJ, Atkinson EJ, Khosla S, Melton LJ., 3rd Trends in fracture incidence: A population-based study over 20 years. J. Bone Miner. Res. 2014;29:581&#x2013;589. doi: 10.1002/jbmr.2072.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jbmr.2072</ArticleId><ArticleId IdType="pmc">PMC3929546</ArticleId><ArticleId IdType="pubmed">23959594</ArticleId></ArticleIdList></Reference><Reference><Citation>Sheehan SE, Michael JYS, Weaver J, Sodickson AD, Khurana B. Proximal femoral fractures: What the orthopedic surgeon wants to know. Radiographics. 2015 doi: 10.1148/rg.2015140301.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/rg.2015140301</ArticleId><ArticleId IdType="pubmed">26186669</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang D, Zhang K, Qiang M, Jia X, Chen Y. Computer-assisted preoperative planning improves the learning curve of PFNA-II in the treatment of intertrochanteric femoral fractures. BMC Musculoskelet. Disord. 2020;21:34. doi: 10.1186/s12891-020-3048-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s12891-020-3048-4</ArticleId><ArticleId IdType="pmc">PMC6966829</ArticleId><ArticleId IdType="pubmed">31948409</ArticleId></ArticleIdList></Reference><Reference><Citation>Lim S-J, Park Y-S. Plain radiography of the hip: A review of radiographic techniques and image features. Hip Pelvis. 2015;27:125&#x2013;134. doi: 10.5371/hp.2015.27.3.125.</Citation><ArticleIdList><ArticleId IdType="doi">10.5371/hp.2015.27.3.125</ArticleId><ArticleId IdType="pmc">PMC4972716</ArticleId><ArticleId IdType="pubmed">27536615</ArticleId></ArticleIdList></Reference><Reference><Citation>Okada T, et al. Computer-assisted preoperative planning for reduction of proximal femoral fracture using 3-D-CT data. IEEE Trans. Biomed. Eng. 2009;56:749&#x2013;759. doi: 10.1109/TBME.2008.2005970.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TBME.2008.2005970</ArticleId><ArticleId IdType="pubmed">19389685</ArticleId></ArticleIdList></Reference><Reference><Citation>Kasban H, El-Bendary M, Salama DH. A comparative study of medical imaging techniques. Int. J. Inf. Sci. Intell. Syst. 2015;4:37&#x2013;58.</Citation></Reference><Reference><Citation>Grignon B, Oldrini G, Walter F. Teaching medical anatomy: What is the role of imaging today? Surg. Radiol. Anat. 2016;38:253&#x2013;260. doi: 10.1007/s00276-015-1548-y.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00276-015-1548-y</ArticleId><ArticleId IdType="pubmed">26298830</ArticleId></ArticleIdList></Reference><Reference><Citation>Jia X, Zhang K, Qiang M, Wu Y, Chen Y. Association of computer-assisted virtual preoperative planning with postoperative mortality and complications in older patients with intertrochanteric hip fracture. JAMA Netw. Open. 2020;3:5830. doi: 10.1001/jamanetworkopen.2020.5830.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jamanetworkopen.2020.5830</ArticleId><ArticleId IdType="pmc">PMC7417968</ArticleId><ArticleId IdType="pubmed">32777058</ArticleId></ArticleIdList></Reference><Reference><Citation>Mezger U, Jendrewski C, Bartels M. Navigation in surgery. Langenbecks Arch. Surg. 2013;398:501&#x2013;514. doi: 10.1007/s00423-013-1059-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00423-013-1059-4</ArticleId><ArticleId IdType="pmc">PMC3627858</ArticleId><ArticleId IdType="pubmed">23430289</ArticleId></ArticleIdList></Reference><Reference><Citation>Reyneke CJF, et al. Review of 2-D/3-D reconstruction using statistical shape and intensity models and X-ray image synthesis: Toward a unified framework. IEEE Rev. Biomed. Eng. 2019;12:269&#x2013;286. doi: 10.1109/RBME.2018.2876450.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/RBME.2018.2876450</ArticleId><ArticleId IdType="pubmed">30334808</ArticleId></ArticleIdList></Reference><Reference><Citation>Heimann T, Meinzer H-P. Statistical shape models for 3D medical image segmentation: A review. Med. Image Anal. 2009;13:543&#x2013;563. doi: 10.1016/j.media.2009.05.004.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2009.05.004</ArticleId><ArticleId IdType="pubmed">19525140</ArticleId></ArticleIdList></Reference><Reference><Citation>Whitmarsh T, Humbert L, Craene MD, Barquero LMDR, Frangi AF. Reconstructing the 3D shape and bone mineral density distribution of the proximal femur from dual-energy X-ray absorptiometry. IEEE Trans. Med. Imaging. 2011;30:2101&#x2013;2114. doi: 10.1109/TMI.2011.2163074.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2011.2163074</ArticleId><ArticleId IdType="pubmed">21803681</ArticleId></ArticleIdList></Reference><Reference><Citation>Shelhamer E, Long J, Darrell T. Fully convolutional networks for semantic segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 2017;39:640&#x2013;651. doi: 10.1109/TPAMI.2016.2572683.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TPAMI.2016.2572683</ArticleId><ArticleId IdType="pubmed">27244717</ArticleId></ArticleIdList></Reference><Reference><Citation>Lin, T. et al. Feature pyramid networks for object detection. in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 936&#x2013;944. 10.1109/CVPR.2017.106 (2017).</Citation></Reference><Reference><Citation>&#xc7;i&#xe7;ek, &#xd6;., Abdulkadir, A., Lienkamp, S. S., Brox, T. &amp; Ronneberger, O. 3D U-Net: Learning dense volumetric segmentation from sparse annotation. in Medical Image Computing and Computer-Assisted Intervention&#x2014;MICCAI 2016. 424&#x2013;432 (2016).</Citation></Reference><Reference><Citation>Henzler P, Rasche V, Ropinski T, Ritschel T. Single-image tomography: 3D volumes from 2D cranial X-rays. Comput. Graph. Forum. 2018;37:377&#x2013;388. doi: 10.1111/cgf.13369.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/cgf.13369</ArticleId></ArticleIdList></Reference><Reference><Citation>Ashish, S., Yohei, S. &amp; Yuichiro, H. GA-GAN: CT reconstruction from biplanar DRRs using GAN with guided attention. Image Video Process. arXiv:1909.12525v2 (2019).</Citation></Reference><Reference><Citation>Lu Y, Uppal HS. Hip fractures: Relevant anatomy, classification, and biomechanics of fracture and fixation. Geriatr. Orthop. Surg. Rehabil. 2019 doi: 10.1177/2151459319859139.</Citation><ArticleIdList><ArticleId IdType="doi">10.1177/2151459319859139</ArticleId><ArticleId IdType="pmc">PMC6610445</ArticleId><ArticleId IdType="pubmed">31321116</ArticleId></ArticleIdList></Reference><Reference><Citation>Ying, X. et al. X2CT-GAN: Reconstructing CT from biplanar X-rays with generative adversarial networks. in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 10611&#x2013;10620. 10.1109/CVPR.2019.01087 (2019).</Citation></Reference><Reference><Citation>Shorten C, Khoshgoftaar TM. A survey on image data augmentation for deep learning. J. Big Data. 2019;6:60. doi: 10.1186/s40537-019-0197-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s40537-019-0197-0</ArticleId><ArticleId IdType="pmc">PMC8287113</ArticleId><ArticleId IdType="pubmed">34306963</ArticleId></ArticleIdList></Reference><Reference><Citation>Ben-Cohen, A., Klang, E., Amitai, M. M., Goldberger, J. &amp; Greenspan, H. 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018). 1096&#x2013;1099 (2018).</Citation></Reference><Reference><Citation>Meinberg EG, Agel J, Roberts CS, Karam MD, Kellam JF. Fracture and dislocation classification compendium&#x2014;2018. J. Orthop. Trauma. 2018;32:1&#x2013;170. doi: 10.1097/BOT.0000000000001063.</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/BOT.0000000000001063</ArticleId><ArticleId IdType="pubmed">29256945</ArticleId></ArticleIdList></Reference><Reference><Citation>Huang, G., Liu, Z., Maaten, L. V. D. &amp; Weinberger, K. Q. Densely connected convolutional networks. in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2261&#x2013;2269. 10.1109/CVPR.2017.243 (2017).</Citation></Reference><Reference><Citation>Gidaris, S., Singh, P. &amp; Komodakis, N. Unsupervised Representation Learning by Predicting Image Rotations. Preprint: arXiv:1803.07728 (2018).</Citation></Reference><Reference><Citation>Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T. &amp; Efros, A. A. Context Encoders: Feature Learning by Inpainting. Preprint: arXiv: 1604.07379 (2016).</Citation></Reference><Reference><Citation>Jing L, Tian Y. Self-supervised visual feature learning with deep neural networks: A survey. IEEE Trans. Pattern Anal. Mach. Intell. 2021;43:4037&#x2013;4058. doi: 10.1109/TPAMI.2020.2992393.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TPAMI.2020.2992393</ArticleId><ArticleId IdType="pubmed">32386141</ArticleId></ArticleIdList></Reference><Reference><Citation>Kanafi, M. M. Surface Generator: Artificial Randomly Rough Surfaces. (2021).</Citation></Reference><Reference><Citation>Lynch JA, et al. Measurement of changes in trabecular bone at fracture sites using X-ray CT and automated image registration and processing. J. Orthop. Res. 2004;22:362&#x2013;367. doi: 10.1016/S0736-0266(03)00197-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0736-0266(03)00197-9</ArticleId><ArticleId IdType="pubmed">15013097</ArticleId></ArticleIdList></Reference><Reference><Citation>Lin, T.-Y., Goyal, P., Girshick, R., He, K. &amp; Doll&#xe1;r, P. Focal Loss for Dense Object Detection. Preprint: arXiv:1708.02002 (2017).</Citation><ArticleIdList><ArticleId IdType="pubmed">30040631</ArticleId></ArticleIdList></Reference><Reference><Citation>Russakoff DB, et al. Fast generation of digitally reconstructed radiographs using attenuation fields with application to 2D&#x2013;3D image registration. IEEE Trans. Med. Imaging. 2005;24:1441&#x2013;1454. doi: 10.1109/TMI.2005.856749.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2005.856749</ArticleId><ArticleId IdType="pubmed">16279081</ArticleId></ArticleIdList></Reference><Reference><Citation>Staub D, Murphy MJ. A digitally reconstructed radiograph algorithm calculated from first principles. Med. Phys. 2013 doi: 10.1118/1.4769413.</Citation><ArticleIdList><ArticleId IdType="doi">10.1118/1.4769413</ArticleId><ArticleId IdType="pmc">PMC3532107</ArticleId><ArticleId IdType="pubmed">23298093</ArticleId></ArticleIdList></Reference><Reference><Citation>Chotai N, Arshad H, Bates P. Radiographic anatomy and imaging of the acetabulum. Orthopaed. Trauma. 2018;32:102&#x2013;109. doi: 10.1016/j.mporth.2018.01.008.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.mporth.2018.01.008</ArticleId></ArticleIdList></Reference><Reference><Citation>Paszke A, et al. PyTorch: An imperative style, high-performance deep learning library. Adv. Neural Inf. Process. Syst. 2019;32:1.</Citation></Reference><Reference><Citation>Kingma, D.P., &amp; Adam, J. L. B. A method for stochastic optimization. in The 3rd International Conference for Learning Representations. arXiv:1412.6980v9 (2015).</Citation></Reference><Reference><Citation>Yeghiazaryan V, Voiculescu I. Family of boundary overlap metrics for the evaluation of medical image segmentation. J. Med. Imaging (Bellingham) 2018;5:015006&#x2013;015006. doi: 10.1117/1.JMI.5.1.015006.</Citation><ArticleIdList><ArticleId IdType="doi">10.1117/1.JMI.5.1.015006</ArticleId><ArticleId IdType="pmc">PMC5817231</ArticleId><ArticleId IdType="pubmed">29487883</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen L, et al. Liver tumor segmentation in CT volumes using an adversarial densely connected network. BMC Bioinform. 2019;20:587. doi: 10.1186/s12859-019-3069-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s12859-019-3069-x</ArticleId><ArticleId IdType="pmc">PMC6886252</ArticleId><ArticleId IdType="pubmed">31787071</ArticleId></ArticleIdList></Reference><Reference><Citation>Kovacevic D, Skretting A. Selecting the correct X-ray tube tilt angle and roof pillar rotation for bedside radiography with combined cranio-caudal and lateral cassette tilt. Radiography. 2008;14:170&#x2013;174. doi: 10.1016/j.radi.2006.11.004.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.radi.2006.11.004</ArticleId></ArticleIdList></Reference><Reference><Citation>Gislason MK, et al. Three dimensional bone mineral density changes in the femur over 1 year in primary total hip arthroplasty patients. Clin. Biomech. 2020;78:105092. doi: 10.1016/j.clinbiomech.2020.105092.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.clinbiomech.2020.105092</ArticleId><ArticleId IdType="pubmed">32590143</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36623323</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>09</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1361-6560</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>09</Day></PubDate></JournalIssue><Title>Physics in medicine and biology</Title><ISOAbbreviation>Phys Med Biol</ISOAbbreviation></Journal><ArticleTitle>Cross-convolutional transformer for automated multi-organs segmentation in a variety of medical images.</ArticleTitle><ELocationID EIdType="doi" ValidYN="Y">10.1088/1361-6560/acb19a</ELocationID><Abstract><AbstractText>Objective It's a huge challenge for multi-organs segmentation in various medical images based on a consistent algorithm with the development of deep learning methods. We therefore develop a deep learning method based on cross-convolutional transformer for these automated- segmentation to obtain better generalization and accuracy. Approach We propose a cross-convolutional transformer network (C2Former) to solve the segmentation problem. Specifically, we first redesign a novel cross-convolutional self-attention mechanism in terms of the algorithm to integrate local and global contexts and model long-distance and short-distance dependencies to enhance the semantic feature understanding of images. Then multi-scale feature edge fusion module (MFEF) is proposed to combine the image edge features, which effectively form multi-scale feature streams and establish reliable relational connections in the global context. Finally, we use three different modalities, imaging three different anatomical regions to train and test multi organs and evaluate segmentation performance. Main results We use the evaluation metrics of Dice Similarity Coefficient (DSC) and 95% Hausdorff distance (HD95) for each dataset. Experiments showed the average DSC of 85.02% and HD95 of 15.79mm on the Synapse dataset (CT images of abdominal multi-organ), the average DSC of 91.42% and HD95 of 1.06mm on the ACDC dataset (MRI of cardiac substructures) and the average DSC of 86.78% and HD95 of 16.85mm on the ISIC 2017 dataset (skin cancer images). In each dataset, our proposed method consistently outperforms the compared networks. Significance The proposed deep learning network provides a generalized and accurate solution method for multi-organ segmentation in the three different datasets. It has the potential to be applied to a variety of medical datasets for structural segmentation.</AbstractText><CopyrightInformation>Creative Commons Attribution license.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Wang</LastName><ForeName>Jing</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Shandong University School of Information Science and Engineering, No. 72, Binhai Road, Jimo, Qingdao, Shandong, China, Qingdao, Shandong, 266237, CHINA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhao</LastName><ForeName>Haiyue</ForeName><Initials>H</Initials><Identifier Source="ORCID">0000-0003-3824-5639</Identifier><AffiliationInfo><Affiliation>Shandong Youth University of Political Science, No.31699 Jing Shi East Road, Li Xia Distict, Jinan, Shandong, China., Jinan, Shandong, 250103, CHINA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Liang</LastName><ForeName>Wei</ForeName><Initials>W</Initials><AffiliationInfo><Affiliation>Environmental Protection Department of Shandong Province, No.3377 Jing Shi Road, Jinan, China., Jinan, Shandong, 250101, CHINA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wang</LastName><ForeName>Shuyu</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Shandong Youth University of Political Science, No.31699 Jing Shi East Road, Li Xia Distict, Jinan, Shandong, China., Jinan, Shandong, 250103, CHINA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Yan</ForeName><Initials>Y</Initials><Identifier Source="ORCID">0000-0002-7315-1446</Identifier><AffiliationInfo><Affiliation>Shandong Mental Health Center, No.49 Wen Hua Dong Road, Li Xia Distict, Jinan, Shandong, China., Jinan, 250014, CHINA.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>09</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>Phys Med Biol</MedlineTA><NlmUniqueID>0401220</NlmUniqueID><ISSNLinking>0031-9155</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Deep Learning</Keyword><Keyword MajorTopicYN="N">Medical image</Keyword><Keyword MajorTopicYN="N">Self-Attention</Keyword><Keyword MajorTopicYN="N">Transformer</Keyword><Keyword MajorTopicYN="N">Visual Attention Mechanism</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>17</Hour><Minute>43</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36623323</ArticleId><ArticleId IdType="doi">10.1088/1361-6560/acb19a</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36623316</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>09</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1361-6560</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>09</Day></PubDate></JournalIssue><Title>Physics in medicine and biology</Title><ISOAbbreviation>Phys Med Biol</ISOAbbreviation></Journal><ArticleTitle>Topologically preserved registration of 3D CT images with deep networks.</ArticleTitle><ELocationID EIdType="doi" ValidYN="Y">10.1088/1361-6560/acb197</ELocationID><Abstract><AbstractText Label="OBJECTIVE" NlmCategory="OBJECTIVE">Computed Tomography (CT) image registration makes fast and accurate imaging-based disease diagnosis possible. We aim to develop a framework which can perform accurate local registration of organs in 3D CT images while preserving the topology of transformation.</AbstractText><AbstractText Label="APPROACH" NlmCategory="METHODS">In this framework, the Faster R-CNN method is first used to detect local areas containing organs from fixed and moving images whose results are then registered with a weakly supervised deep neural network. In this network, a novel 3D channel coordinate attention (CA) module is introduced to reduce the loss of position information. The image edge loss and the organ labelling loss are used to weakly supervise the training process of our deep network, which enables the network learning to focus on registering organs and image structures. An intuitive inverse module is also used to reduce the folding of deformation field. More specifically, the folding is suppressed directly by simultaneously maximizing forward and backward registration accuracy in the image domain rather than indirectly by measuring the consistency of forward and inverse deformation fields as usual.</AbstractText><AbstractText Label="MAIN RESULTS" NlmCategory="RESULTS">Our method achieves an average dice similarity coefficient (DSC) of 0.954 and an average Similarity (Sim) of 0.914 on publicly available liver datasets (LiTS for training and Sliver07 for testing) and achieves an average DSC of 0.914 and an average Sim of 0.947 on our home-built left ventricular myocardium (LVM) dataset.</AbstractText><AbstractText Label="SIGNIFICANCE" NlmCategory="CONCLUSIONS">Experimental results show that our proposed method can significantly improve the registration accuracy of organs such as the liver and LVM. Moreover, our inverse module can intuitively improve the inherent topological preservation of transformations.</AbstractText><CopyrightInformation>&#xa9; 2023 Institute of Physics and Engineering in Medicine.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Liu</LastName><ForeName>Huaying</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>School of Electronic and Information Engineering, Soochow University, GuSu , Suzhou, Jiangsu, Suzhou, 215006, CHINA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Gong</LastName><ForeName>Guanzhong</ForeName><Initials>G</Initials><AffiliationInfo><Affiliation>Department of radiation physics and technology, Shandong Tumor Hospital and Institute, Jiyan Road, Jinan City, Shandong Province, Jinan, Shandong, 250000, CHINA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zou</LastName><ForeName>Wei</ForeName><Initials>W</Initials><AffiliationInfo><Affiliation>Soochow University, Gusu, Suzhou, Jiangsu, Suzhou, 215006, CHINA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Hu</LastName><ForeName>Nan</ForeName><Initials>N</Initials><AffiliationInfo><Affiliation>Soochow University, Gusu, Suzhou, Jiangsu, Suzhou, 215006, CHINA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wang</LastName><ForeName>Jiajun</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Soochow University, GuSu, Suzhou, Jiangsu, Suzhou, 215000, CHINA.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>09</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>Phys Med Biol</MedlineTA><NlmUniqueID>0401220</NlmUniqueID><ISSNLinking>0031-9155</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Attention</Keyword><Keyword MajorTopicYN="N">CT</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Medical image registration</Keyword><Keyword MajorTopicYN="N">Uni-modal</Keyword><Keyword MajorTopicYN="N">Weakly-supervised</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>17</Hour><Minute>42</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36623316</ArticleId><ArticleId IdType="doi">10.1088/1361-6560/acb197</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36622465</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>09</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1618-727X</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>09</Day></PubDate></JournalIssue><Title>Journal of digital imaging</Title><ISOAbbreviation>J Digit Imaging</ISOAbbreviation></Journal><ArticleTitle>Hybrid Optimization Algorithm Enabled Deep Learning Approach Brain Tumor Segmentation and Classification Using MRI.</ArticleTitle><ELocationID EIdType="doi" ValidYN="Y">10.1007/s10278-022-00752-2</ELocationID><Abstract><AbstractText>The unnatural and uncontrolled increase of brain cells is called brain tumors, leading to human health danger. Magnetic resonance imaging (MRI) is widely applied for classifying and detecting brain tumors, due to its better resolution. In general, medical specialists require more details regarding the size, type, and changes in small lesions for effective classification. The timely and exact diagnosis plays a major role in the efficient treatment of patients. Therefore, in this research, an efficient hybrid optimization algorithm is implemented for brain tumor segmentation and classification. The convolutional neural network (CNN) features are extracted to perform a better classification. The classification is performed by considering the extracted features as the input of the deep residual network (DRN), in which the training is performed using the proposed chronological Jaya honey badger algorithm (CJHBA). The proposed CJHBA is the integration of the Jaya algorithm, honey badger algorithm (HBA), and chronological concept. The performance is evaluated using the BRATS 2018 and Figshare datasets, in which the maximum accuracy, sensitivity, and specificity are attained using the BRATS dataset with values 0.9210, 0.9313, and 0.9284, respectively.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s) under exclusive licence to Society for Imaging Informatics in Medicine.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Deepa</LastName><ForeName>S</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Professor, Department of ECE, Panimalar Engineering College, Chennai, India. dineshdeepas1977@gmail.com.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Janet</LastName><ForeName>J</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Professor, Department of CSE, Sri Krishna College of Engineering and Technology, Coimbatore, India.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Sumathi</LastName><ForeName>S</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Professor, Department of EEE, Mahendra Engineering College, Namakkal, India.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Ananth</LastName><ForeName>J P</ForeName><Initials>JP</Initials><AffiliationInfo><Affiliation>Professor, Department of CSE, Sri Krishna College of Engineering and Technology, Coimbatore, India.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>09</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>J Digit Imaging</MedlineTA><NlmUniqueID>9100529</NlmUniqueID><ISSNLinking>0897-1889</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Chronological concept</Keyword><Keyword MajorTopicYN="N">Data augmentation</Keyword><Keyword MajorTopicYN="N">Gaussian noise</Keyword><Keyword MajorTopicYN="N">Honey badger algorithm</Keyword><Keyword MajorTopicYN="N">Normalization</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>7</Month><Day>4</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>4</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>9</Month><Day>16</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>11</Hour><Minute>16</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36622465</ArticleId><ArticleId IdType="doi">10.1007/s10278-022-00752-2</ArticleId><ArticleId IdType="pii">10.1007/s10278-022-00752-2</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>David N. Louis, Arie Perry, Guido Reifenberger, Andreas von Deimling, Dominique Figarella&#x2011;Branger, Webster K. Cavenee, Hiroko Ohgaki, Otmar D. Wiestler, Paul Kleihues, and David W. Ellison, The 2016 World Health Organization Classification of Tumors of the Central Nervous System: a summary, Acta Neuropathol, vol. 131, pp. 803&#x2013;820, 2016.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00401-016-1545-1</ArticleId></ArticleIdList></Reference><Reference><Citation>Jaeyong Kang, Zahid Ullah, and Jeonghwan Gwak, MRI-Based Brain Tumor Classification Using Ensemble of Deep Features and Machine Learning Classifiers, Sensors, vol. 21, no. 6, 2021.</Citation></Reference><Reference><Citation>Gopal S. Tandel, Mainak Biswas, Omprakash G. Kakde, Ashish Tiwari, Harman S. Suri, Monica Turk, John R. Laird, Christopher K. Asare, Annabel A. Ankrah, N. N. Khanna, B. K. Madhusudhan, Luca Saba, and Jasjit S. Suri, A Review on a Deep Learning Perspective in Brain Cancer Classification, Cancers, vol. 11, no. 1, 2019.</Citation></Reference><Reference><Citation>Ahmad M. Sarhan, Brain Tumor Classification in Magnetic Resonance Images Using Deep Learning and Wavelet Transform, Journal of Biomedical Science and Engineering, vol. 13, no. 6, pp. 102-112, 2020.</Citation><ArticleIdList><ArticleId IdType="doi">10.4236/jbise.2020.136010</ArticleId></ArticleIdList></Reference><Reference><Citation>Yurong Guan, Muhammad Aamir, Ziaur Rahman, Ammara Ali, Waheed Ahmed Abro, Zaheer Ahmed Dayo, Muhammad Shoaib Bhutta, and Zhihua Hu, A framework for efficient brain tumor classification using MRI images, Mathematical Biosciences and Engineering, vol. 18, no. 5, pp. 5790-5815, 2021.</Citation><ArticleIdList><ArticleId IdType="doi">10.3934/mbe.2021292</ArticleId></ArticleIdList></Reference><Reference><Citation>Francisco Javier D&#xed;az-Pernas, Mario Mart&#xed;nez-Zarzuela, M&#xed;riam Ant&#xf3;n-Rodr&#xed;guez, and David Gonz&#xe1;lez-Ortega, A Deep Learning Approach for Brain Tumor Classification and Segmentation Using a Multiscale Convolutional Neural Network, Healthcare, vol. 9, no. 2, pp. 1-14, 2021.</Citation></Reference><Reference><Citation>Parnian Afshar, Konstantinos Plataniotis, and Arash Mohammadi, Capsule Networks for Brain Tumor Classification based on MRI Images and Course Tumor Boundaries, 2018.</Citation></Reference><Reference><Citation>Asmita Dixit and Aparajita Nanda, An improved whale optimization algorithm-based radial neural network for multi-grade brain tumor classification, The Visual Computer, 2021.</Citation></Reference><Reference><Citation>Jin Liu, Min Li, Jianxin Wang, Fangxiang Wu, Tianming Liu, and Yi Pan, A survey of MRI-based brain tumor segmentation methods, Tsinghua Science and Technology, vol. 19, no. 6, pp. 578-595, 2014.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TST.2014.6961028</ArticleId></ArticleIdList></Reference><Reference><Citation>G.Gokulkumari, Classification of Brain Tumor using Manta Ray Foraging Optimization-based DeepCNN Classifier, Multimedia Research, vol. 3, no. 4, pp. 32-42, 2020.</Citation><ArticleIdList><ArticleId IdType="doi">10.46253/j.mr.v3i4.a4</ArticleId></ArticleIdList></Reference><Reference><Citation>Xiaoqing Gu, Zongxuan Shen, Jing Xue, Yiqing Fan, and Tongguang Ni, Brain Tumor MR Image Classification Using Convolutional Dictionary Learning With Local Constraint, Front Neuroscience, 2021.</Citation></Reference><Reference><Citation>Avinash Gopal, Hybrid classifier: Brain Tumor Classification and Segmentation using Genetic-based Grey Wolf optimization, Multimedia Research, vol. 3, no. 2, pp. 1-10, 2020.</Citation><ArticleIdList><ArticleId IdType="doi">10.46253/j.mr.v3i2.a1</ArticleId></ArticleIdList></Reference><Reference><Citation>Zeynettin Akkus, Alfiia Galimzianova, Assaf Hoogi, Daniel L. Rubin, and Bradley J. Erickson, Deep Learning for Brain MRI Segmentation: State of the Art and Future Directions, Journal of Digital Imaging, vol. 30, pp. pp. 449&#x2013;459, 2017.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10278-017-9983-4</ArticleId></ArticleIdList></Reference><Reference><Citation>Isselmou Abd El Kader, Guizhi Xu, Zhang Shuai, Sani Saminu, Imran Javaid, and Isah Salim Ahmad, Differential Deep Convolutional Neural Network Model for Brain Tumor Classification, Brain Sciences, vol. 11, no. 3, 2021.</Citation></Reference><Reference><Citation>Heba Mohsen, El-Sayed A. El-Dahshan, El-Sayed M. El-Horbaty, and Abdel-Badeeh M. Salem, Classification using deep learning neural networks for brain tumors, Future Computing and Informatics Journal, vol. 3, no. 1, pp. 68-71, 2018.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.fcij.2017.12.001</ArticleId></ArticleIdList></Reference><Reference><Citation>S&#xe9;rgio Pereira, Adriano Pinto, Victor Alves, and Carlos A. Silva, Brain Tumor Segmentation Using Convolutional Neural Networks in MRI Images, IEEE Transactions on Medical Imaging, vol. 35, no. 5, pp. 1240-1251, 2016.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2016.2538465</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhifang Zhan, Jian-Feng Cai, Di Guo, Yunsong Liu, Zhong Chen, and Xiaobo Qu, Fast Multiclass Dictionaries Learning With Geometrical Directions in MRI Reconstruction, IEEE Transactions on Biomedical Engineering, vol. 63, no. 9, pp. 1850-1861, 2016.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TBME.2015.2503756</ArticleId></ArticleIdList></Reference><Reference><Citation>Jiang Wang, Yi Yang, Junhua Mao, Zhiheng Huang, Chang Huang, and Wei Xu, CNN-RNN: A Unified Framework for Multi-label Image Classification, In the proceeding of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.</Citation></Reference><Reference><Citation>Jimit Doshi, Guray Erus, Mohamad Habes, and Christos Davatzikos, DeepMRSeg: A convolutional deep neural network for anatomy and abnormality segmentation on MR images, arXiv preprint arXiv:1907.02110 , 2019.</Citation></Reference><Reference><Citation>Hashim, F.A., Houssein, E.H., Hussain, K., Mabrouk, M.S. and Al-Atabany, W., Honey Badger Algorithm: New metaheuristic algorithm for solving optimization problems, Mathematics and Computers in Simulation, vol.192, pp.84-110, February 2022.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.matcom.2021.08.013</ArticleId></ArticleIdList></Reference><Reference><Citation>Yao, R., Wang, N., Liu, Z., Chen, P. and Sheng, X., Intrusion detection system in the advanced Metering infrastructure: a cross-layer feature-Fusion CNN-LSTM-Based approach, Sensors, vol.21, no.2, pp.626, January 2021.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s21020626</ArticleId></ArticleIdList></Reference><Reference><Citation>BRATS 2018 database, taken from, https://figshare.com/articles/brain_tumor_dataset/1512427 , accessed on February 2022.</Citation></Reference><Reference><Citation>Rao, R., Jaya: A simple and new optimization algorithm for solving constrained and unconstrained optimization problems, International Journal of Industrial Engineering Computations, vol.7, no.1, pp.19-34, 2016.</Citation></Reference><Reference><Citation>Javaria Amin, Muhammad Sharif, Nadia Gul, Mussarat Yasmin, and Shafqat Ali Shad, Brain tumor classification based on DWT fusion of MRI sequences using convolutional neural network, Pattern Recognition Letters, vol. 129, pp. 115&#x2013;122, 2020.</Citation></Reference><Reference><Citation>Wadhah Ayadi, Imen Charfi, Wajdi Elhamzi, and Mohamed Atri, Brain tumor classification based on hybrid approach, The Visual Computer, vol. 38, pp. 107&#x2013;117, 2022.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00371-020-02005-1</ArticleId></ArticleIdList></Reference><Reference><Citation>Swati, Z.N.K., Zhao, Q., Kabir, M., Ali, F., Ali, Z., Ahmed, S. and Lu, J., Brain tumor classification for MR images using transfer learning and fine-tuning, Computerized Medical Imaging and Graphics, vol.75, pp.34-46, 2019.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compmedimag.2019.05.001</ArticleId></ArticleIdList></Reference><Reference><Citation>Figshare database, taken from, https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=37224922 , accessed on February 2022.</Citation></Reference><Reference><Citation>Chen, Z., Chen, Y., Wu, L., Cheng, S. and Lin, P., Deep residual network based fault detection and diagnosis of photovoltaic arrays using current-voltage curves and ambient conditions, Energy Conversion and Management, vol. 198, pp.111793, October 2019.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.enconman.2019.111793</ArticleId></ArticleIdList></Reference><Reference><Citation>Raja, P.S., Brain tumor classification using a hybrid deep autoencoder with Bayesian fuzzy clustering-based segmentation approach, Biocybernetics and Biomedical Engineering, vol.40, no.1, pp.440-453, 2020.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bbe.2020.01.006</ArticleId></ArticleIdList></Reference><Reference><Citation>Ghassemi, N., Shoeibi, A. and Rouhani, M., Deep neural network with generative adversarial networks pre-training for brain tumor classification based on MR images, Biomedical Signal Processing and Control, vol.57, pp.101678, 2020.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bspc.2019.101678</ArticleId></ArticleIdList></Reference><Reference><Citation>Mzoughi, H., Njeh, I., Wali, A., Slima, M.B., BenHamida, A., Mhiri, C. and Mahfoudhe, K.B., Deep multi-scale 3D convolutional neural network (CNN) for MRI gliomas brain tumor classification, Journal of Digital Imaging, vol.33, pp.903-915, 2020.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10278-020-00347-9</ArticleId></ArticleIdList></Reference><Reference><Citation>Ottorino Catani, Federico Fusini, Fabio Zanchini, Fabrizio Sergio, Giovanni Cautiero, Jorge Hugo Villafane, and Francesco Langella, Functional outcomes of percutaneous correction of hallux valgus in not symptomatic flatfoot: a case series study, Acta Bio Medica: Atenei Parmensis, vol. 91, no. 3, 2020.</Citation></Reference><Reference><Citation>Antonio Bonacaro, Ivan Rubbi, and Dave Sookhoo, The use of wearable devices in preventing hospital readmission and in improving the quality of life of chronic patients in the homecare setting: a narrative literature review, Professioni Infermieristiche, vol. 72, no. 2, pp. 143-151, 2019.</Citation></Reference><Reference><Citation>Maicol Carvello, Filippo Zanotti, Ivan Rubbi, Silvia Bacchetti, Giovanna Artioli, and Antonio Bonacaro, Peer-support: a coping strategy for nurses working at the Emergency Ambulance Service, Acta Biomed for Health Professions, vol. 90, no. 3, pp. 29-37, 2019.</Citation></Reference><Reference><Citation>Giovanni Parente, Tommaso Gargano, Giovanni Ruggeri, Michela Maffi, Simone D'Antonio, Elisa Sacchet, and Mario Lima, Anastomotic Stricture Definition After Esophageal Atresia Repair: Role of Endoscopic Stricture Index, Journal of surgical research, vol. 257, pp. 572-578, 2021.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jss.2020.08.035</ArticleId></ArticleIdList></Reference><Reference><Citation>Giovanni Parente, Tommaso Gargano, Stefania Pavia, Chiara Cordola, Marzia Vastano, Francesco Baccelli, Giulia Gallotta, Laura Bruni, Adelaide Corvaglia, and Mario Lima, Pyelonephritis in pediatric uropathic patients: Differences from community-acquired ones and therapeutic protocol considerations. A 10-year single-center retrospective study, Children, vol. 8, no. 6, 2021.</Citation></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36622401</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>09</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1552-3365</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>09</Day></PubDate></JournalIssue><Title>The American journal of sports medicine</Title><ISOAbbreviation>Am J Sports Med</ISOAbbreviation></Journal><ArticleTitle>Degree of Accuracy With Which Deep Learning for Ultrasound Images Identifies Osteochondritis Dissecans of the Humeral Capitellum.</ArticleTitle><Pagination><StartPage>3635465221142280</StartPage><MedlinePgn>3635465221142280</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1177/03635465221142280</ELocationID><Abstract><AbstractText Label="BACKGROUND" NlmCategory="UNASSIGNED">Medical screening using ultrasonography (US) has been performed on young baseball players for early detection of osteochondritis dissecans (OCD) of the humeral capitellum. Deep learning (DL) and artificial intelligence (AI) techniques are widely adopted in the medical imaging research field.</AbstractText><AbstractText Label="PURPOSE/HYPOTHESIS" NlmCategory="UNASSIGNED">The purpose of this study was to calculate the diagnostic accuracy using DL for US images of OCD. We hypothesized that using DL for US imaging would improve the prediction accuracy of OCD.</AbstractText><AbstractText Label="STUDY DESIGN" NlmCategory="UNASSIGNED">Cohort study (Diagnosis); Level of evidence, 2.</AbstractText><AbstractText Label="METHODS" NlmCategory="UNASSIGNED">A total of 40 elbows (mean age of patients, 12.1 years) that were suspected of having OCD at a medical checkup and later confirmed by radiographs and magnetic resonance imaging were included in the study. The affected elbows were used as the OCD group and the contralateral elbows as the control group. From US videos, 100 images per elbow were captured from different angles, and 4000 images of the elbows were prepared for both groups. Of these, 80% were randomly selected by DL models and used as training data; the remaining were used as test data. Transfer learning was conducted using 3 pretrained DL models. The confusion matrix and the area under the receiver operating characteristic curve (AUC) were used to evaluate the model, and the visualization of the areas deemed important by the DL models was also performed. Furthermore, OCD regions were detected using an automatic image recognition model based on DL.</AbstractText><AbstractText Label="RESULTS" NlmCategory="UNASSIGNED">Classification of the OCD image by the DL model was performed; the best accuracy score was 0.87; the recall was 1.00. AUC was high for all DL models. Visualization of important features showed that AI predicted the presence of OCD by focusing on the irregularity or discontinuity of the surface of subchondral bone. In the detection of OCD task, the mean average precision was 0.83.</AbstractText><AbstractText Label="CONCLUSION" NlmCategory="UNASSIGNED">The DL on US images identified OCD with high accuracy. The important features detected by the DL models correspond to the areas used by clinicians in screening the US images. The OCD was also detected with high accuracy using the object detection model. The AI model may be used in medical screening for OCD.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Shinohara</LastName><ForeName>Issei</ForeName><Initials>I</Initials><AffiliationInfo><Affiliation>Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Japan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Investigation performed at Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Kobe, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yoshikawa</LastName><ForeName>Tomoya</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Japan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Investigation performed at Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Kobe, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Inui</LastName><ForeName>Atsuyuki</ForeName><Initials>A</Initials><Identifier Source="ORCID">0000-0002-0806-6579</Identifier><AffiliationInfo><Affiliation>Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Japan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Investigation performed at Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Kobe, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Mifune</LastName><ForeName>Yutaka</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Japan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Investigation performed at Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Kobe, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Nishimoto</LastName><ForeName>Hanako</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Japan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Investigation performed at Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Kobe, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Mukohara</LastName><ForeName>Shintaro</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Japan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Investigation performed at Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Kobe, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kato</LastName><ForeName>Tatsuo</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Japan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Investigation performed at Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Kobe, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Furukawa</LastName><ForeName>Takahiro</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Japan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Investigation performed at Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Kobe, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Tanaka</LastName><ForeName>Shuya</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Japan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Investigation performed at Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Kobe, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kusunose</LastName><ForeName>Masaya</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Japan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Investigation performed at Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Kobe, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Hoshino</LastName><ForeName>Yuichi</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Japan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Investigation performed at Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Kobe, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Matsushita</LastName><ForeName>Takehiko</ForeName><Initials>T</Initials><Identifier Source="ORCID">0000-0001-6616-0398</Identifier><AffiliationInfo><Affiliation>Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Japan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Investigation performed at Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Kobe, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kuroda</LastName><ForeName>Ryosuke</ForeName><Initials>R</Initials><AffiliationInfo><Affiliation>Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Japan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Investigation performed at Department of Orthopaedic Surgery, Kobe University Graduate School of Medicine, Kobe, Japan.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>09</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>Am J Sports Med</MedlineTA><NlmUniqueID>7609541</NlmUniqueID><ISSNLinking>0363-5465</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">artificial intelligence</Keyword><Keyword MajorTopicYN="N">baseball</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword><Keyword MajorTopicYN="N">osteochondritis dissecans</Keyword><Keyword MajorTopicYN="N">screening</Keyword><Keyword MajorTopicYN="N">ultrasonography</Keyword><Keyword MajorTopicYN="N">visualization</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>11</Hour><Minute>13</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36622401</ArticleId><ArticleId IdType="doi">10.1177/03635465221142280</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36621555</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>09</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1873-5894</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>05</Day></PubDate></JournalIssue><Title>Magnetic resonance imaging</Title><ISOAbbreviation>Magn Reson Imaging</ISOAbbreviation></Journal><ArticleTitle>4D segmentation of the thoracic aorta from 4D flow MRI using deep learning.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">S0730-725X(22)00236-3</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1016/j.mri.2022.12.021</ELocationID><Abstract><AbstractText Label="BACKGROUND" NlmCategory="BACKGROUND">4D flow MRI allows the analysis of hemodynamic changes in the aorta caused by pathologies such as thoracic aortic aneurysms (TAA). For personalized management of TAA, new biomarkers are required to analyze the effect of fluid structure iteration which can be obtained from 4D flow MRI. However, the generation of these biomarkers requires prior 4D segmentation of the aorta.</AbstractText><AbstractText Label="OBJECTIVE" NlmCategory="OBJECTIVE">To develop an automatic deep learning model to segment the aorta in 4D from 4D flow MRI.</AbstractText><AbstractText Label="METHODS" NlmCategory="METHODS">Segmentation is addressed with a U-Net based segmentation model that treats each 4D flow MRI frame as an independent sample. Performance is measured with respect to Dice score (DS) and Hausdorff distance (HD). In addition, the maximum and minimum surface areas at the level of the ascending aorta are measured and compared with those obtained from cine-MRI.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">The segmentation performance was 0.90&#x202f;&#xb1;&#x202f;0.02 for the DS and the mean HD was 9.58&#x202f;&#xb1;&#x202f;4.36&#x202f;mm. A correlation coefficient of r&#x202f;=&#x202f;0.85 was obtained for the maximum surface and r&#x202f;=&#x202f;0.86 for the minimum surface between the 4D flow MRI and cine-MRI.</AbstractText><AbstractText Label="CONCLUSION" NlmCategory="CONCLUSIONS">The proposed automatic approach of 4D aortic segmentation from 4D flow MRI seems to be accurate enough to contribute to the wider use of this imaging technique in the analysis of pathologies such as TAA.</AbstractText><CopyrightInformation>Copyright &#xa9; 2022. Published by Elsevier Inc.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Marin-Castrillon</LastName><ForeName>Diana M</ForeName><Initials>DM</Initials><AffiliationInfo><Affiliation>Imaging and Artificial Vision Laboratory, EA 7535, University of Burgundy, Dijon 21000, France. Electronic address: dianamarin228@gmail.com.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lalande</LastName><ForeName>Alain</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Imaging and Artificial Vision Laboratory, EA 7535, University of Burgundy, Dijon 21000, France; Medical Imaging Department, University Hospital of Dijon, Dijon 21000, France.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Leclerc</LastName><ForeName>Sarah</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Imaging and Artificial Vision Laboratory, EA 7535, University of Burgundy, Dijon 21000, France.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Ambarki</LastName><ForeName>Khalid</ForeName><Initials>K</Initials><AffiliationInfo><Affiliation>Siemens Healthcare SAS, Saint-Denis 93200, France.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Morgant</LastName><ForeName>Marie-Catherine</ForeName><Initials>MC</Initials><AffiliationInfo><Affiliation>Imaging and Artificial Vision Laboratory, EA 7535, University of Burgundy, Dijon 21000, France; Department of cardiovascular and thoracic surgery, University Hospital of Dijon, Dijon 21000, France.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Cochet</LastName><ForeName>Alexandre</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Imaging and Artificial Vision Laboratory, EA 7535, University of Burgundy, Dijon 21000, France; Medical Imaging Department, University Hospital of Dijon, Dijon 21000, France.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lin</LastName><ForeName>Siyu</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Imaging and Artificial Vision Laboratory, EA 7535, University of Burgundy, Dijon 21000, France.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Bouchot</LastName><ForeName>Olivier</ForeName><Initials>O</Initials><AffiliationInfo><Affiliation>Imaging and Artificial Vision Laboratory, EA 7535, University of Burgundy, Dijon 21000, France; Department of cardiovascular and thoracic surgery, University Hospital of Dijon, Dijon 21000, France.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Boucher</LastName><ForeName>Arnaud</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Imaging and Artificial Vision Laboratory, EA 7535, University of Burgundy, Dijon 21000, France.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Presles</LastName><ForeName>Benoit</ForeName><Initials>B</Initials><AffiliationInfo><Affiliation>Imaging and Artificial Vision Laboratory, EA 7535, University of Burgundy, Dijon 21000, France.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>05</Day></ArticleDate></Article><MedlineJournalInfo><Country>Netherlands</Country><MedlineTA>Magn Reson Imaging</MedlineTA><NlmUniqueID>8214883</NlmUniqueID><ISSNLinking>0730-725X</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">4D flow MRI</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Segmentation</Keyword><Keyword MajorTopicYN="N">Thoracic aortic aneurysm</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>8</Month><Day>29</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>24</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>31</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>4</Hour><Minute>56</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36621555</ArticleId><ArticleId IdType="doi">10.1016/j.mri.2022.12.021</ArticleId><ArticleId IdType="pii">S0730-725X(22)00236-3</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36621191</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>09</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1879-0534</ISSN><JournalIssue CitedMedium="Internet"><Volume>153</Volume><PubDate><Year>2023</Year><Month>Jan</Month><Day>04</Day></PubDate></JournalIssue><Title>Computers in biology and medicine</Title><ISOAbbreviation>Comput Biol Med</ISOAbbreviation></Journal><ArticleTitle>Fused deep learning paradigm for the prediction of o6-methylguanine-DNA methyltransferase genotype in glioblastoma patients: A neuro-oncological investigation.</ArticleTitle><Pagination><StartPage>106492</StartPage><MedlinePgn>106492</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1016/j.compbiomed.2022.106492</ELocationID><ELocationID EIdType="pii" ValidYN="Y">S0010-4825(22)01200-8</ELocationID><Abstract><AbstractText Label="BACKGROUND" NlmCategory="BACKGROUND">The O6-methylguanine-DNA methyltransferase (MGMT) is a deoxyribonucleic acid (DNA) repairing enzyme that has been established as an essential clinical brain tumor biomarker for Glioblastoma Multiforme (GBM). Knowing the status of MGMT methylation biomarkers using multi-parametric MRI (mp-MRI) helps neuro-oncologists to analyze GBM and its treatment plan.</AbstractText><AbstractText Label="METHOD" NlmCategory="METHODS">The hand-crafted radiomics feature extraction of GBM's subregions, such as edema(ED), tumor core (TC), and enhancing tumor (ET) in the machine learning (ML) framework, was investigated using support vector machine(SVM), K-Nearest Neighbours (KNN), random forest (RF), LightGBM, and extreme gradient boosting (XGB). For tissue-level analysis of the promotor genes in GBM, we used the deep residual neural network (ResNet-18) with 3D architecture, followed by EfficientNet-based investigation for variants as B0 and B1. Lastly, we analyzed the fused deep learning (FDL) framework that combines ML and DL frameworks.</AbstractText><AbstractText Label="RESULT" NlmCategory="RESULTS">Structural mp-MRI consisting of T1, T2, FLAIR, and T1GD having a size of 400 and 185 patients, respectively, for discovery and replication cohorts. Using the CV protocol in the ResNet-3D framework, MGMT methylation status prediction in mp-MRI gave the AUC of 0.753 (p&#xa0;&lt;&#xa0;0.0001) and 0.72 (p&#xa0;&lt;&#xa0;0.0001) for the discovery and replication cohort, respectively. We presented that the FDL is &#x223c;7% superior to solo DL and &#x223c;15% to solo ML.</AbstractText><AbstractText Label="CONCLUSION" NlmCategory="CONCLUSIONS">The proposed study aims to provide solutions for building an efficient predictive model of MGMT for GBM patients using deep radiomics features obtained from mp-MRI with the end-to-end ResNet-18 3D and FDL imaging signatures.</AbstractText><CopyrightInformation>Copyright &#xa9; 2022. Published by Elsevier Ltd.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Saxena</LastName><ForeName>Sanjay</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Department of Computer Science &amp; Engineering, International Institute of Information Technology, Bhubaneswar, Odisha, India.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Jena</LastName><ForeName>Biswajit</ForeName><Initials>B</Initials><AffiliationInfo><Affiliation>Department of Computer Science &amp; Engineering, Institute of Technical Education and Research, SOA Deemed to be University, Bhubaneswar, India.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Mohapatra</LastName><ForeName>Bibhabasu</ForeName><Initials>B</Initials><AffiliationInfo><Affiliation>Department of Computer Science &amp; Engineering, International Institute of Information Technology, Bhubaneswar, Odisha, India.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Gupta</LastName><ForeName>Neha</ForeName><Initials>N</Initials><AffiliationInfo><Affiliation>Bharati Vidyapeeth's College of Engineering, Paschim Vihar, New Delhi, India.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kalra</LastName><ForeName>Manudeep</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Radiology, Massachusetts General Hospital, Boston, MA, 02114, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Scartozzi</LastName><ForeName>Mario</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Radiology, A.O.U, di Cagliari-Polo di Monserrato s.s, 09124, Cagliari, Italy.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Saba</LastName><ForeName>Luca</ForeName><Initials>L</Initials><AffiliationInfo><Affiliation>Department of Radiology, A.O.U, di Cagliari-Polo di Monserrato s.s, 09124, Cagliari, Italy.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Suri</LastName><ForeName>Jasjit S</ForeName><Initials>JS</Initials><AffiliationInfo><Affiliation>Stroke Monitoring and Diagnostic Division, AtheroPoint&#x2122; LLC, Roseville, CA, USA; Knowledge Engineering Centre, Global Biomedical Technologies, Inc, Roseville, CA, USA. Electronic address: jasjit.suri@atheropoint.com.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>04</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>Comput Biol Med</MedlineTA><NlmUniqueID>1250250</NlmUniqueID><ISSNLinking>0010-4825</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Brain tumor</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Fused deep learning</Keyword><Keyword MajorTopicYN="N">Glioblastoma</Keyword><Keyword MajorTopicYN="N">MGMT</Keyword><Keyword MajorTopicYN="N">Machine learning</Keyword><Keyword MajorTopicYN="N">O6-methylguanine-DNA methyltransferase</Keyword><Keyword MajorTopicYN="N">Prognosis</Keyword></KeywordList><CoiStatement>Declaration of competing interest The authors declare the following financial interests/personal relationships which may be considered as potential competing interests: "The authors would like to declare that Professor Suri has worked with Professor U R Acharya for over 10 years and we would Dr. Acharya not be an Associate Editor for our manuscript. Thank you for your kind understanding".</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>10</Month><Day>23</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>11</Month><Day>29</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>27</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>4</Hour><Minute>39</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36621191</ArticleId><ArticleId IdType="doi">10.1016/j.compbiomed.2022.106492</ArticleId><ArticleId IdType="pii">S0010-4825(22)01200-8</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36620450</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic-eCollection"><Journal><ISSN IssnType="Print">1662-4548</ISSN><JournalIssue CitedMedium="Print"><Volume>16</Volume><PubDate><Year>2022</Year></PubDate></JournalIssue><Title>Frontiers in neuroscience</Title><ISOAbbreviation>Front Neurosci</ISOAbbreviation></Journal><ArticleTitle>Research trends and hotspots on connectomes from 2005 to 2021: A bibliometric and latent Dirichlet allocation application study.</ArticleTitle><Pagination><StartPage>1046562</StartPage><MedlinePgn>1046562</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">1046562</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3389/fnins.2022.1046562</ELocationID><Abstract><AbstractText Label="BACKGROUND" NlmCategory="UNASSIGNED">This study aimed to conduct a bibliometric analysis of publications on connectomes and illustrate its trends and hotspots using a machine-learning-based text mining algorithm.</AbstractText><AbstractText Label="METHODS" NlmCategory="UNASSIGNED">Documents were retrieved from the Web of Science Core Collection (WoSCC) and Scopus databases and analyzed in Rstudio 1.3.1. Through quantitative and qualitative methods, the most productive and impactful academic journals in the field of connectomes were compared in terms of the total number of publications and h-index over time. Meanwhile, the countries/regions and institutions involved in connectome research were compared, as well as their scientific collaboration. The study analyzed topics and research trends by R package "bibliometrix." The major topics of connectomes were classified by Latent Dirichlet allocation (LDA).</AbstractText><AbstractText Label="RESULTS" NlmCategory="UNASSIGNED">A total of 14,140 publications were included in the study. NEUROIMAGE ranked first in terms of publication volume (1,427 articles) and impact factor (h-index:122) among all the relevant journals. The majority of articles were published by developed countries, with the United States having the most. Harvard Medical School and the University of Pennsylvania were the two most productive institutions. Neuroimaging analysis technology and brain functions and diseases were the two major topics of connectome research. The application of machine learning, deep learning, and graph theory analysis in connectome research has become the current trend, while an increasing number of studies were concentrating on dynamic functional connectivity. Meanwhile, researchers have begun investigating alcohol use disorders and migraine in terms of brain connectivity in the past 2 years.</AbstractText><AbstractText Label="CONCLUSION" NlmCategory="UNASSIGNED">This study illustrates a comprehensive overview of connectome research and provides researchers with critical information for understanding the recent trends and hotspots of connectomes.</AbstractText><CopyrightInformation>Copyright &#xa9; 2022 Yan, Fan, Liao and Zhao.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Yan</LastName><ForeName>Yangye</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Tongji University School of Medicine, Shanghai Eastern Hospital Affiliated to Tongji University, Shanghai, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Fan</LastName><ForeName>Guoxin</ForeName><Initials>G</Initials><AffiliationInfo><Affiliation>Department of Pain Medicine, Huazhong University of Science and Technology Union Shenzhen Hospital, Shenzhen, China.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>School of Biomedical Engineering, School of Medicine, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Shenzhen University, Shenzhen, China.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Spine Surgery, Third Affiliated Hospital, Sun Yat-sen University, Guangzhou, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Liao</LastName><ForeName>Xiang</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>Department of Pain Medicine, Huazhong University of Science and Technology Union Shenzhen Hospital, Shenzhen, China.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>School of Biomedical Engineering, School of Medicine, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Shenzhen University, Shenzhen, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhao</LastName><ForeName>Xudong</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>Clinical Research Center for Mental Disorders, Chinese-German Institute of Mental Health, Shanghai Pudong New Area Mental Health Center, School of Medicine, Tongji University, Shanghai, China.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>22</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Front Neurosci</MedlineTA><NlmUniqueID>101478481</NlmUniqueID><ISSNLinking>1662-453X</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Web of Science</Keyword><Keyword MajorTopicYN="N">bibliometric</Keyword><Keyword MajorTopicYN="N">connectome</Keyword><Keyword MajorTopicYN="N">latent Dirichlet allocation</Keyword><Keyword MajorTopicYN="N">neuroscience</Keyword></KeywordList><CoiStatement>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>9</Month><Day>16</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>6</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>4</Hour><Minute>11</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36620450</ArticleId><ArticleId IdType="pmc">PMC9814013</ArticleId><ArticleId IdType="doi">10.3389/fnins.2022.1046562</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Aggarwal A., Agosti E., Singh P. M., Varshini A., Garg K., Chaurasia B., et al. (2021). Scientometric analysis of medical publications during COVID-19 pandemic: The twenty-twenty research boom. Minerva Med. 112 631&#x2013;640. 10.23736/S0026-4806.21.07489-9</Citation><ArticleIdList><ArticleId IdType="doi">10.23736/S0026-4806.21.07489-9</ArticleId><ArticleId IdType="pubmed">34814634</ArticleId></ArticleIdList></Reference><Reference><Citation>Andersson J. L. R., Sotiropoulos S. N. (2016). An integrated approach to correction for off-resonance effects and subject movement in diffusion MR imaging. Neuroimage 125 1063&#x2013;1078. 10.1016/j.neuroimage.2015.10.019</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2015.10.019</ArticleId><ArticleId IdType="pmc">PMC4692656</ArticleId><ArticleId IdType="pubmed">26481672</ArticleId></ArticleIdList></Reference><Reference><Citation>Aria M., Cuccurullo C. (2017). bibliometrix: An R-tool for comprehensive science mapping analysis. J. Informetr. 11 959&#x2013;975. 10.1016/j.joi.2017.08.007</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.joi.2017.08.007</ArticleId></ArticleIdList></Reference><Reference><Citation>Barron D. S., Gao S., Dadashkarimi J., Greene A. S., Spann M. N., Noble S., et al. (2021). Transdiagnostic, connectome-based prediction of memory constructs across psychiatric disorders. Cereb. Cortex 31 2523&#x2013;2533. 10.1093/cercor/bhaa371</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/cercor/bhaa371</ArticleId><ArticleId IdType="pmc">PMC8023861</ArticleId><ArticleId IdType="pubmed">33345271</ArticleId></ArticleIdList></Reference><Reference><Citation>Bernhardt B. C., Fadaie F., Liu M., Caldairou B., Gu S., Jefferies E., et al. (2019). Temporal lobe epilepsy: Hippocampal pathology modulates connectome topology and controllability. Neurology 92 e2209&#x2013;e2220. 10.1212/WNL.0000000000007447</Citation><ArticleIdList><ArticleId IdType="doi">10.1212/WNL.0000000000007447</ArticleId><ArticleId IdType="pmc">PMC6537128</ArticleId><ArticleId IdType="pubmed">31004070</ArticleId></ArticleIdList></Reference><Reference><Citation>Biswal B. B., Mennes M., Zuo X. N., Gohel S., Kelly C., Smith S. M., et al. (2010). Toward discovery science of human brain function. Proc. Natl. Acad. Sci. U.S.A. 107 4734&#x2013;4739. 10.1073/pnas.0911855107</Citation><ArticleIdList><ArticleId IdType="doi">10.1073/pnas.0911855107</ArticleId><ArticleId IdType="pmc">PMC2842060</ArticleId><ArticleId IdType="pubmed">20176931</ArticleId></ArticleIdList></Reference><Reference><Citation>Burke M. J., Joutsa J., Cohen A. L., Soussand L., Cooke D., Burstein R., et al. (2020). Mapping migraine to a common brain network. Brain 143 541&#x2013;553. 10.1093/brain/awz405</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/brain/awz405</ArticleId><ArticleId IdType="pmc">PMC7009560</ArticleId><ArticleId IdType="pubmed">31919494</ArticleId></ArticleIdList></Reference><Reference><Citation>Coppola G., Di Renzo A., Petolicchio B., Tinelli E., Di Lorenzo C., Serrao M., et al. (2020). Increased neural connectivity between the hypothalamus and cortical resting-state functional networks in chronic migraine. J. Neurol. 267 185&#x2013;191. 10.1007/s00415-019-09571-y</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00415-019-09571-y</ArticleId><ArticleId IdType="pubmed">31606759</ArticleId></ArticleIdList></Reference><Reference><Citation>delEtoile J., Adeli H. (2017). Graph theory and brain connectivity in Alzheimer&#x2019;s disease. Neuroscientist 23 616&#x2013;626. 10.1177/1073858417702621</Citation><ArticleIdList><ArticleId IdType="doi">10.1177/1073858417702621</ArticleId><ArticleId IdType="pubmed">28406055</ArticleId></ArticleIdList></Reference><Reference><Citation>Devezas M. A. M. (2021). Shedding light on neuroscience: Two decades of functional near-infrared spectroscopy applications and advances from a bibliometric perspective. J. Neuroimaging 31 641&#x2013;655. 10.1111/jon.12877</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/jon.12877</ArticleId><ArticleId IdType="pubmed">34002425</ArticleId></ArticleIdList></Reference><Reference><Citation>Dick A. S., Bernal B., Tremblay P. (2014). The language connectome: New pathways, new concepts. Neuroscientist 20 453&#x2013;467. 10.1177/1073858413513502</Citation><ArticleIdList><ArticleId IdType="doi">10.1177/1073858413513502</ArticleId><ArticleId IdType="pubmed">24342910</ArticleId></ArticleIdList></Reference><Reference><Citation>Ding H., Wu C., Liao N., Zhan Q., Sun W., Huang Y., et al. (2021). Radiomics in oncology: A 10-year bibliometric analysis. Front. Oncol. 11:689802. 10.3389/fonc.2021.689802</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fonc.2021.689802</ArticleId><ArticleId IdType="pmc">PMC8488302</ArticleId><ArticleId IdType="pubmed">34616671</ArticleId></ArticleIdList></Reference><Reference><Citation>Dong D., Wang Y., Chang X., Luo C., Yao D. (2018). Dysfunction of large-scale brain networks in schizophrenia: A meta-analysis of resting-state functional connectivity. Schizophr. Bull. 44 168&#x2013;181. 10.1093/schbul/sbx034</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/schbul/sbx034</ArticleId><ArticleId IdType="pmc">PMC5767956</ArticleId><ArticleId IdType="pubmed">28338943</ArticleId></ArticleIdList></Reference><Reference><Citation>Elton A., Garbutt J. C., Boettiger C. A. (2021). Risk and resilience for alcohol use disorder revealed in brain functional connectivity. Neuroimage Clin. 32:102801. 10.1016/j.nicl.2021.102801</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.nicl.2021.102801</ArticleId><ArticleId IdType="pmc">PMC8416942</ArticleId><ArticleId IdType="pubmed">34482279</ArticleId></ArticleIdList></Reference><Reference><Citation>Farahani F. V., Karwowski W., Lighthall N. R. (2019). Application of graph theory for identifying connectivity patterns in human brain networks: A systematic review. Front. Neurosci. 13:585. 10.3389/fnins.2019.00585</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fnins.2019.00585</ArticleId><ArticleId IdType="pmc">PMC6582769</ArticleId><ArticleId IdType="pubmed">31249501</ArticleId></ArticleIdList></Reference><Reference><Citation>Fornito A., Zalesky A., Breakspear M. (2013). Graph analysis of the human connectome: Promise, progress, and pitfalls. Neuroimage 80 426&#x2013;444. 10.1016/j.neuroimage.2013.04.087</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2013.04.087</ArticleId><ArticleId IdType="pubmed">23643999</ArticleId></ArticleIdList></Reference><Reference><Citation>Gerchen M. F., Weiss F., Kirsch M., Rentsch A., Halli P., Kiefer F., et al. (2021). Dynamic frontostriatal functional peak connectivity (in alcohol use disorder). Hum. Brain Mapp. 42 36&#x2013;46. 10.1002/hbm.25201</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/hbm.25201</ArticleId><ArticleId IdType="pmc">PMC7721230</ArticleId><ArticleId IdType="pubmed">32885886</ArticleId></ArticleIdList></Reference><Reference><Citation>Glasser M. F., Coalson T. S., Robinson E. C., Hacker C. D., Harwell J., Yacoub E., et al. (2016). A multi-modal parcellation of human cerebral cortex. Nature 536 171&#x2013;178. 10.1038/nature18933</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nature18933</ArticleId><ArticleId IdType="pmc">PMC4990127</ArticleId><ArticleId IdType="pubmed">27437579</ArticleId></ArticleIdList></Reference><Reference><Citation>Glasser M. F., Sotiropoulos S. N., Wilson J. A., Coalson T. S., Fischl B., Andersson J. L., et al. (2013). The minimal preprocessing pipelines for the human connectome project. Neuroimage 80 105&#x2013;124. 10.1016/j.neuroimage.2013.04.127</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2013.04.127</ArticleId><ArticleId IdType="pmc">PMC3720813</ArticleId><ArticleId IdType="pubmed">23668970</ArticleId></ArticleIdList></Reference><Reference><Citation>Gong B., Naveed S., Hafeez D. M., Afzal K. I., Majeed S., Abele J., et al. (2019). Neuroimaging in psychiatric disorders: A bibliometric analysis of the 100 most highly cited articles. J. Neuroimaging 29 14&#x2013;33. 10.1111/jon.12570</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/jon.12570</ArticleId><ArticleId IdType="pubmed">30311320</ArticleId></ArticleIdList></Reference><Reference><Citation>Griffiths T. L., Steyvers M. (2004). Finding scientific topics. Proc. Natl. Acad. Sci. U.S.A. 101 (Suppl. 1) 5228&#x2013;5235. 10.1073/pnas.0307752101</Citation><ArticleIdList><ArticleId IdType="doi">10.1073/pnas.0307752101</ArticleId><ArticleId IdType="pmc">PMC387300</ArticleId><ArticleId IdType="pubmed">14872004</ArticleId></ArticleIdList></Reference><Reference><Citation>Guler A. T., Waaijer C. J., Palmblad M. (2016). Scientific workflows for bibliometrics. Scientometrics 107 385&#x2013;398. 10.1007/s11192-016-1885-6</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11192-016-1885-6</ArticleId><ArticleId IdType="pmc">PMC4833826</ArticleId><ArticleId IdType="pubmed">27122644</ArticleId></ArticleIdList></Reference><Reference><Citation>He Y., Chen Z. J., Evans A. C. (2007). Small-world anatomical networks in the human brain revealed by cortical thickness from MRI. Cereb. Cortex 17 2407&#x2013;2419. 10.1093/cercor/bhl149</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/cercor/bhl149</ArticleId><ArticleId IdType="pubmed">17204824</ArticleId></ArticleIdList></Reference><Reference><Citation>Horn A., Reich M., Vorwerk J., Li N., Wenzel G., Fang Q., et al. (2017). Connectivity predicts deep brain stimulation outcome in Parkinson disease. Ann. Neurol. 82 67&#x2013;78. 10.1002/ana.24974</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/ana.24974</ArticleId><ArticleId IdType="pmc">PMC5880678</ArticleId><ArticleId IdType="pubmed">28586141</ArticleId></ArticleIdList></Reference><Reference><Citation>Hutchison R. M., Womelsdorf T., Allen E. A., Bandettini P. A., Calhoun V. D., Corbetta M., et al. (2013). Dynamic functional connectivity: Promise, issues, and interpretations. Neuroimage 80 360&#x2013;378. 10.1016/j.neuroimage.2013.05.079</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2013.05.079</ArticleId><ArticleId IdType="pmc">PMC3807588</ArticleId><ArticleId IdType="pubmed">23707587</ArticleId></ArticleIdList></Reference><Reference><Citation>Kaestner E., Balachandra A. R., Bahrami N., Reyes A., Lalani S. J., Macari A. C., et al. (2020). The white matter connectome as an individualized biomarker of language impairment in temporal lobe epilepsy. Neuroimage Clin. 25:102125. 10.1016/j.nicl.2019.102125</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.nicl.2019.102125</ArticleId><ArticleId IdType="pmc">PMC6953962</ArticleId><ArticleId IdType="pubmed">31927128</ArticleId></ArticleIdList></Reference><Reference><Citation>Koch P. J., Park C. H., Girard G., Beanato E., Egger P., Evangelista G. G., et al. (2021). The structural connectome and motor recovery after stroke: Predicting natural recovery. Brain 144 2107&#x2013;2119. 10.1093/brain/awab082</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/brain/awab082</ArticleId><ArticleId IdType="pmc">PMC8370413</ArticleId><ArticleId IdType="pubmed">34237143</ArticleId></ArticleIdList></Reference><Reference><Citation>Krismer F., Seppi K. (2021). The Parkinson disease connectome&#x2013;insights from new imaging studies. Nat. Rev. Neurol. 17 527&#x2013;528. 10.1038/s41582-021-00543-3</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41582-021-00543-3</ArticleId><ArticleId IdType="pubmed">34312532</ArticleId></ArticleIdList></Reference><Reference><Citation>Lichtman J. W., Sanes J. R. (2008). Ome sweet ome: What can the genome tell us about the connectome? Curr. Opin. Neurobiol. 18 346&#x2013;353. 10.1016/j.conb.2008.08.010</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.conb.2008.08.010</ArticleId><ArticleId IdType="pmc">PMC2735215</ArticleId><ArticleId IdType="pubmed">18801435</ArticleId></ArticleIdList></Reference><Reference><Citation>Lynall M. E., Bassett D. S., Kerwin R., McKenna P. J., Kitzbichler M., Muller U., et al. (2010). Functional connectivity and brain networks in schizophrenia. J. Neurosci. 30 9477&#x2013;9487. 10.1523/JNEUROSCI.0333-10.2010</Citation><ArticleIdList><ArticleId IdType="doi">10.1523/JNEUROSCI.0333-10.2010</ArticleId><ArticleId IdType="pmc">PMC2914251</ArticleId><ArticleId IdType="pubmed">20631176</ArticleId></ArticleIdList></Reference><Reference><Citation>Malagurski B., Liem F., Oschwald J., M&#xe9;rillat S., J&#xe4;ncke L. (2020). Longitudinal functional brain network reconfiguration in healthy aging. Hum. Brain Mapp. 41 4829&#x2013;4845. 10.1002/hbm.25161</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/hbm.25161</ArticleId><ArticleId IdType="pmc">PMC7643380</ArticleId><ArticleId IdType="pubmed">32857461</ArticleId></ArticleIdList></Reference><Reference><Citation>Malekpour M. R., Abbasi-Kangevari M., Azadnajafabad S., Ghamari S. H., Rezaei N., Rezazadeh-Khadem S., et al. (2021). How the scientific community responded to the COVID-19 pandemic: A subject-level time-trend bibliometric analysis. PLoS One 16:e0258064. 10.1371/journal.pone.0258064</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0258064</ArticleId><ArticleId IdType="pmc">PMC8483337</ArticleId><ArticleId IdType="pubmed">34591941</ArticleId></ArticleIdList></Reference><Reference><Citation>Mill R. D., Ito T., Cole M. W. (2017). From connectome to cognition: The search for mechanism in human functional brain networks. Neuroimage 160 124&#x2013;139. 10.1016/j.neuroimage.2017.01.060</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2017.01.060</ArticleId><ArticleId IdType="pmc">PMC5529276</ArticleId><ArticleId IdType="pubmed">28131891</ArticleId></ArticleIdList></Reference><Reference><Citation>Moguilner S., Garc&#xed;a A. M., Perl Y. S., Tagliazucchi E., Piguet O., Kumfor F., et al. (2021). Dynamic brain fluctuations outperform connectivity measures and mirror pathophysiological profiles across dementia subtypes: A multicenter study. Neuroimage 225:117522. 10.1016/j.neuroimage.2020.117522</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2020.117522</ArticleId><ArticleId IdType="pmc">PMC7832160</ArticleId><ArticleId IdType="pubmed">33144220</ArticleId></ArticleIdList></Reference><Reference><Citation>Neher P. F., C&#xf4;t&#xe9; M. A., Houde J. C., Descoteaux M., Maier-Hein K. H. (2017). Fiber tractography using machine learning. Neuroimage 158 417&#x2013;429. 10.1016/j.neuroimage.2017.07.028</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2017.07.028</ArticleId><ArticleId IdType="pubmed">28716716</ArticleId></ArticleIdList></Reference><Reference><Citation>Oh S. W., Harris J. A., Ng L., Winslow B., Cain N., Mihalas S., et al. (2014). A mesoscale connectome of the mouse brain. Nature 508 207&#x2013;214. 10.1038/nature13186</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nature13186</ArticleId><ArticleId IdType="pmc">PMC5102064</ArticleId><ArticleId IdType="pubmed">24695228</ArticleId></ArticleIdList></Reference><Reference><Citation>Ousdal O. T., Kaufmann T., Kolsk&#xe5;r K., Vik A., Wehling E., Lundervold A. J., et al. (2020). Longitudinal stability of the brain functional connectome is associated with episodic memory performance in aging. Hum. Brain Mapp. 41 697&#x2013;709. 10.1002/hbm.24833</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/hbm.24833</ArticleId><ArticleId IdType="pmc">PMC7268077</ArticleId><ArticleId IdType="pubmed">31652017</ArticleId></ArticleIdList></Reference><Reference><Citation>Sadaghiani S., Wirsich J. (2020). Intrinsic connectome organization across temporal scales: New insights from cross-modal approaches. Netw. Neurosci. 4 1&#x2013;29. 10.1162/netn_a_00114</Citation><ArticleIdList><ArticleId IdType="doi">10.1162/netn_a_00114</ArticleId><ArticleId IdType="pmc">PMC7006873</ArticleId><ArticleId IdType="pubmed">32043042</ArticleId></ArticleIdList></Reference><Reference><Citation>Schwarz C. (2018). Ldagibbs: A command for topic modeling in Stata using latent Dirichlet allocation. Stata J. 18 101&#x2013;117. 10.1177/1536867X1801800107</Citation><ArticleIdList><ArticleId IdType="doi">10.1177/1536867X1801800107</ArticleId></ArticleIdList></Reference><Reference><Citation>Seung H. S. (2009). Reading the book of memory: Sparse sampling versus dense mapping of connectomes. Neuron 62 17&#x2013;29. 10.1016/j.neuron.2009.03.020</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuron.2009.03.020</ArticleId><ArticleId IdType="pubmed">19376064</ArticleId></ArticleIdList></Reference><Reference><Citation>Shatte A. B. R., Hutchinson D. M., Teague S. J. (2019). Machine learning in mental health: A scoping review of methods and applications. Psychol. Med. 49 1426&#x2013;1448. 10.1017/S0033291719000151</Citation><ArticleIdList><ArticleId IdType="doi">10.1017/S0033291719000151</ArticleId><ArticleId IdType="pubmed">30744717</ArticleId></ArticleIdList></Reference><Reference><Citation>Silasi G., Murphy T. H. (2014). Stroke and the connectome: How connectivity guides therapeutic intervention. Neuron 83 1354&#x2013;1368. 10.1016/j.neuron.2014.08.052</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuron.2014.08.052</ArticleId><ArticleId IdType="pubmed">25233317</ArticleId></ArticleIdList></Reference><Reference><Citation>Sporns O. (2013). The human connectome: Origins and challenges. Neuroimage 80 53&#x2013;61. 10.1016/j.neuroimage.2013.03.023</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2013.03.023</ArticleId><ArticleId IdType="pubmed">23528922</ArticleId></ArticleIdList></Reference><Reference><Citation>Sporns O. (2018). Graph theory methods: Applications in brain networks. Dialogues Clin. Neurosci. 20 111&#x2013;121. 10.31887/DCNS.2018.20.2/osporns</Citation><ArticleIdList><ArticleId IdType="doi">10.31887/DCNS.2018.20.2/osporns</ArticleId><ArticleId IdType="pmc">PMC6136126</ArticleId><ArticleId IdType="pubmed">30250388</ArticleId></ArticleIdList></Reference><Reference><Citation>Sporns O., Betzel R. F. (2016). Modular brain networks. Annu. Rev. Psychol. 67 613&#x2013;640. 10.1146/annurev-psych-122414-033634</Citation><ArticleIdList><ArticleId IdType="doi">10.1146/annurev-psych-122414-033634</ArticleId><ArticleId IdType="pmc">PMC4782188</ArticleId><ArticleId IdType="pubmed">26393868</ArticleId></ArticleIdList></Reference><Reference><Citation>Sporns O., Tononi G., Kotter R. (2005). The human connectome: A structural description of the human brain. PLoS Comput. Biol. 1:e42. 10.1371/journal.pcbi.0010042</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pcbi.0010042</ArticleId><ArticleId IdType="pmc">PMC1239902</ArticleId><ArticleId IdType="pubmed">16201007</ArticleId></ArticleIdList></Reference><Reference><Citation>Taylor S. R., Santpere G., Weinreb A., Barrett A., Reilly M. B., Xu C., et al. (2021). Molecular topography of an entire nervous system. Cell 184 4329&#x2013;4347.e23. 10.1016/j.cell.2021.06.023</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cell.2021.06.023</ArticleId><ArticleId IdType="pmc">PMC8710130</ArticleId><ArticleId IdType="pubmed">34237253</ArticleId></ArticleIdList></Reference><Reference><Citation>Thompson P. M., Ge T., Glahn D. C., Jahanshad N., Nichols T. E. (2013). Genetics of the connectome. Neuroimage 80 475&#x2013;488. 10.1016/j.neuroimage.2013.05.013</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2013.05.013</ArticleId><ArticleId IdType="pmc">PMC3905600</ArticleId><ArticleId IdType="pubmed">23707675</ArticleId></ArticleIdList></Reference><Reference><Citation>Towlson E. K., V&#xe9;rtes P. E., Ahnert S. E., Schafer W. R., Bullmore E. T. (2013). The rich club of the C. elegans neuronal connectome. J. Neurosci. 33 6380&#x2013;6387. 10.1523/JNEUROSCI.3784-12.2013</Citation><ArticleIdList><ArticleId IdType="doi">10.1523/JNEUROSCI.3784-12.2013</ArticleId><ArticleId IdType="pmc">PMC4104292</ArticleId><ArticleId IdType="pubmed">23575836</ArticleId></ArticleIdList></Reference><Reference><Citation>Tran B. X., Nghiem S., Sahin O., Vu T. M., Ha G. H., Vu G. T., et al. (2019). Modeling research topics for artificial intelligence applications in medicine: Latent Dirichlet allocation application study. J. Med. Internet Res. 21:e15511. 10.2196/15511</Citation><ArticleIdList><ArticleId IdType="doi">10.2196/15511</ArticleId><ArticleId IdType="pmc">PMC6858616</ArticleId><ArticleId IdType="pubmed">31682577</ArticleId></ArticleIdList></Reference><Reference><Citation>United Nations (2020). World economic situation and prospects 2020. New York, NY: United Nation Publication. 10.18356/ee1a3197-en</Citation><ArticleIdList><ArticleId IdType="doi">10.18356/ee1a3197-en</ArticleId></ArticleIdList></Reference><Reference><Citation>van den Heuvel M. P., Sporns O. (2011). Rich-club organization of the human connectome. J. Neurosci. 31 15775&#x2013;15786. 10.1523/JNEUROSCI.3539-11.2011</Citation><ArticleIdList><ArticleId IdType="doi">10.1523/JNEUROSCI.3539-11.2011</ArticleId><ArticleId IdType="pmc">PMC6623027</ArticleId><ArticleId IdType="pubmed">22049421</ArticleId></ArticleIdList></Reference><Reference><Citation>van den Heuvel M. P., Sporns O. (2019). A cross-disorder connectome landscape of brain dysconnectivity. Nat. Rev. Neurosci. 20 435&#x2013;446. 10.1038/s41583-019-0177-6</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41583-019-0177-6</ArticleId><ArticleId IdType="pmc">PMC8864539</ArticleId><ArticleId IdType="pubmed">31127193</ArticleId></ArticleIdList></Reference><Reference><Citation>Van Dijk K. R., Sabuncu M. R., Buckner R. L. (2012). The influence of head motion on intrinsic functional connectivity MRI. Neuroimage 59 431&#x2013;438. 10.1016/j.neuroimage.2011.07.044</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2011.07.044</ArticleId><ArticleId IdType="pmc">PMC3683830</ArticleId><ArticleId IdType="pubmed">21810475</ArticleId></ArticleIdList></Reference><Reference><Citation>Van Essen D. C., Smith S. M., Barch D. M., Behrens T. E., Yacoub E., Ugurbil K. (2013). The WU-Minn human connectome project: An overview. Neuroimage 80 62&#x2013;79. 10.1016/j.neuroimage.2013.05.041</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2013.05.041</ArticleId><ArticleId IdType="pmc">PMC3724347</ArticleId><ArticleId IdType="pubmed">23684880</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang X., Huang W., Su L., Xing Y., Jessen F., Sun Y., et al. (2020). Neuroimaging advances regarding subjective cognitive decline in preclinical Alzheimer&#x2019;s disease. Mol. Neurodegener. 15:55. 10.1186/s13024-020-00395-3</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s13024-020-00395-3</ArticleId><ArticleId IdType="pmc">PMC7507636</ArticleId><ArticleId IdType="pubmed">32962744</ArticleId></ArticleIdList></Reference><Reference><Citation>Xia M., Wang J., He Y. (2013). BrainNet viewer: A network visualization tool for human brain connectomics. PLoS One 8:e68910. 10.1371/journal.pone.0068910</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0068910</ArticleId><ArticleId IdType="pmc">PMC3701683</ArticleId><ArticleId IdType="pubmed">23861951</ArticleId></ArticleIdList></Reference><Reference><Citation>Yan C. G., Cheung B., Kelly C., Colcombe S., Craddock R. C., Di Martino A., et al. (2013a). A comprehensive assessment of regional variation in the impact of head micromovements on functional connectomics. Neuroimage 76 183&#x2013;201. 10.1016/j.neuroimage.2013.03.004</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2013.03.004</ArticleId><ArticleId IdType="pmc">PMC3896129</ArticleId><ArticleId IdType="pubmed">23499792</ArticleId></ArticleIdList></Reference><Reference><Citation>Yan C. G., Craddock R. C., Zuo X. N., Zang Y. F., Milham M. P. (2013b). Standardizing the intrinsic brain: Towards robust measurement of inter-individual variation in 1000 functional connectomes. Neuroimage 80 246&#x2013;262. 10.1016/j.neuroimage.2013.04.081</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2013.04.081</ArticleId><ArticleId IdType="pmc">PMC4074397</ArticleId><ArticleId IdType="pubmed">23631983</ArticleId></ArticleIdList></Reference><Reference><Citation>Yan W., Zheng K., Weng L., Chen C., Kiartivich S., Jiang X., et al. (2020). Bibliometric evaluation of 2000-2019 publications on functional near-infrared spectroscopy. Neuroimage 220:117121. 10.1016/j.neuroimage.2020.117121</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2020.117121</ArticleId><ArticleId IdType="pubmed">32619709</ArticleId></ArticleIdList></Reference><Reference><Citation>Yu M., Sporns O., Saykin A. J. (2021). The human connectome in Alzheimer disease&#x2013;relationship to biomarkers and genetics. Nat. Rev. Neurol. 17 545&#x2013;563. 10.1038/s41582-021-00529-1</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41582-021-00529-1</ArticleId><ArticleId IdType="pmc">PMC8403643</ArticleId><ArticleId IdType="pubmed">34285392</ArticleId></ArticleIdList></Reference><Reference><Citation>Yun J. Y., Kim Y. K. (2021). Graph theory approach for the structural-functional brain connectome of depression. Prog. Neuropsychopharmacol. Biol. Psychiatry 111:110401. 10.1016/j.pnpbp.2021.110401</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.pnpbp.2021.110401</ArticleId><ArticleId IdType="pubmed">34265367</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang Z., Wang Z., Huang Y. (2021). A bibliometric analysis of 8,276 publications during the past 25 years on cholangiocarcinoma by machine learning. Front. Oncol. 11:687904. 10.3389/fonc.2021.687904</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fonc.2021.687904</ArticleId><ArticleId IdType="pmc">PMC8453170</ArticleId><ArticleId IdType="pubmed">34557406</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36620440</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic-eCollection"><Journal><ISSN IssnType="Print">1662-4548</ISSN><JournalIssue CitedMedium="Print"><Volume>16</Volume><PubDate><Year>2022</Year></PubDate></JournalIssue><Title>Frontiers in neuroscience</Title><ISOAbbreviation>Front Neurosci</ISOAbbreviation></Journal><ArticleTitle>Editorial: Deep learning techniques and their applications to the healthy and disordered brain - during development through adulthood and beyond.</ArticleTitle><Pagination><StartPage>1118233</StartPage><MedlinePgn>1118233</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">1118233</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3389/fnins.2022.1118233</ELocationID><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Shmuel</LastName><ForeName>Amir</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Department of Neurology, McConnell Brain Imaging Centre, Montreal Neurological Institute, McGill University, Montreal, QC, Canada.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Neurosurgery, McConnell Brain Imaging Centre, Montreal Neurological Institute, McGill University, Montreal, QC, Canada.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Physiology and Biomedical Engineering, McConnell Brain Imaging Centre, Montreal Neurological Institute, McGill University, Montreal, QC, Canada.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Park</LastName><ForeName>Hyunjin</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Center for Neuroscience Imaging Research, Institute for Basic Science, Suwon, South Korea.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Rathi</LastName><ForeName>Yogesh</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Department of Psychiatry, Harvard Medical School, Boston, MA, United States.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, Harvard Medical School, Boston, MA, United States.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yang</LastName><ForeName>Albert</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Institute of Brain Science, National Yang-Ming Chiao Tung University, Taipei, Taiwan.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016421">Editorial</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>22</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Front Neurosci</MedlineTA><NlmUniqueID>101478481</NlmUniqueID><ISSNLinking>1662-453X</ISSNLinking></MedlineJournalInfo><CommentsCorrectionsList><CommentsCorrections RefType="CommentOn"><RefSource>Editorial on the Research Topic Deep learning techniques and their applications to the healthy and disordered brain - during development through adulthood and beyond</RefSource></CommentsCorrections></CommentsCorrectionsList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Alzheimer's disease</Keyword><Keyword MajorTopicYN="N">Parkinson's disease</Keyword><Keyword MajorTopicYN="N">autism</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword><Keyword MajorTopicYN="N">medical image segmentation</Keyword><Keyword MajorTopicYN="N">multiple sclerosis</Keyword><Keyword MajorTopicYN="N">neuroimaging-based diagnosis</Keyword><Keyword MajorTopicYN="N">schizophrenia</Keyword></KeywordList><CoiStatement>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>12</Month><Day>7</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>8</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>4</Hour><Minute>11</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36620440</ArticleId><ArticleId IdType="pmc">PMC9815760</ArticleId><ArticleId IdType="doi">10.3389/fnins.2022.1118233</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Feng X., Provenzano F. A., Small S. A. (2022). A deep learning MRI approach outperforms other biomarkers of prodromal Alzheimer's disease. Alzheimers Res. Therapy 14, 45. 10.1186/s13195-022-00985-x</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s13195-022-00985-x</ArticleId><ArticleId IdType="pmc">PMC8966329</ArticleId><ArticleId IdType="pubmed">35351193</ArticleId></ArticleIdList></Reference><Reference><Citation>K&#xfc;stner T., Armanious K., Yang J., Yang B., Schick F., Gatidis S. (2019). Retrospective correction of motion-affected MR images using deep learning frameworks. Magn. Reson. Med. 82, 1527&#x2013;1540. 10.1002/mrm.27783</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.27783</ArticleId><ArticleId IdType="pubmed">31081955</ArticleId></ArticleIdList></Reference><Reference><Citation>Pizarro R., Assemlal H. E., De Nigris D., Elliot C., Antel S., Arnold D., Shmuel A. (2019). Using deep learning algorithms to automatically identify the brain MRI contrast: implications for managing large databases. Neuroinformatics 17, 115&#x2013;130. 10.1007/s12021-018-9387-8</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s12021-018-9387-8</ArticleId><ArticleId IdType="pubmed">29956131</ArticleId></ArticleIdList></Reference><Reference><Citation>Zeng G., Guo Y., Zhan J., Wang Z., Lai Z., Du X., Qu X., Guo D. (2021). A review on deep learning MRI reconstruction without fully sampled k-space. BMC Med. Imaging 21, 195. 10.1186/s12880-021-00727-9</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s12880-021-00727-9</ArticleId><ArticleId IdType="pmc">PMC8710001</ArticleId><ArticleId IdType="pubmed">34952572</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36620152</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Print">2223-4292</ISSN><JournalIssue CitedMedium="Print"><Volume>13</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>01</Day></PubDate></JournalIssue><Title>Quantitative imaging in medicine and surgery</Title><ISOAbbreviation>Quant Imaging Med Surg</ISOAbbreviation></Journal><ArticleTitle>Deep learning-assisted classification of calcaneofibular ligament injuries in the ankle joint.</ArticleTitle><Pagination><StartPage>80</StartPage><EndPage>93</EndPage><MedlinePgn>80-93</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.21037/qims-22-470</ELocationID><Abstract><AbstractText Label="BACKGROUND" NlmCategory="UNASSIGNED">The classification of calcaneofibular ligament (CFL) injuries on magnetic resonance imaging (MRI) is time-consuming and subject to substantial interreader variability. This study explores the feasibility of classifying CFL injuries using deep learning methods by comparing them with the classifications of musculoskeletal (MSK) radiologists and further examines image cropping screening and calibration methods.</AbstractText><AbstractText Label="METHODS" NlmCategory="UNASSIGNED">The imaging data of 1,074 patients who underwent ankle arthroscopy and MRI examinations in our hospital were retrospectively analyzed. According to the arthroscopic findings, patients were divided into normal (class 0, n=475); degeneration, strain, and partial tear (class 1, n=217); and complete tear (class 2, n=382) groups. All patients were divided into training, validation, and test sets at a ratio of 8:1:1. After preprocessing, the images were cropped using Mask region-based convolutional neural network (R-CNN), followed by the application of an attention algorithm for image screening and calibration and the implementation of LeNet-5 for CFL injury classification. The diagnostic effects of the axial, coronal, and combined models were compared, and the best method was selected for outgroup validation. The diagnostic results of the models in the intragroup and outgroup test sets were compared with those results of 4 MSK radiologists of different seniorities.</AbstractText><AbstractText Label="RESULTS" NlmCategory="UNASSIGNED">The mean average precision (mAP) of the Mask R-CNN using the attention algorithm for the left and right image cropping of axial and coronal sequences was 0.90-0.96. The accuracy of LeNet-5 for classifying classes 0-2 was 0.92, 0.93, and 0.92, respectively, for the axial sequences and 0.89, 0.92, and 0.90, respectively, for the coronal sequences. After sequence combination, the classification accuracy for classes 0-2 was 0.95, 0.97, and 0.96, respectively. The mean accuracies of the 4 MSK radiologists in classifying the intragroup test set as classes 0-2 were 0.94, 0.91, 0.86, and 0.85, all of which were significantly different from the model. The mean accuracies of the MSK radiologists in classifying the outgroup test set as classes 0-2 were 0.92, 0.91, 0.87, and 0.85, with the 2 senior MSK radiologists demonstrating similar diagnostic performance to the model and the junior MSK radiologists demonstrating worse accuracy.</AbstractText><AbstractText Label="CONCLUSIONS" NlmCategory="UNASSIGNED">Deep learning can be used to classify CFL injuries at similar levels to those of MSK radiologists. Adding an attention algorithm after cropping is helpful for accurately cropping CFL images.</AbstractText><CopyrightInformation>2023 Quantitative Imaging in Medicine and Surgery. All rights reserved.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Ni</LastName><ForeName>Ming</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Radiology, Peking University Third Hospital, Beijing, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhao</LastName><ForeName>Yuqing</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Department of Radiology, Peking University Third Hospital, Beijing, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wen</LastName><ForeName>Xiaoyi</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>Institute of Statistics and Big Data, Renmin University of China, Beijing, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lang</LastName><ForeName>Ning</ForeName><Initials>N</Initials><AffiliationInfo><Affiliation>Department of Radiology, Peking University Third Hospital, Beijing, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wang</LastName><ForeName>Qizheng</ForeName><Initials>Q</Initials><AffiliationInfo><Affiliation>Department of Radiology, Peking University Third Hospital, Beijing, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Chen</LastName><ForeName>Wen</ForeName><Initials>W</Initials><AffiliationInfo><Affiliation>Department of Radiology, Peking University Third Hospital, Beijing, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zeng</LastName><ForeName>Xiangzhu</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>Department of Radiology, Peking University Third Hospital, Beijing, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yuan</LastName><ForeName>Huishu</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>Department of Radiology, Peking University Third Hospital, Beijing, China.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>10</Month><Day>13</Day></ArticleDate></Article><MedlineJournalInfo><Country>China</Country><MedlineTA>Quant Imaging Med Surg</MedlineTA><NlmUniqueID>101577942</NlmUniqueID><ISSNLinking>2223-4306</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">ankle</Keyword><Keyword MajorTopicYN="N">diagnosis</Keyword><Keyword MajorTopicYN="N">magnetic resonance imaging (MRI)</Keyword></KeywordList><CoiStatement>Conflicts of Interest: All authors have completed the ICMJE uniform disclosure form (available at https://qims.amegroups.com/article/view/10.21037/qims-22-470/coif). The authors report that this study received funding from the National Natural Science Foundation of China (No. 81871326). The authors have no other conflicts of interest to declare.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>5</Month><Day>12</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>9</Month><Day>7</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>4</Hour><Minute>6</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>ppublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36620152</ArticleId><ArticleId IdType="pmc">PMC9816759</ArticleId><ArticleId IdType="doi">10.21037/qims-22-470</ArticleId><ArticleId IdType="pii">qims-13-01-80</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Fong DT, Ha SC, Mok KM, Chan CW, Chan KM. Kinematics analysis of ankle inversion ligamentous sprain injuries in sports: five cases from televised tennis competitions. Am J Sports Med 2012;40:2627-32. 10.1177/0363546512458259</Citation><ArticleIdList><ArticleId IdType="doi">10.1177/0363546512458259</ArticleId><ArticleId IdType="pubmed">22967824</ArticleId></ArticleIdList></Reference><Reference><Citation>Brostr&#xf6;m L. Sprained ankles. V. Treatment and prognosis in recent ligament ruptures. Acta Chir Scand 1966;132:537-50.</Citation><ArticleIdList><ArticleId IdType="pubmed">5972556</ArticleId></ArticleIdList></Reference><Reference><Citation>Pereira BS, van Dijk CN, Andrade R, Casaroli-Marano RP, Espregueira-Mendes J, Oliva XM. The calcaneofibular ligament has distinct anatomic morphological variants: an anatomical cadaveric study. Knee Surg Sports Traumatol Arthrosc 2020;28:40-7. 10.1007/s00167-019-05797-5</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00167-019-05797-5</ArticleId><ArticleId IdType="pubmed">31776625</ArticleId></ArticleIdList></Reference><Reference><Citation>Cao S, Wang C, Ma X, Wang X, Huang J, Zhang C. Imaging diagnosis for chronic lateral ankle ligament injury: a systemic review with meta-analysis. J Orthop Surg Res 2018;13:122. 10.1186/s13018-018-0811-4</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s13018-018-0811-4</ArticleId><ArticleId IdType="pmc">PMC5964890</ArticleId><ArticleId IdType="pubmed">29788978</ArticleId></ArticleIdList></Reference><Reference><Citation>Hunt KJ, Pereira H, Kelley J, Anderson N, Fuld R, Baldini T, Kumparatana P, D'Hooghe P. The Role of Calcaneofibular Ligament Injury in Ankle Instability: Implications for Surgical Management. Am J Sports Med 2019;47:431-7. 10.1177/0363546518815160</Citation><ArticleIdList><ArticleId IdType="doi">10.1177/0363546518815160</ArticleId><ArticleId IdType="pubmed">30571138</ArticleId></ArticleIdList></Reference><Reference><Citation>Michels F, Pereira H, Calder J, Matricali G, Glazebrook M, Guillo S, et al. Searching for consensus in the approach to patients with chronic lateral ankle instability: ask the expert. Knee Surg Sports Traumatol Arthrosc 2018;26:2095-102. 10.1007/s00167-017-4556-0</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00167-017-4556-0</ArticleId><ArticleId IdType="pubmed">28439639</ArticleId></ArticleIdList></Reference><Reference><Citation>De Leeuw PAJ, Vega J, Karlsson J, Dalmau-Pastor M. The posterior fibulotalocalcaneal ligament complex: a forgotten ligament. Knee Surg Sports Traumatol Arthrosc 2021;29:1627-34. 10.1007/s00167-020-06431-5</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00167-020-06431-5</ArticleId><ArticleId IdType="pmc">PMC8038989</ArticleId><ArticleId IdType="pubmed">33486559</ArticleId></ArticleIdList></Reference><Reference><Citation>Shakked R, Sheskier S. Acute and Chronic Lateral Ankle Instability Diagnosis, Management, and New Concepts. Bull Hosp Jt Dis (2013) 2017;75:71-80.</Citation><ArticleIdList><ArticleId IdType="pubmed">28214465</ArticleId></ArticleIdList></Reference><Reference><Citation>Park HJ, Lee SY, Park NH, Kim E, Chung EC, Kook SH, Lee JW. Usefulness of the oblique coronal plane in ankle MRI of the calcaneofibular ligament. Clin Radiol 2015;70:416-23. 10.1016/j.crad.2014.12.008</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.crad.2014.12.008</ArticleId><ArticleId IdType="pubmed">25573813</ArticleId></ArticleIdList></Reference><Reference><Citation>Park HJ, Cha SD, Kim SS, Rho MH, Kwag HJ, Park NH, Lee SY. Accuracy of MRI findings in chronic lateral ankle ligament injury: comparison with surgical findings. Clin Radiol 2012;67:313-8. 10.1016/j.crad.2011.08.025</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.crad.2011.08.025</ArticleId><ArticleId IdType="pubmed">22078461</ArticleId></ArticleIdList></Reference><Reference><Citation>Kumar V, Triantafyllopoulos I, Panagopoulos A, Fitzgerald S, Van Niekerk L. Deficiencies of MRI in the diagnosis of chronic symptomatic lateral ankle ligament injuries. Foot Ankle Surg 2007;13:171-6. 10.1016/j.fas.2007.04.002</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.fas.2007.04.002</ArticleId></ArticleIdList></Reference><Reference><Citation>Hattori S, Nimura A, Koyama M, Tsutsumi M, Amaha K, Ohuchi H, Akita K. Dorsiflexion is more feasible than plantar flexion in ultrasound evaluation of the calcaneofibular ligament: a combination study of ultrasound and cadaver. Knee Surg Sports Traumatol Arthrosc 2020;28:262-9. 10.1007/s00167-019-05630-z</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00167-019-05630-z</ArticleId><ArticleId IdType="pubmed">31327035</ArticleId></ArticleIdList></Reference><Reference><Citation>Schork NJ. Artificial Intelligence and Personalized Medicine. Cancer Treat Res 2019;178:265-83. 10.1007/978-3-030-16391-4_11</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-030-16391-4_11</ArticleId><ArticleId IdType="pmc">PMC7580505</ArticleId><ArticleId IdType="pubmed">31209850</ArticleId></ArticleIdList></Reference><Reference><Citation>Currie G. Intelligent Imaging: Anatomy of Machine Learning and Deep Learning. J Nucl Med Technol 2019;47:273-81. 10.2967/jnmt.119.232470</Citation><ArticleIdList><ArticleId IdType="doi">10.2967/jnmt.119.232470</ArticleId><ArticleId IdType="pubmed">31401617</ArticleId></ArticleIdList></Reference><Reference><Citation>Olveres J, Gonz&#xe1;lez G, Torres F, Moreno-Tagle JC, Carbajal-Degante E, Valencia-Rodr&#xed;guez A, M&#xe9;ndez-S&#xe1;nchez N, Escalante-Ram&#xed;rez B. What is new in computer vision and artificial intelligence in medical image analysis applications. Quant Imaging Med Surg 2021;11:3830-53. 10.21037/qims-20-1151</Citation><ArticleIdList><ArticleId IdType="doi">10.21037/qims-20-1151</ArticleId><ArticleId IdType="pmc">PMC8245941</ArticleId><ArticleId IdType="pubmed">34341753</ArticleId></ArticleIdList></Reference><Reference><Citation>Ng D, Du H, Yao MM, Kosik RO, Chan WP, Feng M. Today's radiologists meet tomorrow's AI: the promises, pitfalls, and unbridled potential. Quant Imaging Med Surg 2021;11:2775-9. 10.21037/qims-20-1083</Citation><ArticleIdList><ArticleId IdType="doi">10.21037/qims-20-1083</ArticleId><ArticleId IdType="pmc">PMC8107304</ArticleId><ArticleId IdType="pubmed">34079741</ArticleId></ArticleIdList></Reference><Reference><Citation>Leung K, Zhang B, Tan J, Shen Y, Geras KJ, Babb JS, Cho K, Chang G, Deniz CM. Prediction of Total Knee Replacement and Diagnosis of Osteoarthritis by Using Deep Learning on Knee Radiographs: Data from the Osteoarthritis Initiative. Radiology 2020;296:584-93. 10.1148/radiol.2020192091</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2020192091</ArticleId><ArticleId IdType="pmc">PMC7434649</ArticleId><ArticleId IdType="pubmed">32573386</ArticleId></ArticleIdList></Reference><Reference><Citation>Chea P, Mandell JC. Current applications and future directions of deep learning in musculoskeletal radiology. Skeletal Radiol 2020;49:183-97. 10.1007/s00256-019-03284-z</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00256-019-03284-z</ArticleId><ArticleId IdType="pubmed">31377836</ArticleId></ArticleIdList></Reference><Reference><Citation>Kijowski R, Liu F, Caliva F, Pedoia V. Deep Learning for Lesion Detection, Progression, and Prediction of Musculoskeletal Disease. J Magn Reson Imaging 2020;52:1607-19. 10.1002/jmri.27001</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmri.27001</ArticleId><ArticleId IdType="pmc">PMC7251925</ArticleId><ArticleId IdType="pubmed">31763739</ArticleId></ArticleIdList></Reference><Reference><Citation>Pei Y, Yang W, Wei S, Cai R, Li J, Guo S, Li Q, Wang J, Li X. Automated measurement of hip-knee-ankle angle on the unilateral lower limb X-rays using deep learning. Phys Eng Sci Med 2021;44:53-62. 10.1007/s13246-020-00951-7</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s13246-020-00951-7</ArticleId><ArticleId IdType="pmc">PMC7701936</ArticleId><ArticleId IdType="pubmed">33252719</ArticleId></ArticleIdList></Reference><Reference><Citation>Hesamian MH, Jia W, He X, Kennedy P. Deep Learning Techniques for Medical Image Segmentation: Achievements and Challenges. J Digit Imaging 2019;32:582-96. 10.1007/s10278-019-00227-x</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10278-019-00227-x</ArticleId><ArticleId IdType="pmc">PMC6646484</ArticleId><ArticleId IdType="pubmed">31144149</ArticleId></ArticleIdList></Reference><Reference><Citation>Yang F, Weng X, Miao Y, Wu Y, Xie H, Lei P. Deep learning approach for automatic segmentation of ulna and radius in dual-energy X-ray imaging. Insights Imaging 2021;12:191. 10.1186/s13244-021-01137-9</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s13244-021-01137-9</ArticleId><ArticleId IdType="pmc">PMC8688680</ArticleId><ArticleId IdType="pubmed">34928449</ArticleId></ArticleIdList></Reference><Reference><Citation>Astuto B, Flament I, K Namiri N, Shah R, Bharadwaj U, M Link T, D Bucknor M, Pedoia V, Majumdar S. Automatic Deep Learning-assisted Detection and Grading of Abnormalities in Knee MRI Studies. Radiol Artif Intell 2021;3:e200165. 10.1148/ryai.2021200165</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/ryai.2021200165</ArticleId><ArticleId IdType="pmc">PMC8166108</ArticleId><ArticleId IdType="pubmed">34142088</ArticleId></ArticleIdList></Reference><Reference><Citation>Ashkani-Esfahani S, Mojahed Yazdi R, Bhimani R, Kerkhoffs GM, Maas M, DiGiovanni CW, Lubberts B, Guss D. Detection of ankle fractures using deep learning algorithms. Foot Ankle Surg 2022. [Epub ahead of print]. pii: S1268-7731(22)00102-3. doi: .10.1016/j.fas.2022.05.005</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.fas.2022.05.005</ArticleId><ArticleId IdType="pubmed">35659710</ArticleId></ArticleIdList></Reference><Reference><Citation>Tang A, Tam R, Cadrin-Ch&#xea;nevert A, Guest W, Chong J, Barfett J, Chepelev L, Cairns R, Mitchell JR, Cicero MD, Poudrette MG, Jaremko JL, Reinhold C, Gallix B, Gray B, Geis R, Canadian Association of Radiologists (CAR) Artificial Intelligence Working Group . Canadian Association of Radiologists White Paper on Artificial Intelligence in Radiology. Can Assoc Radiol J 2018;69:120-35. 10.1016/j.carj.2018.02.002</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.carj.2018.02.002</ArticleId><ArticleId IdType="pubmed">29655580</ArticleId></ArticleIdList></Reference><Reference><Citation>Lee JG, Jun S, Cho YW, Lee H, Kim GB, Seo JB, Kim N. Deep Learning in Medical Imaging: General Overview. Korean J Radiol 2017;18:570-84. 10.3348/kjr.2017.18.4.570</Citation><ArticleIdList><ArticleId IdType="doi">10.3348/kjr.2017.18.4.570</ArticleId><ArticleId IdType="pmc">PMC5447633</ArticleId><ArticleId IdType="pubmed">28670152</ArticleId></ArticleIdList></Reference><Reference><Citation>Masoudi S, Harmon SA, Mehralivand S, Walker SM, Raviprakash H, Bagci U, Choyke PL, Turkbey B. Quick guide on radiology image pre-processing for deep learning applications in prostate cancer research. J Med Imaging (Bellingham) 2021;8:010901. 10.1117/1.JMI.8.1.010901</Citation><ArticleIdList><ArticleId IdType="doi">10.1117/1.JMI.8.1.010901</ArticleId><ArticleId IdType="pmc">PMC7790158</ArticleId><ArticleId IdType="pubmed">33426151</ArticleId></ArticleIdList></Reference><Reference><Citation>Flannery SW, Kiapour AM, Edgar DJ, Murray MM, Fleming BC. Automated magnetic resonance image segmentation of the anterior cruciate ligament. J Orthop Res 2021;39:831-40. 10.1002/jor.24926</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jor.24926</ArticleId><ArticleId IdType="pmc">PMC8005419</ArticleId><ArticleId IdType="pubmed">33241856</ArticleId></ArticleIdList></Reference><Reference><Citation>He K, Gkioxari G, Dollar P, Girshick R. Mask R-CNN. IEEE Trans Pattern Anal Mach Intell 2020;42:386-97. 10.1109/TPAMI.2018.2844175</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TPAMI.2018.2844175</ArticleId><ArticleId IdType="pubmed">29994331</ArticleId></ArticleIdList></Reference><Reference><Citation>Couteaux V, Si-Mohamed S, Nempont O, Lefevre T, Popoff A, Pizaine G, Villain N, Bloch I, Cotten A, Boussel L. Automatic knee meniscus tear detection and orientation classification with Mask-RCNN. Diagn Interv Imaging 2019;100:235-42. 10.1016/j.diii.2019.03.002</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.diii.2019.03.002</ArticleId><ArticleId IdType="pubmed">30910620</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang R, Cheng C, Zhao X, Li X. Multiscale Mask R-CNN-Based Lung Tumor Detection Using PET Imaging. Mol Imaging 2019;18:1536012119863531. 10.1177/1536012119863531</Citation><ArticleIdList><ArticleId IdType="doi">10.1177/1536012119863531</ArticleId><ArticleId IdType="pmc">PMC6669841</ArticleId><ArticleId IdType="pubmed">31364467</ArticleId></ArticleIdList></Reference><Reference><Citation>Vuurberg G, Hoorntje A, Wink LM, van der Doelen BFW, van den Bekerom MP, Dekker R, van Dijk CN, Krips R, Loogman MCM, Ridderikhof ML, Smithuis FF, Stufkens SAS, Verhagen EALM, de Bie RA, Kerkhoffs GMMJ. Diagnosis, treatment and prevention of ankle sprains: update of an evidence-based clinical guideline. Br J Sports Med 2018;52:956. 10.1136/bjsports-2017-098106</Citation><ArticleIdList><ArticleId IdType="doi">10.1136/bjsports-2017-098106</ArticleId><ArticleId IdType="pubmed">29514819</ArticleId></ArticleIdList></Reference><Reference><Citation>Choisne J, Hoch MC, Bawab S, Alexander I, Ringleb SI. The effects of a semi-rigid ankle brace on a simulated isolated subtalar joint instability. J Orthop Res 2013;31:1869-75. 10.1002/jor.22468</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jor.22468</ArticleId><ArticleId IdType="pubmed">24038108</ArticleId></ArticleIdList></Reference><Reference><Citation>Ringleb SI, Udupa JK, Siegler S, Imhauser CW, Hirsch BE, Liu J, Odhner D, Okereke E, Roach N. The effect of ankle ligament damage and surgical reconstructions on the mechanics of the ankle and subtalar joints revealed by three-dimensional stress MRI. J Orthop Res 2005;23:743-9. 10.1016/j.orthres.2005.02.001</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.orthres.2005.02.001</ArticleId><ArticleId IdType="pubmed">16022985</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36619816</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>10</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic-eCollection"><Journal><ISSN IssnType="Electronic">1687-5273</ISSN><JournalIssue CitedMedium="Internet"><Volume>2022</Volume><PubDate><Year>2022</Year></PubDate></JournalIssue><Title>Computational intelligence and neuroscience</Title><ISOAbbreviation>Comput Intell Neurosci</ISOAbbreviation></Journal><ArticleTitle>Application of Machine Learning in Intelligent Medical Image Diagnosis and Construction of Intelligent Service Process.</ArticleTitle><Pagination><StartPage>9152605</StartPage><MedlinePgn>9152605</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">9152605</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1155/2022/9152605</ELocationID><Abstract><AbstractText>The introduction of digital technology in the healthcare industry is marked by ongoing difficulties with implementation and use. Slow progress has been made in unifying different healthcare systems, and much of the globe still lacks a fully integrated healthcare system. As a result, it is critical and advantageous for healthcare providers to comprehend the fundamental ideas of AI in order to design and deliver their own AI-powered technology. AI is commonly defined as the capacity of machines to mimic human cognitive functions. It can tackle jobs with equivalent or superior performance to humans by combining computer science, algorithms, machine learning, and data science. The healthcare system is a dynamic and evolving environment, and medical experts are constantly confronted with new issues, shifting duties, and frequent interruptions. Because of this variation, illness diagnosis frequently becomes a secondary concern for healthcare professionals. Furthermore, clinical interpretation of medical information is a cognitively demanding endeavor. This applies not just to seasoned experts, but also to individuals with varying or limited skills, such as young assistant doctors. In this paper, we proposed the comparative analysis of various state-of-the-art methods of deep learning for medical imaging diagnosis and evaluated various important characteristics. The methodology is to evaluate various important factors such as interpretability, visualization, semantic data, and quantification of logical relationships in medical data. Furthermore, the glaucoma diagnosis system is discussed in detail via qualitative and quantitative approaches. Finally, the applications and future prospects were also discussed.</AbstractText><CopyrightInformation>Copyright &#xa9; 2022 Zhihong Gao et al.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Gao</LastName><ForeName>Zhihong</ForeName><Initials>Z</Initials><AffiliationInfo><Affiliation>Department of Big Data in Health Science, The First Affiliated Hospital of Wenzhou Medical University, Wenzhou, Zhejiang 325000, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lou</LastName><ForeName>Lihua</ForeName><Initials>L</Initials><AffiliationInfo><Affiliation>Department of Burn, Wound Repair and Regenerative Medicine Center, The First Affiliated Hospital of Wenzhou Medical University, Wenzhou 325000, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wang</LastName><ForeName>Meihao</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Radiology, The First Affiliated Hospital of Wenzhou Medical University, Wenzhou 325000, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Sun</LastName><ForeName>Zhen</ForeName><Initials>Z</Initials><AffiliationInfo><Affiliation>Department of Big Data in Health Science, The First Affiliated Hospital of Wenzhou Medical University, Wenzhou, Zhejiang 325000, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Chen</LastName><ForeName>Xiaodong</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>Department of Gastrointestinal Surgery, The First Affiliated Hospital of Wenzhou Medical University, Wenzhou 325000, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Xiang</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>Wenzhou Data Management and Development Group Co., Ltd., Wenzhou, Zhejiang 325000, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Pan</LastName><ForeName>Zhifang</ForeName><Initials>Z</Initials><AffiliationInfo><Affiliation>Department of Big Data in Health Science, The First Affiliated Hospital of Wenzhou Medical University, Wenzhou, Zhejiang 325000, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Hao</LastName><ForeName>Haibin</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>Department of Big Data in Health Science, The First Affiliated Hospital of Wenzhou Medical University, Wenzhou, Zhejiang 325000, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Yu</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Department of Big Data in Health Science, The First Affiliated Hospital of Wenzhou Medical University, Wenzhou, Zhejiang 325000, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Quan</LastName><ForeName>Shichao</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Department of Big Data in Health Science, The First Affiliated Hospital of Wenzhou Medical University, Wenzhou, Zhejiang 325000, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yin</LastName><ForeName>Shaobo</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Department of Burn, Wound Repair and Regenerative Medicine Center, The First Affiliated Hospital of Wenzhou Medical University, Wenzhou 325000, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lin</LastName><ForeName>Cai</ForeName><Initials>C</Initials><Identifier Source="ORCID">0000-0003-1462-7771</Identifier><AffiliationInfo><Affiliation>Department of Burn, Wound Repair and Regenerative Medicine Center, The First Affiliated Hospital of Wenzhou Medical University, Wenzhou 325000, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Shen</LastName><ForeName>Xian</ForeName><Initials>X</Initials><Identifier Source="ORCID">0000-0001-7974-830X</Identifier><AffiliationInfo><Affiliation>Department of Gastrointestinal Surgery, The First Affiliated Hospital of Wenzhou Medical University, Wenzhou 325000, China.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType><PublicationType UI="D016454">Review</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>28</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>Comput Intell Neurosci</MedlineTA><NlmUniqueID>101279357</NlmUniqueID></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000069550" MajorTopicYN="Y">Machine Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000465" MajorTopicYN="Y">Algorithms</DescriptorName></MeshHeading></MeshHeadingList><CoiStatement>The authors declare that they have no conflicts of interest to report regarding the present study.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>11</Month><Day>6</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>11</Month><Day>23</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>6</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>3</Hour><Minute>59</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36619816</ArticleId><ArticleId IdType="pmc">PMC9812610</ArticleId><ArticleId IdType="doi">10.1155/2022/9152605</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Chamoun N., Matta S., Aderian S. S., et al. A prospective observational cohort of clinical outcomes in medical inpatients prescribed pharmacological thromboprophylaxis using different clinical risk assessment models (COMPT RAMs) Scientific Reports . 2019;9(1) doi: 10.1038/s41598-019-54842-3.18366</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-019-54842-3</ArticleId><ArticleId IdType="pmc">PMC6892868</ArticleId><ArticleId IdType="pubmed">31797897</ArticleId></ArticleIdList></Reference><Reference><Citation>Ali T. M., Nawaz A., Rehman A., et al. A sequential machine learning-cum-attentions mechanism for effective sementation of brain tumor. Frontiers in Oncology . 2022;12:1&#x2013;10.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC9202559</ArticleId><ArticleId IdType="pubmed">35719987</ArticleId></ArticleIdList></Reference><Reference><Citation>Abbas S., Jalil Z., Javed A. R., et al. BCD-WERT: a novel approach for breast cancer detection using whale optimization based efficient features and extremely randomized tree algorithm. Peer J Computer Science . 2021;7:p. e390. doi: 10.7717/peerj-cs.390.</Citation><ArticleIdList><ArticleId IdType="doi">10.7717/peerj-cs.390</ArticleId><ArticleId IdType="pmc">PMC7959601</ArticleId><ArticleId IdType="pubmed">33817036</ArticleId></ArticleIdList></Reference><Reference><Citation>Tsiakmaki M., Kostopoulos G., Kotsiantis S., Ragos O. Transfer learning from deep neural networks for predicting student performance. Applied Sciences . 2020;10(6):2145&#x2013;2218. doi: 10.3390/app10062145.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/app10062145</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhou L., Pan S., Wang J., Vasilakos A. V. Machine learning on big data: opportunities and challenges. Neurocomputing . 2017;237:350&#x2013;361. doi: 10.1016/j.neucom.2017.01.026.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neucom.2017.01.026</ArticleId></ArticleIdList></Reference><Reference><Citation>Ghazal M., Ghazal M., Mahmoud A., et al. Alzheimer rsquo s disease diagnostics by a 3D deeply supervised adaptable convolutional network. Frontiers in Bioscience . 2018;23(2):4606&#x2013;5596. doi: 10.2741/4606.</Citation><ArticleIdList><ArticleId IdType="doi">10.2741/4606</ArticleId><ArticleId IdType="pubmed">28930562</ArticleId></ArticleIdList></Reference><Reference><Citation>Suk H. I., Lee S. W., Shen D. Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis. NeuroImage . 2014;101:569&#x2013;582. doi: 10.1016/j.neuroimage.2014.06.077.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2014.06.077</ArticleId><ArticleId IdType="pmc">PMC4165842</ArticleId><ArticleId IdType="pubmed">25042445</ArticleId></ArticleIdList></Reference><Reference><Citation>Hao P., Zheng X., Huang J. Z. An effective approach for robust lung cancer cell detection. Proceedings of the International Workshop on Patch Based Techniques in Medical Imaging; October 2015; Munich, Germany.</Citation></Reference><Reference><Citation>Yan R., Ren F., Wang Z., et al. Breast cancer histopathological image classification using a hybrid deep neural network. Methods . 2020;173(3):52&#x2013;60. doi: 10.1016/j.ymeth.2019.06.014.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ymeth.2019.06.014</ArticleId><ArticleId IdType="pubmed">31212016</ArticleId></ArticleIdList></Reference><Reference><Citation>Yao J., Lei Z., Yue W., et al. DeepThy-Net: a multimodal deep learning method for predicting cervical lymph node metastasis in papillary thyroid cancer. Advanced Intelligent Systems . 2022;4(10):2200100&#x2013;2202235. doi: 10.1002/aisy.202200100.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/aisy.202200100</ArticleId></ArticleIdList></Reference><Reference><Citation>Haloi M. Improved microaneurysm detection using deep neural networks. 2015.  
 https://arxiv.org/abs/1505.04424
.</Citation></Reference><Reference><Citation>Chandrakumar T., Kathirvel R. Classifying diabetic retinopathy using deep learning architecture. International Journal of Engineering Research and Technology . 2016;5(6):19&#x2013;24.</Citation></Reference><Reference><Citation>Biran O., Cotton C. Explanation and justification in machine learning: a survey. Proceedings of the IJCAI- 17 Workshop on Explainable AI (XAI); January 2017; Melbourn, UK. pp. 1&#x2013;5.</Citation></Reference><Reference><Citation>Miller T. Explanation in artificial intelligence: insights from the social sciences. Artificial Intelligence . 2019;267:1&#x2013;38. doi: 10.1016/j.artint.2018.07.007.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.artint.2018.07.007</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang X., Chen Y., Yang J., Wu L., Wu Z., Xie X. A reinforcement learning framework for explainable recommendation. Proceedings of the 2018 IEEE International Conference on Data Mining (ICDM); November 2018; Singapore. pp. 587&#x2013;596.</Citation></Reference><Reference><Citation>Adomavicius G., Tuzhilin A. Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions. IEEE Transactions on Knowledge and Data Engineering . 2005;17:734&#x2013;749. doi: 10.1109/tkde.2005.99.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/tkde.2005.99</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang Y., Chen X. Explainable recommendation: a survey and new perspectives. 2018.  
 https://arxiv.org/abs/1804.11192
.</Citation></Reference><Reference><Citation>Teach R. L., Shortliffe E. H. An analysis of physician attitudes regarding computer-based clinical consultation systems. Computers and Biomedical Research . 1981;14(6):542&#x2013;558. doi: 10.1016/0010-4809(81)90012-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/0010-4809(81)90012-4</ArticleId><ArticleId IdType="pubmed">7035062</ArticleId></ArticleIdList></Reference><Reference><Citation>Smith L. B., Slone L. K. A developmental approach to machine learning? Frontiers in Psychology . 2017;8:p. 2124. doi: 10.3389/fpsyg.2017.02124.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fpsyg.2017.02124</ArticleId><ArticleId IdType="pmc">PMC5723343</ArticleId><ArticleId IdType="pubmed">29259573</ArticleId></ArticleIdList></Reference><Reference><Citation>Binkowski M., Sutherl D., Arbel M., Gretton A. Demystifying MMD GANs. Proceedings of the International Conference on Learning Representations; March 2018; Vancouver, Canada.</Citation></Reference><Reference><Citation>Zeiler M., Fergus R. Visualizing and Understanding Convolutional Networks. Proceedings of the European Conference on Computer Vision; November 2014; Munich, Germany. pp. 818&#x2013;833.</Citation></Reference><Reference><Citation>Mahendran A., Vedaldi A. Understanding deep image representations by inverting them. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; June 2015; Boston, MA, USA. pp. 5188&#x2013;5196.</Citation></Reference><Reference><Citation>Liu M., Shi J., Li Z., Li C., Zhu J., Liu S. Towards better analysis of deep convolutional neural networks. IEEE Transactions on Visualization and Computer Graphics . 2017;23(1):91&#x2013;100. doi: 10.1109/tvcg.2016.2598831.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/tvcg.2016.2598831</ArticleId><ArticleId IdType="pubmed">27576252</ArticleId></ArticleIdList></Reference><Reference><Citation>Ding Y., Liu C., Zhu H., Liu J., Chen Q. Visualization deep networks using segmentation recognition and interpretation algorithm. Information Sciences . 2022;609:1381&#x2013;1396.</Citation></Reference><Reference><Citation>Olah C., Mordvintsev A., Schubert L. Feature visualization. Distill . 2017;2(11):p. e7. doi: 10.23915/distill.00007.</Citation><ArticleIdList><ArticleId IdType="doi">10.23915/distill.00007</ArticleId></ArticleIdList></Reference><Reference><Citation>Bau D., Zhou B., Khosla A., Oliva A., Torralba A. Network dissection: quantifying interpretability of deep visual representations. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; July 2017; Hawaii, HI, USA. pp. 6541&#x2013;6549.</Citation></Reference><Reference><Citation>Fong R., Vedaldi A. Net2Vec: quantifying and explaining how concepts are encoded by filters in deep neural networks. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; June 2018; Salt Lake, UT, USA. pp. 8730&#x2013;8738.</Citation></Reference><Reference><Citation>Kim B., Wattenberg M., Gilmer J., et al.  Interpretability beyond feature attribution: quantitative testing with concept activation vectors (tcav) 2017.  
 https://arxiv.org/abs/1711.11279
.</Citation></Reference><Reference><Citation>Wang S., Yin Y., Cao G., Wei B., Zheng Y., Yang G. Hierarchical retinal blood vessel segmentation based on feature and ensemble learning. Neurocomputing . 2015;149:708&#x2013;717. doi: 10.1016/j.neucom.2014.07.059.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neucom.2014.07.059</ArticleId></ArticleIdList></Reference><Reference><Citation>Selvaraju R., Cogswell M., Das A., Vedantam R., Parikh D., Batra D. Grad-CAM: visual explanations from deep networks via gradient-based localization. Proceedings of the IEEE International Conference on Computer Vision; October 2017; Venice, Italy. pp. 618&#x2013;626.</Citation></Reference><Reference><Citation>Gao X., Wang Y., Sun J. Intravascular ultrasound image plaque recognition based on improved ResNet network. Proceedings of the IEEE International Conference on Intelligent Computing and Signal Processing (ICSP); April 2022; Xi&#x2019;an, China. pp. 1759&#x2013;1764.</Citation></Reference><Reference><Citation>Gao X., Li W., Loomes M., Wang L. A fused deep learning architecture for viewpoint classification of echocardiography. Information Fusion . 2017;36:103&#x2013;113. doi: 10.1016/j.inffus.2016.11.007.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.inffus.2016.11.007</ArticleId></ArticleIdList></Reference><Reference><Citation>Pasyar P., Mahmoudi T., Kouzehkanan S. Z. M., et al. Hybrid classification of diffuse liver diseases in ultrasound images using deep convolutional neural networks. Informatics in Medicine Unlocked . 2021;22(3):100496&#x2013;110517. doi: 10.1016/j.imu.2020.100496.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.imu.2020.100496</ArticleId></ArticleIdList></Reference><Reference><Citation>Blanco P. J., Ziemer P. G., Bulant C. A., et al. Fully automated lumen and vessel contour segmentation in intravascular ultrasound datasets. Medical Image Analysis . 2022;75:102262&#x2013;111035. doi: 10.1016/j.media.2021.102262.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2021.102262</ArticleId><ArticleId IdType="pubmed">34670148</ArticleId></ArticleIdList></Reference><Reference><Citation>Han W., Peng M., Xie Q., et al. DTC: transfer learning for commonsense machine comprehension. Neurocomputing . 2020;396(4):102&#x2013;112. doi: 10.1016/j.neucom.2019.07.110.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neucom.2019.07.110</ArticleId></ArticleIdList></Reference><Reference><Citation>Olah C., Satyanarayan A., Johnson I., et al. The building blocks of interpretability. Distill . 2018;3(3) doi: 10.23915/distill.00010.</Citation><ArticleIdList><ArticleId IdType="doi">10.23915/distill.00010</ArticleId></ArticleIdList></Reference><Reference><Citation>Sabour S., Frosst N., Hinton G. Dynamic routing between capsules. 2017.  
 https://arxiv.org/abs/1710.09829
.</Citation></Reference><Reference><Citation>Chen X., Duan Y., Houthooft R., Schulman J., Sutskever I., Abbeel P. Infogan: interpretable representation learning by information maximizing generative adversarial nets. 2016.  
 https://arxiv.org/abs/1606.03657
.</Citation></Reference><Reference><Citation>Lecun Y., Bottou L., Bengio Y., Haffner P. Gradient -Based learning applied to document recognition. Proceedings of the IEEE . 1998;86(11):2278&#x2013;2324. doi: 10.1109/5.726791.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/5.726791</ArticleId></ArticleIdList></Reference><Reference><Citation>Liu Z., Luop P., Wang X., Tang X. Deep learning face attributes in the wild. Proceedings of the IEEE International Conference on Computer Vision; December 2015; Santiago, Chile. pp. 3730&#x2013;3738.</Citation></Reference><Reference><Citation>Netzer Y., Wang T., Coates A., Bissacco A., Wu A. Reading Digits in Natural Images with Unsupervised Feature Learning. Proceedings of the Conference And Workshop On Neural Information Processing Systems; April 2011; Granada, Spain.</Citation></Reference><Reference><Citation>Paysan P., Knothe R., Amberg B., Romdhani S., Vetter T. A 3D face model for pose and illumination invariant face recognition. Proceedings of the 2009 16thIEEE International Conference on Advanced Video and Signal Based Surveillance; September 2009; Genova, Italy. pp. 296&#x2013;301.</Citation></Reference><Reference><Citation>Guo Y., Duan X., Wang C., Guo H. Segmentation and recognition of breast ultrasound images based on an expanded U-Net. PLoS One . 2021;16(6):e0253202&#x2013;e0253217. doi: 10.1371/journal.pone.0253202.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0253202</ArticleId><ArticleId IdType="pmc">PMC8205136</ArticleId><ArticleId IdType="pubmed">34129619</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhou B., Khosla A., Lapedriza A., Oliva A., Torralba A. Learning deep features for discriminative localization. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; June 2016; Las Vegas, NV, USA. pp. 2921&#x2013;2929.</Citation></Reference><Reference><Citation>Wu M., Hughes M., Parbhoos S., Michael C., Sonali Z. Beyond sparsity: tree regularization of deep models for interpretability. Proceedings of the Thirty Second AAAI Conference on Artificial Intelligence; February 2018; Louisiana, LA, USA.</Citation></Reference><Reference><Citation>Zhang Q., Wu Y., Zhu S. Interpretable convolutional neural networks. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; June 2018; Salt Lake, UT, USA. pp. 8827&#x2013;8836.</Citation></Reference><Reference><Citation>Zhang Q., Yang Y., Ma H., Wu Y. N. Interpreting CNNs via decision trees. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; June 2019; California, CA, USA. pp. 6261&#x2013;6270.</Citation></Reference><Reference><Citation>Hou B., Zhou Z. Learning with interpretable structure from rnn. 2018.  
 https://arxiv.org/abs/1810.10708
.</Citation><ArticleIdList><ArticleId IdType="pubmed">32071002</ArticleId></ArticleIdList></Reference><Reference><Citation>Wu T., Sun W., Li X., Song X., Li B. Towards interpretable r-cnn by unfolding latent structures. 2017.  
 https://arxiv.org/abs/1711.05226
.</Citation></Reference><Reference><Citation>Madumal P., Miller T., Sonenberg L., Vetere F. Explainable reinforcement learning through a causal lens. Proceedings of the AAAI Conference on Artificial Intelligence . 2020;34(03):2493&#x2013;2500. doi: 10.1609/aaai.v34i03.5631.</Citation><ArticleIdList><ArticleId IdType="doi">10.1609/aaai.v34i03.5631</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang Y., Su H., Zhang B., Hu X. Interpret neural networks by identifying critical data routing paths. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; June 2018; Salt Lake, UT, USA. pp. 8906&#x2013;8914.</Citation></Reference><Reference><Citation>Chu L., Hu X., Hu J., Lanjun W., Jian P. Exact and Consistent Interpretation for Piecewise Linear Neural Networks: A Closed Form Solution. Proceedings of the Knowledge Discovery and Data Mining; July 2018; Alaska, AK, USA. pp. 1244&#x2013;1253.</Citation></Reference><Reference><Citation>Zhang Q., Cao R., Shi F., Nian Y., Song-Chun Z. Interpreting CNN knowledge via an explanatory graph. Proceedings of the 32nd AAAI Conference on Artificial Intelligence; February 2018; Louisiana, LA, USA.</Citation></Reference><Reference><Citation>Zhang Q., Wang X., Cao R., Wu Y. N., Shi F., Song-Chun Z. Explanatory graphs for CNNs. 2018.  
 https://arxiv.org/abs/1812.07997
.</Citation></Reference><Reference><Citation>Selvaraju R., Chattopadhyay P., Elhoseiny M., et al. Choose Your Neuron: Incorporating Domain Knowledge through Neuron-Importance. Proceedings of the European Conference on Computer Vision; August 2018; Munich, Germany. pp. 540&#x2013;556.</Citation></Reference><Reference><Citation>Bau D., Zhu J., Strobelt H., Zhou B., Tenenbaum J. B. GAN dissection: visualizing and understanding generative adversarial networks. Proceedings of the International Conference on Learning Representations; May 2019; Louisiana, LA, USA.</Citation></Reference><Reference><Citation>Paschali M., Ferjadnaeem M., Simson W., Steiger K., Mollenhauer M., Navab N. Improving the interpretability of medical imaging neural networks. 2019.  
 https://arxiv.org/abs/1904.03127
.</Citation></Reference><Reference><Citation>Lee H., Yune S., Mansouri M., et al. An explainable deep-learning algorithm for the detection of acute intracranial haemorrhage from small datasets. Nature Biomedical Engineering . 2018;3(3):173&#x2013;182. doi: 10.1038/s41551-018-0324-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41551-018-0324-9</ArticleId><ArticleId IdType="pubmed">30948806</ArticleId></ArticleIdList></Reference><Reference><Citation>Liao W., Zou B., Zhao R., Chen Y., He Z., Zhou M. Clinical interpretable deep learning model for glaucoma diagnosis. IEEE Journal of Biomedical and Health Informatics . 2020;24(5):1405&#x2013;1412. doi: 10.1109/jbhi.2019.2949075.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/jbhi.2019.2949075</ArticleId><ArticleId IdType="pubmed">31647449</ArticleId></ArticleIdList></Reference><Reference><Citation>Herrera L., Everson M., Li W., et al.  Interpretable fully convolutional classification of intrapapillary capillary loops for real-time detection of early squamous neoplasia. 2018.  
 https://arxiv.org/abs/1805.00632
.</Citation></Reference><Reference><Citation>Cruzroa A., Ovalle J., Madabhushi A. A deep learning architecture for image representation, visual interpretability and automated basal-cell carcinoma cancer detection. Proceedings of the Medical Image Computing and Computer Assisted Intervention; March 2013; Nagoya, Japan. pp. 403&#x2013;410.</Citation><ArticleIdList><ArticleId IdType="pubmed">24579166</ArticleId></ArticleIdList></Reference><Reference><Citation>Biffi C., Oktay O., Tarroni G., Bai W., Marvao A. D., Doumou G. Learning interpretable Anatomical features through deep generative models: application to cardiac remodeling. Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention; July 2018; Granada, Spain. pp. 464&#x2013;471.</Citation></Reference><Reference><Citation>Zhang Z., Chen P., Sapkota M., Yan L. TandemNet: distilling knowledge from medical images using diagnostic reports as optional semantic references. Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention; September 2017; Quebe, Canada. pp. 320&#x2013;328.</Citation></Reference><Reference><Citation>Wang X., Peng Y., Lu L., Lu Z., Summers R. M. TieNet: text-image embedding network for common thorax disease classification and reporting in chest X-rays. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; January 2018; Salt Lake, UT, USA. pp. 9049&#x2013;9058.</Citation></Reference><Reference><Citation>Shen S., Han S. X., Aberle D. R., Bui A. A., Hsu W. An interpretable deep hierarchical semantic convolutional neural network for lung nodule malignancy classification. Expert Systems with Applications . 2019;128:84&#x2013;95. doi: 10.1016/j.eswa.2019.01.048.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.eswa.2019.01.048</ArticleId><ArticleId IdType="pmc">PMC6623975</ArticleId><ArticleId IdType="pubmed">31296975</ArticleId></ArticleIdList></Reference><Reference><Citation>Kim S., Lee H., Kim H., Man Ro Y. ICADx: interpretable computer aided diagnosis of breast masses. 2018.  
 https://arxiv.org/abs/1805.08960v1
.</Citation></Reference><Reference><Citation>Zhang Z., Xie Y., Xing F., McGough M. MDNet: a semantically and visually interpretable medical image diagnosis network. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; September 2017; Hawaii, HI, USA. pp. 6428&#x2013;6436.</Citation></Reference><Reference><Citation>De Fauw J., Ledsam J. R., Romera-Paredes B., et al. Clinically applicable deep learning for diagnosis and referral in retinal disease. Nature Medicine . 2018;24(9):1342&#x2013;1350. doi: 10.1038/s41591-018-0107-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41591-018-0107-6</ArticleId><ArticleId IdType="pubmed">30104768</ArticleId></ArticleIdList></Reference><Reference><Citation>Niu Y., Gu L., Lu F., et al. Pathological evidence exploration in deep retinal image diagnosis. Proceedings of the AAAI Conference on Artificial Intelligence . 2019;33(01):1093&#x2013;1101. doi: 10.1609/aaai.v33i01.33011093.</Citation><ArticleIdList><ArticleId IdType="doi">10.1609/aaai.v33i01.33011093</ArticleId></ArticleIdList></Reference><Reference><Citation>Li X., Dvornek N., Zhou Y. Efficient interpretation of deep learning models using graph structure and cooperative game theory: application to ASD biomarker discovery. Proceedings of the International Conference Information Processing in Medical Imaging; May 2019; Hong Kong, China. pp. 718&#x2013;730.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7519580</ArticleId><ArticleId IdType="pubmed">32982121</ArticleId></ArticleIdList></Reference><Reference><Citation>Alaa A., Schaar M. Forecasting individualized disease trajectories using interpretable deep learning. 2018.  
 https://arxiv.org/abs/1810.10489
.</Citation></Reference><Reference><Citation>Gohorbani A., Wexler J., Zou J. Towards automatic concept-Based explanations. Proceedings of the Neural Information Processing Systems; March 2019; Vancouver, Canada. pp. 9273&#x2013;9282.</Citation></Reference><Reference><Citation>Wang S., Qureshi M. A., Pechuan L., The T., Gadekallu T., Liyanage M. Explainable ai for b5g/6g: technical aspects, use cases, and research challenges. 2021.  
 https://arxiv.org/abs/2112.04698
.</Citation><ArticleIdList><ArticleId IdType="doi">10.48550/arXiv.2112.04698</ArticleId></ArticleIdList></Reference><Reference><Citation>Ramana K., Kumar M. R., Sreenivasulu K., et al. Early prediction of lung cancers using deep saliency capsule and pre-trained deep learning frameworks. Frontiers in Oncology . 2022;12:886739&#x2013;886812. doi: 10.3389/fonc.2022.886739.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fonc.2022.886739</ArticleId><ArticleId IdType="pmc">PMC9247339</ArticleId><ArticleId IdType="pubmed">35785184</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36617971</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>09</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1475-5793</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>09</Day></PubDate></JournalIssue><Title>Journal of medical screening</Title><ISOAbbreviation>J Med Screen</ISOAbbreviation></Journal><ArticleTitle>Test accuracy of artificial intelligence-based grading of fundus images in diabetic retinopathy screening: A systematic review.</ArticleTitle><Pagination><StartPage>9691413221144382</StartPage><MedlinePgn>9691413221144382</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1177/09691413221144382</ELocationID><Abstract><AbstractText Label="OBJECTIVES" NlmCategory="OBJECTIVE">To systematically review the accuracy of artificial intelligence (AI)-based systems for grading of fundus images in diabetic retinopathy (DR) screening.</AbstractText><AbstractText Label="METHODS" NlmCategory="METHODS">We searched MEDLINE, EMBASE, the Cochrane Library and the ClinicalTrials.gov from 1st January 2000 to 27th August 2021. Accuracy studies published in English were included if they met the pre-specified inclusion criteria. Selection of studies for inclusion, data extraction and quality assessment were conducted by one author with a second reviewer independently screening and checking 20% of titles. Results were analysed narratively.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">Forty-three studies evaluating 15 deep learning (DL) and 4 machine learning (ML) systems were included. Nine systems were evaluated in a single study each. Most studies were judged to be at high or unclear risk of bias in at least one QUADAS-2 domain. Sensitivity for referable DR and higher grades was &#x2265;85% while specificity varied and was &lt;80% for all ML systems and in 6/31 studies evaluating DL systems. Studies reported high accuracy for detection of ungradable images, but the latter were analysed and reported inconsistently. Seven studies reported that AI was more sensitive but less specific than human graders.</AbstractText><AbstractText Label="CONCLUSIONS" NlmCategory="CONCLUSIONS">AI-based systems are more sensitive than human graders and could be safe to use in clinical practice but have variable specificity. However, for many systems evidence is limited, at high risk of bias and may not generalise across settings. Therefore, pre-implementation assessment in the target clinical pathway is essential to obtain reliable and applicable accuracy estimates.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Zhelev</LastName><ForeName>Zhivko</ForeName><Initials>Z</Initials><Identifier Source="ORCID">0000-0002-0106-2401</Identifier><AffiliationInfo><Affiliation>Exeter Test Group, University of Exeter Medical School, 3286University of Exeter, Exeter, UK.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Peters</LastName><ForeName>Jaime</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Exeter Test Group, University of Exeter Medical School, 3286University of Exeter, Exeter, UK.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Rogers</LastName><ForeName>Morwenna</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>NIHR ARC South West Peninsula (PenARC), 171002University of Exeter Medical School, University of Exeter, Exeter, UK.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Allen</LastName><ForeName>Michael</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>University of Exeter Medical School, 3286University of Exeter, Exeter, UK.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kijauskaite</LastName><ForeName>Goda</ForeName><Initials>G</Initials><AffiliationInfo><Affiliation>UK National Screening Committee, London, UK.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Seedat</LastName><ForeName>Farah</ForeName><Initials>F</Initials><AffiliationInfo><Affiliation>UK National Screening Committee, London, UK.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wilkinson</LastName><ForeName>Elizabeth</ForeName><Initials>E</Initials><AffiliationInfo><Affiliation>9553Northern Devon Healthcare NHS Trust, Devon, UK.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Hyde</LastName><ForeName>Christopher</ForeName><Initials>C</Initials><Identifier Source="ORCID">0000-0002-7349-0616</Identifier><AffiliationInfo><Affiliation>Exeter Test Group, University of Exeter Medical School, 3286University of Exeter, Exeter, UK.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType><PublicationType UI="D016454">Review</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>09</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>J Med Screen</MedlineTA><NlmUniqueID>9433359</NlmUniqueID><ISSNLinking>0969-1413</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Artificial intelligence</Keyword><Keyword MajorTopicYN="N">diabetic retinopathy</Keyword><Keyword MajorTopicYN="N">fundus imaging</Keyword><Keyword MajorTopicYN="N">screening</Keyword><Keyword MajorTopicYN="N">sensitivity and specificity</Keyword><Keyword MajorTopicYN="N">systematic review</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>3</Hour><Minute>13</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36617971</ArticleId><ArticleId IdType="doi">10.1177/09691413221144382</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36617620</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>10</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>11</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">2509-9280</ISSN><JournalIssue CitedMedium="Internet"><Volume>7</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>09</Day></PubDate></JournalIssue><Title>European radiology experimental</Title><ISOAbbreviation>Eur Radiol Exp</ISOAbbreviation></Journal><ArticleTitle>Comparison of image quality of two versions of deep-learning image reconstruction algorithm on a rapid kV-switching CT: a phantom study.</ArticleTitle><Pagination><StartPage>1</StartPage><MedlinePgn>1</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">1</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1186/s41747-022-00314-9</ELocationID><Abstract><AbstractText Label="BACKGROUND" NlmCategory="BACKGROUND">To assess the impact of the new version of a deep learning (DL) spectral reconstruction on image quality of virtual monoenergetic images (VMIs) for contrast-enhanced abdominal computed tomography in the rapid kV-switching platform.</AbstractText><AbstractText Label="METHODS" NlmCategory="METHODS">Two phantoms were scanned with a rapid kV-switching CT using abdomen-pelvic CT examination parameters at dose of 12.6 mGy. Images were reconstructed using two versions of DL spectral reconstruction algorithms (DLSR V1 and V2) for three reconstruction levels. The noise power spectrum (NSP) and task-based transfer function at 50% (TTF<sub>50</sub>) were computed at 40/50/60/70 keV. A detectability index (d') was calculated for enhanced lesions at low iodine concentrations: 2, 1, and 0.5 mg/mL.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">The noise magnitude was significantly lower with DLSR V2 compared to DLSR V1 for energy levels between 40 and 60 keV by -36.5% &#xb1; 1.4% (mean &#xb1; standard deviation) for the&#xa0;standard level. The average NPS frequencies increased significantly with DLSR V2 by 23.7% &#xb1; 4.2% for&#xa0;the standard level. The highest difference in TTF<sub>50</sub> was observed at the mild level with a significant increase of 61.7% &#xb1; 11.8% over 40-60 keV energy with DLSR V2. The d' values were significantly higher for DLSR V2 versus DLSR V1.</AbstractText><AbstractText Label="CONCLUSIONS" NlmCategory="CONCLUSIONS">The DLSR V2 improves image quality and detectability of low iodine concentrations in VMIs compared to DLSR V1. This suggests a great potential of DLSR V2 to reduce iodined contrast doses.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Dabli</LastName><ForeName>Djamel</ForeName><Initials>D</Initials><Identifier Source="ORCID">0000-0003-1003-1196</Identifier><AffiliationInfo><Affiliation>Department of Medical Imaging, IMAGINE UR UM 103, Montpellier University, N&#xee;mes University Hospital, Bd Prof Robert Debr&#xe9;, 30029, N&#xee;mes Cedex 9, France. Djamel.dabli@chu-nimes.fr.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Loisy</LastName><ForeName>Maeliss</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Medical Imaging, IMAGINE UR UM 103, Montpellier University, N&#xee;mes University Hospital, Bd Prof Robert Debr&#xe9;, 30029, N&#xee;mes Cedex 9, France.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Frandon</LastName><ForeName>Julien</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Department of Medical Imaging, IMAGINE UR UM 103, Montpellier University, N&#xee;mes University Hospital, Bd Prof Robert Debr&#xe9;, 30029, N&#xee;mes Cedex 9, France.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>de Oliveira</LastName><ForeName>Fabien</ForeName><Initials>F</Initials><AffiliationInfo><Affiliation>Department of Medical Imaging, IMAGINE UR UM 103, Montpellier University, N&#xee;mes University Hospital, Bd Prof Robert Debr&#xe9;, 30029, N&#xee;mes Cedex 9, France.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Meerun</LastName><ForeName>Azhar Mohamad</ForeName><Initials>AM</Initials><AffiliationInfo><Affiliation>Saint-Eloi University Hospital, Montpellier, France.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Guiu</LastName><ForeName>Boris</ForeName><Initials>B</Initials><AffiliationInfo><Affiliation>Saint-Eloi University Hospital, Montpellier, France.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Beregi</LastName><ForeName>Jean-Paul</ForeName><Initials>JP</Initials><AffiliationInfo><Affiliation>Department of Medical Imaging, IMAGINE UR UM 103, Montpellier University, N&#xee;mes University Hospital, Bd Prof Robert Debr&#xe9;, 30029, N&#xee;mes Cedex 9, France.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Greffier</LastName><ForeName>Jo&#xeb;l</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Department of Medical Imaging, IMAGINE UR UM 103, Montpellier University, N&#xee;mes University Hospital, Bd Prof Robert Debr&#xe9;, 30029, N&#xee;mes Cedex 9, France.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>09</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>Eur Radiol Exp</MedlineTA><NlmUniqueID>101721752</NlmUniqueID><ISSNLinking>2509-9280</ISSNLinking></MedlineJournalInfo><ChemicalList><Chemical><RegistryNumber>9679TC07X4</RegistryNumber><NameOfSubstance UI="D007455">Iodine</NameOfSubstance></Chemical></ChemicalList><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D014057" MajorTopicYN="N">Tomography, X-Ray Computed</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D000465" MajorTopicYN="N">Algorithms</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D007455" MajorTopicYN="Y">Iodine</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D007091" MajorTopicYN="N">Image Processing, Computer-Assisted</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Abdomen</Keyword><Keyword MajorTopicYN="N">Contrast media</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Image processing (computer assisted)</Keyword><Keyword MajorTopicYN="N">Phantoms (imaging)</Keyword></KeywordList><CoiStatement>The authors declare that they have no competing interests.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>6</Month><Day>25</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>11</Month><Day>5</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>23</Hour><Minute>22</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36617620</ArticleId><ArticleId IdType="pmc">PMC9826773</ArticleId><ArticleId IdType="doi">10.1186/s41747-022-00314-9</ArticleId><ArticleId IdType="pii">10.1186/s41747-022-00314-9</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Sch&#xf6;ckel L, Jost G, Seidensticker P, et al. Developments in x-ray contrast media and the potential impact on computed tomography. Invest Radiol. 2020;55:592&#x2013;597. doi: 10.1097/RLI.0000000000000696.</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/RLI.0000000000000696</ArticleId><ArticleId IdType="pubmed">32701620</ArticleId></ArticleIdList></Reference><Reference><Citation>Geenen RWF, Kingma HJ, van der Molen AJ. Contrast-induced nephropathy: pharmacology, pathophysiology and prevention. Insights Imaging. 2013;4:811&#x2013;820. doi: 10.1007/s13244-013-0291-3.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s13244-013-0291-3</ArticleId><ArticleId IdType="pmc">PMC3846935</ArticleId><ArticleId IdType="pubmed">24092564</ArticleId></ArticleIdList></Reference><Reference><Citation>Ribitsch W, Horina JH, Quehenberger F, et al. Contrast induced acute kidney injury and its impact on mid-term kidney function, cardiovascular events and mortality. Sci Rep. 2019;9:16896. doi: 10.1038/s41598-019-53040-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-019-53040-5</ArticleId><ArticleId IdType="pmc">PMC6858434</ArticleId><ArticleId IdType="pubmed">31729409</ArticleId></ArticleIdList></Reference><Reference><Citation>van der Molen AJ, Reimer P, Dekkers IA, et al. Post-contrast acute kidney injury &#x2013; Part 1: Definition, clinical features, incidence, role of contrast medium and risk factors: Recommendations for updated ESUR Contrast Medium Safety Committee guidelines. Eur Radiol. 2018;28:2845&#x2013;2855. doi: 10.1007/s00330-017-5246-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-017-5246-5</ArticleId><ArticleId IdType="pmc">PMC5986826</ArticleId><ArticleId IdType="pubmed">29426991</ArticleId></ArticleIdList></Reference><Reference><Citation>van der Molen AJ, Reimer P, Dekkers IA, et al. Post-contrast acute kidney injury. Part 2: risk stratification, role of hydration and other prophylactic measures, patients taking metformin and chronic dialysis patients: Recommendations for updated ESUR Contrast Medium Safety Committee guidelines. Eur Radiol. 2018;28:2856&#x2013;2869. doi: 10.1007/s00330-017-5247-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-017-5247-4</ArticleId><ArticleId IdType="pmc">PMC5986837</ArticleId><ArticleId IdType="pubmed">29417249</ArticleId></ArticleIdList></Reference><Reference><Citation>Gottumukkala RV, Kalra MK, Tabari A, et al. Advanced CT techniques for decreasing radiation dose, reducing sedation requirements, and optimizing image quality in children. Radiographics. 2019;39:709&#x2013;726. doi: 10.1148/rg.2019180082.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/rg.2019180082</ArticleId><ArticleId IdType="pubmed">30924753</ArticleId></ArticleIdList></Reference><Reference><Citation>Solbak MS, Henning MK, England A, et al. Impact of iodine concentration and scan parameters on image quality, contrast enhancement and radiation dose in thoracic CT. Eur Radiol Exp. 2020;4:57. doi: 10.1186/s41747-020-00184-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s41747-020-00184-z</ArticleId><ArticleId IdType="pmc">PMC7486352</ArticleId><ArticleId IdType="pubmed">32915405</ArticleId></ArticleIdList></Reference><Reference><Citation>Li J, Wang Y, Zheng F, et al. Feasibility of utilizing ultra-low-dose contrast medium for pancreatic artery depiction using the combination of advanced virtual monoenergetic imaging and high-concentration contrast medium: an intra-patient study. Insights Imaging. 2021;12:166. doi: 10.1186/s13244-021-01079-2.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s13244-021-01079-2</ArticleId><ArticleId IdType="pmc">PMC8589906</ArticleId><ArticleId IdType="pubmed">34767101</ArticleId></ArticleIdList></Reference><Reference><Citation>Dabli D, Frandon J, Belaouni A, et al. Optimization of image quality and accuracy of low iodine concentration quantification as function of dose level and reconstruction algorithm for abdominal imaging using dual-source CT: a phantom study. Diagn Interv Imaging. 2022;103:31&#x2013;40. doi: 10.1016/j.diii.2021.08.004.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.diii.2021.08.004</ArticleId><ArticleId IdType="pubmed">34625394</ArticleId></ArticleIdList></Reference><Reference><Citation>Dabli D, Frandon J, Hamard A, et al. Optimization of image quality and accuracy of low iodine concentration quantification as function of kVp pairs for abdominal imaging using dual-source CT: a phantom study. Phys Med. 2021;88:285&#x2013;292. doi: 10.1016/j.ejmp.2021.07.008.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejmp.2021.07.008</ArticleId><ArticleId IdType="pubmed">34358863</ArticleId></ArticleIdList></Reference><Reference><Citation>Albrecht MH, Vogl TJ, Martin SS, et al. Review of clinical applications for virtual monoenergetic dual-energy CT. Radiology. 2019;293:260&#x2013;271. doi: 10.1148/radiol.2019182297.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2019182297</ArticleId><ArticleId IdType="pubmed">31502938</ArticleId></ArticleIdList></Reference><Reference><Citation>Adam SZ, Rabinowich A, Kessner R, et al. Spectral CT of the abdomen: Where are we now? Insights Imaging. 2021;12:138. doi: 10.1186/s13244-021-01082-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s13244-021-01082-7</ArticleId><ArticleId IdType="pmc">PMC8476679</ArticleId><ArticleId IdType="pubmed">34580788</ArticleId></ArticleIdList></Reference><Reference><Citation>Liang H, Zhou Y, Zw Z, et al. Dual-energy CT with virtual monoenergetic images to improve the visualization of pancreatic supplying arteries: the normal anatomy and variations. Insights Imaging. 2022;13:21. doi: 10.1186/s13244-022-01157-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s13244-022-01157-z</ArticleId><ArticleId IdType="pmc">PMC8816990</ArticleId><ArticleId IdType="pubmed">35122162</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang X, Zhang G, Xu L, et al. Utilisation of virtual non-contrast images and virtual mono-energetic images acquired from dual-layer spectral CT for renal cell carcinoma: image quality and radiation dose. Insights Imaging. 2022;13:12. doi: 10.1186/s13244-021-01146-8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s13244-021-01146-8</ArticleId><ArticleId IdType="pmc">PMC8787008</ArticleId><ArticleId IdType="pubmed">35072807</ArticleId></ArticleIdList></Reference><Reference><Citation>Sun K, Han R, Han Y, et al. Accuracy of combined computed tomography colonography and dual energy iodine map imaging for detecting colorectal masses using high-pitch dual-source CT. Sci Rep. 2018;8:3790. doi: 10.1038/s41598-018-22188-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-018-22188-x</ArticleId><ArticleId IdType="pmc">PMC5830575</ArticleId><ArticleId IdType="pubmed">29491380</ArticleId></ArticleIdList></Reference><Reference><Citation>Mileto A, Marin D, Alfaro-Cordoba M, et al. Iodine quantification to distinguish clear cell from papillary renal cell carcinoma at dual-energy multidetector CT: a multireader diagnostic performance study. Radiology. 2014;273:813&#x2013;820. doi: 10.1148/radiol.14140171.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.14140171</ArticleId><ArticleId IdType="pubmed">25162309</ArticleId></ArticleIdList></Reference><Reference><Citation>Zarzour JG, Milner D, Valentin R, et al. Quantitative iodine content threshold for discrimination of renal cell carcinomas using rapid kV-switching dual-energy CT. Abdom Radiol (NY) 2017;42:727&#x2013;734. doi: 10.1007/s00261-016-0967-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00261-016-0967-5</ArticleId><ArticleId IdType="pmc">PMC8983094</ArticleId><ArticleId IdType="pubmed">27847998</ArticleId></ArticleIdList></Reference><Reference><Citation>Martin SS, Weidinger S, Czwikla R, et al. Iodine and fat quantification for differentiation of adrenal gland adenomas from metastases using third-generation dual-source dual-energy computed tomography. Invest Radiol. 2018;53:173&#x2013;178. doi: 10.1097/RLI.0000000000000425.</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/RLI.0000000000000425</ArticleId><ArticleId IdType="pubmed">28990974</ArticleId></ArticleIdList></Reference><Reference><Citation>Yue X, Jiang Q, Hu X, et al. Quantitative dual-energy CT for evaluating hepatocellular carcinoma after transarterial chemoembolization. Sci Rep. 2021;11:11127. doi: 10.1038/s41598-021-90508-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-021-90508-9</ArticleId><ArticleId IdType="pmc">PMC8160271</ArticleId><ArticleId IdType="pubmed">34045528</ArticleId></ArticleIdList></Reference><Reference><Citation>Jacobsen MC, Cressman ENK, Tamm EP, et al. Dual-energy CT: Lower limits of iodine detection and quantification. Radiology. 2019;292:414&#x2013;419. doi: 10.1148/radiol.2019182870.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2019182870</ArticleId><ArticleId IdType="pmc">PMC6694721</ArticleId><ArticleId IdType="pubmed">31237496</ArticleId></ArticleIdList></Reference><Reference><Citation>McCollough CH, Boedeker K, Cody D et al (2020) Principles and applications of multienergy CT: report of AAPM task group 291. Med Phys 47. 10.1002/mp.14157</Citation><ArticleIdList><ArticleId IdType="pubmed">32215937</ArticleId></ArticleIdList></Reference><Reference><Citation>Greffier J, Si-Mohamed S, Dabli D, et al. Performance of four dual-energy CT platforms for abdominal imaging: a task-based image quality assessment based on phantom data. Eur Radiol. 2021;31:5324&#x2013;5334. doi: 10.1007/s00330-020-07671-2.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-020-07671-2</ArticleId><ArticleId IdType="pubmed">33449188</ArticleId></ArticleIdList></Reference><Reference><Citation>Jacobsen MC, Schellingerhout D, Wood CA, et al. Intermanufacturer comparison of dual-energy CT Iodine quantification and monochromatic attenuation: a phantom study. Radiology. 2018;287:224&#x2013;234. doi: 10.1148/radiol.2017170896.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2017170896</ArticleId><ArticleId IdType="pubmed">29185902</ArticleId></ArticleIdList></Reference><Reference><Citation>Sellerer T, No&#xeb;l PB, Patino M, et al. Dual-energy CT: a phantom comparison of different platforms for abdominal imaging. Eur Radiol. 2018;28:2745&#x2013;2755. doi: 10.1007/s00330-017-5238-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-017-5238-5</ArticleId><ArticleId IdType="pubmed">29404773</ArticleId></ArticleIdList></Reference><Reference><Citation>Harsaker V, Jensen K, Andersen HK, Martinsen AC. Quantitative benchmarking of iodine imaging for two CT spectral imaging technologies: a phantom study. Eur Radiol Exp. 2021;5:24. doi: 10.1186/s41747-021-00224-2.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s41747-021-00224-2</ArticleId><ArticleId IdType="pmc">PMC8219825</ArticleId><ArticleId IdType="pubmed">34159477</ArticleId></ArticleIdList></Reference><Reference><Citation>Kojima T, Shirasaka T, Kondo M, et al. A novel fast kilovoltage switching dual-energy CT with deep learning: accuracy of CT number on virtual monochromatic imaging and iodine quantification. Phys Med. 2021;81:253&#x2013;261. doi: 10.1016/j.ejmp.2020.12.018.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejmp.2020.12.018</ArticleId><ArticleId IdType="pubmed">33508738</ArticleId></ArticleIdList></Reference><Reference><Citation>Greffier J, Hamard A, Pereira F, et al. Image quality and dose reduction opportunity of deep learning image reconstruction algorithm for CT: a phantom study. Eur Radiol. 2020;30:3951&#x2013;3959. doi: 10.1007/s00330-020-06724-w.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-020-06724-w</ArticleId><ArticleId IdType="pubmed">32100091</ArticleId></ArticleIdList></Reference><Reference><Citation>Racine D, Becce F, Viry A, et al. Task-based characterization of a deep learning image reconstruction and comparison with filtered back-projection and a partial model-based iterative reconstruction in abdominal CT: a phantom study. Phys Med. 2020;76:28&#x2013;37. doi: 10.1016/j.ejmp.2020.06.004.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejmp.2020.06.004</ArticleId><ArticleId IdType="pubmed">32574999</ArticleId></ArticleIdList></Reference><Reference><Citation>Solomon J, Lyu P, Marin D, Samei E. Noise and spatial resolution properties of a commercially available deep learning-based CT reconstruction algorithm. Med Phys. 2020;47:3961&#x2013;3971. doi: 10.1002/mp.14319.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mp.14319</ArticleId><ArticleId IdType="pubmed">32506661</ArticleId></ArticleIdList></Reference><Reference><Citation>Higaki T, Nakamura Y, Zhou J, et al. Deep learning reconstruction at CT: phantom Study of the image characteristics. Acad Radiol. 2020;27:82&#x2013;87. doi: 10.1016/j.acra.2019.09.008.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.acra.2019.09.008</ArticleId><ArticleId IdType="pubmed">31818389</ArticleId></ArticleIdList></Reference><Reference><Citation>Greffier J, Si-Mohamed S, Guiu B, et al. Comparison of virtual monoenergetic imaging between a rapid kilovoltage switching dual-energy computed tomography with deep-learning and four dual-energy CTs with iterative reconstruction. Quant Imaging Med Surg. 2022;12:1149&#x2013;1162. doi: 10.21037/qims-21-708.</Citation><ArticleIdList><ArticleId IdType="doi">10.21037/qims-21-708</ArticleId><ArticleId IdType="pmc">PMC8739122</ArticleId><ArticleId IdType="pubmed">35111612</ArticleId></ArticleIdList></Reference><Reference><Citation>Greffier J, Viry A, Barbotteau Y, et al. Phantom task-based image quality assessment of three generations of rapid kV-switching dual-energy CT systems on virtual monoenergetic images. Med Phys. 2022;49(4):2233&#x2013;2244. doi: 10.1002/mp.15558.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mp.15558</ArticleId><ArticleId IdType="pubmed">35184293</ArticleId></ArticleIdList></Reference><Reference><Citation>Boedeker K, Hayes M, Zhou J, Zhang R, Yu Z . (2019) Deep Learning Spectral CT &#x2013; Faster, easier and more intelligent. Whitepaper - Canon Medical Systems. Accessed 2019- 12 ????? https://global.medical.canon/products/computed-tomography/spectral</Citation></Reference><Reference><Citation>Journal officiel de la r&#xe9;publique fran&#xe7;aise Arr&#xea;t&#xe9; du 23 mai 2019 portant homologation de la d&#xe9;cision n&#xb0; 2019-DC-0667 de l'Autorit&#xe9; de s&#xfb;ret&#xe9; nucl&#xe9;aire du 18 avril 2019 relative aux modalit&#xe9;s d'&#xe9;valuation des doses de rayonnements ionisants d&#xe9;livr&#xe9;es aux patients lors d'un acte de radiologie, de pratiques interventionnelles radioguid&#xe9;es ou de m&#xe9;decine nucl&#xe9;aire et &#xe0; la mise &#xe0; jour des niveaux de r&#xe9;f&#xe9;rence diagnostiques associ&#xe9;s. Accessed 31 mai 2019. https://www.legifrance.gouv.fr/</Citation></Reference><Reference><Citation>Samei E, Richard S. Assessment of the dose reduction potential of a model-based iterative reconstruction algorithm using a task-based performance metrology: CT task-based performance metrology. Med Phys. 2014;42:314&#x2013;323. doi: 10.1118/1.4903899.</Citation><ArticleIdList><ArticleId IdType="doi">10.1118/1.4903899</ArticleId><ArticleId IdType="pubmed">25563271</ArticleId></ArticleIdList></Reference><Reference><Citation>Samei E, Bakalyar D, Boedeker KL et al (2019) Performance evaluation of computed tomography systems: Summary of AAPM task group 233. Med Phys 46. 10.1002/mp.13763</Citation><ArticleIdList><ArticleId IdType="pubmed">31408540</ArticleId></ArticleIdList></Reference><Reference><Citation>Richard S, Husarik DB, Yadava G, et al. Towards task-based assessment of CT performance: system and object MTF across different reconstruction algorithms: towards task-based assessment of CT performance. Med Phys. 2012;39:4115&#x2013;4122. doi: 10.1118/1.4725171.</Citation><ArticleIdList><ArticleId IdType="doi">10.1118/1.4725171</ArticleId><ArticleId IdType="pubmed">22830744</ArticleId></ArticleIdList></Reference><Reference><Citation>Eckstein M, Bartroff J, Abbey C, et al. Automated computer evaluation and optimization of image compression of x-ray coronary angiograms for signal known exactly detection tasks. Opt Express. 2003;11:460. doi: 10.1364/OE.11.000460.</Citation><ArticleIdList><ArticleId IdType="doi">10.1364/OE.11.000460</ArticleId><ArticleId IdType="pubmed">19461753</ArticleId></ArticleIdList></Reference><Reference><Citation>Kalender WA, Perman WH, Vetter JR, Klotz E. Evaluation of a prototype dual-energy computed tomographic apparatus. I. Phantom studies. Med Phys. 1986;13:334&#x2013;339. doi: 10.1118/1.595958.</Citation><ArticleIdList><ArticleId IdType="doi">10.1118/1.595958</ArticleId><ArticleId IdType="pubmed">3724693</ArticleId></ArticleIdList></Reference><Reference><Citation>D&#x2019;Angelo T, Cicero G, Mazziotti S et al (2019) Dual energy computed tomography virtual monoenergetic imaging: technique and clinical applications. BJR:20180546. 10.1259/bjr.20180546</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6592074</ArticleId><ArticleId IdType="pubmed">30919651</ArticleId></ArticleIdList></Reference><Reference><Citation>Greffier J, Dabli D, Frandon J, et al. Comparison of two versions of a deep learning image reconstruction algorithm on CT image quality and dose reduction: a phantom study. Med Phys. 2021;48:5743&#x2013;5755. doi: 10.1002/mp.15180.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mp.15180</ArticleId><ArticleId IdType="pubmed">34418110</ArticleId></ArticleIdList></Reference><Reference><Citation>Hsiao C-Y, Chen T-H, Lee Y-C, Wang M-C. Ureteral stone with hydronephrosis and urolithiasis alone are risk factors for acute kidney injury in patients with urinary tract infection. Sci Rep. 2021;11:23333. doi: 10.1038/s41598-021-02647-8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-021-02647-8</ArticleId><ArticleId IdType="pmc">PMC8639828</ArticleId><ArticleId IdType="pubmed">34857804</ArticleId></ArticleIdList></Reference><Reference><Citation>Lenhard DC, Frisk A-L, Lengsfeld P, et al. The effect of iodinated contrast agent properties on renal kinetics and oxygenation. Invest Radiol. 2013;48:175&#x2013;182. doi: 10.1097/RLI.0b013e31827b70f9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/RLI.0b013e31827b70f9</ArticleId><ArticleId IdType="pubmed">23262792</ArticleId></ArticleIdList></Reference><Reference><Citation>Greffier J, Frandon J, Si-Mohamed S, et al. Comparison of two deep learning image reconstruction algorithms in chest CT images: a task-based image quality assessment on phantom data. Diagn Interv Imaging. 2022;103:21&#x2013;30. doi: 10.1016/j.diii.2021.08.001.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.diii.2021.08.001</ArticleId><ArticleId IdType="pubmed">34493475</ArticleId></ArticleIdList></Reference><Reference><Citation>Gro&#xdf;e Hokamp N, H&#xf6;ink AJ, Doerner J, et al. Assessment of arterially hyper-enhancing liver lesions using virtual monoenergetic images from spectral detector CT: phantom and patient experience. Abdom Radiol (NY) 2018;43:2066&#x2013;2074. doi: 10.1007/s00261-017-1411-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00261-017-1411-1</ArticleId><ArticleId IdType="pubmed">29185013</ArticleId></ArticleIdList></Reference><Reference><Citation>McNamara MM, Little MD, Alexander LF, et al. Multireader evaluation of lesion conspicuity in small pancreatic adenocarcinomas: complimentary value of iodine material density and low keV simulated monoenergetic images using multiphasic rapid kVp-switching dual energy CT. Abdom Imaging. 2015;40:1230&#x2013;1240. doi: 10.1007/s00261-014-0274-y.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00261-014-0274-y</ArticleId><ArticleId IdType="pubmed">25331567</ArticleId></ArticleIdList></Reference><Reference><Citation>Patel BN, Farjat A, Schabel C, et al. Energy-specific optimization of attenuation thresholds for low-energy virtual monoenergetic images in renal lesion evaluation. AJR Am J Roentgenol. 2018;210:W205&#x2013;W217. doi: 10.2214/AJR.17.1864.</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/AJR.17.1864</ArticleId><ArticleId IdType="pubmed">29547057</ArticleId></ArticleIdList></Reference><Reference><Citation>Lee SM, Kim SH, Ahn SJ, et al. Virtual monoenergetic dual-layer, dual-energy CT enterography: optimization of keV settings and its added value for Crohn&#x2019;s disease. Eur Radiol. 2018;28:2525&#x2013;2534. doi: 10.1007/s00330-017-5215-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-017-5215-z</ArticleId><ArticleId IdType="pubmed">29294151</ArticleId></ArticleIdList></Reference><Reference><Citation>Martin SS, Pfeifer S, Wichmann JL, et al. Noise-optimized virtual monoenergetic dual-energy computed tomography: optimization of kiloelectron volt settings in patients with gastrointestinal stromal tumors. Abdom Radiol (NY) 2017;42:718&#x2013;726. doi: 10.1007/s00261-016-1011-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00261-016-1011-5</ArticleId><ArticleId IdType="pubmed">27999889</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36617595</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>10</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>11</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">2365-7464</ISSN><JournalIssue CitedMedium="Internet"><Volume>8</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>08</Day></PubDate></JournalIssue><Title>Cognitive research: principles and implications</Title><ISOAbbreviation>Cogn Res Princ Implic</ISOAbbreviation></Journal><ArticleTitle>Using global feedback to induce learning of gist of abnormality in mammograms.</ArticleTitle><Pagination><StartPage>3</StartPage><MedlinePgn>3</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">3</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1186/s41235-022-00457-8</ELocationID><Abstract><AbstractText>Extraction of global structural regularities provides general 'gist' of our everyday visual environment as it does the gist of abnormality for medical experts reviewing medical images. We investigated whether na&#xef;ve observers could learn this gist of medical abnormality. Fifteen participants completed nine adaptive training sessions viewing four categories of unilateral mammograms: normal, obvious-abnormal, subtle-abnormal, and global signals of abnormality (mammograms with no visible lesions but from breasts contralateral to or years prior to the development of cancer) and receiving only categorical feedback. Performance was tested pre-training, post-training, and after a week's retention on 200 mammograms viewed for 500&#xa0;ms without feedback. Performance measured as d' was modulated by mammogram category, with the highest performance for mammograms with visible lesions. Post-training, twelve observed showed increased d' for all mammogram categories but a subset of nine, labelled learners also showed a positive correlation of d' across training. Critically, learners learned to detect abnormality in mammograms with only the global signals, but improvements were poorly retained. A state-of-the-art breast cancer classifier detected mammograms with lesions but struggled to detect cancer in mammograms with the global signal of abnormality. The gist of abnormality can be learned through perceptual/incidental learning in mammograms both with and without visible lesions, subject to individual differences. Poor retention suggests perceptual tuning to gist needs maintenance, converging with findings that radiologists' gist performance correlates with the number of cases reviewed per year, not years of experience. The human visual system can tune itself to complex global signals not easily captured by current deep neural networks.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Raat</LastName><ForeName>E M</ForeName><Initials>EM</Initials><Identifier Source="ORCID">0000-0001-6748-5186</Identifier><AffiliationInfo><Affiliation>University of York, Heslington, York, YO10 5DD, UK. emma.raat@york.ac.uk.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kyle-Davidson</LastName><ForeName>C</ForeName><Initials>C</Initials><AffiliationInfo><Affiliation>University of York, Heslington, York, YO10 5DD, UK.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Evans</LastName><ForeName>K K</ForeName><Initials>KK</Initials><AffiliationInfo><Affiliation>University of York, Heslington, York, YO10 5DD, UK. karla.evans@york.ac.uk.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>08</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>Cogn Res Princ Implic</MedlineTA><NlmUniqueID>101697632</NlmUniqueID><ISSNLinking>2365-7464</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D005260" MajorTopicYN="N">Female</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D005246" MajorTopicYN="N">Feedback</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D008327" MajorTopicYN="Y">Mammography</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D001943" MajorTopicYN="Y">Breast Neoplasms</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D001940" MajorTopicYN="N">Breast</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D000072177" MajorTopicYN="N">Radiologists</DescriptorName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Deep neural network</Keyword><Keyword MajorTopicYN="N">Gist extraction</Keyword><Keyword MajorTopicYN="N">Gist of abnormality</Keyword><Keyword MajorTopicYN="N">Implicit learning</Keyword><Keyword MajorTopicYN="N">Medical expertise</Keyword><Keyword MajorTopicYN="N">Medical image perception</Keyword><Keyword MajorTopicYN="N">Medical imaging</Keyword><Keyword MajorTopicYN="N">Perceptual learning</Keyword><Keyword MajorTopicYN="N">Statistical learning</Keyword></KeywordList><CoiStatement>No competing interests to disclose.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2021</Year><Month>12</Month><Day>21</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>19</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>23</Hour><Minute>19</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36617595</ArticleId><ArticleId IdType="pmc">PMC9826776</ArticleId><ArticleId IdType="doi">10.1186/s41235-022-00457-8</ArticleId><ArticleId IdType="pii">10.1186/s41235-022-00457-8</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Aberg KC, Herzog MH. Different types of feedback change decision criterion and sensitivity differently in perceptual learning. Journal of Vision. 2012;12(3):3&#x2013;3.</Citation><ArticleIdList><ArticleId IdType="pubmed">22396463</ArticleId></ArticleIdList></Reference><Reference><Citation>Bacon-Mac&#xe9; N, Mac&#xe9; MJM, Fabre-Thorpe M, Thorpe SJ. The time course of visual processing: Backward masking and natural scene categorisation. Vision Research. 2005;45:1459&#x2013;1469. doi: 10.1016/j.visres.2005.01.004.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.visres.2005.01.004</ArticleId><ArticleId IdType="pubmed">15743615</ArticleId></ArticleIdList></Reference><Reference><Citation>Bau D, Zhu J-Y, Strobelt H, Lapedriza A, Zhou B, Torralba A. Understanding the role of individual units in a deep neural network. Proceedings of the National Academy of Sciences. 2020;117(48):30071&#x2013;30078.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7720226</ArticleId><ArticleId IdType="pubmed">32873639</ArticleId></ArticleIdList></Reference><Reference><Citation>Bavelier D, Green CS, Pouget A, Schrater P. Brain plasticity through the life span: Learning to learn and action video games. Annual Review of Neuroscience. 2012;35:391&#x2013;416.</Citation><ArticleIdList><ArticleId IdType="pubmed">22715883</ArticleId></ArticleIdList></Reference><Reference><Citation>Bejjanki VR, Zhang R, Li R, Pouget A, Green CS, Lu Z-L, Bavelier D. Action video game play facilitates the development of better perceptual templates. Proceedings of the National Academy of Sciences. 2014;111(47):16961&#x2013;16966.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4250112</ArticleId><ArticleId IdType="pubmed">25385590</ArticleId></ArticleIdList></Reference><Reference><Citation>Bi T, Chen J, Zhou T, He Y, Fang F. Function and structure of human left fusiform cortex are closely associated with perceptual learning of faces. Current Biology. 2014;24(2):222&#x2013;227.</Citation><ArticleIdList><ArticleId IdType="pubmed">24412207</ArticleId></ArticleIdList></Reference><Reference><Citation>Boucart M, Moroni C, Thibaut M, Szaffarczyk S, Greene M. Scene categorization at large visual eccentricities. Vision Research. 2013;86:35&#x2013;42.</Citation><ArticleIdList><ArticleId IdType="pubmed">23597581</ArticleId></ArticleIdList></Reference><Reference><Citation>Brady TF, Oliva A. Statistical learning of temporal predictability in scene gist. Journal of Vision. 2007;7(9):1050&#x2013;1050.</Citation></Reference><Reference><Citation>Brennan PC, Gandomkar Z, Ekpo EU, Tapia K, Trieu PD, Lewis SJ, Evans KK. Radiologists can detect the &#x2018;gist&#x2019;of breast cancer before any overt signs of cancer appear. Scientific Reports. 2018;8(1):1&#x2013;12.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5992208</ArticleId><ArticleId IdType="pubmed">29880817</ArticleId></ArticleIdList></Reference><Reference><Citation>Carrigan AJ, Wardle SG, Rich AN. Finding cancer in mammograms: If you know it&#x2019;s there, do you know where? Cognitive Research: Principles and Implications. 2018;3(1):10.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5904219</ArticleId><ArticleId IdType="pubmed">29707615</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen X, Hegd&#xe9; J. Learning to break camouflage by learning the background. Psychological Science. 2012;23(11):1395&#x2013;1403.</Citation><ArticleIdList><ArticleId IdType="pubmed">23064405</ArticleId></ArticleIdList></Reference><Reference><Citation>Chun MM. Contextual cueing of visual attention. Trends in Cognitive Sciences. 2000;4(5):170&#x2013;178.</Citation><ArticleIdList><ArticleId IdType="pubmed">10782102</ArticleId></ArticleIdList></Reference><Reference><Citation>Chun MM, Jiang Y. Contextual cueing: Implicit learning and memory of visual context guides spatial attention. Cognitive Psychology. 1998;36(1):28&#x2013;71.</Citation><ArticleIdList><ArticleId IdType="pubmed">9679076</ArticleId></ArticleIdList></Reference><Reference><Citation>D&#x2019;Orsi C, Bassett L, Feig S. Breast imaging reporting and data system (BI-RADS). Breast imaging atlas. 4. American College of Radiology; 2018.</Citation><ArticleIdList><ArticleId IdType="pubmed">12202727</ArticleId></ArticleIdList></Reference><Reference><Citation>Devillez H, Mollison MV, Hagen S, Tanaka JW, Scott LS, Curran T. Color and spatial frequency differentially impact early stages of perceptual expertise training. Neuropsychologia. 2019;122:62&#x2013;75.</Citation><ArticleIdList><ArticleId IdType="pubmed">30471254</ArticleId></ArticleIdList></Reference><Reference><Citation>Dobres J, Seitz AR. Perceptual learning of oriented gratings as revealed by classification images. Journal of Vision. 2010;10(13):8&#x2013;8.</Citation><ArticleIdList><ArticleId IdType="pubmed">21071575</ArticleId></ArticleIdList></Reference><Reference><Citation>Emery KJ, Webster MA. Individual differences and their implications for color perception. Current Opinion in Behavioral Sciences. 2019;30:28&#x2013;33.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7430749</ArticleId><ArticleId IdType="pubmed">32832586</ArticleId></ArticleIdList></Reference><Reference><Citation>Evans, K. K., Birdwell, R. L., &amp; Wolfe, J. M. (2013a). If you don&#x2019;t find it often, you often don&#x2019;t find it: why some cancers are missed in breast cancer screening. PloS one, 8(5).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC3667799</ArticleId><ArticleId IdType="pubmed">23737980</ArticleId></ArticleIdList></Reference><Reference><Citation>Evans KK, Cohen MA, Tambouret R, Horowitz T, Kreindel E, Wolfe JM. Does visual expertise improve visual recognition memory? Attention, Perception, &amp; Psychophysics. 2011;73(1):30&#x2013;35.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC3140200</ArticleId><ArticleId IdType="pubmed">21258906</ArticleId></ArticleIdList></Reference><Reference><Citation>Evans, K. K., Culpan, A. M., &amp; Wolfe, J. M. (2019). Detecting the "GIST" of breast cancer in mammograms three years before localized signs of cancer are visible. British Journal of Radiology, 92. doi:10.1259/bjr.20190136</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6636261</ArticleId><ArticleId IdType="pubmed">31166769</ArticleId></ArticleIdList></Reference><Reference><Citation>Evans KK, Georgian-Smith D, Tambouret R, Birdwell RL, Wolfe JM. The gist of the abnormal: Above-chance medical decision making in the blink of an eye. Psychonomic Bulletin and Review. 2013;20:1170&#x2013;1175. doi: 10.3758/s13423-013-0459-3.</Citation><ArticleIdList><ArticleId IdType="doi">10.3758/s13423-013-0459-3</ArticleId><ArticleId IdType="pmc">PMC3851597</ArticleId><ArticleId IdType="pubmed">23771399</ArticleId></ArticleIdList></Reference><Reference><Citation>Evans KK, Haygood TM, Cooper J, Culpan AM, Wolfe JM. A half-second glimpse often lets radiologists identify breast cancer cases even when viewing the mammogram of the opposite breast. Proceedings of the National Academy of Sciences of the United States of America. 2016;113:10292&#x2013;10297. doi: 10.1073/pnas.1606187113.</Citation><ArticleIdList><ArticleId IdType="doi">10.1073/pnas.1606187113</ArticleId><ArticleId IdType="pmc">PMC5027466</ArticleId><ArticleId IdType="pubmed">27573841</ArticleId></ArticleIdList></Reference><Reference><Citation>Evans KK, Horowitz TS, Wolfe JM. When categories collide: Accumulation of information about multiple categories in rapid scene perception. Journal of Psychological Science. 2011;22(6):739&#x2013;746.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC3140830</ArticleId><ArticleId IdType="pubmed">21555522</ArticleId></ArticleIdList></Reference><Reference><Citation>Evans KK, Treisman AM. Perception of objects in natural scenes: Is it really attention free? Journal of Experimental Psychology: Human Perception and Performance. 2005;31(6):1476.</Citation><ArticleIdList><ArticleId IdType="pubmed">16366803</ArticleId></ArticleIdList></Reference><Reference><Citation>Fabre-Thorpe M, Delorme A, Marlot C, Thorpe S. A limit to the speed of processing in ultra-rapid visual categorization of novel natural scenes. Journal of Cognitive Neuroscience. 2001;13(2):171&#x2013;180.</Citation><ArticleIdList><ArticleId IdType="pubmed">11244543</ArticleId></ArticleIdList></Reference><Reference><Citation>Fiser J, Aslin RN. Unsupervised statistical learning of higher-order spatial structures from visual scenes. Psychological Science. 2001;12(6):499&#x2013;504.</Citation><ArticleIdList><ArticleId IdType="pubmed">11760138</ArticleId></ArticleIdList></Reference><Reference><Citation>Fiser J, Aslin RN. Statistical learning of higher-order temporal structure from visual shape sequences. Journal of Experimental Psychology: Learning, Memory, and Cognition. 2002;28(3):458.</Citation><ArticleIdList><ArticleId IdType="pubmed">12018498</ArticleId></ArticleIdList></Reference><Reference><Citation>Fiser J, Aslin RN. Statistical learning of new visual feature combinations by infants. Proceedings of the National Academy of Sciences. 2002;99(24):15822&#x2013;15826.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC137800</ArticleId><ArticleId IdType="pubmed">12429858</ArticleId></ArticleIdList></Reference><Reference><Citation>Frank SM, Reavis EA, Greenlee MW, Tse PU. Pretraining cortical thickness predicts subsequent perceptual learning rate in a visual search task. Cerebral Cortex. 2016;26(3):1211&#x2013;1220.</Citation><ArticleIdList><ArticleId IdType="pubmed">25576537</ArticleId></ArticleIdList></Reference><Reference><Citation>Freeman J, Simoncelli EP. Metamers of the ventral stream. Nature Neuroscience. 2011;14(9):1195.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC3164938</ArticleId><ArticleId IdType="pubmed">21841776</ArticleId></ArticleIdList></Reference><Reference><Citation>Gandomkar Z, Siviengphanom S, Ekpo EU, Suleiman MA, Li T, Xu D, Brennan PC. Global processing provides malignancy evidence complementary to the information captured by humans or machines following detailed mammogram inspection. Scientific reports. 2021;11(1):1&#x2013;12.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8505651</ArticleId><ArticleId IdType="pubmed">34635726</ArticleId></ArticleIdList></Reference><Reference><Citation>Greene MR, Oliva A. Recognition of natural scenes from global properties: Seeing the forest without representing the trees. Cognitive Psychology. 2009;58:137&#x2013;176. doi: 10.1016/j.cogpsych.2008.06.001.Recognition.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cogpsych.2008.06.001.Recognition</ArticleId><ArticleId IdType="pmc">PMC2759758</ArticleId><ArticleId IdType="pubmed">18762289</ArticleId></ArticleIdList></Reference><Reference><Citation>Hanley JA, McNeil BJ. The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology. 1982;143(1):29&#x2013;36.</Citation><ArticleIdList><ArticleId IdType="pubmed">7063747</ArticleId></ArticleIdList></Reference><Reference><Citation>Hegd&#xe9; J. Deep learning can be used to train na&#xef;ve, nonprofessional observers to detect diagnostic visual patterns of certain cancers in mammograms: A proof-of-principle study. Journal of Medical Imaging. 2020;7(2):022410.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6998757</ArticleId><ArticleId IdType="pubmed">32042860</ArticleId></ArticleIdList></Reference><Reference><Citation>Johnson SP. Development of visual perception. Wiley Interdisciplinary Reviews: Cognitive Science. 2011;2(5):515&#x2013;528.</Citation><ArticleIdList><ArticleId IdType="pubmed">26302303</ArticleId></ArticleIdList></Reference><Reference><Citation>Jones T, Hadley H, Cataldo AM, Arnold E, Curran T, Tanaka JW, Scott LS. Neural and behavioral effects of subordinate-level training of novel objects across manipulations of color and spatial frequency. European Journal of Neuroscience. 2020;52(11):4468&#x2013;4479.</Citation><ArticleIdList><ArticleId IdType="pubmed">29499088</ArticleId></ArticleIdList></Reference><Reference><Citation>Joubert OR, Rousselet GA, Fabre-Thorpe M, Fize D. Rapid visual categorization of natural scene contexts with equalized amplitude spectrum and increasing phase noise. Journal of Vision. 2009;9(1):1&#x2013;16.</Citation><ArticleIdList><ArticleId IdType="pubmed">19271872</ArticleId></ArticleIdList></Reference><Reference><Citation>Kundel HL, Nodine CF. Interpreting chest radiographs without visual search. Radiology. 1975;116(3):527&#x2013;532.</Citation><ArticleIdList><ArticleId IdType="pubmed">125436</ArticleId></ArticleIdList></Reference><Reference><Citation>Larson AM, Loschky LC. The contributions of central versus peripheral vision to scene gist recognition. Journal of Vision. 2009;9(10):6&#x2013;6.</Citation><ArticleIdList><ArticleId IdType="pubmed">19810787</ArticleId></ArticleIdList></Reference><Reference><Citation>Levenson RM, Krupinski EA, Navarro VM, Wasserman EA. Pigeons (Columba livia) as trainable observers of pathology and radiology breast cancer images. PLoS ONE. 2015;10(11):e0141357.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4651348</ArticleId><ArticleId IdType="pubmed">26581091</ArticleId></ArticleIdList></Reference><Reference><Citation>Li FF, VanRullen R, Koch C, Perona P. Natural scene categorization in the near absence of attention: Further explorations. Journal of Vision. 2003;3:331&#x2013;331. doi: 10.1167/3.9.331.</Citation><ArticleIdList><ArticleId IdType="doi">10.1167/3.9.331</ArticleId></ArticleIdList></Reference><Reference><Citation>Li Q, Joo SJ, Yeatman JD, Reinecke K. Controlling for participants&#x2019; viewing distance in large-scale, psychophysical online experiments using a virtual chinrest. Scientific Reports. 2020;10(1):1&#x2013;11.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6976612</ArticleId><ArticleId IdType="pubmed">31969579</ArticleId></ArticleIdList></Reference><Reference><Citation>Lloyd R, Hodgson ME, Stokes A. Visual categorization with aerial photographs. Annals of the Association of American Geographers. 2002;92(2):241&#x2013;266.</Citation></Reference><Reference><Citation>Loschky LC, Ringer RV, Ellis K, Hansen BC. Comparing rapid scene categorization of aerial and terrestrial views: A new perspective on scene gist. Journal of Vision. 2015;15(6):11&#x2013;11.</Citation><ArticleIdList><ArticleId IdType="pubmed">26024458</ArticleId></ArticleIdList></Reference><Reference><Citation>Maurer D. Chapter 1: Infant Visual Perception: Methods of Study. Infant Perception: From Sensation to Cognition: Basic Visual Processes. 2013;1:1.</Citation></Reference><Reference><Citation>Oliva A, Torralba A. Modeling the shape of the scene: A holistic representation of the spatial envelope. International Journal of Computer Vision. 2001;42(3):145&#x2013;175.</Citation></Reference><Reference><Citation>Ouyang W, Zeng X, Wang X, Qiu S, Luo P, Tian Y, Li H. DeepID-Net: Object detection with deformable part based convolutional neural networks. IEEE Transactions on Pattern Analysis and Machine Intelligence. 2016;39(7):1320&#x2013;1334.</Citation><ArticleIdList><ArticleId IdType="pubmed">27392342</ArticleId></ArticleIdList></Reference><Reference><Citation>Palmeri TJ, Wong AC, Gauthier I. Computational approaches to the development of perceptual expertise. Trends in Cognitive Sciences. 2004;8(8):378&#x2013;386.</Citation><ArticleIdList><ArticleId IdType="pubmed">15335465</ArticleId></ArticleIdList></Reference><Reference><Citation>Portilla J, Simoncelli EP. A parametric texture model based on joint statistics of complex wavelet coefficients. International Journal of Computer Vision. 2000;40(1):49&#x2013;70.</Citation></Reference><Reference><Citation>Potter MC, Wyble B, Hagmann CE, McCourt ES. Detecting meaning in RSVP at 13 ms per picture. Attention, Perception, &amp; Psychophysics. 2014;76(2):270&#x2013;279.</Citation><ArticleIdList><ArticleId IdType="pubmed">24374558</ArticleId></ArticleIdList></Reference><Reference><Citation>Pringle, H. L., Kramer, A. F., &amp; Irwin, D. E. (2004). Individual differences in the visual representation of scenes: MIT Press.</Citation></Reference><Reference><Citation>Raat E, Farr I, Wolfe J, Evans K. Comparable prediction of breast cancer risk from a glimpse or a first impression of a mammogram. Cognitive Research: Principles and Implications. 2021;6(1):1&#x2013;14.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8572261</ArticleId><ArticleId IdType="pubmed">34743266</ArticleId></ArticleIdList></Reference><Reference><Citation>Reeder RR, Stein T, Peelen MV. Perceptual expertise improves category detection in natural scenes. Psychonomic Bulletin &amp; Review. 2016;23(1):172&#x2013;179.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4742498</ArticleId><ArticleId IdType="pubmed">26106059</ArticleId></ArticleIdList></Reference><Reference><Citation>Rousselet GA, Thorpe SJ, Fabre-Thorpe M. Processing of one, two or four natural scenes in humans: The limits of parallelism. Vision Research. 2004;44(9):877&#x2013;894.</Citation><ArticleIdList><ArticleId IdType="pubmed">14992832</ArticleId></ArticleIdList></Reference><Reference><Citation>Semizer, Y., Michel, M., Evans, K., &amp; Wolfe, J. (2018). Texture as a diagnostic signal in mammograms.</Citation></Reference><Reference><Citation>Sharma P. Biology and management of patients with triple-negative breast cancer. The Oncologist. 2016;21(9):1050&#x2013;1062.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5016071</ArticleId><ArticleId IdType="pubmed">27401886</ArticleId></ArticleIdList></Reference><Reference><Citation>Shinn-Cunningham, B., Varghese, L., Wang, L., &amp; Bharadwaj, H. (2017). Individual differences in temporal perception and their implications for everyday listening. The Frequency-Following Response, 159&#x2013;192.</Citation></Reference><Reference><Citation>Simoncelli EP. Vision and the statistics of the visual environment. Current Opinion in Neurobiology. 2003;13(2):144&#x2013;149.</Citation><ArticleIdList><ArticleId IdType="pubmed">12744966</ArticleId></ArticleIdList></Reference><Reference><Citation>Simonyan, K., &amp; Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.</Citation></Reference><Reference><Citation>Sweeny TD, Wurnitsch N, Gopnik A, Whitney D. Ensemble perception of size in 4&#x2013;5-year-old children. Developmental Science. 2015;18(4):556&#x2013;568.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5282927</ArticleId><ArticleId IdType="pubmed">25442844</ArticleId></ArticleIdList></Reference><Reference><Citation>Taigman, Y., Yang, M., Ranzato, M. A., &amp; Wolf, L. (2014). Deepface: Closing the gap to human-level performance in face verification. Paper presented at the proceedings of the IEEE conference on computer vision and pattern recognition.</Citation></Reference><Reference><Citation>Turk-Browne, N. B. (2012). Statistical learning and its consequences. In The influence of attention, learning, and motivation on visual search (pp. 117&#x2013;146): Springer.</Citation><ArticleIdList><ArticleId IdType="pubmed">23437632</ArticleId></ArticleIdList></Reference><Reference><Citation>Turk-Browne NB, Jung&#xe9; JA, Scholl BJ. The automaticity of visual statistical learning. Journal of Experimental Psychology: General. 2005;134(4):552.</Citation><ArticleIdList><ArticleId IdType="pubmed">16316291</ArticleId></ArticleIdList></Reference><Reference><Citation>VanRullen R, Thorpe SJ. The time course of visual processing: From early perception to decision-making. Journal of Cognitive Neuroscience. 2001;13:454&#x2013;461. doi: 10.1162/08989290152001880.</Citation><ArticleIdList><ArticleId IdType="doi">10.1162/08989290152001880</ArticleId><ArticleId IdType="pubmed">11388919</ArticleId></ArticleIdList></Reference><Reference><Citation>Voss MW, Kramer AF, Basak C, Prakash RS, Roberts B. Are expert athletes &#x2018;expert&#x2019;in the cognitive laboratory? A meta-analytic review of cognition and sport expertise. Applied Cognitive Psychology. 2010;24(6):812&#x2013;826.</Citation></Reference><Reference><Citation>Voulodimos, A., Doulamis, N., Doulamis, A., &amp; Protopapadakis, E. (2018). Deep learning for computer vision: A brief review. Computational Intelligence and Neuroscience, 2018.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5816885</ArticleId><ArticleId IdType="pubmed">29487619</ArticleId></ArticleIdList></Reference><Reference><Citation>White D, Burton AM. Individual differences and the multidimensional nature of face perception. Nature Reviews Psychology. 2022;1(5):287&#x2013;300.</Citation></Reference><Reference><Citation>Wong AC-N, Palmeri TJ, Gauthier I. Conditions for facelike expertise with objects: Becoming a Ziggerin expert&#x2014;but which type? Psychological Science. 2009;20(9):1108&#x2013;1117.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC2919853</ArticleId><ArticleId IdType="pubmed">19694980</ArticleId></ArticleIdList></Reference><Reference><Citation>Wu N, Phang J, Park J, Shen Y, Huang Z, Zorin M, Kim E. Deep neural networks improve radiologists&#x2019; performance in breast cancer screening. IEEE Transactions on Medical Imaging. 2019;39(4):1184&#x2013;1194.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7427471</ArticleId><ArticleId IdType="pubmed">31603772</ArticleId></ArticleIdList></Reference><Reference><Citation>Wurster, S. W., Sitek, A., Chen, J., Evans, K., Kim, G., &amp; Wolfe, J. M. (2019). Human gist processing augments deep learning breast cancer risk assessment. arXiv preprint arXiv:1912.05470.</Citation></Reference><Reference><Citation>Xu B, Rourke L, Robinson JK, Tanaka JW. Training melanoma detection in photographs using the perceptual expertise training approach. Applied Cognitive Psychology. 2016;30(5):750&#x2013;756.</Citation></Reference><Reference><Citation>Yang J, Yan F-F, Chen L, Xi J, Fan S, Zhang P, Huang C-B. General learning ability in perceptual learning. Proceedings of the National Academy of Sciences. 2020;117(32):19092&#x2013;19100.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7430974</ArticleId><ArticleId IdType="pubmed">32703813</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36617076</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>10</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">1424-8220</ISSN><JournalIssue CitedMedium="Internet"><Volume>23</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>02</Day></PubDate></JournalIssue><Title>Sensors (Basel, Switzerland)</Title><ISOAbbreviation>Sensors (Basel)</ISOAbbreviation></Journal><ArticleTitle>LDDNet: A Deep Learning Framework for the Diagnosis of Infectious Lung Diseases.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">480</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/s23010480</ELocationID><Abstract><AbstractText>This paper proposes a new deep learning (DL) framework for the analysis of lung diseases, including COVID-19 and pneumonia, from chest CT scans and X-ray (CXR) images. This framework is termed optimized DenseNet201 for lung diseases (LDDNet). The proposed LDDNet was developed using additional layers of 2D global average pooling, dense and dropout layers, and batch normalization to the base DenseNet201 model. There are 1024 Relu-activated dense layers and 256 dense layers using the sigmoid activation method. The hyper-parameters of the model, including the learning rate, batch size, epochs, and dropout rate, were tuned for the model. Next, three datasets of lung diseases were formed from separate open-access sources. One was a CT scan dataset containing 1043 images. Two X-ray datasets comprising images of COVID-19-affected lungs, pneumonia-affected lungs, and healthy lungs exist, with one being an imbalanced dataset with 5935 images and the other being a balanced dataset with 5002 images. The performance of each model was analyzed using the Adam, Nadam, and SGD optimizers. The best results have been obtained for both the CT scan and CXR datasets using the Nadam optimizer. For the CT scan images, LDDNet showed a COVID-19-positive classification accuracy of 99.36%, a 100% precision recall of 98%, and an F1 score of 99%. For the X-ray dataset of 5935 images, LDDNet provides a 99.55% accuracy, 73% recall, 100% precision, and 85% F1 score using the Nadam optimizer in detecting COVID-19-affected patients. For the balanced X-ray dataset, LDDNet provides a 97.07% classification accuracy. For a given set of parameters, the performance results of LDDNet are better than the existing algorithms of ResNet152V2 and XceptionNet.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Podder</LastName><ForeName>Prajoy</ForeName><Initials>P</Initials><Identifier Source="ORCID">0000-0002-7564-4738</Identifier><AffiliationInfo><Affiliation>Institute of Information and Communication Technology, Bangladesh University of Engineering and Technology, Dhaka 1205, Bangladesh.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Das</LastName><ForeName>Sanchita Rani</ForeName><Initials>SR</Initials><AffiliationInfo><Affiliation>Institute of Information and Communication Technology, Bangladesh University of Engineering and Technology, Dhaka 1205, Bangladesh.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Mondal</LastName><ForeName>M Rubaiyat Hossain</ForeName><Initials>MRH</Initials><Identifier Source="ORCID">0000-0002-8582-9197</Identifier><AffiliationInfo><Affiliation>Institute of Information and Communication Technology, Bangladesh University of Engineering and Technology, Dhaka 1205, Bangladesh.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Bharati</LastName><ForeName>Subrato</ForeName><Initials>S</Initials><Identifier Source="ORCID">0000-0001-8849-4313</Identifier><AffiliationInfo><Affiliation>Institute of Information and Communication Technology, Bangladesh University of Engineering and Technology, Dhaka 1205, Bangladesh.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Maliha</LastName><ForeName>Azra</ForeName><Initials>A</Initials><Identifier Source="ORCID">0000-0002-4855-5836</Identifier><AffiliationInfo><Affiliation>Faculty of Engineering and IT, The British University in Dubai, Dubai P.O. Box 345015, United Arab Emirates.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Hasan</LastName><ForeName>Md Junayed</ForeName><Initials>MJ</Initials><Identifier Source="ORCID">0000-0003-4578-952X</Identifier><AffiliationInfo><Affiliation>National Subsea Centre, Robert Gordon University, Aberdeen AB10 7AQ, UK.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Piltan</LastName><ForeName>Farzin</ForeName><Initials>F</Initials><AffiliationInfo><Affiliation>Ulsan Industrial Artificial Intelligence (UIAI) Lab, Department of Electrical, Electronics and Computer Engineering, University of Ulsan, Ulsan 44610, Republic of Korea.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>02</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Sensors (Basel)</MedlineTA><NlmUniqueID>101204366</NlmUniqueID><ISSNLinking>1424-8220</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000086382" MajorTopicYN="Y">COVID-19</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D011014" MajorTopicYN="Y">Pneumonia</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D014057" MajorTopicYN="N">Tomography, X-Ray Computed</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000465" MajorTopicYN="N">Algorithms</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000086742" MajorTopicYN="N">COVID-19 Testing</DescriptorName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">COVID-19</Keyword><Keyword MajorTopicYN="N">CT scan</Keyword><Keyword MajorTopicYN="N">DenseNet201</Keyword><Keyword MajorTopicYN="N">ResNet152V2</Keyword><Keyword MajorTopicYN="N">X-ray</Keyword><Keyword MajorTopicYN="N">XceptionNet</Keyword><Keyword MajorTopicYN="N">infectious disease</Keyword></KeywordList><CoiStatement>All the authors in this paper have no conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>11</Month><Day>8</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>25</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>27</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>1</Hour><Minute>45</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36617076</ArticleId><ArticleId IdType="pmc">PMC9824583</ArticleId><ArticleId IdType="doi">10.3390/s23010480</ArticleId><ArticleId IdType="pii">s23010480</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Novel Coronavirus&#x2014;China. Online. 2020.  [(accessed on 11 October 2022)].  Available online:  http://www.who.int/csr/don/12-january-2020-novel-coronaviruschina/en/</Citation></Reference><Reference><Citation>Tomar A., Gupta N. Prediction for the spread of COVID-19 in India and effectiveness of preventive measures. Sci. Total. Environ. 2020;728:138762. doi: 10.1016/j.scitotenv.2020.138762.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.scitotenv.2020.138762</ArticleId><ArticleId IdType="pmc">PMC7169890</ArticleId><ArticleId IdType="pubmed">32334157</ArticleId></ArticleIdList></Reference><Reference><Citation>Ahuja S., Panigrahi B.K., Dey N., Rajinikanth V., Gandhi T.K. Deep transfer learning-based automated detection of COVID-19 from lung CT scan slices. Appl. Intell. 2020;51:571&#x2013;585. doi: 10.1007/s10489-020-01826-w.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10489-020-01826-w</ArticleId><ArticleId IdType="pmc">PMC7440966</ArticleId><ArticleId IdType="pubmed">34764547</ArticleId></ArticleIdList></Reference><Reference><Citation>Srivatsan S., Han P.D., van Raay K., Wolf C.R., McCulloch D.J., Kim A.E., Brandstetter E., Martin B., Gehring J., Chen W., et al. Preliminary support for a &#x201c;dry swab, extraction free&#x201d; protocol for SARS-CoV-2 testing via RT-qPCR. BioRxiv. 2020 doi: 10.1101/2020.04.22.056283.</Citation><ArticleIdList><ArticleId IdType="doi">10.1101/2020.04.22.056283</ArticleId></ArticleIdList></Reference><Reference><Citation>Jangam E., Barreto A.A.D., Annavarapu C.S.R. Automatic detection of COVID-19 from chest CT scan and chest X-Rays images using deep learning, transfer learning and stacking. Appl. Intell. 2021;52:2243&#x2013;2259. doi: 10.1007/s10489-021-02393-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10489-021-02393-4</ArticleId><ArticleId IdType="pmc">PMC8180385</ArticleId><ArticleId IdType="pubmed">34764605</ArticleId></ArticleIdList></Reference><Reference><Citation>Benmalek E., Elmhamdi J., Jilbab A. Comparing CT scan and chest X-ray imaging for COVID-19 diagnosis. Biomed. Eng. Adv. 2021;1:100003. doi: 10.1016/j.bea.2021.100003.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bea.2021.100003</ArticleId><ArticleId IdType="pmc">PMC7992299</ArticleId><ArticleId IdType="pubmed">34786568</ArticleId></ArticleIdList></Reference><Reference><Citation>Kanne J.P., Little B.P., Chung J.H., Elicker B.M., Ketai L.H. Essentials for Radiologists on COVID-19: An Update&#x2014;Radiology Scientific Expert Panel. Radiology. 2020;296:200527. doi: 10.1148/radiol.2020200527.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2020200527</ArticleId><ArticleId IdType="pmc">PMC7233379</ArticleId><ArticleId IdType="pubmed">32105562</ArticleId></ArticleIdList></Reference><Reference><Citation>Bai H.X., Hsieh B., Xiong Z., Halsey K., Choi J.W., Tran T.M.L., Pan I., Shi L.-B., Wang D.-C., Mei J., et al. Performance of Radiologists in Differentiating COVID-19 from Non-COVID-19 Viral Pneumonia at Chest CT. Radiology. 2020;296:200823. doi: 10.1148/radiol.2020200823.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2020200823</ArticleId><ArticleId IdType="pmc">PMC7233414</ArticleId><ArticleId IdType="pubmed">32155105</ArticleId></ArticleIdList></Reference><Reference><Citation>Long C., Xu H., Shen Q., Zhang X., Fan B., Wang C., Zeng B., Li Z., Li X., Li H. Diagnosis of the Coronavirus disease (COVID-19): rRT-PCR or CT? Eur. J. Radiol. 2020;126:108961. doi: 10.1016/j.ejrad.2020.108961.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejrad.2020.108961</ArticleId><ArticleId IdType="pmc">PMC7102545</ArticleId><ArticleId IdType="pubmed">32229322</ArticleId></ArticleIdList></Reference><Reference><Citation>Lawton S., Viriri S. Detection of COVID-19 from CT Lung Scans Using Transfer Learning. Comput. Intell. Neurosci. 2021;2021:1&#x2013;14. doi: 10.1155/2021/5527923.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2021/5527923</ArticleId><ArticleId IdType="pmc">PMC8042993</ArticleId><ArticleId IdType="pubmed">33936188</ArticleId></ArticleIdList></Reference><Reference><Citation>Ardakani A.A., Kanafi A.R., Acharya U.R., Khadem N., Mohammadi A. Application of deep learning technique to manage COVID-19 in routine clinical practice using CT images: Results of 10 convolutional neural networks. Comput. Biol. Med. 2020;121:103795. doi: 10.1016/j.compbiomed.2020.103795.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2020.103795</ArticleId><ArticleId IdType="pmc">PMC7190523</ArticleId><ArticleId IdType="pubmed">32568676</ArticleId></ArticleIdList></Reference><Reference><Citation>Bharati S., Podder P., Mondal M.R., Podder P., Kose U. A review on epidemiology, genomic characteristics, spread, and treatments of COVID-19. Data Sci. COVID-19. 2022;2:487&#x2013;505.</Citation></Reference><Reference><Citation>Kim M., Lee B.-D. Automatic Lung Segmentation on Chest X-rays Using Self-Attention Deep Neural Network. Sensors. 2021;21:369. doi: 10.3390/s21020369.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s21020369</ArticleId><ArticleId IdType="pmc">PMC7826788</ArticleId><ArticleId IdType="pubmed">33430480</ArticleId></ArticleIdList></Reference><Reference><Citation>Souza J.C., Diniz J.O.B., Ferreira J.L., da Silva G.L.F., Silva A.C., de Paiva A.C. An automatic method for lung segmentation and reconstruction in chest X-ray using deep neural networks. Comput. Methods Programs Biomed. 2019;177:285&#x2013;296. doi: 10.1016/j.cmpb.2019.06.005.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cmpb.2019.06.005</ArticleId><ArticleId IdType="pubmed">31319957</ArticleId></ArticleIdList></Reference><Reference><Citation>Dong D., Tang Z., Wang S., Hui H., Gong L., Lu Y., Xue Z., Liao H., Chen F., Yang F., et al. The Role of Imaging in the Detection and Management of COVID-19: A Review. IEEE Rev. Biomed. Eng. 2020;14:16&#x2013;29. doi: 10.1109/RBME.2020.2990959.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/RBME.2020.2990959</ArticleId><ArticleId IdType="pubmed">32356760</ArticleId></ArticleIdList></Reference><Reference><Citation>RSNA Pneumonia Detection Challenge.  [(accessed on 16 June 2020)].  Available online:  https://www.kaggle.com/c/rsna-pneumoniadetection-challenge/data.</Citation></Reference><Reference><Citation>Mishra S. Deep Transfer Learning-Based Framework for COVID-19 Diagnosis Using Chest CT Scans and Clinical Information. SN Comput. Sci. 2021;2:1&#x2013;11. doi: 10.1007/s42979-021-00785-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s42979-021-00785-4</ArticleId><ArticleId IdType="pmc">PMC8308084</ArticleId><ArticleId IdType="pubmed">34337433</ArticleId></ArticleIdList></Reference><Reference><Citation>Mukherjee H., Ghosh S., Dhar A., Obaidullah S., Santosh K.C., Roy K. Deep neural network to detect COVID-19: One architecture for both CT Scans and Chest X-rays. Appl. Intell. 2020;51:2777&#x2013;2789. doi: 10.1007/s10489-020-01943-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10489-020-01943-6</ArticleId><ArticleId IdType="pmc">PMC7646727</ArticleId><ArticleId IdType="pubmed">34764562</ArticleId></ArticleIdList></Reference><Reference><Citation>Arora V., Ng E.Y.-K., Leekha R.S., Darshan M., Singh A. Transfer learning-based approach for detecting COVID-19 ailment in lung CT scan. Comput. Biol. Med. 2021;135:104575. doi: 10.1016/j.compbiomed.2021.104575.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2021.104575</ArticleId><ArticleId IdType="pmc">PMC8196483</ArticleId><ArticleId IdType="pubmed">34153789</ArticleId></ArticleIdList></Reference><Reference><Citation>Bharati S., Podder P., Mondal M.R.H., Prasath V.S. CO-ResNet: Optimized ResNet model for COVID-19 diagnosis from X-ray images. Int. J. Hybrid Intell. Syst. 2021;17:71&#x2013;85. doi: 10.3233/HIS-210008.</Citation><ArticleIdList><ArticleId IdType="doi">10.3233/HIS-210008</ArticleId></ArticleIdList></Reference><Reference><Citation>Bharati S., Podder P., Mondal M., Gandhi N. Optimized NASNet for diagnosis of COVID-19 from lung CT images; Proceedings of the 20th International Conference on Intelligent Systems Design and Applications (ISDA 2020); online. 12&#x2013;15 December 2020.</Citation></Reference><Reference><Citation>Soares E., Angelov P., Biaso S., Froes M.H., Abe D.K. SARS-CoV-2 CT-scan Dataset: A Large Dataset of Real Patients CT Scans for SARS-CoV-2 Identification. medRxiv. 2020 doi: 10.1101/2020.04.24.20078584.</Citation><ArticleIdList><ArticleId IdType="doi">10.1101/2020.04.24.20078584</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhao J., Zhang Y., He X., Xie P. COVID-ct-dataset: A ct scan dataset about COVID-19. arXiv. 20202003.13865</Citation></Reference><Reference><Citation>Zhao J., Zhang Y., He X., Xie P.  [(accessed on 2 May 2020)].  Available online:  https://github.com/UCSD-AI4H/COVID-CT.</Citation></Reference><Reference><Citation>Bharati S., Podder P., Mondal M.R.H., Prasath V.B.S. Medical imaging with deep learning for COVID-19 diagnosis: A comprehensive review. arXiv. 20212107.09602</Citation></Reference><Reference><Citation>Mondal M.R.H., Bharati S., Podder P. Diagnosis of COVID-19 Using Machine Learning and Deep Learning: A Review. Curr. Med. Imaging. 2021;17:1403&#x2013;1418. doi: 10.2174/1573405617666210713113439.</Citation><ArticleIdList><ArticleId IdType="doi">10.2174/1573405617666210713113439</ArticleId><ArticleId IdType="pubmed">34259149</ArticleId></ArticleIdList></Reference><Reference><Citation>Cohen J.P., Morrison P., Dao L., Roth K., Duong T.Q., Ghassemi M. COVID-19 image data collection: Prospective predictions are the future. arXiv. 20202006.11988</Citation></Reference><Reference><Citation>Rahimzadeh M., Attar A., Sakhaei S.M. A fully automated deep learning-based network for detecting COVID-19 from a new and large lung CT scan dataset. Biomed. Signal Process. Control. 2021;68:102588. doi: 10.1016/j.bspc.2021.102588.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bspc.2021.102588</ArticleId><ArticleId IdType="pmc">PMC8011666</ArticleId><ArticleId IdType="pubmed">33821166</ArticleId></ArticleIdList></Reference><Reference><Citation>Chowdhury M.E.H., Rahman T., Khandakar A., Mazhar R., Kadir M.A., Mahbub Z.B., Islam K.R., Khan M.S., Iqbal A., Emadi N.A., et al. Can ai help in screening viral and COVID-19 pneumonia? IEEE Access. 2020;8:132665&#x2013;132676. doi: 10.1109/ACCESS.2020.3010287.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2020.3010287</ArticleId></ArticleIdList></Reference><Reference><Citation>COVID Chest XRay. Online. 2020.  [(accessed on 1 October 2022)].  Available online:  https://github.com/ieee8023/covid-chestxraydataset.</Citation></Reference><Reference><Citation>Chest XRay (Pneumonia). Online. 2020.  [(accessed on 4 November 2020)].  Available online:  https://www.kaggle.com/paultimothymooney/chest-xraypneumonia.</Citation></Reference><Reference><Citation>Dataset SARS-COV-2 CT.  [(accessed on 4 November 2020)].  Available online:  https://www.kaggle.com/plameneduardo/sarscov2-ctscan-dataset.</Citation></Reference><Reference><Citation>Kumar P., Kumari S. Detection of coronavirus Disease (COVID-19) based on Deep Features. Preprints. 2020:2020030300. doi: 10.20944/preprints202003.0300.v1.</Citation><ArticleIdList><ArticleId IdType="doi">10.20944/preprints202003.0300.v1</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang X., Peng Y., Lu L., Lu Z., Bagheri M., Summers R.M. ChestX-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases; Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Honolulu, HI, USA. 21&#x2013;26 July 2017; pp. 3462&#x2013;3471.</Citation></Reference><Reference><Citation>Shah V., Keniya R., Shridharani A., Punjabi M., Shah J., Mehendale N. Diagnosis of COVID-19 using CT scan images and deep learning techniques. Emerg. Radiol. 2021;28:497&#x2013;505. doi: 10.1007/s10140-020-01886-y.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10140-020-01886-y</ArticleId><ArticleId IdType="pmc">PMC7848247</ArticleId><ArticleId IdType="pubmed">33523309</ArticleId></ArticleIdList></Reference><Reference><Citation>Zheng C., Deng X., Fu Q., Zhou Q., Feng J., Ma H., Liu W., Wang X. Deep learning-based detection for COVID-19 from chest CT using weak label. medRxiv. 2020 doi: 10.1101/2020.03.12.20027185.</Citation><ArticleIdList><ArticleId IdType="doi">10.1101/2020.03.12.20027185</ArticleId></ArticleIdList></Reference><Reference><Citation>COVID-19 CT Segmentation Dataset.  [(accessed on 2 November 2022)].  Available online:  http://medicalsegmentation.com/covid19/</Citation></Reference><Reference><Citation>Nisar Z. COVID-19. 2020.  [(accessed on 12 October 2020)].  Available online:  https://github.com/zeeshannisar/COVID-19.</Citation></Reference><Reference><Citation>Kingma D.P., Ba J. Adam: A method for stochastic optimization. arXiv. 20141412.6980</Citation></Reference><Reference><Citation>Wang S., Kang B., Ma J., Zeng X., Xiao M., Guo J., Cai M., Yang J., Li Y., Meng X., et al. A deep learning algorithm using CT images to screen for Corona Virus Disease (COVID-19) Eur. Radiol. 2021;31:6096&#x2013;6104. doi: 10.1007/s00330-021-07715-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-021-07715-1</ArticleId><ArticleId IdType="pmc">PMC7904034</ArticleId><ArticleId IdType="pubmed">33629156</ArticleId></ArticleIdList></Reference><Reference><Citation>Mondal M.R.H., Bharati S., Podder P. CO-IRv2: Optimized InceptionResNetV2 for COVID-19 detection from chest CT images. PLoS ONE. 2021;16:e0259179. doi: 10.1371/journal.pone.0259179.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0259179</ArticleId><ArticleId IdType="pmc">PMC8553063</ArticleId><ArticleId IdType="pubmed">34710175</ArticleId></ArticleIdList></Reference><Reference><Citation>Loey M., El-Sappagh S., Mirjalili S. Bayesian-based optimized deep learning model to detect COVID-19 patients using chest X-ray image data. Comput. Biol. Med. 2022;142:105213. doi: 10.1016/j.compbiomed.2022.105213.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2022.105213</ArticleId><ArticleId IdType="pmc">PMC8730711</ArticleId><ArticleId IdType="pubmed">35026573</ArticleId></ArticleIdList></Reference><Reference><Citation>Khan I.U., Aslam N., Anwar T., Alsaif H.S., Chrouf S.M.B., Alzahrani N.A., Alamoudi F.A., Kamaleldin M.M.A., Awary K.B. Using a Deep Learning Model to Explore the Impact of Clinical Data on COVID-19 Diagnosis Using Chest X-ray. Sensors. 2022;22:669. doi: 10.3390/s22020669.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s22020669</ArticleId><ArticleId IdType="pmc">PMC8779361</ArticleId><ArticleId IdType="pubmed">35062629</ArticleId></ArticleIdList></Reference><Reference><Citation>Ahmad M., Sadiq S., Eshmawi A.A., Alluhaidan A.S., Umer M., Ullah S., Nappi M. Industry 4.0 technologies and their applications in fighting COVID-19 pandemic using deep learning techniques. Comput. Biol. Med. 2022;145:105418. doi: 10.1016/j.compbiomed.2022.105418.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2022.105418</ArticleId><ArticleId IdType="pmc">PMC8935962</ArticleId><ArticleId IdType="pubmed">35334315</ArticleId></ArticleIdList></Reference><Reference><Citation>Khan I., Aslam N. A Deep-Learning-Based Framework for Automated Diagnosis of COVID-19 Using X-ray Images. Information. 2020;11:419. doi: 10.3390/info11090419.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/info11090419</ArticleId></ArticleIdList></Reference><Reference><Citation>Bharati S., Podder P., Mondal M.R.H. X-Ray Images Three Levels. Figshare. 2021.  [(accessed on 25 July 2021)].  Available online:  https://figshare.com/articles/dataset/X-ray_images_three_levels/14755965/1.</Citation></Reference><Reference><Citation>Podder P., Bharati S., Mondal M., Khamparia A. Biomedical Data Analysis and Processing Using Explainable (XAI) and Responsive Artificial Intelligence (RAI) Springer; Singapore: 2022. Rethinking the Transfer Learning Architecture for Respiratory Diseases and COVID-19 Diagnosis; pp. 105&#x2013;121.</Citation></Reference><Reference><Citation>Afshar P., Heidarian S., Enshaei N., Naderkhani F., Rafiee M.J., Oikonomou A., Fard F.B., Samimi K., Plataniotis K.N., Mohammadi A. COVID-CT-MD, COVID-19 computed tomography scan dataset applicable in machine learning and deep learning. arXiv. 2020 doi: 10.1038/s41597-021-00900-3.2009.14623</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41597-021-00900-3</ArticleId><ArticleId IdType="pmc">PMC8085195</ArticleId><ArticleId IdType="pubmed">33927208</ArticleId></ArticleIdList></Reference><Reference><Citation>Morozov S.P., Andreychenko A.E., Pavlov N.A., Vladzymyrskyy A.V., Ledikhova N.V., Gombolevskiy V.A., Blokhin I.A., Gelezhe P.B., Gonchar A.V., Chernina V.Y. MosMedData: Chest CT Scans with COVID-19 Related Findings Dataset. arXiv. 20202005.06465</Citation></Reference><Reference><Citation>Jun M., Cheng G., Yixin W., Xingle A., Jiantao G., Ziqi Y., Minqing Z., Xin L., Xueyuan D., Shucheng C., et al. COVID-19 CT Lung and Infection Segmentation Dataset. Zenodo. 2020 doi: 10.5281/zenodo.3757476.</Citation><ArticleIdList><ArticleId IdType="doi">10.5281/zenodo.3757476</ArticleId></ArticleIdList></Reference><Reference><Citation>COVID19-Pneumonia-Normal Chest X-ray Images.  [(accessed on 2 November 2022)].  Available online:  https://data.mendeley.com/datasets/dvntn9yhd2/1.</Citation></Reference><Reference><Citation>Hasan J., Shon D., Im K., Choi H.-K., Yoo D.-S., Kim J.-M. Sleep State Classification Using Power Spectral Density and Residual Neural Network with Multichannel EEG Signals. Appl. Sci. 2020;10:7639. doi: 10.3390/app10217639.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/app10217639</ArticleId></ArticleIdList></Reference><Reference><Citation> [(accessed on 24 December 2022)].  Available online:  https://www.kaggle.com/nabeelsajid917/covid-19-x-ray-10000-images.</Citation></Reference><Reference><Citation> [(accessed on 24 December 2022)].  Available online:  https://www.kaggle.com/tarandee97/covid19-normal-posteroanteriorpa-xrays.</Citation></Reference><Reference><Citation> [(accessed on 24 December 2022)].  Available online:  https://www.kaggle.com/pranavraikokte/covid19-image-dataset.</Citation></Reference><Reference><Citation> [(accessed on 24 December 2022)].  Available online:  https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia.</Citation></Reference><Reference><Citation>Lahsaini I., Daho M.E.H., Chikh M.A. Deep transfer learning based classification model for COVID-19 using chest CT-scans. Pattern Recognit. Lett. 2021;152:122&#x2013;128. doi: 10.1016/j.patrec.2021.08.035.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.patrec.2021.08.035</ArticleId><ArticleId IdType="pmc">PMC8455169</ArticleId><ArticleId IdType="pubmed">34566222</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhao H., Fang Z., Ren J., MacLellan C., Xia Y., Li S., Sun M., Ren K. SC2Net: A novel segmentation-based classification network for detection of COVID-19 in chest X-ray images. IEEE J. Biomed. Health Inform. 2022;26:4032&#x2013;4043. doi: 10.1109/JBHI.2022.3177854.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/JBHI.2022.3177854</ArticleId><ArticleId IdType="pubmed">35613061</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36616983</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>10</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">1424-8220</ISSN><JournalIssue CitedMedium="Internet"><Volume>23</Volume><Issue>1</Issue><PubDate><Year>2022</Year><Month>Dec</Month><Day>29</Day></PubDate></JournalIssue><Title>Sensors (Basel, Switzerland)</Title><ISOAbbreviation>Sensors (Basel)</ISOAbbreviation></Journal><ArticleTitle>Vision-Based Eye Image Classification for Ophthalmic Measurement Systems.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">386</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/s23010386</ELocationID><Abstract><AbstractText>The accuracy and the overall performances of ophthalmic instrumentation, where specific analysis of eye images is involved, can be negatively influenced by invalid or incorrect frames acquired during everyday measurements of unaware or non-collaborative human patients and non-technical operators. Therefore, in this paper, we investigate and compare the adoption of several vision-based classification algorithms belonging to different fields, i.e., Machine Learning, Deep Learning, and Expert Systems, in order to improve the performance of an ophthalmic instrument designed for the <i>Pupillary Light Reflex</i> measurement. To test the implemented solutions, we collected and publicly released <i>PopEYE</i> as one of the first datasets consisting of 15 k eye images belonging to 22 different subjects acquired through the aforementioned specialized ophthalmic device. Finally, we discuss the experimental results in terms of classification accuracy of the eye status, as well as computational load analysis, since the proposed solution is designed to be implemented in embedded boards, which have limited hardware resources in computational power and memory size.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Gibertoni</LastName><ForeName>Giovanni</ForeName><Initials>G</Initials><Identifier Source="ORCID">0000-0003-3312-3616</Identifier><AffiliationInfo><Affiliation>Department of Engineering "Enzo Ferrari", University of Modena and Reggio Emilia, 41125 Modena, Italy.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Borghi</LastName><ForeName>Guido</ForeName><Initials>G</Initials><Identifier Source="ORCID">0000-0003-2441-7524</Identifier><AffiliationInfo><Affiliation>Department of Computer Science and Engineering, University of Bologna, 40126 Bologna, Italy.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Rovati</LastName><ForeName>Luigi</ForeName><Initials>L</Initials><Identifier Source="ORCID">0000-0002-1743-3043</Identifier><AffiliationInfo><Affiliation>Department of Engineering "Enzo Ferrari", University of Modena and Reggio Emilia, 41125 Modena, Italy.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>29</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Sensors (Basel)</MedlineTA><NlmUniqueID>101204366</NlmUniqueID><ISSNLinking>1424-8220</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000465" MajorTopicYN="N">Algorithms</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000069550" MajorTopicYN="N">Machine Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D005123" MajorTopicYN="N">Eye</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">computer vision-based classification</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword><Keyword MajorTopicYN="N">expert systems</Keyword><Keyword MajorTopicYN="N">eye status classification</Keyword><Keyword MajorTopicYN="N">machine learning</Keyword><Keyword MajorTopicYN="N">ophthalmic instrumentation</Keyword><Keyword MajorTopicYN="N">pupillary light reflex</Keyword></KeywordList><CoiStatement>The authors declare no conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>12</Month><Day>7</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>20</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>23</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>1</Hour><Minute>44</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36616983</ArticleId><ArticleId IdType="pmc">PMC9823474</ArticleId><ArticleId IdType="doi">10.3390/s23010386</ArticleId><ArticleId IdType="pii">s23010386</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Dos Santos E.S., Xavier W.B., Rodrigues R.N., da C., Botelho S.S., Werhli A.V. Vision Based Measurement applied to Industrial Instrumentation. IFAC-PapersOnLine. 2017;50:788&#x2013;793. doi: 10.1016/j.ifacol.2017.08.509.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ifacol.2017.08.509</ArticleId></ArticleIdList></Reference><Reference><Citation>Shapiro L.G., Stockman G.C. Computer Vision. Volume 3 Prentice Hall; Hoboken, NJ, USA: 2001.</Citation></Reference><Reference><Citation>Mahesh B. Machine learning algorithms-a review. Int. J. Sci. Res. (IJSR) 2020;9:381&#x2013;386.</Citation></Reference><Reference><Citation>LeCun Y., Bengio Y., Hinton G. Deep learning. Nature. 2015;521:436&#x2013;444. doi: 10.1038/nature14539.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nature14539</ArticleId><ArticleId IdType="pubmed">26017442</ArticleId></ArticleIdList></Reference><Reference><Citation>Buchanan B.G., Smith R.G. Fundamentals of expert systems. Annu. Rev. Comput. Sci. 1988;3:23&#x2013;58. doi: 10.1146/annurev.cs.03.060188.000323.</Citation><ArticleIdList><ArticleId IdType="doi">10.1146/annurev.cs.03.060188.000323</ArticleId></ArticleIdList></Reference><Reference><Citation>Gallagher C.V., Bruton K., Leahy K., O&#x2019;Sullivan D.T. The suitability of machine learning to minimise uncertainty in the measurement and verification of energy savings. Energy Build. 2018;158:647&#x2013;655. doi: 10.1016/j.enbuild.2017.10.041.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.enbuild.2017.10.041</ArticleId></ArticleIdList></Reference><Reference><Citation>Yao T., Qu C., Liu Q., Deng R., Tian Y., Xu J., Jha A., Bao S., Zhao M., Fogo A.B., et al. Compound Figure Separation of Biomedical Images with Side Loss. In: Engelhardt S., Oksuz I., Zhu D., Yuan Y., Mukhopadhyay A., Heller N., Huang S.X., Nguyen H., Sznitman R., Xue Y., editors. Proceedings of the Deep Generative Models, and Data Augmentation, Labelling, and Imperfections, Strasbourg, France, 1 October 2021. Springer International Publishing; Cham, Switzerland: 2021. pp. 173&#x2013;183. Lecture Notes in Computer Science.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-030-88210-5_16</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhao M., Jha A., Liu Q., Millis B.A., Mahadevan-Jansen A., Lu L., Landman B.A., Tyska M.J., Huo Y. Faster Mean-shift: GPU-accelerated clustering for cosine embedding-based cell segmentation and tracking. Med. Image Anal. 2021;71:102048. doi: 10.1016/j.media.2021.102048.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2021.102048</ArticleId><ArticleId IdType="pubmed">33872961</ArticleId></ArticleIdList></Reference><Reference><Citation>Kim S.J., Cho K.J., Oh S. Development of machine learning models for diagnosis of glaucoma. PLoS ONE. 2017;12:e0177726. doi: 10.1371/journal.pone.0177726.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0177726</ArticleId><ArticleId IdType="pmc">PMC5441603</ArticleId><ArticleId IdType="pubmed">28542342</ArticleId></ArticleIdList></Reference><Reference><Citation>Asaoka R., Murata H., Iwase A., Araie M. Detecting Preperimetric Glaucoma with Standard Automated Perimetry Using a Deep Learning Classifier. Ophthalmology. 2016;123:1974&#x2013;1980. doi: 10.1016/j.ophtha.2016.05.029.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ophtha.2016.05.029</ArticleId><ArticleId IdType="pubmed">27395766</ArticleId></ArticleIdList></Reference><Reference><Citation>Li Q., Feng B., Xie L., Liang P., Zhang H., Wang T. A Cross-Modality Learning Approach for Vessel Segmentation in Retinal Images. IEEE Trans. Med. Imaging. 2015;35:109&#x2013;118. doi: 10.1109/TMI.2015.2457891.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2015.2457891</ArticleId><ArticleId IdType="pubmed">26208306</ArticleId></ArticleIdList></Reference><Reference><Citation>Varun Gulshan P. Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal. JAMA. 2016;316:2402&#x2013;2410. doi: 10.1001/jama.2016.17216.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jama.2016.17216</ArticleId><ArticleId IdType="pubmed">27898976</ArticleId></ArticleIdList></Reference><Reference><Citation>Daniel Shu Wei Ting M. Development and Validation of a Deep Learning System for Diabetic Retinopathy and Related Eye Diseases Using. JAMA. 2017;318:2211&#x2013;2223. doi: 10.1001/jama.2017.18152.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jama.2017.18152</ArticleId><ArticleId IdType="pmc">PMC5820739</ArticleId><ArticleId IdType="pubmed">29234807</ArticleId></ArticleIdList></Reference><Reference><Citation>Balyen L., Peto T. Promising Artificial Intelligence-Machine Learning-Deep Learning Algorithms in Ophthalmology. Asia-Pac. J. Ophthalmol. 2019;8:264&#x2013;272. doi: 10.22608/APO.2018479.</Citation><ArticleIdList><ArticleId IdType="doi">10.22608/APO.2018479</ArticleId><ArticleId IdType="pubmed">31149787</ArticleId></ArticleIdList></Reference><Reference><Citation>Schmidt T.A. On slit-lamp microscopy. Doc. Ophthalmol. 1975;39:117&#x2013;153. doi: 10.1007/BF00578760.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/BF00578760</ArticleId><ArticleId IdType="pubmed">1201695</ArticleId></ArticleIdList></Reference><Reference><Citation>Podoleanu A.G. Optical coherence tomography. J. Microsc. 2012;247:209&#x2013;219. doi: 10.1111/j.1365-2818.2012.03619.x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/j.1365-2818.2012.03619.x</ArticleId><ArticleId IdType="pmc">PMC3563006</ArticleId><ArticleId IdType="pubmed">22708800</ArticleId></ArticleIdList></Reference><Reference><Citation>Roberts E., Morgan R., King D., Clerkin L. Funduscopy: A forgotten art? Postgrad. Med. J. 1999;75:282&#x2013;284. doi: 10.1136/pgmj.75.883.282.</Citation><ArticleIdList><ArticleId IdType="doi">10.1136/pgmj.75.883.282</ArticleId><ArticleId IdType="pmc">PMC1741240</ArticleId><ArticleId IdType="pubmed">10533632</ArticleId></ArticleIdList></Reference><Reference><Citation>Sirois S., Brisson J. Pupillometry. Wiley Interdiscip. Rev. Cogn. Sci. 2014;5:679&#x2013;692. doi: 10.1002/wcs.1323.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/wcs.1323</ArticleId><ArticleId IdType="pubmed">26308873</ArticleId></ArticleIdList></Reference><Reference><Citation>Osman E. Laser refractive surgery in glaucoma patients. Saudi J. Ophthalmol. 2011;25:169&#x2013;173. doi: 10.1016/j.sjopt.2010.04.003.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.sjopt.2010.04.003</ArticleId><ArticleId IdType="pmc">PMC3729399</ArticleId><ArticleId IdType="pubmed">23960918</ArticleId></ArticleIdList></Reference><Reference><Citation>Davis G. The evolution of cataract surgery. Mo. Med. 2016;113:58.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6139750</ArticleId><ArticleId IdType="pubmed">27039493</ArticleId></ArticleIdList></Reference><Reference><Citation>Gibertoni G., Pinto V.D., Cattini S., Tramarin F., Geiser M., Rovati L. Ophthalmic Technologies XXXII. Vol. 11941. SPIE; Bellingham, WA, USA: 2022. A simple Maxwellian optical system to investigate the photoreceptors contribution to pupillary light reflex; pp. 52&#x2013;60.</Citation><ArticleIdList><ArticleId IdType="doi">10.1117/12.2608129</ArticleId></ArticleIdList></Reference><Reference><Citation>Kardon R. Pupillary light reflex. Curr. Opin. Ophthalmol. 1995;6:20&#x2013;26. doi: 10.1097/00055735-199512000-00004.</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/00055735-199512000-00004</ArticleId><ArticleId IdType="pubmed">10160414</ArticleId></ArticleIdList></Reference><Reference><Citation>Lee J., Stanley M., Spanias A., Tepedelenlioglu C. Integrating machine learning in embedded sensor systems for Internet-of-Things applications; Proceedings of the 2016 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT); Limassol, Cyprus. 12&#x2013;14 December 2016; pp. 290&#x2013;294.</Citation></Reference><Reference><Citation>Krafka K., Khosla A., Kellnhofer P., Kannan H., Bhandarkar S., Matusik W., Torralba A. Eye tracking for everyone; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Las Vegas, NV, USA. 27 June 2016; pp. 2176&#x2013;2184.</Citation></Reference><Reference><Citation>Kar A., Corcoran P. A review and analysis of eye-gaze estimation systems, algorithms and performance evaluation methods in consumer platforms. IEEE Access. 2017;5:16495&#x2013;16519. doi: 10.1109/ACCESS.2017.2735633.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2017.2735633</ArticleId></ArticleIdList></Reference><Reference><Citation>Chennamma H.R., Yuan X. A Survey on Eye-Gaze Tracking Techniques. arXiv. 20131312.6410</Citation></Reference><Reference><Citation>Fedullo T., Masetti E., Gibertoni G., Tramarin F., Rovati L. On the Use of an Hyperspectral Imaging Vision Based Measurement System and Machine Learning for Iris Pigmentation Grading; Proceedings of the 2022 IEEE International Instrumentation and Measurement Technology Conference (I2MTC); Ottawa, ON, Canada. 16&#x2013;19 May 2022; pp. 1&#x2013;6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/I2MTC48687.2022.9806509</ArticleId></ArticleIdList></Reference><Reference><Citation>Winkler S., Subramanian R. Overview of Eye tracking Datasets; Proceedings of the 2013 Fifth International Workshop on Quality of Multimedia Experience (QoMEX); Klagenfurt am W&#xf6;rthersee, Austria. 3&#x2013;5 July 2013; pp. 212&#x2013;217.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/QoMEX.2013.6603239</ArticleId></ArticleIdList></Reference><Reference><Citation>Fischer T., Chang H.J., Demiris Y. Rt-gene: Real-time eye gaze estimation in natural environments; Proceedings of the European Conference on Computer Vision (ECCV); Munich, Germany. 8&#x2013;14 September 2018; pp. 334&#x2013;352.</Citation></Reference><Reference><Citation>Brousseau B., Rose J., Eizenman M. Hybrid Eye-Tracking on a Smartphone with CNN Feature Extraction and an Infrared 3D Model. Sensors. 2020;20:543. doi: 10.3390/s20020543.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s20020543</ArticleId><ArticleId IdType="pmc">PMC7014547</ArticleId><ArticleId IdType="pubmed">31963823</ArticleId></ArticleIdList></Reference><Reference><Citation>Rahman H., Ahmed M.U., Barua S., Funk P., Begum S. Vision-Based Driver&#x2019;s Cognitive Load Classification Considering Eye Movement Using Machine Learning and Deep Learning. Sensors. 2021;21:8019. doi: 10.3390/s21238019.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s21238019</ArticleId><ArticleId IdType="pmc">PMC8659461</ArticleId><ArticleId IdType="pubmed">34884021</ArticleId></ArticleIdList></Reference><Reference><Citation>Vajs I., Kovi&#x107; V., Papi&#x107; T., Savi&#x107; A.M., Jankovi&#x107; M.M. Spatiotemporal Eye-Tracking Feature Set for Improved Recognition of Dyslexic Reading Patterns in Children. Sensors. 2022;22:4900. doi: 10.3390/s22134900.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s22134900</ArticleId><ArticleId IdType="pmc">PMC9269601</ArticleId><ArticleId IdType="pubmed">35808394</ArticleId></ArticleIdList></Reference><Reference><Citation>Granka L.A., Joachims T., Gay G. Eye-tracking analysis of user behavior in WWW search; Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval; Sheffield, UK. 25&#x2013;29 July 2004; New York, NY, USA: Association for Computing Machinery; 2004. pp. 478&#x2013;479. SIGIR &#x2019;04.</Citation><ArticleIdList><ArticleId IdType="doi">10.1145/1008992.1009079</ArticleId></ArticleIdList></Reference><Reference><Citation>Agarwal M., Sivakumar R. Blink: A Fully Automated Unsupervised Algorithm for Eye-Blink Detection in EEG Signals; Proceedings of the 2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton); Monticello, IL, USA. 24&#x2013;27 September 2019; pp. 1113&#x2013;1121.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ALLERTON.2019.8919795</ArticleId></ArticleIdList></Reference><Reference><Citation>Cech J., Soukupova T. Real-time eye blink detection using facial landmarks.  [(accessed on 7 December 2022)];Cent. Mach. Perception, Dep. Cybern. Fac. Electr. Eng. Czech Tech. Univ. Prague. 2016 :1&#x2013;8. Available online:  https://vision.fe.uni-lj.si/cvww2016/proceedings/papers/05.pdf.</Citation></Reference><Reference><Citation>Cortacero K., Fischer T., Demiris Y. RT-BENE: A dataset and baselines for real-time blink estimation in natural environments; Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops; Seoul, Republic of Korea. 27&#x2013;28 October 2019.</Citation></Reference><Reference><Citation>Fuhl W., Kasneci G., Kasneci E. TEyeD: Over 20 million real-world eye images with Pupil, Eyelid, and Iris 2D and 3D Segmentations, 2D and 3D Landmarks, 3D Eyeball, Gaze Vector, and Eye Movement Types. arXiv. 20212102.02115</Citation></Reference><Reference><Citation>Tonsen M., Zhang X., Sugano Y., Bulling A. Labelled pupils in the wild: A dataset for studying pupil detection in unconstrained environments; Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research &amp; Applications; Charleston, SC, USA. 14&#x2013;17 March 2016; New York, NY, USA: Association for Computing Machinery; 2016. pp. 139&#x2013;142. ETRA &#x2019;16.</Citation><ArticleIdList><ArticleId IdType="doi">10.1145/2857491.2857520</ArticleId></ArticleIdList></Reference><Reference><Citation>Omelina L., Goga J., Pavlovicova J., Oravec M., Jansen B. A survey of iris datasets. Image Vis. Comput. 2021;108:104109. doi: 10.1016/j.imavis.2021.104109.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.imavis.2021.104109</ArticleId></ArticleIdList></Reference><Reference><Citation>Timm F., Barth E. Accurate eye centre localisation by means of gradients. Visapp. 2011;11:125&#x2013;130.</Citation></Reference><Reference><Citation>&#x15a;wirski L., Bulling A., Dodgson N. Robust real-time pupil tracking in highly off-axis images; Proceedings of the Symposium on Eye Tracking Research and Applications; Santa Barbara, CA, USA. 28&#x2013;30 March 2012; New York, NY, USA: Association for Computing Machinery; 2012. pp. 173&#x2013;176. ETRA &#x2019;12.</Citation><ArticleIdList><ArticleId IdType="doi">10.1145/2168556.2168585</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen S., Liu C. Eye detection using discriminatory Haar features and a new efficient SVM. Image Vis. Comput. 2015;33:68&#x2013;77. doi: 10.1016/j.imavis.2014.10.007.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.imavis.2014.10.007</ArticleId></ArticleIdList></Reference><Reference><Citation>Jesorsky O., Kirchberg K.J., Frischholz R.W. Robust Face Detection Using the Hausdorff Distance. In: Bigun J., Smeraldi F., editors. Proceedings of the Audio- and Video-Based Biometric Person Authentication; Halmstad, Sweden. 6&#x2013;8 June 2001; Berlin/Heidelberg, Germany: Springer; 2001. pp. 90&#x2013;95. Lecture Notes in Computer Science.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/3-540-45344-X_14</ArticleId></ArticleIdList></Reference><Reference><Citation>Fuhl W., Santini T., Kasneci G., Kasneci E. PupilNet: Convolutional Neural Networks for Robust Pupil Detection. arXiv. 20161601.04902</Citation></Reference><Reference><Citation>Vera-Olmos F.J., Pardo E., Melero H., Malpica N. DeepEye: Deep convolutional network for pupil detection in real environments. Integr. Comput.-Aided Eng. 2019;26:85&#x2013;95. doi: 10.3233/ICA-180584.</Citation><ArticleIdList><ArticleId IdType="doi">10.3233/ICA-180584</ArticleId></ArticleIdList></Reference><Reference><Citation>Fedullo T., Cassanelli D., Gibertoni G., Tramarin F., Quaranta L., Riva I., Tanga L., Oddone F., Rovati L. Assessment of a Vision-Based Technique for an Automatic Van Herick Measurement System. IEEE Trans. Instrum. Meas. 2022;71:1&#x2013;11. doi: 10.1109/TIM.2022.3196323.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TIM.2022.3196323</ArticleId></ArticleIdList></Reference><Reference><Citation>Khan W., Hussain A., Kuru K., Al-askar H. Pupil Localisation and Eye Centre Estimation Using Machine Learning and Computer Vision. Sensors. 2020;20:3785. doi: 10.3390/s20133785.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s20133785</ArticleId><ArticleId IdType="pmc">PMC7374404</ArticleId><ArticleId IdType="pubmed">32640589</ArticleId></ArticleIdList></Reference><Reference><Citation>Talking Face Video. 2008.  [(accessed on 30 November 2022)].  Available online:  https://personalpages.manchester.ac.uk/staff/timothy.f.cootes/data/talking_face/talking_face.html.</Citation></Reference><Reference><Citation>Villanueva A., Ponz V., Sesma-Sanchez L., Ariz M., Porta S., Cabeza R. Hybrid method based on topography for robust detection of iris center and eye corners. ACM Trans. Multimed. Comput. Commun. Appl. 2013;9:1&#x2013;20. doi: 10.1145/2501643.2501647.</Citation><ArticleIdList><ArticleId IdType="doi">10.1145/2501643.2501647</ArticleId></ArticleIdList></Reference><Reference><Citation>Kacete A., Royan J., Seguier R., Collobert M., Soladie C. Real-time eye pupil localization using Hough regression forest; Proceedings of the 2016 IEEE Winter Conference on Applications of Computer Vision (WACV); Lake Placid, NY, USA. 7&#x2013;10 March 2016; pp. 1&#x2013;8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/WACV.2016.7477666</ArticleId></ArticleIdList></Reference><Reference><Citation>Jin B., Cruz L., Gon&#xe7;alves N. Pseudo RGB-D Face Recognition. IEEE Sensors J. 2022;22:21780&#x2013;21794. doi: 10.1109/JSEN.2022.3197235.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/JSEN.2022.3197235</ArticleId></ArticleIdList></Reference><Reference><Citation>Zheng Q., Yang M., Yang J., Zhang Q., Zhang X. Improvement of Generalization Ability of Deep CNN via Implicit Regularization in Two-Stage Training Process. IEEE Access. 2018;6:15844&#x2013;15869. doi: 10.1109/ACCESS.2018.2810849.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2018.2810849</ArticleId></ArticleIdList></Reference><Reference><Citation>Westheimer G. The maxwellian view. Vis. Res. 1966;6:669&#x2013;682. doi: 10.1016/0042-6989(66)90078-2.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/0042-6989(66)90078-2</ArticleId><ArticleId IdType="pubmed">6003389</ArticleId></ArticleIdList></Reference><Reference><Citation>Graffieti G., Borghi G., Maltoni D. Continual Learning in Real-Life Applications. IEEE Robot. Autom. Lett. 2022;7:6195&#x2013;6202. doi: 10.1109/LRA.2022.3167736.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/LRA.2022.3167736</ArticleId></ArticleIdList></Reference><Reference><Citation>Dalal N., Triggs B. Histograms of oriented gradients for human detection; Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&#x2019;05); San Diego, CA, USA. 20&#x2013;25 June 2005; pp. 886&#x2013;893.</Citation></Reference><Reference><Citation>Ojala T., Pietik&#xe4;inen M., Harwood D. A comparative study of texture measures with classification based on featured distributions. Pattern Recognit. 1996;29:51&#x2013;59. doi: 10.1016/0031-3203(95)00067-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/0031-3203(95)00067-4</ArticleId></ArticleIdList></Reference><Reference><Citation>Andriluka M., Roth S., Schiele B. People-tracking-by-detection and people-detection-by-tracking; Proceedings of the 2008 IEEE Conference on Computer Vision and Pattern Recognition; Anchorage, AK, USA. 23&#x2013;28 June 2008; pp. 1&#x2013;8.</Citation></Reference><Reference><Citation>Bolelli F., Borghi G., Grana C. Historical handwritten text images word spotting through sliding window HOG features; Proceedings of the International Conference on Image Analysis and Processing; Catania, Italy. 11&#x2013;15 September 2017; Berlin/Heidelberg, Germany: Springer; 2017. pp. 729&#x2013;738.</Citation></Reference><Reference><Citation>Liao S., Zhu X., Lei Z., Zhang L., Li S.Z. Learning multi-scale block local binary patterns for face recognition; Proceedings of the International Conference on Biometrics; Seoul, Republic of Korea. 27&#x2013;29 August 2007; Berlin/Heidelberg, Germany: Springer; 2007. pp. 828&#x2013;837.</Citation></Reference><Reference><Citation>Hearst M.A., Dumais S.T., Osuna E., Platt J., Scholkopf B. Support vector machines. IEEE Intell. Syst. Their Appl. 1998;13:18&#x2013;28. doi: 10.1109/5254.708428.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/5254.708428</ArticleId></ArticleIdList></Reference><Reference><Citation>Riedmiller M., Lernen A. Multi Layer Perceptron. Machine Learning Lab Special Lecture, University of Freiburg; Freiburg, Germany: 2014. pp. 7&#x2013;24.</Citation></Reference><Reference><Citation>Rigatti S.J. Random forest. J. Insur. Med. 2017;47:31&#x2013;39. doi: 10.17849/insm-47-01-31-39.1.</Citation><ArticleIdList><ArticleId IdType="doi">10.17849/insm-47-01-31-39.1</ArticleId><ArticleId IdType="pubmed">28836909</ArticleId></ArticleIdList></Reference><Reference><Citation>Pal M., Mather P.M. An assessment of the effectiveness of decision tree methods for land cover classification. Remote Sens. Environ. 2003;86:554&#x2013;565. doi: 10.1016/S0034-4257(03)00132-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0034-4257(03)00132-9</ArticleId></ArticleIdList></Reference><Reference><Citation>Raschka S. Naive bayes and text classification i-introduction and theory. arXiv. 20141410.5329</Citation></Reference><Reference><Citation>Peterson L.E. K-nearest neighbor. Scholarpedia. 2009;4:1883. doi: 10.4249/scholarpedia.1883.</Citation><ArticleIdList><ArticleId IdType="doi">10.4249/scholarpedia.1883</ArticleId></ArticleIdList></Reference><Reference><Citation>Sun Y., Liu Z., Todorovic S., Li J. Adaptive boosting for SAR automatic target recognition. IEEE Trans. Aerosp. Electron. Syst. 2007;43:112&#x2013;125. doi: 10.1109/TAES.2007.357120.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TAES.2007.357120</ArticleId></ArticleIdList></Reference><Reference><Citation>Srivastava S., Gupta M.R., Frigyik B.A. Bayesian quadratic discriminant analysis. J. Mach. Learn. Res. 2007;8:1277&#x2013;1305.</Citation></Reference><Reference><Citation>Ding X., Liu J., Yang F., Cao J. Random radial basis function kernel-based support vector machine. J. Frankl. Inst. 2021;358:10121&#x2013;10140. doi: 10.1016/j.jfranklin.2021.10.005.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jfranklin.2021.10.005</ArticleId></ArticleIdList></Reference><Reference><Citation>Agarap A.F. Deep learning using rectified linear units (relu) arXiv. 20181803.08375</Citation></Reference><Reference><Citation>Kingma D.P., Ba J. Adam: A method for stochastic optimization. arXiv. 20141412.6980</Citation></Reference><Reference><Citation>Lerman R.I., Yitzhaki S. A note on the calculation and interpretation of the Gini index. Econ. Lett. 1984;15:363&#x2013;368. doi: 10.1016/0165-1765(84)90126-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/0165-1765(84)90126-5</ArticleId></ArticleIdList></Reference><Reference><Citation>Howard A.G., Zhu M., Chen B., Kalenichenko D., Wang W., Weyand T., Andreetto M., Adam H. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv. 20171704.04861</Citation></Reference><Reference><Citation>Deng J., Dong W., Socher R., Li L.J., Li K., Fei-Fei L. Imagenet: A large-scale hierarchical image database; Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition; Miami, FL, USA. 20&#x2013;25 June 2009; pp. 248&#x2013;255.</Citation></Reference><Reference><Citation>Sandler M., Howard A., Zhu M., Zhmoginov A., Chen L.C. Mobilenetv2: Inverted residuals and linear bottlenecks; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Salt Lake City, UT, USA. 18&#x2013;23 June 2018; pp. 4510&#x2013;4520.</Citation></Reference><Reference><Citation>He K., Zhang X., Ren S., Sun J. Deep residual learning for image recognition; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Las Vegas, NV, USA. 27&#x2013;30 June 2016; pp. 770&#x2013;778.</Citation></Reference><Reference><Citation>Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A. Going deeper with convolutions; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Boston, MA, USA. 7&#x2013;12 June 2015; pp. 1&#x2013;9.</Citation></Reference><Reference><Citation>Tan M., Le Q. Efficientnet: Rethinking model scaling for convolutional neural networks; Proceedings of the International Conference on Machine Learning; Long Beach, CA, USA. 9&#x2013;15 June 2019; pp. 6105&#x2013;6114.</Citation></Reference><Reference><Citation>Szegedy C., Vanhoucke V., Ioffe S., Shlens J., Wojna Z. Rethinking the inception architecture for computer vision; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Las Vegas, NV, USA. 27&#x2013;30 June 2016; pp. 2818&#x2013;2826.</Citation></Reference><Reference><Citation>Abadi M., Agarwal A., Barham P., Brevdo E., Chen Z., Citro C., Corrado G.S., Davis A., Dean J., Devin M., et al. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. arXiv. 20161603.04467</Citation></Reference><Reference><Citation> [(accessed on 30 November 2022)].  Available online:  https://github.com/keras-team/keras.</Citation></Reference><Reference><Citation>Couret D., Boumaza D., Grisotto C., Triglia T., Pellegrini L., Ocquidant P., Bruder N.J., Velly L.J. Reliability of standard pupillometry practice in neurocritical care: An observational, double-blinded study. Crit. Care. 2016;20:1&#x2013;9. doi: 10.1186/s13054-016-1239-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s13054-016-1239-z</ArticleId><ArticleId IdType="pmc">PMC4828754</ArticleId><ArticleId IdType="pubmed">27072310</ArticleId></ArticleIdList></Reference><Reference><Citation>Christina L., Master M.D. Utility of Pupillary Light Reflex Metrics as a Physiologic Biomarker for Adolescent Sport-Related Concussion. JAMA Ophthalmol. 2020;138:1135&#x2013;1141. doi: 10.1001/jamaophthalmol.2020.3466.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jamaophthalmol.2020.3466</ArticleId><ArticleId IdType="pmc">PMC7516812</ArticleId><ArticleId IdType="pubmed">32970102</ArticleId></ArticleIdList></Reference><Reference><Citation>Antoniou E., Bozios P., Christou V., Tzimourta K.D., Kalafatakis K., Tsipouras M.G., Giannakeas N., Tzallas A.T. EEG-Based Eye Movement Recognition Using Brain&#x2014;Computer Interface and Random Forests. Sensors. 2021;21:2339. doi: 10.3390/s21072339.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s21072339</ArticleId><ArticleId IdType="pmc">PMC8036672</ArticleId><ArticleId IdType="pubmed">33801663</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36616931</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>10</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">1424-8220</ISSN><JournalIssue CitedMedium="Internet"><Volume>23</Volume><Issue>1</Issue><PubDate><Year>2022</Year><Month>Dec</Month><Day>28</Day></PubDate></JournalIssue><Title>Sensors (Basel, Switzerland)</Title><ISOAbbreviation>Sensors (Basel)</ISOAbbreviation></Journal><ArticleTitle>Honeycomb Artifact Removal Using Convolutional Neural Network for Fiber Bundle Imaging.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">333</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/s23010333</ELocationID><Abstract><AbstractText>We present a new deep learning framework for removing honeycomb artifacts yielded by optical path blocking of cladding layers in fiber bundle imaging. The proposed framework, HAR-CNN, provides an end-to-end mapping from a raw fiber bundle image to an artifact-free image via a convolution neural network (CNN). The synthesis of honeycomb patterns on ordinary images allows conveniently learning and validating the network without the enormous ground truth collection by extra hardware setups. As a result, HAR-CNN shows significant performance improvement in honeycomb pattern removal and also detailed preservation for the 1961 USAF chart sample, compared with other conventional methods. Finally, HAR-CNN is GPU-accelerated for real-time processing and enhanced image mosaicking performance.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Kim</LastName><ForeName>Eunchan</ForeName><Initials>E</Initials><AffiliationInfo><Affiliation>Center for Intelligent and Interactive Robotics, Korea Institute of Science and Technology, Seoul 02792, Republic of Korea.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Mechanical Convergence Engineering, Hanyang University, Seoul 04763, Republic of Korea.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kim</LastName><ForeName>Seonghoon</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Department of Biological Sciences, Seoul National University, Seoul 03080, Republic of Korea.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Choi</LastName><ForeName>Myunghwan</ForeName><Initials>M</Initials><Identifier Source="ORCID">0000-0002-4235-7003</Identifier><AffiliationInfo><Affiliation>Department of Biological Sciences, Seoul National University, Seoul 03080, Republic of Korea.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Seo</LastName><ForeName>Taewon</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>Department of Mechanical Convergence Engineering, Hanyang University, Seoul 04763, Republic of Korea.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yang</LastName><ForeName>Sungwook</ForeName><Initials>S</Initials><Identifier Source="ORCID">0000-0002-9394-5358</Identifier><AffiliationInfo><Affiliation>Center for Intelligent and Interactive Robotics, Korea Institute of Science and Technology, Seoul 02792, Republic of Korea.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>28</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Sensors (Basel)</MedlineTA><NlmUniqueID>101204366</NlmUniqueID><ISSNLinking>1424-8220</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D007091" MajorTopicYN="N">Image Processing, Computer-Assisted</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D016571" MajorTopicYN="N">Neural Networks, Computer</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D003952" MajorTopicYN="N">Diagnostic Imaging</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D016477" MajorTopicYN="N">Artifacts</DescriptorName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">convolution neural network (CNN)</Keyword><Keyword MajorTopicYN="N">fiber bundle imaging</Keyword><Keyword MajorTopicYN="N">honeycomb artifact</Keyword><Keyword MajorTopicYN="N">pattern synthesis</Keyword></KeywordList><CoiStatement>The authors declare no conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>11</Month><Day>29</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>14</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>24</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>1</Hour><Minute>44</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36616931</ArticleId><ArticleId IdType="pmc">PMC9824069</ArticleId><ArticleId IdType="doi">10.3390/s23010333</ArticleId><ArticleId IdType="pii">s23010333</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Orth A., Ploschner M., Wilson E., Maksymov I., Gibson B. Optical fiber bundles: Ultra-slim light field imaging probes. Sci. Adv. 2019;5:eaav1555. doi: 10.1126/sciadv.aav1555.</Citation><ArticleIdList><ArticleId IdType="doi">10.1126/sciadv.aav1555</ArticleId><ArticleId IdType="pmc">PMC6486219</ArticleId><ArticleId IdType="pubmed">31032405</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang Z., Bovik A.C., Sheikh H.R., Simoncelli E.P. Image quality assessment: From error visibility to structural similarity. IEEE Trans. Image Process. 2004;13:600&#x2013;612. doi: 10.1109/TIP.2003.819861.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TIP.2003.819861</ArticleId><ArticleId IdType="pubmed">15376593</ArticleId></ArticleIdList></Reference><Reference><Citation>Yserbyt J., Dooms C., Janssens W., Verleden G. Endoscopic advanced imaging of the respiratory tract: Exploring probe-based confocal laser endomicroscopy in emphysema. Thorax. 2018;73:188&#x2013;190. doi: 10.1136/thoraxjnl-2016-209746.</Citation><ArticleIdList><ArticleId IdType="doi">10.1136/thoraxjnl-2016-209746</ArticleId><ArticleId IdType="pubmed">28411249</ArticleId></ArticleIdList></Reference><Reference><Citation>Wu J., Wang T., Uckermann O., Galli R., Schackert G., Cao L., Czarske J., Kuschmierz R. Learned end-to-end high-resolution lensless fiber imaging towards real-time cancer diagnosis. Sci. Rep. 2022;12:18846. doi: 10.1038/s41598-022-23490-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-022-23490-5</ArticleId><ArticleId IdType="pmc">PMC9640670</ArticleId><ArticleId IdType="pubmed">36344626</ArticleId></ArticleIdList></Reference><Reference><Citation>Han J.H., Lee J., Kang J.U. Pixelation effect removal from fiber bundle probe based optical coherence tomography imaging. Opt. Express. 2010;18:7427&#x2013;7439. doi: 10.1364/OE.18.007427.</Citation><ArticleIdList><ArticleId IdType="doi">10.1364/OE.18.007427</ArticleId><ArticleId IdType="pmc">PMC3359145</ArticleId><ArticleId IdType="pubmed">20389766</ArticleId></ArticleIdList></Reference><Reference><Citation>Winter C., Rupp S., Elter M., Munzenmayer C., Gerhauser H., Wittenberg T. Automatic adaptive enhancement for images obtained with fiberscopic endoscopes. IEEE Trans. Biomed. Eng. 2006;53:2035&#x2013;2046. doi: 10.1109/TBME.2006.877110.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TBME.2006.877110</ArticleId><ArticleId IdType="pubmed">17019868</ArticleId></ArticleIdList></Reference><Reference><Citation>Dumripatanachod M., Piyawattanametha W. A fast depixelation method of fiber bundle image for an embedded system; Proceedings of the 2015 8th Biomedical Engineering International Conference (BMEiCON); Pattaya, Thailand. 25&#x2013;27 November 2015; pp. 1&#x2013;4.</Citation></Reference><Reference><Citation>Regeling B., Thies B., Gerstner A.O., Westermann S., M&#xfc;ller N.A., Bendix J., Laffers W. Hyperspectral imaging using flexible endoscopy for laryngeal cancer detection. Sensors. 2016;16:1288. doi: 10.3390/s16081288.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s16081288</ArticleId><ArticleId IdType="pmc">PMC5017453</ArticleId><ArticleId IdType="pubmed">27529255</ArticleId></ArticleIdList></Reference><Reference><Citation>Perperidis A., Dhaliwal K., McLaughlin S., Vercauteren T. Image computing for fibre-bundle endomicroscopy: A review. Med. Image Anal. 2020;62:101620. doi: 10.1016/j.media.2019.101620.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2019.101620</ArticleId><ArticleId IdType="pmc">PMC7611433</ArticleId><ArticleId IdType="pubmed">32279053</ArticleId></ArticleIdList></Reference><Reference><Citation>Elter M., Rupp S., Winter C. Physically motivated reconstruction of fiberscopic images; Proceedings of the 18th International Conference on Pattern Recognition (ICPR&#x2019;06); Hong Kong, China. 20&#x2013;24 August 2006; pp. 599&#x2013;602.</Citation></Reference><Reference><Citation>Wang P., Turcatel G., Arnesano C., Warburton D., Fraser S.E., Cutrale F. Fiber pattern removal and image reconstruction method for snapshot mosaic hyperspectral endoscopic images. Biomed. Opt. Express. 2018;9:780&#x2013;790. doi: 10.1364/BOE.9.000780.</Citation><ArticleIdList><ArticleId IdType="doi">10.1364/BOE.9.000780</ArticleId><ArticleId IdType="pmc">PMC5854078</ArticleId><ArticleId IdType="pubmed">29552412</ArticleId></ArticleIdList></Reference><Reference><Citation>Zheng Z., Cai B., Kou J., Liu W., Wang Z. A Honeycomb Artifacts Removal and Super Resolution Method for Fiber-Optic Images; Proceedings of the International Conference on Intelligent Autonomous Systems; Shanghai, China. 3&#x2013;7 July 2016; pp. 771&#x2013;779.</Citation></Reference><Reference><Citation>Lee C.Y., Han J.H. Elimination of honeycomb patterns in fiber bundle imaging by a superimposition method. Opt. Lett. 2013;38:2023&#x2013;2025. doi: 10.1364/OL.38.002023.</Citation><ArticleIdList><ArticleId IdType="doi">10.1364/OL.38.002023</ArticleId><ArticleId IdType="pubmed">23938964</ArticleId></ArticleIdList></Reference><Reference><Citation>Cheon G.W., Cha J., Kang J.U. Random transverse motion-induced spatial compounding for fiber bundle imaging. Opt. Lett. 2014;39:4368&#x2013;4371. doi: 10.1364/OL.39.004368.</Citation><ArticleIdList><ArticleId IdType="doi">10.1364/OL.39.004368</ArticleId><ArticleId IdType="pubmed">25078179</ArticleId></ArticleIdList></Reference><Reference><Citation>Renteria C., Su&#xe1;rez J., Licudine A., Boppart S.A. Depixelation and enhancement of fiber bundle images by bundle rotation. Appl. Opt. 2020;59:536&#x2013;544. doi: 10.1364/AO.59.000536.</Citation><ArticleIdList><ArticleId IdType="doi">10.1364/AO.59.000536</ArticleId><ArticleId IdType="pmc">PMC7286003</ArticleId><ArticleId IdType="pubmed">32225338</ArticleId></ArticleIdList></Reference><Reference><Citation>Jiang J., Zhou X., Liu J., Pan L., Pan Z., Zou F., Li Z., Li F., Ma X., Geng C., et al. Optical Fiber Bundle-Based High-Speed and Precise Micro-Scanning for Image High-Resolution Reconstruction. Sensors. 2021;22:127. doi: 10.3390/s22010127.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s22010127</ArticleId><ArticleId IdType="pmc">PMC8747347</ArticleId><ArticleId IdType="pubmed">35009670</ArticleId></ArticleIdList></Reference><Reference><Citation>Shao J., Zhang J., Huang X., Liang R., Barnard K. Fiber bundle image restoration using deep learning. Opt. Lett. 2019;44:1080&#x2013;1083. doi: 10.1364/OL.44.001080.</Citation><ArticleIdList><ArticleId IdType="doi">10.1364/OL.44.001080</ArticleId><ArticleId IdType="pubmed">30821775</ArticleId></ArticleIdList></Reference><Reference><Citation>Shao J., Zhang J., Liang R., Barnard K. Fiber bundle imaging resolution enhancement using deep learning. Opt. Express. 2019;27:15880&#x2013;15890. doi: 10.1364/OE.27.015880.</Citation><ArticleIdList><ArticleId IdType="doi">10.1364/OE.27.015880</ArticleId><ArticleId IdType="pmc">PMC6825616</ArticleId><ArticleId IdType="pubmed">31163777</ArticleId></ArticleIdList></Reference><Reference><Citation>Russakovsky O., Deng J., Su H., Krause J., Satheesh S., Ma S., Huang Z., Karpathy A., Khosla A., Bernstein M., et al. Imagenet large scale visual recognition challenge. Int. J. Comput. Vis. 2015;115:211&#x2013;252. doi: 10.1007/s11263-015-0816-y.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11263-015-0816-y</ArticleId></ArticleIdList></Reference><Reference><Citation>Dong C., Loy C.C., He K., Tang X. Image super-resolution using deep convolutional networks. IEEE Trans. Pattern Anal. Mach. Intell. 2015;38:295&#x2013;307. doi: 10.1109/TPAMI.2015.2439281.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TPAMI.2015.2439281</ArticleId><ArticleId IdType="pubmed">26761735</ArticleId></ArticleIdList></Reference><Reference><Citation>Lim B., Son S., Kim H., Nah S., Mu Lee K. Enhanced deep residual networks for single image super-resolution; Proceedings of the Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops; Honolulu, HI, USA. 21&#x2013;26 July 2017; pp. 136&#x2013;144.</Citation></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36616820</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>10</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">1424-8220</ISSN><JournalIssue CitedMedium="Internet"><Volume>23</Volume><Issue>1</Issue><PubDate><Year>2022</Year><Month>Dec</Month><Day>26</Day></PubDate></JournalIssue><Title>Sensors (Basel, Switzerland)</Title><ISOAbbreviation>Sensors (Basel)</ISOAbbreviation></Journal><ArticleTitle>Semantic Segmentation of Terrestrial Laser Scans of Railway Catenary Arches: A Use Case Perspective.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">222</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/s23010222</ELocationID><Abstract><AbstractText>Having access to accurate and recent <i>digital twins</i> of infrastructure assets benefits the renovation, maintenance, condition monitoring, and construction planning of infrastructural projects. There are many cases where such a digital twin does not yet exist, such as for legacy structures. In order to create such a digital twin, a mobile laser scanner can be used to capture the geometric representation of the structure. With the aid of <i>semantic segmentation</i>, the scene can be decomposed into different object classes. This decomposition can then be used to retrieve cad models from a cad library to create an accurate digital twin. This study explores three deep-learning-based models for semantic segmentation of point clouds in a practical real-world setting: PointNet++, SuperPoint Graph, and Point Transformer. This study focuses on the use case of catenary arches of the Dutch railway system in collaboration with Strukton Rail, a major contractor for rail projects. A challenging, varied, high-resolution, and annotated dataset for evaluating point cloud segmentation models in railway settings is presented. The dataset contains 14 individually labelled classes and is the first of its kind to be made publicly available. A modified PointNet++ model achieved the best mean class Intersection over Union (IoU) of 71% for the semantic segmentation task on this new, diverse, and challenging dataset.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Ton</LastName><ForeName>Bram</ForeName><Initials>B</Initials><Identifier Source="ORCID">0000-0002-9525-5633</Identifier><AffiliationInfo><Affiliation>Ambient Intelligence, Saxion University of Applied Sciences, 7513 AB Enschede, The Netherlands.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Ahmed</LastName><ForeName>Faizan</ForeName><Initials>F</Initials><Identifier Source="ORCID">0000-0002-2760-6892</Identifier><AffiliationInfo><Affiliation>Ambient Intelligence, Saxion University of Applied Sciences, 7513 AB Enschede, The Netherlands.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Formal Methods and Tools, University of Twente, 7522 NB Enschede, The Netherlands.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Linssen</LastName><ForeName>Jeroen</ForeName><Initials>J</Initials><Identifier Source="ORCID">0000-0002-2626-1837</Identifier><AffiliationInfo><Affiliation>Ambient Intelligence, Saxion University of Applied Sciences, 7513 AB Enschede, The Netherlands.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>26</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Sensors (Basel)</MedlineTA><NlmUniqueID>101204366</NlmUniqueID><ISSNLinking>1424-8220</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D012660" MajorTopicYN="Y">Semantics</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D011211" MajorTopicYN="Y">Electric Power Supplies</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D007834" MajorTopicYN="N">Lasers</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D011877" MajorTopicYN="N">Radionuclide Imaging</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D011996" MajorTopicYN="N">Records</DescriptorName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">catenary arch</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword><Keyword MajorTopicYN="N">point cloud</Keyword><Keyword MajorTopicYN="N">railway infrastructure</Keyword><Keyword MajorTopicYN="N">semantic segmentation</Keyword><Keyword MajorTopicYN="N">terrestrial laser scanner</Keyword></KeywordList><CoiStatement>The data was collected by Strukton Rail. The authors declare no further conflict of interest. The funders had no role in the design of the study; the analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the results.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>11</Month><Day>18</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>20</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>22</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>1</Hour><Minute>43</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36616820</ArticleId><ArticleId IdType="pmc">PMC9824632</ArticleId><ArticleId IdType="doi">10.3390/s23010222</ArticleId><ArticleId IdType="pii">s23010222</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Uddin W., Hudson W.R., Haas R. Public Infrastructure Asset Management. McGraw-Hill Education; New York, NY, USA: 2013.</Citation></Reference><Reference><Citation>Tang P., Huber D., Akinci B., Lipman R., Lytle A. Automatic reconstruction of as-built building information models from laser-scanned point clouds: A review of related techniques. Autom. Constr. 2010;19:829&#x2013;843. doi: 10.1016/j.autcon.2010.06.007.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.autcon.2010.06.007</ArticleId></ArticleIdList></Reference><Reference><Citation>Baltsavias E.P. A comparison between photogrammetry and laser scanning. ISPRS J. Photogramm. Remote Sens. 1999;54:83&#x2013;94. doi: 10.1016/S0924-2716(99)00014-3.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0924-2716(99)00014-3</ArticleId></ArticleIdList></Reference><Reference><Citation>Kalvoda P., Nosek J., Kuruc M., Volarik T. IOP Conference Series: Earth and Environmental Science. IOP Publishing; Bristol, UK: 2020. Accuracy Evaluation and Comparison of Mobile Laser Scanning and Mobile Photogrammetry Data Accuracy Evaluation and Comparison of Mobile Laser Scanning and Mobile Photogrammetry Data.</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/1755-1315/609/1/012091</ArticleId></ArticleIdList></Reference><Reference><Citation>Qi C.R., Yi L., Su H., Guibas L.J. PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space; Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&#x2019;17; Long Beach, CA, USA. 4&#x2013;9 December 2017; Red Hook, NY, USA: Curran Associates Inc.; 2017. pp. 5105&#x2013;5114.</Citation></Reference><Reference><Citation>Landrieu L., Simonovsky M. Large-Scale Point Cloud Semantic Segmentation with Superpoint Graphs; Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition; Salt Lake City, UT, USA. 18&#x2013;23 June 2018; pp. 4558&#x2013;4567.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/CVPR.2018.00479</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhao H., Jiang L., Jia J., Torr P.H., Koltun V. Point Transformer; Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV); Montreal, QC, Canada. 10&#x2013;17 October 2021; pp. 16259&#x2013;16268.</Citation></Reference><Reference><Citation>Armeni I., Sener O., Zamir A.R., Jiang H., Brilakis I., Fischer M., Savarese S. 3D Semantic Parsing of Large-Scale Indoor Spaces Supplementary Material; Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Las Vegas, NV, USA. 27&#x2013;30 June 2016; pp. 1534&#x2013;1543.</Citation></Reference><Reference><Citation>Wang Q., Kim M.k. Applications of 3D point cloud data in the construction industry: A fifteen-year review from 2004 to 2018. Adv. Eng. Inform. 2019;39:306&#x2013;319. doi: 10.1016/j.aei.2019.02.007.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.aei.2019.02.007</ArticleId></ArticleIdList></Reference><Reference><Citation>Callahan M.A., LeBlanc B., Vreeland R., Bretting G. Close-Range Photogrammetry with Laser Scan Point Clouds. SAE Technical Paper; Warrendale, PA, USA: 2012. Technical Report.</Citation></Reference><Reference><Citation>Valero E., Bosch&#xe9; F., Forster A. Automatic segmentation of 3D point clouds of rubble masonry walls, and its application to building surveying, repair and maintenance. Autom. Constr. 2018;96:29&#x2013;39. doi: 10.1016/j.autcon.2018.08.018.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.autcon.2018.08.018</ArticleId></ArticleIdList></Reference><Reference><Citation>Mahler J., Matl M., Satish V., Danielczuk M., DeRose B., McKinley S., Goldberg K. Learning ambidextrous robot grasping policies. Sci. Robot. 2019;4:eaau4984. doi: 10.1126/scirobotics.aau4984.</Citation><ArticleIdList><ArticleId IdType="doi">10.1126/scirobotics.aau4984</ArticleId><ArticleId IdType="pubmed">33137754</ArticleId></ArticleIdList></Reference><Reference><Citation>Bello S.A., Yu S., Wang C., Adam J.M., Li J. Review: Deep Learning on 3D Point Clouds. Remote Sens. 2020;12:1729. doi: 10.3390/rs12111729.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/rs12111729</ArticleId></ArticleIdList></Reference><Reference><Citation>Burume D.M., Du S. Deep Learning Methods Applied to 3D Point Clouds Based Instance Segmentation: A Review. Preprints. 2021:2021110228. doi: 10.20944/preprints202111.0228.v1.</Citation><ArticleIdList><ArticleId IdType="doi">10.20944/preprints202111.0228.v1</ArticleId></ArticleIdList></Reference><Reference><Citation>Guo Y., Wang H., Hu Q., Liu H., Liu L., Bennamoun M. Deep learning for 3D point clouds: A survey. IEEE Trans. Pattern Anal. Mach. Intell. 2020;43:4338&#x2013;4364. doi: 10.1109/TPAMI.2020.3005434.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TPAMI.2020.3005434</ArticleId><ArticleId IdType="pubmed">32750799</ArticleId></ArticleIdList></Reference><Reference><Citation>Liu W., Sun J., Li W., Hu T., Wang P. Deep Learning on Point Clouds and Its Application: A Survey. Sensors. 2019;19:4188. doi: 10.3390/s19194188.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s19194188</ArticleId><ArticleId IdType="pmc">PMC6806315</ArticleId><ArticleId IdType="pubmed">31561639</ArticleId></ArticleIdList></Reference><Reference><Citation>Liu S., Zhang M., Kadam P., Kuo C.C.J. 3D Point Cloud Analysis. Springer; Berlin/Heidelberg, Germany: 2021. Deep Learning-Based Point Cloud Analysis; pp. 53&#x2013;86.</Citation></Reference><Reference><Citation>Zhang J., Zhao X., Chen Z., Lu Z. A review of deep learning-based semantic segmentation for point cloud. IEEE Access. 2019;7:179118&#x2013;179133. doi: 10.1109/ACCESS.2019.2958671.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2019.2958671</ArticleId></ArticleIdList></Reference><Reference><Citation>Bentley J.L. Multidimensional binary search trees used for associative searching. Commun. ACM. 1975;18:509&#x2013;517. doi: 10.1145/361002.361007.</Citation><ArticleIdList><ArticleId IdType="doi">10.1145/361002.361007</ArticleId></ArticleIdList></Reference><Reference><Citation>Zeng W., Gevers T. 3DContextNet: K-d tree guided hierarchical learning of point clouds using local and global contextual cues; Proceedings of the European Conference on Computer Vision (ECCV) Workshops; Munich, Germany. 8&#x2013;14 September 2018.</Citation></Reference><Reference><Citation>Arastounia M. Automated Recognition of Railroad Infrastructure in Rural Areas from LiDAR Data. Remote Sens. 2015;7:14916&#x2013;14938. doi: 10.3390/rs71114916.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/rs71114916</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen L., Jung J., Sohn G. Multi-Scale HierarchicalCRF for Railway Electrification Asset Classification From Mobile Laser Scanning Data. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2019;12:3131&#x2013;3148. doi: 10.1109/JSTARS.2019.2918272.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/JSTARS.2019.2918272</ArticleId></ArticleIdList></Reference><Reference><Citation>Charles R.Q., Su H., Kaichun M., Guibas L.J. PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation; Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Honolulu, HI, USA. 21&#x2013;26 July 2017; pp. 77&#x2013;85.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/CVPR.2017.16</ArticleId></ArticleIdList></Reference><Reference><Citation>Thomas H., Qi C.R., Deschaud J.E., Marcotegui B., Goulette F., Guibas L. KPConv: Flexible and Deformable Convolution for Point Clouds; Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision (ICCV); Seoul, Republic of Korea. 27 October&#x2013;2 November 2019; pp. 6410&#x2013;6419.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ICCV.2019.00651</ArticleId></ArticleIdList></Reference><Reference><Citation>Soil&#xe1;n M., S&#xe1;nchez-Rodr&#xed;guez A., del R&#xed;o-Barral P., Perez-Collazo C., Arias P., Riveiro B. Review of Laser Scanning Technologies and Their Applications for Road and Railway Infrastructure Monitoring. Infrastructures. 2019;4:58. doi: 10.3390/infrastructures4040058.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/infrastructures4040058</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen L., Xu C., Lin S., Li S., Tu X. A Deep Learning-Based Method for Overhead Contact System Component Recognition Using Mobile 2D LiDAR. Sensors. 2020;20:2224. doi: 10.3390/s20082224.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s20082224</ArticleId><ArticleId IdType="pmc">PMC7218868</ArticleId><ArticleId IdType="pubmed">32326438</ArticleId></ArticleIdList></Reference><Reference><Citation>Lin S., Xu C., Chen L., Li S., Tu X. LiDAR Point Cloud Recognition of Overhead Catenary System with Deep Learning. Sensors. 2020;20:2212. doi: 10.3390/s20082212.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s20082212</ArticleId><ArticleId IdType="pmc">PMC7218916</ArticleId><ArticleId IdType="pubmed">32295187</ArticleId></ArticleIdList></Reference><Reference><Citation>Bruijne A.d., Buren J.V., Marel H.V.D. Geodetic Reference Frames in the Netherlands. NCG, Nederlandse Commissie voor Geodesie, Netherlands Geodetic Commission; Delft, The Netherlands: 2005. pp. 1&#x2013;117.</Citation></Reference><Reference><Citation>Zhu L., Hyyppa J. The Use of Airborne and Mobile Laser Scanning for Modeling Railway Environments in 3D. Remote Sens. 2014;6:3075&#x2013;3100. doi: 10.3390/rs6043075.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/rs6043075</ArticleId></ArticleIdList></Reference><Reference><Citation>Corongiu M., Masiero A., Tucci G. Classification of Railway Assets in Mobile Mapping Point Clouds. Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2020;XLIII-B1-2:219&#x2013;225. doi: 10.5194/isprs-archives-XLIII-B1-2020-219-2020.</Citation><ArticleIdList><ArticleId IdType="doi">10.5194/isprs-archives-XLIII-B1-2020-219-2020</ArticleId></ArticleIdList></Reference><Reference><Citation>LAS Specification Version 1.4-R13. ASPRS; Bethesda, MD, USA: 2013. American Society for Photogrammetry and Remote Sensing. Technical Report.</Citation></Reference><Reference><Citation>Zhan K., Chen S., Whitman D., Shyu M., Yan J., Zhang C. A progressive morphological filter for removing nonground measurements from airborne LiDAR data. IEEE Trans. Geosci. Remote Sens. 2003;41:872&#x2013;882. doi: 10.1109/TGRS.2003.810682.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TGRS.2003.810682</ArticleId></ArticleIdList></Reference><Reference><Citation>Ioffe S., Szegedy C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift; Proceedings of the 32nd International Conference on International Conference on Machine Learning&#x2014;JMLR.org, ICML&#x2019;15; Lille, France. 7&#x2013;9 July 2015; pp. 448&#x2013;456.</Citation><ArticleIdList><ArticleId IdType="doi">10.5555/3045118.3045167</ArticleId></ArticleIdList></Reference><Reference><Citation>Barber C.B., Dobkin D.P., Huhdanpaa H. The quickhull algorithm for convex hulls. ACM Trans. Math. Softw. 1996;22:469&#x2013;483. doi: 10.1145/235815.235821.</Citation><ArticleIdList><ArticleId IdType="doi">10.1145/235815.235821</ArticleId></ArticleIdList></Reference><Reference><Citation>Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A.N., Kaiser L., Polosukhin I. Attention is All You Need; Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&#x2019;17; Long Beach, CA, USA. 4&#x2013;9 December 2017; Red Hook, NY, USA: Curran Associates Inc.; 2017. pp. 6000&#x2013;6010.</Citation></Reference><Reference><Citation>Devlin J., Chang M., Lee K., Toutanova K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In: Burstein J., Doran C., Solorio T., editors. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT; Minneapolis, MN, USA. 2&#x2013;7 June 2019; Minneapolis, MN, USA: Association for Computational Linguistics; 2019. pp. 4171&#x2013;4186.</Citation><ArticleIdList><ArticleId IdType="doi">10.18653/v1/n19-1423</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang X., Girshick R., Gupta A., He K. Non-local Neural Networks; Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); Salt Lake City, UT, USA. 18&#x2013;22 June 2018; Los Alamitos, CA, USA: IEEE Computer Society; 2018. pp. 7794&#x2013;7803.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/CVPR.2018.00813</ArticleId></ArticleIdList></Reference><Reference><Citation>Samek W., M&#xfc;ller K.R. Towards Explainable Artificial Intelligence. In: Samek W., Montavon G., Vedaldi A., Hansen L.K., M&#xfc;ller K.R., editors. Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Springer International Publishing; Cham, Switzerland: 2019. pp. 5&#x2013;22.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-030-28954-6_1</ArticleId></ArticleIdList></Reference><Reference><Citation>Linardatos P., Papastefanopoulos V., Kotsiantis S. Explainable AI: A review of machine learning interpretability methods. Entropy. 2021;23:18. doi: 10.3390/e23010018.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/e23010018</ArticleId><ArticleId IdType="pmc">PMC7824368</ArticleId><ArticleId IdType="pubmed">33375658</ArticleId></ArticleIdList></Reference><Reference><Citation>Pan H., Wang Z., Zhan W., Tomizuka M. Towards Better Performance and More Explainable Uncertainty for 3D Object Detection of Autonomous Vehicles; Proceedings of the 2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC); Rhodes, Greece. 20&#x2013;23 September 2020; pp. 1&#x2013;7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ITSC45102.2020.9294177</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang M., You H., Kadam P., Liu S., Kuo C.C.J. PointHop: An Explainable Machine Learning Method for Point Cloud Classification. IEEE Trans. Multimed. 2020;22:1744&#x2013;1755. doi: 10.1109/TMM.2019.2963592.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMM.2019.2963592</ArticleId></ArticleIdList></Reference><Reference><Citation>Matrone F., Paolanti M., Felicetti A., Martini M., Pierdicca R. BubblEX: An Explainable Deep Learning Framework for Point-Cloud Classification. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2022;15:6571&#x2013;6587. doi: 10.1109/JSTARS.2022.3195200.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/JSTARS.2022.3195200</ArticleId></ArticleIdList></Reference><Reference><Citation>Verburg F.M. Bachelor&#x2019;s Thesis. University of Twente; Enschede, The Netherlands: 2022. Exploring Explainability and Robustness of Point Cloud Segmentation Deep Learning Model by Visualization.</Citation></Reference><Reference><Citation>Burton T., Heuckelbach D. Fugro vegetation control: A remote solution for lineside vegetation management. Perm. Way Inst. 2020;138:34&#x2013;37.</Citation></Reference><Reference><Citation>Guti&#xe9;rrez-Fern&#xe1;ndez A., Fern&#xe1;ndez-Llamas C., Matell&#xe1;n-Olivera V., Su&#xe1;rez-Gonz&#xe1;lez A. Automatic extraction of power cables location in railways using surface lidar systems. Sensors. 2020;20:6222. doi: 10.3390/s20216222.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s20216222</ArticleId><ArticleId IdType="pmc">PMC7663307</ArticleId><ArticleId IdType="pubmed">33142776</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang L., Wang J., Shen Y., Liang J., Chen Y., Chen L., Zhou M. A Deep Learning Based Method for Railway Overhead Wire Reconstruction from Airborne LiDAR Data. Remote Sens. 2022;14:5272. doi: 10.3390/rs14205272.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/rs14205272</ArticleId></ArticleIdList></Reference><Reference><Citation>Marwati A., Wang C.K. Automatic retrieval of railway masts tilt angle from Mobile Laser Scanning data; Proceedings of the 42nd Asian Conference on Remote Sensing, ACRS 2021; Can Tho City, Vietnam. 22&#x2013;24 November 2021; Can Tho, Vietnam: Asian Association on Remote Sensing (AARS); 2021.</Citation></Reference><Reference><Citation>Vock R., Dieckmann A., Ochmann S., Klein R. Fast template matching and pose estimation in 3D point clouds. Comput. Graph. 2019;79:36&#x2013;45. doi: 10.1016/j.cag.2018.12.007.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cag.2018.12.007</ArticleId></ArticleIdList></Reference><Reference><Citation>Vieth Z.J. Bachelor&#x2019;s Thesis. University of Twente; Enschede, The Netherlands: 2022. Point Cloud Classification and Segmentation of Catenary Systems.</Citation></Reference><Reference><Citation>Sayin B., Krivosheev E., Yang J., Passerini A., Casati F. A review and experimental analysis of active learning over crowd sourced data. Artif. Intell. Rev. 2021;54:5283&#x2013;5305. doi: 10.1007/s10462-021-10021-3.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10462-021-10021-3</ArticleId></ArticleIdList></Reference><Reference><Citation>Budd S., Robinson E.C., Kainz B. A survey on active learning and human-in-the-loop deep learning for medical image analysis. Med. Image Anal. 2021;71:102062. doi: 10.1016/j.media.2021.102062.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2021.102062</ArticleId><ArticleId IdType="pubmed">33901992</ArticleId></ArticleIdList></Reference><Reference><Citation>Meng Q., Wang W., Zhou T., Shen J., Jia Y., Van Gool L. Towards a weakly supervised framework for 3D point cloud object detection and annotation. IEEE Trans. Pattern Anal. Mach. Intell. 2021;44:4454&#x2013;4468. doi: 10.1109/TPAMI.2021.3063611.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TPAMI.2021.3063611</ArticleId><ArticleId IdType="pubmed">33656990</ArticleId></ArticleIdList></Reference><Reference><Citation>Strukton R., Ton B. High resolution labelled point cloud dataset of catenary arches in the Netherlands. 4TU.ResearchData. 2021.</Citation><ArticleIdList><ArticleId IdType="doi">10.4121/17048816</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36615186</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Print">2077-0383</ISSN><JournalIssue CitedMedium="Print"><Volume>12</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>03</Day></PubDate></JournalIssue><Title>Journal of clinical medicine</Title><ISOAbbreviation>J Clin Med</ISOAbbreviation></Journal><ArticleTitle>Segmentation-Assisted Fully Convolutional Neural Network Enhances Deep Learning Performance to Identify Proliferative Diabetic Retinopathy.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">385</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/jcm12010385</ELocationID><Abstract><AbstractText>With the progression of diabetic retinopathy (DR) from the non-proliferative (NPDR) to proliferative (PDR) stage, the possibility of vision impairment increases significantly. Therefore, it is clinically important to detect the progression to PDR stage for proper intervention. We propose a segmentation-assisted DR classification methodology, that builds on (and improves) current methods by using a fully convolutional network (FCN) to segment retinal neovascularizations (NV) in retinal images prior to image classification. This study utilizes the Kaggle EyePacs dataset, containing retinal photographs from patients with varying degrees of DR (mild, moderate, severe NPDR and PDR. Two graders annotated the NV (a board-certified ophthalmologist and a trained medical student). Segmentation was performed by training an FCN to locate neovascularization on 669 retinal fundus photographs labeled with PDR status according to NV presence. The trained segmentation model was used to locate probable NV in images from the classification dataset. Finally, a CNN was trained to classify the combined images and probability maps into categories of PDR. The mean accuracy of segmentation-assisted classification was 87.71% on the test set (SD = 7.71%). Segmentation-assisted classification of PDR achieved accuracy that was 7.74% better than classification alone. Our study shows that segmentation assistance improves identification of the most severe stage of diabetic retinopathy and has the potential to improve deep learning performance in other imaging problems with limited data availability.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Alam</LastName><ForeName>Minhaj</ForeName><Initials>M</Initials><Identifier Source="ORCID">0000-0003-3095-2232</Identifier><AffiliationInfo><Affiliation>School of Medicine, Stanford University, Stanford, CA 94305, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Electrical and Computer Engineering, University of North Carolina at Charlotte, Charlotte, NC 28223, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhao</LastName><ForeName>Emma J</ForeName><Initials>EJ</Initials><AffiliationInfo><Affiliation>School of Medicine, Stanford University, Stanford, CA 94305, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lam</LastName><ForeName>Carson K</ForeName><Initials>CK</Initials><AffiliationInfo><Affiliation>School of Medicine, Stanford University, Stanford, CA 94305, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Rubin</LastName><ForeName>Daniel L</ForeName><Initials>DL</Initials><AffiliationInfo><Affiliation>School of Medicine, Stanford University, Stanford, CA 94305, USA.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>03</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>J Clin Med</MedlineTA><NlmUniqueID>101606588</NlmUniqueID><ISSNLinking>2077-0383</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">AI in ophthalmology</Keyword><Keyword MajorTopicYN="N">computer aided diagnosis</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword><Keyword MajorTopicYN="N">diabetic retinopathy</Keyword><Keyword MajorTopicYN="N">segmentation aided classification</Keyword></KeywordList><CoiStatement>The authors declare no conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>10</Month><Day>26</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>27</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>29</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>1</Hour><Minute>32</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36615186</ArticleId><ArticleId IdType="pmc">PMC9821182</ArticleId><ArticleId IdType="doi">10.3390/jcm12010385</ArticleId><ArticleId IdType="pii">jcm12010385</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Early Treatment Diabetic Retinopathy Study Research Group Grading diabetic retinopathy from stereoscopic color fundus photographs&#x2014;An extension of the modified Airlie House classification. ETDRS report number 10. Ophthalmology. 1991;98:786&#x2013;806. doi: 10.1016/S0161-6420(13)38012-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0161-6420(13)38012-9</ArticleId><ArticleId IdType="pubmed">2062513</ArticleId></ArticleIdList></Reference><Reference><Citation>Chudzik P., Majumdar S., Caliv&#xe1; F., Al-Diri B., Hunter A. Microaneurysm detection using fully convolutional neural networks. Comput. Methods Programs Biomed. 2018;158:185&#x2013;192. doi: 10.1016/j.cmpb.2018.02.016.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cmpb.2018.02.016</ArticleId><ArticleId IdType="pubmed">29544784</ArticleId></ArticleIdList></Reference><Reference><Citation>The Diabetic Retinopathy Study Research Group Photocoagulation treatment of proliferative diabetic retinopathy. Clinical application of Diabetic Retinopathy Study (DRS) findings, DRS Report Number 8. Ophthalmology. 1981;88:583&#x2013;600.</Citation><ArticleIdList><ArticleId IdType="pubmed">7196564</ArticleId></ArticleIdList></Reference><Reference><Citation>Writing Committee for the Diabetic Retinopathy Clinical Research Network Panretinal Photocoagulation vs Intravitreous Ranibizumab for Proliferative Diabetic Retinopathy: A Randomized Clinical Trial. JAMA. 2015;314:2137&#x2013;2146. doi: 10.1001/jama.2015.15217.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jama.2015.15217</ArticleId><ArticleId IdType="pmc">PMC5567801</ArticleId><ArticleId IdType="pubmed">26565927</ArticleId></ArticleIdList></Reference><Reference><Citation>Esteva A., Kuprel B., Novoa R.A., Ko J., Swetter S.M., Blau H.M., Thrun S. Corrigendum: Dermatologist-level classification of skin cancer with deep neural networks. Nature. 2017;546:686. doi: 10.1038/nature22985.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nature22985</ArticleId><ArticleId IdType="pubmed">28658222</ArticleId></ArticleIdList></Reference><Reference><Citation>Gulshan V., Peng L., Coram M., Stumpe M.C., Wu D., Narayanaswamy A., Venugopalan S., Widner K., Madams T., Cuadro J., et al. Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs. JAMA. 2016;316:2402&#x2013;2410. doi: 10.1001/jama.2016.17216.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jama.2016.17216</ArticleId><ArticleId IdType="pubmed">27898976</ArticleId></ArticleIdList></Reference><Reference><Citation>Bejnordi B.E., Veta M., Van Diest P.J., Van Ginneken B., Karssemeijer N., Litjens G., Van Der Laak J.A.W.M., Hermsen M., Manson Q.F., Balkenhol M., et al. Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women with Breast Cancer. JAMA. 2017;318:2199&#x2013;2210. doi: 10.1001/jama.2017.14585.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jama.2017.14585</ArticleId><ArticleId IdType="pmc">PMC5820737</ArticleId><ArticleId IdType="pubmed">29234806</ArticleId></ArticleIdList></Reference><Reference><Citation>Nie D., Wang L., Gao Y., Shen D. Fully Convolutional Networks for Multi-Modality Isointense Infant Brain Image Segmentation. Proc. IEEE Int. Symp. Biomed. Imaging. 2016;2016:1342&#x2013;1345.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5031138</ArticleId><ArticleId IdType="pubmed">27668065</ArticleId></ArticleIdList></Reference><Reference><Citation>Li Y., Shen L., Yu S. HEp-2 Specimen Image Segmentation and Classification Using Very Deep Fully Convolutional Network. IEEE Trans. Med. Imaging. 2017;36:1561&#x2013;1572. doi: 10.1109/TMI.2017.2672702.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2017.2672702</ArticleId><ArticleId IdType="pubmed">28237925</ArticleId></ArticleIdList></Reference><Reference><Citation>Li X., Dou Q., Chen H., Fu C.W., Qi X., Belav&#xfd; D.L., Armbrecht G., Felsenberg D., Zheng G., Heng P.A. 3D multi-scale FCN with random modality voxel dropout learning for Intervertebral Disc Localization and Segmentation from Multi-modality MR Images. Med. Image Anal. 2018;45:41&#x2013;54. doi: 10.1016/j.media.2018.01.004.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2018.01.004</ArticleId><ArticleId IdType="pubmed">29414435</ArticleId></ArticleIdList></Reference><Reference><Citation>Wong T.Y., Cheung C.M.G., Larsen M., Sharma S., Sim&#xf3; R. Diabetic retinopathy. Nat. Rev. Dis. Primer. 2016;2:16012. doi: 10.1038/nrdp.2016.12.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nrdp.2016.12</ArticleId><ArticleId IdType="pubmed">27159554</ArticleId></ArticleIdList></Reference><Reference><Citation>Zheng Y., He M., Congdon N. The worldwide epidemic of diabetic retinopathy. Indian J. Ophthalmol. 2012;60:428&#x2013;431.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC3491270</ArticleId><ArticleId IdType="pubmed">22944754</ArticleId></ArticleIdList></Reference><Reference><Citation>Ting D.S.W., Cheung G.C.M., Wong T.Y. Diabetic retinopathy: Global prevalence, major risk factors, screening practices and public health challenges: A review. Clin. Exp. Ophthalmol. 2016;44:260&#x2013;277. doi: 10.1111/ceo.12696.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/ceo.12696</ArticleId><ArticleId IdType="pubmed">26716602</ArticleId></ArticleIdList></Reference><Reference><Citation>Kiani A., Uyumazturk B., Rajpurkar P., Wang A., Gao R., Jones E., Yu Y., Langlotz C.P., Ball R.L., Montine T.J., et al. Impact of a deep learning assistant on the histopathologic classification of liver cancer. NPJ Digit. Med. 2020;3:1&#x2013;8. doi: 10.1038/s41746-020-0232-8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41746-020-0232-8</ArticleId><ArticleId IdType="pmc">PMC7044422</ArticleId><ArticleId IdType="pubmed">32140566</ArticleId></ArticleIdList></Reference><Reference><Citation>LeCun Y., Bengio Y., Hinton G. Deep learning. Nature. 2015;521:436&#x2013;444. doi: 10.1038/nature14539.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nature14539</ArticleId><ArticleId IdType="pubmed">26017442</ArticleId></ArticleIdList></Reference><Reference><Citation>Shelhamer E., Long J., Darrell T. Fully Convolutional Networks for Semantic Segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 2017;39:640&#x2013;651. doi: 10.1109/TPAMI.2016.2572683.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TPAMI.2016.2572683</ArticleId><ArticleId IdType="pubmed">27244717</ArticleId></ArticleIdList></Reference><Reference><Citation>Salongcay R.P., Silva P.S. The Role of Teleophthalmology in the Management of Diabetic Retinopathy. Asia-Pac. J. Ophthalmol. 2018;7:17&#x2013;21.</Citation><ArticleIdList><ArticleId IdType="pubmed">29376232</ArticleId></ArticleIdList></Reference><Reference><Citation>Grisolia A.B.D., Abalem M.F., Lu Y., Aoki L., Matayoshi S. Teleophthalmology: Where are we now? Arq. Bras. Oftalmol. 2017;80:401&#x2013;406. doi: 10.5935/0004-2749.20170099.</Citation><ArticleIdList><ArticleId IdType="doi">10.5935/0004-2749.20170099</ArticleId><ArticleId IdType="pubmed">29267581</ArticleId></ArticleIdList></Reference><Reference><Citation>Ting D.S.W., Cheung C.Y.L., Lim G., Tan G.S.W., Quang N.D., Gan A., Hamzah H., Garcia-Franco R., San Yeo I.Y., Lee S.Y., et al. Development and Validation of a Deep Learning System for Diabetic Retinopathy and Related Eye Diseases Using Retinal Images From Multiethnic Populations with Diabetes. JAMA. 2017;318:2211&#x2013;2223. doi: 10.1001/jama.2017.18152.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jama.2017.18152</ArticleId><ArticleId IdType="pmc">PMC5820739</ArticleId><ArticleId IdType="pubmed">29234807</ArticleId></ArticleIdList></Reference><Reference><Citation>Ardiyanto I., Nugroho H.A., Buana R.L.B. Deep learning-based Diabetic Retinopathy assessment on embedded system; Proceedings of the 2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC); Jeju Island, Republic of Korea. 11&#x2013;15 July 2017; pp. 1760&#x2013;1763.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/EMBC.2017.8037184</ArticleId><ArticleId IdType="pubmed">29060228</ArticleId></ArticleIdList></Reference><Reference><Citation>Weiss K., Khoshgoftaar T.M., Wang D. A survey of transfer learning. J. Big Data. 2016;3:9. doi: 10.1186/s40537-016-0043-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s40537-016-0043-6</ArticleId></ArticleIdList></Reference><Reference><Citation>Samala R.K., Chan H.-P., Hadjiiski L., Helvie M.A., Wei J., Cha K. Mass detection in digital breast tomosynthesis: Deep convolutional neural network with transfer learning from mammography. Med. Phys. 2016;43:6654. doi: 10.1118/1.4967345.</Citation><ArticleIdList><ArticleId IdType="doi">10.1118/1.4967345</ArticleId><ArticleId IdType="pmc">PMC5135717</ArticleId><ArticleId IdType="pubmed">27908154</ArticleId></ArticleIdList></Reference><Reference><Citation>Coutinho E., Schuller B. Shared acoustic codes underlie emotional communication in music and speech-Evidence from deep transfer learning. PLoS ONE. 2017;12:e0179289. doi: 10.1371/journal.pone.0179289.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0179289</ArticleId><ArticleId IdType="pmc">PMC5489171</ArticleId><ArticleId IdType="pubmed">28658285</ArticleId></ArticleIdList></Reference><Reference><Citation>Paul R., Hawkins S., Balagurunathan Y., Schabath M., Gillies R., Hall L., Goldgof D. Deep Feature Transfer Learning in Combination with Traditional Features Predicts Survival Among Patients with Lung Adenocarcinoma. Tomography. 2016;2:388&#x2013;395. doi: 10.18383/j.tom.2016.00211.</Citation><ArticleIdList><ArticleId IdType="doi">10.18383/j.tom.2016.00211</ArticleId><ArticleId IdType="pmc">PMC5218828</ArticleId><ArticleId IdType="pubmed">28066809</ArticleId></ArticleIdList></Reference><Reference><Citation>Desautels T., Calvert J., Hoffman J., Mao Q., Jay M., Fletcher G., Barton C., Chettipally U., Kerem Y., Das R. Using Transfer Learning for Improved Mortality Prediction in a Data-Scarce Hospital Setting. Biomed. Inform. Insights. 2017;9:1178222617712994. doi: 10.1177/1178222617712994.</Citation><ArticleIdList><ArticleId IdType="doi">10.1177/1178222617712994</ArticleId><ArticleId IdType="pmc">PMC5470861</ArticleId><ArticleId IdType="pubmed">28638239</ArticleId></ArticleIdList></Reference><Reference><Citation>Mei S. Probability weighted ensemble transfer learning for predicting interactions between HIV-1 and human proteins. PLoS ONE. 2013;8:e79606. doi: 10.1371/journal.pone.0079606.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0079606</ArticleId><ArticleId IdType="pmc">PMC3832534</ArticleId><ArticleId IdType="pubmed">24260261</ArticleId></ArticleIdList></Reference><Reference><Citation>Lee C.S., Lee A., Sim D.A., Keane P.A., Mehta H., Zarranz-Ventura J., Fruttiger M., Egan C., Tufail A. Reevaluating the definition of intraretinal microvascular abnormalities and neovascularization elsewhere in diabetic retinopathy using optical coherence tomography and fluorescein angiography. Am. J. Ophthalmol. 2015;159:101&#x2013;110.e1. doi: 10.1016/j.ajo.2014.09.041.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ajo.2014.09.041</ArticleId><ArticleId IdType="pmc">PMC5006953</ArticleId><ArticleId IdType="pubmed">25284762</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36614953</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Print">2077-0383</ISSN><JournalIssue CitedMedium="Print"><Volume>12</Volume><Issue>1</Issue><PubDate><Year>2022</Year><Month>Dec</Month><Day>24</Day></PubDate></JournalIssue><Title>Journal of clinical medicine</Title><ISOAbbreviation>J Clin Med</ISOAbbreviation></Journal><ArticleTitle>Application of Deep Learning to Retinal-Image-Based Oculomics for Evaluation of Systemic Health: A Review.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">152</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/jcm12010152</ELocationID><Abstract><AbstractText>The retina is a window to the human body. Oculomics is the study of the correlations between ophthalmic biomarkers and systemic health or disease states. Deep learning (DL) is currently the cutting-edge machine learning technique for medical image analysis, and in recent years, DL techniques have been applied to analyze retinal images in oculomics studies. In this review, we summarized oculomics studies that used DL models to analyze retinal images-most of the published studies to date involved color fundus photographs, while others focused on optical coherence tomography images. These studies showed that some systemic variables, such as age, sex and cardiovascular disease events, could be consistently robustly predicted, while other variables, such as thyroid function and blood cell count, could not be. DL-based oculomics has demonstrated fascinating, "super-human" predictive capabilities in certain contexts, but it remains to be seen how these models will be incorporated into clinical care and whether management decisions influenced by these models will lead to improved clinical outcomes.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Wu</LastName><ForeName>Jo-Hsuan</ForeName><Initials>JH</Initials><Identifier Source="ORCID">0000-0002-9435-746X</Identifier><AffiliationInfo><Affiliation>Shiley Eye Institute and Viterbi Family Department of Ophthalmology, University of California, San Diego, CA 92093, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Liu</LastName><ForeName>Tin Yan Alvin</ForeName><Initials>TYA</Initials><AffiliationInfo><Affiliation>Wilmer Eye Institute, Johns Hopkins University, Baltimore, MD 21287, USA.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType><PublicationType UI="D016454">Review</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>24</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>J Clin Med</MedlineTA><NlmUniqueID>101606588</NlmUniqueID><ISSNLinking>2077-0383</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">artificial intelligence</Keyword><Keyword MajorTopicYN="N">cardiovascular diseases</Keyword><Keyword MajorTopicYN="N">color fundus photograph</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword><Keyword MajorTopicYN="N">machine learning</Keyword><Keyword MajorTopicYN="N">neurodegenerative diseases</Keyword><Keyword MajorTopicYN="N">oculomics</Keyword><Keyword MajorTopicYN="N">optical coherence tomography</Keyword><Keyword MajorTopicYN="N">retinal imaging</Keyword><Keyword MajorTopicYN="N">systemic diseases</Keyword></KeywordList><CoiStatement>The authors declare no conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>11</Month><Day>20</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>17</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>22</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>1</Hour><Minute>31</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36614953</ArticleId><ArticleId IdType="pmc">PMC9821402</ArticleId><ArticleId IdType="doi">10.3390/jcm12010152</ArticleId><ArticleId IdType="pii">jcm12010152</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Wagner S.K., Fu D.J., Faes L., Liu X., Huemer J., Khalid H., Ferraz D., Korot E., Kelly C., Balaskas K., et al. Insights into Systemic Disease through Retinal Imaging-Based Oculomics. Transl. Vis. Sci. Technol. 2020;9:6. doi: 10.1167/tvst.9.2.6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1167/tvst.9.2.6</ArticleId><ArticleId IdType="pmc">PMC7343674</ArticleId><ArticleId IdType="pubmed">32704412</ArticleId></ArticleIdList></Reference><Reference><Citation>Gupta K., Reddy S. Heart, Eye, and Artificial Intelligence: A Review. Cardiol. Res. 2021;12:132&#x2013;139. doi: 10.14740/cr1179.</Citation><ArticleIdList><ArticleId IdType="doi">10.14740/cr1179</ArticleId><ArticleId IdType="pmc">PMC8139752</ArticleId><ArticleId IdType="pubmed">34046105</ArticleId></ArticleIdList></Reference><Reference><Citation>Vujosevic S., Parra M.M., Hartnett M.E., O&#x2019;Toole L., Nuzzi A., Limoli C., Villani E., Nucci P. Optical coherence tomography as retinal imaging biomarker of neuroinflammation/neurodegeneration in systemic disorders in adults and children. Eye. 2022 doi: 10.1038/s41433-022-02056-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41433-022-02056-9</ArticleId><ArticleId IdType="pmc">PMC9012155</ArticleId><ArticleId IdType="pubmed">35428871</ArticleId></ArticleIdList></Reference><Reference><Citation>MacGillivray T.J., Trucco E., Cameron J.R., Dhillon B., Houston J.G., van Beek E.J. Retinal imaging as a source of biomarkers for diagnosis, characterization and prognosis of chronic illness or long-term conditions. Br. J. Radiol. 2014;87:20130832. doi: 10.1259/bjr.20130832.</Citation><ArticleIdList><ArticleId IdType="doi">10.1259/bjr.20130832</ArticleId><ArticleId IdType="pmc">PMC4112401</ArticleId><ArticleId IdType="pubmed">24936979</ArticleId></ArticleIdList></Reference><Reference><Citation>London A., Benhar I., Schwartz M. The retina as a window to the brain&#x2014;From eye research to CNS disorders. Nat. Rev. Neurol. 2013;9:44&#x2013;53. doi: 10.1038/nrneurol.2012.227.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nrneurol.2012.227</ArticleId><ArticleId IdType="pubmed">23165340</ArticleId></ArticleIdList></Reference><Reference><Citation>Country M.W. Retinal metabolism: A comparative look at energetics in the retina. Brain Res. 2017;1672:50&#x2013;57. doi: 10.1016/j.brainres.2017.07.025.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.brainres.2017.07.025</ArticleId><ArticleId IdType="pubmed">28760441</ArticleId></ArticleIdList></Reference><Reference><Citation>Honavar S.G. Oculomics&#x2014;The eyes talk a great deal. Indian J. Ophthalmol. 2022;70:713. doi: 10.4103/ijo.IJO_474_22.</Citation><ArticleIdList><ArticleId IdType="doi">10.4103/ijo.IJO_474_22</ArticleId><ArticleId IdType="pmc">PMC9114618</ArticleId><ArticleId IdType="pubmed">35225499</ArticleId></ArticleIdList></Reference><Reference><Citation>Fujimoto J.G., Pitris C., Boppart S.A., Brezinski M.E. Optical coherence tomography: An emerging technology for biomedical imaging and optical biopsy. Neoplasia. 2000;2:9&#x2013;25. doi: 10.1038/sj.neo.7900071.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/sj.neo.7900071</ArticleId><ArticleId IdType="pmc">PMC1531864</ArticleId><ArticleId IdType="pubmed">10933065</ArticleId></ArticleIdList></Reference><Reference><Citation>Bille J.F., editor. High Resolution Imaging in Microscopy and Ophthalmology: New Frontiers in Biomedical Optics. Springer; Cham, Switzerland: 2019.</Citation><ArticleIdList><ArticleId IdType="pubmed">32091677</ArticleId></ArticleIdList></Reference><Reference><Citation>Snyder P.J., Alber J., Alt C., Bain L.J., Bouma B.E., Bouwman F.H., DeBuc D.C., Campbell M.C.W., Carrillo M.C., Chew E.Y., et al. Retinal imaging in Alzheimer&#x2019;s and neurodegenerative diseases. Alzheimer&#x2019;s Dement. 2021;17:103&#x2013;111. doi: 10.1002/alz.12179.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/alz.12179</ArticleId><ArticleId IdType="pmc">PMC8062064</ArticleId><ArticleId IdType="pubmed">33090722</ArticleId></ArticleIdList></Reference><Reference><Citation>Christinaki E., Kulenovic H., Hadoux X., Baldassini N., Van Eijgen J., De Groef L., Stalmans I., van Wijngaarden P. Retinal imaging biomarkers of neurodegenerative diseases. Clin. Exp. Optom. 2022;105:194&#x2013;204. doi: 10.1080/08164622.2021.1984179.</Citation><ArticleIdList><ArticleId IdType="doi">10.1080/08164622.2021.1984179</ArticleId><ArticleId IdType="pubmed">34751086</ArticleId></ArticleIdList></Reference><Reference><Citation>Owen C.G., Rudnicka A.R., Welikala R.A., Fraz M.M., Barman S.A., Luben R., Hayat S.A., Khaw K.T., Strachan D.P., Whincup P.H., et al. Retinal Vasculometry Associations with Cardiometabolic Risk Factors in the European Prospective Investigation of Cancer-Norfolk Study. Ophthalmology. 2019;126:96&#x2013;106. doi: 10.1016/j.ophtha.2018.07.022.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ophtha.2018.07.022</ArticleId><ArticleId IdType="pmc">PMC6302796</ArticleId><ArticleId IdType="pubmed">30075201</ArticleId></ArticleIdList></Reference><Reference><Citation>Liew G., Mitchell P., Rochtchina E., Wong T.Y., Hsu W., Lee M.L., Wainwright A., Wang J.J. Fractal analysis of retinal microvasculature and coronary heart disease mortality. Eur. Heart J. 2010;32:422&#x2013;429. doi: 10.1093/eurheartj/ehq431.</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/eurheartj/ehq431</ArticleId><ArticleId IdType="pubmed">21138936</ArticleId></ArticleIdList></Reference><Reference><Citation>Witt N., Wong T.Y., Hughes A.D., Chaturvedi N., Klein B.E., Evans R., McNamara M., Thom S.A.M., Klein R. Abnormalities of Retinal Microvascular Structure and Risk of Mortality from Ischemic Heart Disease and Stroke. Hypertension. 2006;47:975&#x2013;981. doi: 10.1161/01.HYP.0000216717.72048.6c.</Citation><ArticleIdList><ArticleId IdType="doi">10.1161/01.HYP.0000216717.72048.6c</ArticleId><ArticleId IdType="pubmed">16585415</ArticleId></ArticleIdList></Reference><Reference><Citation>McGeechan K., Liew G., Macaskill P., Irwig L., Klein R., Klein B.E., Wang J.J., Mitchell P., Vingerling J.R., Dejong P.T., et al. Meta-analysis: Retinal vessel caliber and risk for coronary heart disease. Ann. Intern. Med. 2009;151:404&#x2013;413. doi: 10.7326/0003-4819-151-6-200909150-00005.</Citation><ArticleIdList><ArticleId IdType="doi">10.7326/0003-4819-151-6-200909150-00005</ArticleId><ArticleId IdType="pmc">PMC2887687</ArticleId><ArticleId IdType="pubmed">19755365</ArticleId></ArticleIdList></Reference><Reference><Citation>McGeechan K., Liew G., Macaskill P., Irwig L., Klein R., Klein B.E., Wang J.J., Mitchell P., Vingerling J.R., de Jong P.T., et al. Prediction of incident stroke events based on retinal vessel caliber: A systematic review and individual-participant meta-analysis. Am. J. Epidemiol. 2009;170:1323&#x2013;1332. doi: 10.1093/aje/kwp306.</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/aje/kwp306</ArticleId><ArticleId IdType="pmc">PMC2800263</ArticleId><ArticleId IdType="pubmed">19884126</ArticleId></ArticleIdList></Reference><Reference><Citation>Wong T.Y., Klein R., Couper D.J., Cooper L.S., Shahar E., Hubbard L.D., Wofford M.R., Sharrett A.R. Retinal microvascular abnormalities and incident stroke: The Atherosclerosis Risk in Communities Study. Lancet. 2001;358:1134&#x2013;1140. doi: 10.1016/S0140-6736(01)06253-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0140-6736(01)06253-5</ArticleId><ArticleId IdType="pubmed">11597667</ArticleId></ArticleIdList></Reference><Reference><Citation>Wong T.Y., Klein R., Sharrett A.R., Manolio T.A., Hubbard L.D., Marino E.K., Kuller L., Burke G., Tracy R.P., Polak J.F., et al. The prevalence and risk factors of retinal microvascular abnormalities in older persons: The Cardiovascular Health Study. Ophthalmology. 2003;110:658&#x2013;666. doi: 10.1016/S0161-6420(02)01931-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0161-6420(02)01931-0</ArticleId><ArticleId IdType="pubmed">12689883</ArticleId></ArticleIdList></Reference><Reference><Citation>Lim L.S., Cheung C.Y.-l., Sabanayagam C., Lim S.C., Tai E.S., Huang L., Wong T.Y. Structural Changes in the Retinal Microvasculature and Renal Function. Investig. Ophthalmol. Vis. Sci. 2013;54:2970&#x2013;2976. doi: 10.1167/iovs.13-11941.</Citation><ArticleIdList><ArticleId IdType="doi">10.1167/iovs.13-11941</ArticleId><ArticleId IdType="pubmed">23572105</ArticleId></ArticleIdList></Reference><Reference><Citation>Liew G., Mitchell P., Wong T.Y., Wang J.J. Retinal microvascular signs are associated with chronic kidney disease in persons with and without diabetes. Kidney Blood Press Res. 2012;35:589&#x2013;594. doi: 10.1159/000339173.</Citation><ArticleIdList><ArticleId IdType="doi">10.1159/000339173</ArticleId><ArticleId IdType="pubmed">22922377</ArticleId></ArticleIdList></Reference><Reference><Citation>Lupton S.J., Chiu C.L., Hodgson L.A., Tooher J., Ogle R., Wong T.Y., Hennessy A., Lind J.M. Changes in retinal microvascular caliber precede the clinical onset of preeclampsia. Hypertension. 2013;62:899&#x2013;904. doi: 10.1161/HYPERTENSIONAHA.113.01890.</Citation><ArticleIdList><ArticleId IdType="doi">10.1161/HYPERTENSIONAHA.113.01890</ArticleId><ArticleId IdType="pubmed">24019405</ArticleId></ArticleIdList></Reference><Reference><Citation>Petzold A., de Boer J.F., Schippling S., Vermersch P., Kardon R., Green A., Calabresi P.A., Polman C. Optical coherence tomography in multiple sclerosis: A systematic review and meta-analysis. Lancet Neurol. 2010;9:921&#x2013;932. doi: 10.1016/S1474-4422(10)70168-X.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S1474-4422(10)70168-X</ArticleId><ArticleId IdType="pubmed">20723847</ArticleId></ArticleIdList></Reference><Reference><Citation>Britze J., Frederiksen J.L. Optical coherence tomography in multiple sclerosis. Eye. 2018;32:884&#x2013;888. doi: 10.1038/s41433-017-0010-2.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41433-017-0010-2</ArticleId><ArticleId IdType="pmc">PMC5944645</ArticleId><ArticleId IdType="pubmed">29391574</ArticleId></ArticleIdList></Reference><Reference><Citation>Paul F., Calabresi P.A., Barkhof F., Green A.J., Kardon R., Sastre-Garriga J., Schippling S., Vermersch P., Saidha S., Gerendas B.S., et al. Optical coherence tomography in multiple sclerosis: A 3-year prospective multicenter study. Ann. Clin. Transl. Neurol. 2021;8:2235&#x2013;2251. doi: 10.1002/acn3.51473.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/acn3.51473</ArticleId><ArticleId IdType="pmc">PMC8670323</ArticleId><ArticleId IdType="pubmed">34792863</ArticleId></ArticleIdList></Reference><Reference><Citation>Marziani E., Pomati S., Ramolfo P., Cigada M., Giani A., Mariani C., Staurenghi G. Evaluation of Retinal Nerve Fiber Layer and Ganglion Cell Layer Thickness in Alzheimer&#x2019;s Disease Using Spectral-Domain Optical Coherence Tomography. Investig. Ophthalmol. Vis. Sci. 2013;54:5953&#x2013;5958. doi: 10.1167/iovs.13-12046.</Citation><ArticleIdList><ArticleId IdType="doi">10.1167/iovs.13-12046</ArticleId><ArticleId IdType="pubmed">23920375</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang M., Zhu Y., Shi Z., Li C., Shen Y. Meta-analysis of the relationship of peripheral retinal nerve fiber layer thickness to Alzheimer&#x2019;s disease and mild cognitive impairment. Shanghai Arch. Psychiatry. 2015;27:263&#x2013;279.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4764001</ArticleId><ArticleId IdType="pubmed">26977124</ArticleId></ArticleIdList></Reference><Reference><Citation>Lian T.-H., Jin Z., Qu Y.-Z., Guo P., Guan H.-Y., Zhang W.-J., Ding D.-Y., Li D.-N., Li L.-X., Wang X.-M., et al. The Relationship Between Retinal Nerve Fiber Layer Thickness and Clinical Symptoms of Alzheimer&#x2019;s Disease. Front. Aging Neurosci. 2021;12:584244. doi: 10.3389/fnagi.2020.584244.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fnagi.2020.584244</ArticleId><ArticleId IdType="pmc">PMC7878673</ArticleId><ArticleId IdType="pubmed">33584241</ArticleId></ArticleIdList></Reference><Reference><Citation>Ko F., Muthy Z.A., Gallacher J., Sudlow C., Rees G., Yang Q., Keane P.A., Petzold A., Khaw P.T., Reisman C., et al. Association of Retinal Nerve Fiber Layer Thinning With Current and Future Cognitive Decline: A Study Using Optical Coherence Tomography. JAMA Neurol. 2018;75:1198&#x2013;1205. doi: 10.1001/jamaneurol.2018.1578.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jamaneurol.2018.1578</ArticleId><ArticleId IdType="pmc">PMC6233846</ArticleId><ArticleId IdType="pubmed">29946685</ArticleId></ArticleIdList></Reference><Reference><Citation>Mutlu U., Colijn J.M., Ikram M.A., Bonnemaijer P.W.M., Licher S., Wolters F.J., Tiemeier H., Koudstaal P.J., Klaver C.C.W., Ikram M.K. Association of Retinal Neurodegeneration on Optical Coherence Tomography with Dementia: A Population-Based Study. JAMA Neurol. 2018;75:1256&#x2013;1263. doi: 10.1001/jamaneurol.2018.1563.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jamaneurol.2018.1563</ArticleId><ArticleId IdType="pmc">PMC6233847</ArticleId><ArticleId IdType="pubmed">29946702</ArticleId></ArticleIdList></Reference><Reference><Citation>Chan H.P., Samala R.K., Hadjiiski L.M., Zhou C. Deep Learning in Medical Image Analysis. Adv. Exp. Med. Biol. 2020;1213:3&#x2013;21.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7442218</ArticleId><ArticleId IdType="pubmed">32030660</ArticleId></ArticleIdList></Reference><Reference><Citation>Shen D., Wu G., Suk H.I. Deep Learning in Medical Image Analysis. Annu. Rev. Biomed. Eng. 2017;19:221&#x2013;248. doi: 10.1146/annurev-bioeng-071516-044442.</Citation><ArticleIdList><ArticleId IdType="doi">10.1146/annurev-bioeng-071516-044442</ArticleId><ArticleId IdType="pmc">PMC5479722</ArticleId><ArticleId IdType="pubmed">28301734</ArticleId></ArticleIdList></Reference><Reference><Citation>LeCun Y., Bengio Y., Hinton G. Deep learning. Nature. 2015;521:436&#x2013;444. doi: 10.1038/nature14539.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nature14539</ArticleId><ArticleId IdType="pubmed">26017442</ArticleId></ArticleIdList></Reference><Reference><Citation>Wu J.H., Liu T.Y.A., Hsu W.T., Ho J.H., Lee C.C. Performance and Limitation of Machine Learning Algorithms for Diabetic Retinopathy Screening: Meta-analysis. J. Med. Internet Res. 2021;23:e23863. doi: 10.2196/23863.</Citation><ArticleIdList><ArticleId IdType="doi">10.2196/23863</ArticleId><ArticleId IdType="pmc">PMC8406115</ArticleId><ArticleId IdType="pubmed">34407500</ArticleId></ArticleIdList></Reference><Reference><Citation>Abr&#xe0;moff M.D., Lou Y., Erginay A., Clarida W., Amelon R., Folk J.C., Niemeijer M. Improved Automated Detection of Diabetic Retinopathy on a Publicly Available Dataset Through Integration of Deep Learning. Invest. Ophthalmol. Vis. Sci. 2016;57:5200&#x2013;5206. doi: 10.1167/iovs.16-19964.</Citation><ArticleIdList><ArticleId IdType="doi">10.1167/iovs.16-19964</ArticleId><ArticleId IdType="pubmed">27701631</ArticleId></ArticleIdList></Reference><Reference><Citation>De Fauw J., Ledsam J.R., Romera-Paredes B., Nikolov S., Tomasev N., Blackwell S., Askham H., Glorot X., O&#x2019;Donoghue B., Visentin D., et al. Clinically applicable deep learning for diagnosis and referral in retinal disease. Nat. Med. 2018;24:1342&#x2013;1350. doi: 10.1038/s41591-018-0107-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41591-018-0107-6</ArticleId><ArticleId IdType="pubmed">30104768</ArticleId></ArticleIdList></Reference><Reference><Citation>Lee C.S., Baughman D.M., Lee A.Y. Deep Learning Is Effective for Classifying Normal versus Age-Related Macular Degeneration OCT Images. Ophthalmol. Retin. 2017;1:322&#x2013;327. doi: 10.1016/j.oret.2016.12.009.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.oret.2016.12.009</ArticleId><ArticleId IdType="pmc">PMC6347658</ArticleId><ArticleId IdType="pubmed">30693348</ArticleId></ArticleIdList></Reference><Reference><Citation>Gulshan V., Peng L., Coram M., Stumpe M.C., Wu D., Narayanaswamy A., Venugopalan S., Widner K., Madams T., Cuadros J., et al. Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs. JAMA. 2016;316:2402&#x2013;2410. doi: 10.1001/jama.2016.17216.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jama.2016.17216</ArticleId><ArticleId IdType="pubmed">27898976</ArticleId></ArticleIdList></Reference><Reference><Citation>Grassmann F., Mengelkamp J., Brandl C., Harsch S., Zimmermann M.E., Linkohr B., Peters A., Heid I.M., Palm C., Weber B.H.F. A Deep Learning Algorithm for Prediction of Age-Related Eye Disease Study Severity Scale for Age-Related Macular Degeneration from Color Fundus Photography. Ophthalmology. 2018;125:1410&#x2013;1420. doi: 10.1016/j.ophtha.2018.02.037.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ophtha.2018.02.037</ArticleId><ArticleId IdType="pubmed">29653860</ArticleId></ArticleIdList></Reference><Reference><Citation>Ting D.S.W., Cheung C.Y., Lim G., Tan G.S.W., Quang N.D., Gan A., Hamzah H., Garcia-Franco R., San Yeo I.Y., Lee S.Y., et al. Development and Validation of a Deep Learning System for Diabetic Retinopathy and Related Eye Diseases Using Retinal Images from Multiethnic Populations with Diabetes. JAMA. 2017;318:2211&#x2013;2223. doi: 10.1001/jama.2017.18152.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jama.2017.18152</ArticleId><ArticleId IdType="pmc">PMC5820739</ArticleId><ArticleId IdType="pubmed">29234807</ArticleId></ArticleIdList></Reference><Reference><Citation>Abr&#xe0;moff M.D., Lavin P.T., Birch M., Shah N., Folk J.C. Pivotal trial of an autonomous AI-based diagnostic system for detection of diabetic retinopathy in primary care offices. npj Digit. Med. 2018;1:39. doi: 10.1038/s41746-018-0040-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41746-018-0040-6</ArticleId><ArticleId IdType="pmc">PMC6550188</ArticleId><ArticleId IdType="pubmed">31304320</ArticleId></ArticleIdList></Reference><Reference><Citation>Poplin R., Varadarajan A.V., Blumer K., Liu Y., McConnell M.V., Corrado G.S., Peng L., Webster D.R. Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning. Nat. Biomed. Eng. 2018;2:158&#x2013;164. doi: 10.1038/s41551-018-0195-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41551-018-0195-0</ArticleId><ArticleId IdType="pubmed">31015713</ArticleId></ArticleIdList></Reference><Reference><Citation>Chang J., Ko A., Park S.M., Choi S., Kim K., Kim S.M., Yun J.M., Kang U., Shin I.H., Shin J.Y., et al. Association of Cardiovascular Mortality and Deep Learning-Funduscopic Atherosclerosis Score derived from Retinal Fundus Images. Am. J. Ophthalmol. 2020;217:121&#x2013;130. doi: 10.1016/j.ajo.2020.03.027.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ajo.2020.03.027</ArticleId><ArticleId IdType="pubmed">32222370</ArticleId></ArticleIdList></Reference><Reference><Citation>Son J., Shin J.Y., Chun E.J., Jung K.-H., Park K.H., Park S.J. Predicting High Coronary Artery Calcium Score from Retinal Fundus Images With Deep Learning Algorithms. Transl. Vis. Sci. Technol. 2020;9:28. doi: 10.1167/tvst.9.2.28.</Citation><ArticleIdList><ArticleId IdType="doi">10.1167/tvst.9.2.28</ArticleId><ArticleId IdType="pmc">PMC7410115</ArticleId><ArticleId IdType="pubmed">33184590</ArticleId></ArticleIdList></Reference><Reference><Citation>Khan N.C., Perera C., Dow E.R., Chen K.M., Mahajan V.B., Mruthyunjaya P., Do D.V., Leng T., Myung D. Predicting Systemic Health Features from Retinal Fundus Images Using Transfer-Learning-Based Artificial Intelligence Models. Diagnostics. 2022;12:1714. doi: 10.3390/diagnostics12071714.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/diagnostics12071714</ArticleId><ArticleId IdType="pmc">PMC9322827</ArticleId><ArticleId IdType="pubmed">35885619</ArticleId></ArticleIdList></Reference><Reference><Citation>Cheung C.Y., Xu D., Cheng C.-Y., Sabanayagam C., Tham Y.-C., Yu M., Rim T.H., Chai C.Y., Gopinath B., Mitchell P., et al. A deep-learning system for the assessment of cardiovascular disease risk via the measurement of retinal-vessel calibre. Nat. Biomed. Eng. 2021;5:498&#x2013;508. doi: 10.1038/s41551-020-00626-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41551-020-00626-4</ArticleId><ArticleId IdType="pubmed">33046867</ArticleId></ArticleIdList></Reference><Reference><Citation>Ma Y., Xiong J., Zhu Y., Ge Z., Hua R., Fu M., Li C., Wang B., Dong L., Zhao X., et al. Development and validation of a deep learning algorithm using fundus photographs to predict 10-year risk of ischemic cardiovascular diseases among Chinese population. medRxiv. 2021 medRxiv:2021.04.15.21255176.</Citation></Reference><Reference><Citation>Rim T.H., Lee G., Kim Y., Tham Y.C., Lee C.J., Baik S.J., Kim Y.A., Yu M., Deshmukh M., Lee B.K., et al. Prediction of systemic biomarkers from retinal photographs: Development and validation of deep-learning algorithms. Lancet Digit. Health. 2020;2:e526&#x2013;e536. doi: 10.1016/S2589-7500(20)30216-8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S2589-7500(20)30216-8</ArticleId><ArticleId IdType="pubmed">33328047</ArticleId></ArticleIdList></Reference><Reference><Citation>Gerrits N., Elen B., Craenendonck T.V., Triantafyllidou D., Petropoulos I.N., Malik R.A., De Boever P. Age and sex affect deep learning prediction of cardiometabolic risk factors from retinal images. Sci. Rep. 2020;10:9432. doi: 10.1038/s41598-020-65794-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-020-65794-4</ArticleId><ArticleId IdType="pmc">PMC7287116</ArticleId><ArticleId IdType="pubmed">32523046</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang L., Yuan M., An Z., Zhao X., Wu H., Li H., Wang Y., Sun B., Li H., Ding S., et al. Prediction of hypertension, hyperglycemia and dyslipidemia from retinal fundus photographs via deep learning: A cross-sectional study of chronic diseases in central China. PLoS ONE. 2020;15:e0233166. doi: 10.1371/journal.pone.0233166.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0233166</ArticleId><ArticleId IdType="pmc">PMC7224473</ArticleId><ArticleId IdType="pubmed">32407418</ArticleId></ArticleIdList></Reference><Reference><Citation>Dai G., He W., Xu L., Pazo E.E., Lin T., Liu S., Zhang C. Exploring the effect of hypertension on retinal microvasculature using deep learning on East Asian population. PLoS ONE. 2020;15:e0230111. doi: 10.1371/journal.pone.0230111.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0230111</ArticleId><ArticleId IdType="pmc">PMC7058325</ArticleId><ArticleId IdType="pubmed">32134976</ArticleId></ArticleIdList></Reference><Reference><Citation>Korot E., Pontikos N., Liu X., Wagner S.K., Faes L., Huemer J., Balaskas K., Denniston A.K., Khawaja A., Keane P.A. Predicting sex from retinal fundus photographs using automated deep learning. Sci. Rep. 2021;11:10286. doi: 10.1038/s41598-021-89743-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-021-89743-x</ArticleId><ArticleId IdType="pmc">PMC8119673</ArticleId><ArticleId IdType="pubmed">33986429</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhu Z., Shi D., Guankai P., Tan Z., Shang X., Hu W., Liao H., Zhang X., Huang Y., Yu H., et al. Retinal age gap as a predictive biomarker for mortality risk. Br. J. Ophthalmol. 2022 doi: 10.1136/bjophthalmol-2021-319807.</Citation><ArticleIdList><ArticleId IdType="doi">10.1136/bjophthalmol-2021-319807</ArticleId><ArticleId IdType="pubmed">35042683</ArticleId></ArticleIdList></Reference><Reference><Citation>Mitani A., Huang A., Venugopalan S., Corrado G.S., Peng L., Webster D.R., Hammel N., Liu Y., Varadarajan A.V. Detection of anaemia from retinal fundus images via deep learning. Nat. Biomed. Eng. 2020;4:18&#x2013;27. doi: 10.1038/s41551-019-0487-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41551-019-0487-z</ArticleId><ArticleId IdType="pubmed">31873211</ArticleId></ArticleIdList></Reference><Reference><Citation>Vaghefi E., Yang S., Hill S., Humphrey G., Walker N., Squirrell D. Detection of smoking status from retinal images; a Convolutional Neural Network study. Sci. Rep. 2019;9:7180. doi: 10.1038/s41598-019-43670-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-019-43670-0</ArticleId><ArticleId IdType="pmc">PMC6509122</ArticleId><ArticleId IdType="pubmed">31073220</ArticleId></ArticleIdList></Reference><Reference><Citation>Sabanayagam C., Xu D., Ting D.S.W., Nusinovici S., Banu R., Hamzah H., Lim C., Tham Y.C., Cheung C.Y., Tai E.S., et al. A deep learning algorithm to detect chronic kidney disease from retinal photographs in community-based populations. Lancet Digit. Health. 2020;2:e295&#x2013;e302. doi: 10.1016/S2589-7500(20)30063-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S2589-7500(20)30063-7</ArticleId><ArticleId IdType="pubmed">33328123</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang K., Liu X., Xu J., Yuan J., Cai W., Chen T., Wang K., Gao Y., Nie S., Xu X., et al. Deep-learning models for the detection and incidence prediction of chronic kidney disease and type 2 diabetes from retinal fundus images. Nat. Biomed. Eng. 2021;5:533&#x2013;545. doi: 10.1038/s41551-021-00745-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41551-021-00745-6</ArticleId><ArticleId IdType="pubmed">34131321</ArticleId></ArticleIdList></Reference><Reference><Citation>Tian J., Smith G., Guo H., Liu B., Pan Z., Wang Z., Xiong S., Fang R. Modular machine learning for Alzheimer&#x2019;s disease classification from retinal vasculature. Sci. Rep. 2021;11:238. doi: 10.1038/s41598-020-80312-2.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-020-80312-2</ArticleId><ArticleId IdType="pmc">PMC7794289</ArticleId><ArticleId IdType="pubmed">33420208</ArticleId></ArticleIdList></Reference><Reference><Citation>Montol&#xed;o A., Mart&#xed;n-Gallego A., Cego&#xf1;ino J., Orduna E., Vilades E., Garcia-Martin E., Palomar A.P.d. Machine learning in diagnosis and disability prediction of multiple sclerosis using optical coherence tomography. Comput. Biol. Med. 2021;133:104416. doi: 10.1016/j.compbiomed.2021.104416.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2021.104416</ArticleId><ArticleId IdType="pubmed">33946022</ArticleId></ArticleIdList></Reference><Reference><Citation>McDonald W.I., Compston A., Edan G., Goodkin D., Hartung H.P., Lublin F.D., McFarland H.F., Paty D.W., Polman C.H., Reingold S.C. Recommended diagnostic criteria for multiple sclerosis: Guidelines from the International Panel on the diagnosis of multiple sclerosis. Ann. Neurol. Off. J. Am. Neurol. Assoc. Child Neurol. Soc. 2001;50:121&#x2013;127. doi: 10.1002/ana.1032.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/ana.1032</ArticleId><ArticleId IdType="pubmed">11456302</ArticleId></ArticleIdList></Reference><Reference><Citation>Dietterich T.G., editor. Ensemble Methods in Machine Learning. Multiple Classifier Systems. Springer; Berlin/Heidelberg, Germany: 2000.</Citation></Reference><Reference><Citation>L&#xf3;pez-Dorado A., Ortiz M., Satue M., Rodrigo M.J., Barea R., S&#xe1;nchez-Morla E.M., Cavaliere C., Rodr&#xed;guez-Ascariz J.M., Orduna-Hospital E., Boquete L., et al. Early Diagnosis of Multiple Sclerosis Using Swept-Source Optical Coherence Tomography and Convolutional Neural Networks Trained with Data Augmentation. Sensors. 2022;22:167. doi: 10.3390/s22010167.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s22010167</ArticleId><ArticleId IdType="pmc">PMC8747672</ArticleId><ArticleId IdType="pubmed">35009710</ArticleId></ArticleIdList></Reference><Reference><Citation>Shigueoka L.S., Mariottoni E.B., Thompson A.C., Jammal A.A., Costa V.P., Medeiros F.A. Predicting Age From Optical Coherence Tomography Scans With Deep Learning. Transl. Vis. Sci. Technol. 2021;10:12. doi: 10.1167/tvst.10.1.12.</Citation><ArticleIdList><ArticleId IdType="doi">10.1167/tvst.10.1.12</ArticleId><ArticleId IdType="pmc">PMC7804495</ArticleId><ArticleId IdType="pubmed">33510951</ArticleId></ArticleIdList></Reference><Reference><Citation>Mendoza L., Christopher M., Brye N., Proudfoot J.A., Belghith A., Bowd C., Rezapour J., Fazio M.A., Goldbaum M.H., Weinreb R.N., et al. Deep Learning Predicts Demographic and Clinical Characteristics from Optic Nerve Head OCT Circle and Radial Scans. Investig. Ophthalmol. Vis. Sci. 2021;62:2120.</Citation></Reference><Reference><Citation>Chueh K.-M., Hsieh Y.-T., Chen H.H., Ma I.H., Huang S.-L. Identification of Sex and Age from Macular Optical Coherence Tomography and Feature Analysis Using Deep Learning. Am. J. Ophthalmol. 2022;235:221&#x2013;228. doi: 10.1016/j.ajo.2021.09.015.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ajo.2021.09.015</ArticleId><ArticleId IdType="pubmed">34582766</ArticleId></ArticleIdList></Reference><Reference><Citation>Hassan O.N., Menten M.J., Bogunovic H., Schmidt-Erfurth U., Lotery A., Rueckert D., editors. Deep Learning Prediction Of Age And Sex From Optical Coherence Tomography; Proceedings of the 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI); Nice, France. 13&#x2013;16 April 2021.</Citation></Reference><Reference><Citation>Wisely C.E., Wang D., Henao R., Grewal D.S., Thompson A.C., Robbins C.B., Yoon S.P., Soundararajan S., Polascik B.W., Burke J.R., et al. Convolutional neural network to identify symptomatic Alzheimer&#x2019;s disease using multimodal retinal imaging. Br. J. Ophthalmol. 2022;106:388&#x2013;395. doi: 10.1136/bjophthalmol-2020-317659.</Citation><ArticleIdList><ArticleId IdType="doi">10.1136/bjophthalmol-2020-317659</ArticleId><ArticleId IdType="pubmed">33243829</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36612010</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Print">2072-6694</ISSN><JournalIssue CitedMedium="Print"><Volume>15</Volume><Issue>1</Issue><PubDate><Year>2022</Year><Month>Dec</Month><Day>20</Day></PubDate></JournalIssue><Title>Cancers</Title><ISOAbbreviation>Cancers (Basel)</ISOAbbreviation></Journal><ArticleTitle>Squeeze-MNet: Precise Skin Cancer Detection Model for Low Computing IoT Devices Using Transfer Learning.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">12</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/cancers15010012</ELocationID><Abstract><AbstractText>Cancer remains a deadly disease. We developed a lightweight, accurate, general-purpose deep learning algorithm for skin cancer classification. Squeeze-MNet combines a Squeeze algorithm for digital hair removal during preprocessing and a MobileNet deep learning model with predefined weights. The Squeeze algorithm extracts important image features from the image, and the black-hat filter operation removes noise. The MobileNet model (with a dense neural network) was developed using the International Skin Imaging Collaboration (ISIC) dataset to fine-tune the model. The proposed model is lightweight; the prototype was tested on a Raspberry Pi 4 Internet of Things device with a Neo pixel 8-bit LED ring; a medical doctor validated the device. The average precision (AP) for benign and malignant diagnoses was 99.76% and 98.02%, respectively. Using our approach, the required dataset size decreased by 66%. The hair removal algorithm increased the accuracy of skin cancer detection to 99.36% with the ISIC dataset. The area under the receiver operating curve was 98.9%.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Shinde</LastName><ForeName>Rupali Kiran</ForeName><Initials>RK</Initials><Identifier Source="ORCID">0000-0002-4631-5353</Identifier><AffiliationInfo><Affiliation>Department of Information and Communication Engineering, Chungbuk National University, Cheongju 28644, Republic of Korea.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Alam</LastName><ForeName>Md Shahinur</ForeName><Initials>MS</Initials><Identifier Source="ORCID">0000-0001-8413-5428</Identifier><AffiliationInfo><Affiliation>VL2 Center, Gallaudet University, Washington, DC 20002, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Hossain</LastName><ForeName>Md Biddut</ForeName><Initials>MB</Initials><Identifier Source="ORCID">0000-0001-6000-9128</Identifier><AffiliationInfo><Affiliation>Department of Information and Communication Engineering, Chungbuk National University, Cheongju 28644, Republic of Korea.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Md Imtiaz</LastName><ForeName>Shariar</ForeName><Initials>S</Initials><Identifier Source="ORCID">0000-0002-7001-1667</Identifier><AffiliationInfo><Affiliation>Department of Information and Communication Engineering, Chungbuk National University, Cheongju 28644, Republic of Korea.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kim</LastName><ForeName>JoonHyun</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Department of Information and Communication Engineering, Chungbuk National University, Cheongju 28644, Republic of Korea.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Padwal</LastName><ForeName>Anuja Anil</ForeName><Initials>AA</Initials><AffiliationInfo><Affiliation>Ashwini Rural Collage, Solapur 413006, India.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kim</LastName><ForeName>Nam</ForeName><Initials>N</Initials><Identifier Source="ORCID">0000-0001-8109-2055</Identifier><AffiliationInfo><Affiliation>Department of Information and Communication Engineering, Chungbuk National University, Cheongju 28644, Republic of Korea.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>20</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Cancers (Basel)</MedlineTA><NlmUniqueID>101526829</NlmUniqueID><ISSNLinking>2072-6694</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">AUC-ROC</Keyword><Keyword MajorTopicYN="N">IoT</Keyword><Keyword MajorTopicYN="N">MobileNet</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword><Keyword MajorTopicYN="N">malignant</Keyword><Keyword MajorTopicYN="N">skin cancer detection</Keyword><Keyword MajorTopicYN="N">squeezed dataset</Keyword><Keyword MajorTopicYN="N">transfer learning</Keyword></KeywordList><CoiStatement>The authors declare no conflict of interest. If the device is produced commercially, the authors (including Dr. Anuja Padwal) do not have any intention to gain economic or any other benefit.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>10</Month><Day>23</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>15</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>16</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>1</Hour><Minute>6</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36612010</ArticleId><ArticleId IdType="pmc">PMC9817940</ArticleId><ArticleId IdType="doi">10.3390/cancers15010012</ArticleId><ArticleId IdType="pii">cancers15010012</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Ansari U.B. Skin Cancer Detection Using Image Processing. Int. Res. J. Eng. Technol. 2017;4:2875&#x2013;2881.</Citation></Reference><Reference><Citation>Goyal M., Knackstedt T., Yan S., Hassanpour S. Artificial intelligence-based image classification methods for diagnosis of skin cancer: Challenges and opportunities. Comput. Biol. Med. 2020;127:104065. doi: 10.1016/j.compbiomed.2020.104065.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2020.104065</ArticleId><ArticleId IdType="pmc">PMC8290363</ArticleId><ArticleId IdType="pubmed">33246265</ArticleId></ArticleIdList></Reference><Reference><Citation>Leiter U., Claus G. Sunlight Vitamin D and Skin Cancer. Springer; New York, NY, USA: 2008. Epidemiology of melanoma and nonmelanoma skin cancer&#x2014;The role of sunlight; pp. 89&#x2013;103.</Citation><ArticleIdList><ArticleId IdType="pubmed">18348450</ArticleId></ArticleIdList></Reference><Reference><Citation>The Skin Cancer Foundation.  [(accessed on 15 October 2020)].  Available online:  https://www.skincancer.org/</Citation></Reference><Reference><Citation>How Much Does a Biopsy Cost? CostHelper.  [(accessed on 20 October 2020)].  Available online:  https://health.costhelper.com.</Citation></Reference><Reference><Citation>Esteva A., Kuprel B., Novoa R.A., Ko J., Swetter S.M., Blau H.M., Thrun S. Dermatologist-level classification of skin cancer with deep neural networks. Nature. 2017;542:115&#x2013;118. doi: 10.1038/nature21056.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nature21056</ArticleId><ArticleId IdType="pmc">PMC8382232</ArticleId><ArticleId IdType="pubmed">28117445</ArticleId></ArticleIdList></Reference><Reference><Citation>Shinde R., Alam M., Park S., Park S., Kim N. Intelligent IoT (IIoT) Device to Identifying Suspected COVID-19 Infections Using Sensor Fusion Algorithm and Real-Time Mask Detection Based on the Enhanced MobileNetV2 Model. Healthcare. 2022;10:454. doi: 10.3390/healthcare10030454.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/healthcare10030454</ArticleId><ArticleId IdType="pmc">PMC8955349</ArticleId><ArticleId IdType="pubmed">35326932</ArticleId></ArticleIdList></Reference><Reference><Citation>Jacobs C., Ginneken B. Google&#x2019;s lung cancer AI: A promising tool that needs further validation. Nat. Rev. Clin. Oncol. 2019;16:532&#x2013;533. doi: 10.1038/s41571-019-0248-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41571-019-0248-7</ArticleId><ArticleId IdType="pubmed">31249401</ArticleId></ArticleIdList></Reference><Reference><Citation>McKinney S.M., Sieniek M., Godbole V., Godwin J., Antropova N., Ashrafian H., Back T., Chesus M., Corrado G.S., Darzi A., et al. International evaluation of an AI system for breast cancer screening. Nature. 2020;577:89&#x2013;94. doi: 10.1038/s41586-019-1799-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41586-019-1799-6</ArticleId><ArticleId IdType="pubmed">31894144</ArticleId></ArticleIdList></Reference><Reference><Citation>Polap D. Analysis of Skin Marks Through the Use of Intelligent Things. IEEE Access. 2019;7:149355&#x2013;149363. doi: 10.1109/ACCESS.2019.2947354.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2019.2947354</ArticleId></ArticleIdList></Reference><Reference><Citation>Shirazi A.Z., Fornaciari E., Bagherian N.S., Ebert L.M., Koszyca B., Gomez G.A. DeepSurvNet: Deep survival convolutional network for brain cancer survival rate classification based on histopathological images. Med. Biol. Eng. Comput. 2020;58:1031&#x2013;1045. doi: 10.1007/s11517-020-02147-3.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11517-020-02147-3</ArticleId><ArticleId IdType="pmc">PMC7188709</ArticleId><ArticleId IdType="pubmed">32124225</ArticleId></ArticleIdList></Reference><Reference><Citation>Abr&#xe0;moff M., Lavin P., Birch M. Pivotal trial of an autonomous AI-based diagnostic system for detection of diabetic reti-nopathy in primary care offices. NPJ Digit. Med. 2018;1:39&#x2013;49. doi: 10.1038/s41746-018-0040-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41746-018-0040-6</ArticleId><ArticleId IdType="pmc">PMC6550188</ArticleId><ArticleId IdType="pubmed">31304320</ArticleId></ArticleIdList></Reference><Reference><Citation>Ali S., El-Sappagh S., Ali F., Imran M., Abuhmed T. Multitask Deep Learning for Cost-Effective Prediction of Patient&#x2019;s Length of Stay and Readmission State Using Multimodal Physical Activity Sensory Data. IEEE J. Biomed. Health Informatics. 2022;26:5793&#x2013;5804. doi: 10.1109/JBHI.2022.3202178.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/JBHI.2022.3202178</ArticleId><ArticleId IdType="pubmed">36037451</ArticleId></ArticleIdList></Reference><Reference><Citation>Subhan F., Aziz M.A., Khan I.U., Fayaz M., Wozniak M., Shafi J., Ijaz M.F. Cancerous Tumor Controlled Treatment Using Search Heuristic (GA)-Based Sliding Mode and Synergetic Controller. Cancers. 2022;14:4191. doi: 10.3390/cancers14174191.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/cancers14174191</ArticleId><ArticleId IdType="pmc">PMC9454425</ArticleId><ArticleId IdType="pubmed">36077727</ArticleId></ArticleIdList></Reference><Reference><Citation>ISIC Archive  International Skin Imaging Collaboration.  [(accessed on 3 June 2022)].  Available online:  https://www.isic-archive.com/#!/topWithHeader/wideContentTop/main.</Citation></Reference><Reference><Citation>Robert F., Darrell R., Alfred W. Early Detection of Malignant Melanoma: The Role of Physician Examination and Self-Examination of the Skin. A Cancer J. Clin. 1985;35:130&#x2013;151.</Citation><ArticleIdList><ArticleId IdType="pubmed">3921200</ArticleId></ArticleIdList></Reference><Reference><Citation>Abbasi N.R., Shaw H.M., Rigel D.S., Friedman R., McCarthy W.H., Osman, Kopf A.W., Polsky D. Early diagnosis of cutaneous melanoma: Revisiting the ABCD criteria. J. Am. Med. Assoc. 2004;292:2771&#x2013;2776. doi: 10.1001/jama.292.22.2771.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jama.292.22.2771</ArticleId><ArticleId IdType="pubmed">15585738</ArticleId></ArticleIdList></Reference><Reference><Citation>Jensen J.D., Elewski B. The ABCDEF Rule: Combining the &#x201c;ABCDE Rule&#x201d; and the &#x201c;Ugly Duckling Sign&#x201d; in an Effort to Improve Patient Self-Screening Examinations. J. Clin. Aesthetic Derm. 2015;8:15&#x2013;25.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4345927</ArticleId><ArticleId IdType="pubmed">25741397</ArticleId></ArticleIdList></Reference><Reference><Citation>Ain Q.U., Xue B., Al-Sahaf H., Zhang M. Genetic Programming for Feature Selection and Feature Construction in Skin Cancer Image Classification; Proceedings of the Pacific Rim International Conference on Artificial Intelligence; Nanjing, China. 28&#x2013;31 August 2018; pp. 732&#x2013;745.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-319-97304-3_56</ArticleId></ArticleIdList></Reference><Reference><Citation>Shahi P., Yadav S., Singh N., Singh N.P. Melanoma skin cancer detection using various classifiers; Proceedings of the 5th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON); Go-rakhpur, India. 2&#x2013;4 November 2018.</Citation></Reference><Reference><Citation>Dey N., Rajinikanth A., Shour S., Tavares M.R. Social group optimization supported segmentation and evaluation of skin melanoma images. Symmetry. 2018;10:51. doi: 10.3390/sym10020051.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/sym10020051</ArticleId></ArticleIdList></Reference><Reference><Citation>Brinker T.J., Hekler A., Enk A.H., Klode J., Hauschild A., Berking C., Schilling B., Haferkamp S., Schadendorf D., Fr&#xf6;hling S., et al. A convolutional neural network trained with dermoscopic images performed on par with 145 dermatologists in a clinical melanoma image classification task. Eur. J. Cancer. 2019;111:148&#x2013;154. doi: 10.1016/j.ejca.2019.02.005.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejca.2019.02.005</ArticleId><ArticleId IdType="pubmed">30852421</ArticleId></ArticleIdList></Reference><Reference><Citation>Lopez-Leyva J.A., Guerra-Rosas E., Alvarez-Borrego J. Multi-Class Diagnosis of Skin Lesions Using the Fourier Spectral Information of Images on Additive Color Model by Artificial Neural Network. IEEE Access. 2021;9:35207&#x2013;35216. doi: 10.1109/ACCESS.2021.3061873.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2021.3061873</ArticleId></ArticleIdList></Reference><Reference><Citation>Albahar M.A. Skin Lesion Classification Using Convolutional Neural Network With Novel Regularizer. IEEE Access. 2019;7:38306&#x2013;38313. doi: 10.1109/ACCESS.2019.2906241.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2019.2906241</ArticleId></ArticleIdList></Reference><Reference><Citation>Sharma A.K., Tiwari S., Aggarwal G., Goenka N., Kumar A., Chakrabarti P., Chakrabarti T., Gono R., Leonowicz Z., Jasinski M. Dermatologist-Level Classification of Skin Cancer Using Cascaded Ensembling of Convolutional Neural Network and Handcrafted Features Based Deep Neural Network. IEEE Access. 2022;10:17920&#x2013;17932. doi: 10.1109/ACCESS.2022.3149824.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2022.3149824</ArticleId></ArticleIdList></Reference><Reference><Citation>Rehman M.U., Khan S.H., Danish R., Abbas Z., Zafar A. Classification of Skin Lesion by Interference of Segmentation and Convolution Neural Network; Proceedings of the 2nd International Conference on Engineering Innovation (ICEI); Bangkok, Thailand. 5&#x2013;7 July 2018.</Citation></Reference><Reference><Citation>Talavera-Martinez L., Bibiloni P., Gonzalez-Hidalgo M. Hair Segmentation and Removal in Dermoscopic Images Using Deep Learning. IEEE Access. 2020;9:2694&#x2013;2704. doi: 10.1109/ACCESS.2020.3047258.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2020.3047258</ArticleId></ArticleIdList></Reference><Reference><Citation>Bian J., Zhang S., Wang S., Zhang J., Guo J. Skin Lesion Classification by Multi-View Filtered Transfer Learning. IEEE Access. 2021;9:66052&#x2013;66061. doi: 10.1109/ACCESS.2021.3076533.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2021.3076533</ArticleId></ArticleIdList></Reference><Reference><Citation>Mahbod A., Schaefer G., Wang C., Ecker R., Dorffner G., Ellinger I. Investigating and Exploiting Image Resolution for Transfer Learning-based Skin Lesion Classification. arXiv. 2021;1:4047&#x2013;4053. doi: 10.1109/icpr48806.2021.9412307.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/icpr48806.2021.9412307</ArticleId></ArticleIdList></Reference><Reference><Citation>Kassem M.A., Hosny K.M., Fouad M.M. Skin Lesions Classification Into Eight Classes for ISIC 2019 Using Deep Convolutional Neural Network and Transfer Learning. IEEE Access. 2020;8:114822&#x2013;114832. doi: 10.1109/ACCESS.2020.3003890.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2020.3003890</ArticleId></ArticleIdList></Reference><Reference><Citation>Hosny K.M., Kassem M.A., Foaud M.M. Classification of skin lesions using transfer learning and augmentation with Alex-net. PLoS ONE. 2019;14:e0217293. doi: 10.1371/journal.pone.0217293.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0217293</ArticleId><ArticleId IdType="pmc">PMC6529006</ArticleId><ArticleId IdType="pubmed">31112591</ArticleId></ArticleIdList></Reference><Reference><Citation>Alqudah A.M., Alquraan H., Abu Qasmieh I. Segmented and Non-Segmented Skin Lesions Classification Using Transfer Learning and Adaptive Moment Learning Rate Technique Using Pretrained Convolutional Neural Network. J. Biomim. Biomater. Biomed. Eng. 2019;42:67&#x2013;78. doi: 10.4028/www.scientific.net/jbbbe.42.67.</Citation><ArticleIdList><ArticleId IdType="doi">10.4028/www.scientific.net/jbbbe.42.67</ArticleId></ArticleIdList></Reference><Reference><Citation>Hosny K.M., Kassem M.A., Fouad M.M. Classification of Skin Lesions into Seven Classes Using Transfer Learning with AlexNet. J. Digit. Imaging. 2020;33:1325&#x2013;1334. doi: 10.1007/s10278-020-00371-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10278-020-00371-9</ArticleId><ArticleId IdType="pmc">PMC7573031</ArticleId><ArticleId IdType="pubmed">32607904</ArticleId></ArticleIdList></Reference><Reference><Citation>Jain S., Singhania U., Tripathy B., Nasr E., Aboudaif M., Kamrani A. Deep Learning-Based Transfer Learning for Classi-fication of Skin Cancer. Sensors. 2021;23:8142. doi: 10.3390/s21238142.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s21238142</ArticleId><ArticleId IdType="pmc">PMC8662405</ArticleId><ArticleId IdType="pubmed">34884146</ArticleId></ArticleIdList></Reference><Reference><Citation>Kondaveeti H.K., Edupuganti P. Skin Cancer Classification using Transfer Learning; Proceedings of the IEEE International Conference on Advent Trends in Multidisciplinary Research and Innovation (ICATMRI); Buldhana, India. 30 December 2020.</Citation></Reference><Reference><Citation>Skin Cancer Malignant vs benign.  [(accessed on 20 March 2022)].  Available online:  https://www.kaggle.com/datasets/abhikray/skin-cancer-malignant-vs-benign?select=test.</Citation></Reference><Reference><Citation>Kim D., Hong B.-W. Unsupervised Feature Elimination via Generative Adversarial Networks: Application to Hair Removal in Melanoma Classification. IEEE Access. 2021;9:42610&#x2013;42620. doi: 10.1109/ACCESS.2021.3065701.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2021.3065701</ArticleId></ArticleIdList></Reference><Reference><Citation>Jaworek J., Ryszard T. Hair removal from dermoscopic color images. Bio-Algorithms Med-Syst. 2013;9:pp. 53&#x2013;58.</Citation></Reference><Reference><Citation>Soans R.V., Fukumizu Y. Improved Facial Keypoint Regression Using Attention Modules; Proceedings of the Communi-cations in Computer and Information Science, Frontiers of Computer Vision; Hiroshima, Japan. 21&#x2013;22 February 2022.</Citation></Reference><Reference><Citation>Ramteke N., Jain S. ABCD rule based automatic computer-aided skin cancer. Int. J. Comput. Technol. Appl. 2013;4:691&#x2013;697.</Citation></Reference><Reference><Citation>Dang T., Prasath B., Hieu L., Nguyen H. Melanoma Skin Cancer Detection Method Based on Adaptive Principal Curvature, Colour Normalisation and Feature Extraction with the ABCD Rule. J. Digit. Imaging. 2020;33:574&#x2013;585.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7256173</ArticleId><ArticleId IdType="pubmed">31848895</ArticleId></ArticleIdList></Reference><Reference><Citation>Bandic J., Kovacevic S., Karabeg R., Lazarov A., Opric D. Teledermoscopy for Skin Cancer Prevention: A Comparative Study of Clinical and Teledermoscopic Diagnosis. Acta Inform. Med. 2020;28:37&#x2013;41. doi: 10.5455/aim.2020.28.37-41.</Citation><ArticleIdList><ArticleId IdType="doi">10.5455/aim.2020.28.37-41</ArticleId><ArticleId IdType="pmc">PMC7085326</ArticleId><ArticleId IdType="pubmed">32210513</ArticleId></ArticleIdList></Reference><Reference><Citation>Manoj K., Mohammed A., Rayed A., Purushottam S., Vikas D. A DE-ANN Inspired Skin Cancer Detection Approach Using Fuzzy C-Means Clustering. Mob. Netw. Appl. 2020;25:1319&#x2013;1329.</Citation></Reference><Reference><Citation>Adegun A., Viriri S. FCN-Based DenseNet Framework for Automated Detection and Classification of Skin Lesions in Der-moscopy Images. IEEE Access. 2020;8:150377&#x2013;150396. doi: 10.1109/ACCESS.2020.3016651.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2020.3016651</ArticleId></ArticleIdList></Reference><Reference><Citation>Pham T.-C., Doucet A., Luong C.-M., Tran C.-T., Hoang V.-D. Improving Skin-Disease Classification Based on Customized Loss Function Combined With Balanced Mini-Batch Logic and Real-Time Image Augmentation. IEEE Access. 2020;8:150725&#x2013;150737. doi: 10.1109/ACCESS.2020.3016653.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2020.3016653</ArticleId></ArticleIdList></Reference><Reference><Citation>Diaz S., Krohmer T., Moreira A., Godoy S.E., Figueroa M. An Instrument for Accurate and Non-Invasive Screening of Skin Cancer Based on Multimodal Imaging. IEEE Access. 2019;7:176646&#x2013;176657. doi: 10.1109/ACCESS.2019.2956898.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2019.2956898</ArticleId></ArticleIdList></Reference><Reference><Citation>Mazoure B., Mazoure A., B&#xe9;dard J., Makarenkov V. DUNEScan: A web server for uncertainty estimation in skin cancer detection with deep neural networks. Sci. Rep. 2022;12:179. doi: 10.1038/s41598-021-03889-2.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-021-03889-2</ArticleId><ArticleId IdType="pmc">PMC8741961</ArticleId><ArticleId IdType="pubmed">34996997</ArticleId></ArticleIdList></Reference><Reference><Citation>Ashraf R., Afzal S., Rehman A.U., Gul S., Baber J., Bakhtyar M., Mehmood I., Song O.-Y., Maqsood M. Region-of-Interest Based Transfer Learning Assisted Framework for Skin Cancer Detection. IEEE Access. 2020;8:147858&#x2013;147871. doi: 10.1109/ACCESS.2020.3014701.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2020.3014701</ArticleId></ArticleIdList></Reference><Reference><Citation>Saleh A., Nudrat N., Aun I., Muhammad H.Y., Muhammad T.M. Melanoma Lesion Detection and Segmentation using YOLOv4-DarkNet and Active Contour. IEEE Access. 2020;8:198403&#x2013;198414.</Citation></Reference><Reference><Citation>Srinivasu P.N., SivaSai J.G., Ijaz M.F., Bhoi A.K., Kim W., Kang J.J. Classification of skin disease using deep learning neural networks with MobileNet V2 and LSTM. Sensors. 2021;21:2852. doi: 10.3390/s21082852.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s21082852</ArticleId><ArticleId IdType="pmc">PMC8074091</ArticleId><ArticleId IdType="pubmed">33919583</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36611583</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Print">2227-9032</ISSN><JournalIssue CitedMedium="Print"><Volume>11</Volume><Issue>1</Issue><PubDate><Year>2022</Year><Month>Dec</Month><Day>30</Day></PubDate></JournalIssue><Title>Healthcare (Basel, Switzerland)</Title><ISOAbbreviation>Healthcare (Basel)</ISOAbbreviation></Journal><ArticleTitle>Bandwidth Improvement in Ultrasound Image Reconstruction Using Deep Learning Techniques.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">123</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/healthcare11010123</ELocationID><Abstract><AbstractText>Ultrasound (US) imaging is a medical imaging modality that uses the reflection of sound in the range of 2-18 MHz to image internal body structures. In US, the frequency bandwidth (BW) is directly associated with image resolution. BW is a property of the transducer and more bandwidth comes at a higher cost. Thus, methods that can transform strongly bandlimited ultrasound data into broadband data are essential. In this work, we propose a deep learning (DL) technique to improve the image quality for a given bandwidth by learning features provided by broadband data of the same field of view. Therefore, the performance of several DL architectures and conventional state-of-the-art techniques for image quality improvement and artifact removal have been compared on in vitro US datasets. Two training losses have been utilized on three different architectures: a super resolution convolutional neural network (SRCNN), U-Net, and a residual encoder decoder network (REDNet) architecture. The models have been trained to transform low-bandwidth image reconstructions to high-bandwidth image reconstructions, to reduce the artifacts, and make the reconstructions visually more attractive. Experiments were performed for 20%, 40%, and 60% fractional bandwidth on the original images and showed that the improvements obtained are as high as 45.5% in RMSE, and 3.85 dB in PSNR, in datasets with a 20% bandwidth limitation.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Awasthi</LastName><ForeName>Navchetan</ForeName><Initials>N</Initials><Identifier Source="ORCID">0000-0001-8153-2786</Identifier><AffiliationInfo><Affiliation>Photoacoustics and Ultrasound Laboratory Eindhoven (PULS/e), Department of Biomedical Engineering, Eindhoven University of Technology, 5612 AZ Eindhoven, The Netherlands.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Medical Image Analysis Group (IMAG/e), Department of Biomedical Engineering, Eindhoven University of Technology, 5612 AZ Eindhoven, The Netherlands.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>van Anrooij</LastName><ForeName>Laslo</ForeName><Initials>L</Initials><AffiliationInfo><Affiliation>Medical Image Analysis Group (IMAG/e), Department of Biomedical Engineering, Eindhoven University of Technology, 5612 AZ Eindhoven, The Netherlands.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Jansen</LastName><ForeName>Gino</ForeName><Initials>G</Initials><Identifier Source="ORCID">0000-0003-1685-7714</Identifier><AffiliationInfo><Affiliation>Department of Biomedical Engineering and Physics, Amsterdam University Medical Center, 1105 AZ Amsterdam, The Netherlands.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Schwab</LastName><ForeName>Hans-Martin</ForeName><Initials>HM</Initials><AffiliationInfo><Affiliation>Photoacoustics and Ultrasound Laboratory Eindhoven (PULS/e), Department of Biomedical Engineering, Eindhoven University of Technology, 5612 AZ Eindhoven, The Netherlands.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Pluim</LastName><ForeName>Josien P W</ForeName><Initials>JPW</Initials><AffiliationInfo><Affiliation>Medical Image Analysis Group (IMAG/e), Department of Biomedical Engineering, Eindhoven University of Technology, 5612 AZ Eindhoven, The Netherlands.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Image Sciences Institute, University Medical Center, 3584 CX Utrecht, The Netherlands.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lopata</LastName><ForeName>Richard G P</ForeName><Initials>RGP</Initials><AffiliationInfo><Affiliation>Photoacoustics and Ultrasound Laboratory Eindhoven (PULS/e), Department of Biomedical Engineering, Eindhoven University of Technology, 5612 AZ Eindhoven, The Netherlands.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>30</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Healthcare (Basel)</MedlineTA><NlmUniqueID>101666525</NlmUniqueID><ISSNLinking>2227-9032</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">bandwidth (BW) improvement</Keyword><Keyword MajorTopicYN="N">convolutional neural networks</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword><Keyword MajorTopicYN="N">image reconstruction</Keyword><Keyword MajorTopicYN="N">ultrasound imaging</Keyword></KeywordList><CoiStatement>The authors have no conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>12</Month><Day>6</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>24</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>29</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>1</Hour><Minute>4</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36611583</ArticleId><ArticleId IdType="pmc">PMC9819580</ArticleId><ArticleId IdType="doi">10.3390/healthcare11010123</ArticleId><ArticleId IdType="pii">healthcare11010123</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Carovac A., Smajlovic F., Junuzovic D. Application of Ultrasound in Medicine. Acta Inform. Medica. 2011;19:168&#x2013;171. doi: 10.5455/aim.2011.19.168-171.</Citation><ArticleIdList><ArticleId IdType="doi">10.5455/aim.2011.19.168-171</ArticleId><ArticleId IdType="pmc">PMC3564184</ArticleId><ArticleId IdType="pubmed">23408755</ArticleId></ArticleIdList></Reference><Reference><Citation>Wells P.N. Ultrasound imaging. Phys. Med. Biol. 2006;51:R83. doi: 10.1088/0031-9155/51/13/R06.</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/0031-9155/51/13/R06</ArticleId><ArticleId IdType="pubmed">16790922</ArticleId></ArticleIdList></Reference><Reference><Citation>Cikes M., Tong L., Sutherland G.R., D&#x2019;Hooge J. Ultrafast cardiac ultrasound imaging: Technical principles, applications, and clinical benefits. JACC Cardiovasc. Imaging. 2014;7:812&#x2013;823. doi: 10.1016/j.jcmg.2014.06.004.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jcmg.2014.06.004</ArticleId><ArticleId IdType="pubmed">25124014</ArticleId></ArticleIdList></Reference><Reference><Citation>Nicolau C., Ripoll&#xe9;s T. Contrast-enhanced ultrasound in abdominal imaging. Abdom. Imaging. 2012;37:1&#x2013;19. doi: 10.1007/s00261-011-9796-8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00261-011-9796-8</ArticleId><ArticleId IdType="pubmed">21879317</ArticleId></ArticleIdList></Reference><Reference><Citation>Woo J. A short history of the development of ultrasound in obstetrics and gynecology. Hist. Ultrasound Obstet. Gynecol. 2002;3:1&#x2013;25.</Citation></Reference><Reference><Citation>Szabo T.L., Lewin P.A. Ultrasound transducer selection in clinical imaging practice. J. Ultrasound Med. 2013;32:573&#x2013;582. doi: 10.7863/jum.2013.32.4.573.</Citation><ArticleIdList><ArticleId IdType="doi">10.7863/jum.2013.32.4.573</ArticleId><ArticleId IdType="pubmed">23525382</ArticleId></ArticleIdList></Reference><Reference><Citation>Szabo T.L. Diagnostic Ultrasound Imaging: Inside Out. Academic Press; New York, NY, USA: 2004.</Citation></Reference><Reference><Citation>Szabo T.L., Lewin P.A. Piezoelectric materials for imaging. J. Ultrasound Med. 2007;26:283&#x2013;288. doi: 10.7863/jum.2007.26.3.283.</Citation><ArticleIdList><ArticleId IdType="doi">10.7863/jum.2007.26.3.283</ArticleId><ArticleId IdType="pubmed">17324977</ArticleId></ArticleIdList></Reference><Reference><Citation>Reid J., Lewin P. Ultrasound imaging transducers. Encycl. Electr. Electron. Eng. 1999;22:664&#x2013;672.</Citation></Reference><Reference><Citation>Maresca D., Renaud G., van Soest G., Li X., Zhou Q., Shung K.K., De Jong N., Van der Steen A.F. Contrast-Enhanced Intravascular Ultrasound Pulse Sequences for Bandwidth-Limited Transducers. Ultrasound Med. Biol. 2013;39:706&#x2013;713. doi: 10.1016/j.ultrasmedbio.2012.10.020.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ultrasmedbio.2012.10.020</ArticleId><ArticleId IdType="pmc">PMC3760231</ArticleId><ArticleId IdType="pubmed">23384459</ArticleId></ArticleIdList></Reference><Reference><Citation>Wong C.M., Chen Y., Luo H., Dai J., Lam K.H., Chan H.L.W. Development of a 20-MHz wide-bandwidth PMN-PT single crystal phased-array ultrasound transducer. Ultrasonics. 2017;73:181&#x2013;186. doi: 10.1016/j.ultras.2016.09.012.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ultras.2016.09.012</ArticleId><ArticleId IdType="pubmed">27664869</ArticleId></ArticleIdList></Reference><Reference><Citation>Foster F., Zhang M., Zhou Y., Liu G., Mehi J., Cherin E., Harasiewicz K., Starkoski B., Zan L., Knapik D., et al. A new ultrasound instrument for in vivo microimaging of mice. Ultrasound Med. Biol. 2002;28:1165&#x2013;1172. doi: 10.1016/S0301-5629(02)00567-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0301-5629(02)00567-7</ArticleId><ArticleId IdType="pubmed">12401387</ArticleId></ArticleIdList></Reference><Reference><Citation>Rashid M.W., Carpenter T., Tekes C., Pirouz A., Jung G., Cowell D., Freear S., Ghovanloo M., Degertekin F.L. Front-end electronics for cable reduction in Intracardiac Echocardiography (ICE) catheters; Proceedings of the 2016 IEEE International Ultrasonics Symposium, IUS; Tours, France. 18&#x2013;21 September 2016;</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ULTSYM.2016.7728506</ArticleId></ArticleIdList></Reference><Reference><Citation>Benane Y.M., Lavarello R., Bujoreanu D., Cachard C., Varray F., Savoia A.S., Franceschini E., Basset O. Ultrasound bandwidth enhancement through pulse compression using a CMUT probe; Proceedings of the 2017 IEEE International Ultrasonics Symposium, IUS; Washington, DC, USA. 6&#x2013;9 September 2017;</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ULTSYM.2017.8092839</ArticleId></ArticleIdList></Reference><Reference><Citation>Costa-Felix R., Machado J.C. Output bandwidth enhancement of a pulsed ultrasound system using a flat envelope and compensated frequency-modulated input signal: Theory and experimental applications. Measurement. 2015;69:146&#x2013;154. doi: 10.1016/j.measurement.2015.03.019.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.measurement.2015.03.019</ArticleId></ArticleIdList></Reference><Reference><Citation>Sainath T.N., Kingsbury B., Saon G., Soltau H., Mohamed A.R., Dahl G., Ramabhadran B. Deep Convolutional Neural Networks for Large-scale Speech Tasks. Neural Netw. 2015;64:39&#x2013;48. doi: 10.1016/j.neunet.2014.08.005.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neunet.2014.08.005</ArticleId><ArticleId IdType="pubmed">25439765</ArticleId></ArticleIdList></Reference><Reference><Citation>Bordes A., Chopra S., Weston J. Question answering with subgraph embeddings; Proceedings of the EMNLP 2014&#x2014;2014 Conference on Empirical Methods in Natural Language Processing; Doha, Qatar. 25&#x2013;29 October 2014;</Citation><ArticleIdList><ArticleId IdType="doi">10.3115/v1/d14-1067</ArticleId></ArticleIdList></Reference><Reference><Citation>Rastegari M., Ordonez V., Redmon J., Farhadi A. Computer Vision&#x2014;ECCV 2016, Proceedings of the 14th European Conference, Amsterdam, The Netherlands, 11&#x2013;14 October 2016. Springer; Cham, Switzerland: 2016. XNOR-net: Imagenet classification using binary convolutional neural networks.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-319-46493-0_32</ArticleId></ArticleIdList></Reference><Reference><Citation>Minaee S., Boykov Y.Y., Porikli F., Plaza A.J., Kehtarnavaz N., Terzopoulos D. Image segmentation using deep learning: A survey. IEEE Trans. Pattern Anal. Mach. Intell. 2022;44:3523&#x2013;3542. doi: 10.1109/TPAMI.2021.3059968.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TPAMI.2021.3059968</ArticleId><ArticleId IdType="pubmed">33596172</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang Y., Li K., Li K., Zhong B., Fu Y. Residual non-local attention networks for image restoration; Proceedings of the 7th International Conference on Learning Representations, ICLR 2019; New Orleans, LA, USA. 6&#x2013;9 May 2019.</Citation></Reference><Reference><Citation>Tai Y., Yang J., Liu X. Image super-resolution via deep recursive residual network; Proceedings of the 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017; Honolulu, HI, USA. 21&#x2013;26 July 2017;</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/CVPR.2017.298</ArticleId></ArticleIdList></Reference><Reference><Citation>Cheng G., Matsune A., Li Q., Zhu L., Zang H., Zhan S. Encoder-decoder residual network for real super-resolution; Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops; Long Beach, CA, USA. 16&#x2013;17 June 2019;</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/CVPRW.2019.00270</ArticleId></ArticleIdList></Reference><Reference><Citation>Mao X.J., Shen C., Yang Y.B. Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections; Proceedings of the Advances in Neural Information Processing Systems; Barcelona, Spain. 5&#x2013;10 December 2016.</Citation></Reference><Reference><Citation>Shiri I., Akhavanallaf A., Sanaat A., Salimi Y., Askari D., Mansouri Z., Shayesteh S.P., Hasanian M., Rezaei-Kalantari K., Salahshour A., et al. Ultra-low-dose chest CT imaging of COVID-19 patients using a deep residual neural network. Eur. Radiol. 2021;31:1420&#x2013;1431. doi: 10.1007/s00330-020-07225-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-020-07225-6</ArticleId><ArticleId IdType="pmc">PMC7467843</ArticleId><ArticleId IdType="pubmed">32879987</ArticleId></ArticleIdList></Reference><Reference><Citation>Christensen-Jeffries K., Couture O., Dayton P.A., Eldar Y.C., Hynynen K., Kiessling F., O&#x2019;Reilly M., Pinton G.F., Schmitz G., Tang M.X., et al. Super-resolution ultrasound imaging. Ultrasound Med. Biol. 2020;46:865&#x2013;891. doi: 10.1016/j.ultrasmedbio.2019.11.013.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ultrasmedbio.2019.11.013</ArticleId><ArticleId IdType="pmc">PMC8388823</ArticleId><ArticleId IdType="pubmed">31973952</ArticleId></ArticleIdList></Reference><Reference><Citation>Van Sloun R.J., Cohen R., Eldar Y.C. Deep learning in ultrasound imaging. Proc. IEEE. 2019;108:11&#x2013;29. doi: 10.1109/JPROC.2019.2932116.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/JPROC.2019.2932116</ArticleId></ArticleIdList></Reference><Reference><Citation>Awasthi N., Vermeer L., Fixsen L.S., Lopata R.G., Pluim J.P. LVNet: Lightweight Model for Left Ventricle Segmentation for Short Axis Views in Echocardiographic Imaging. IEEE Trans. Ultrason. Ferroelectr. Freq. Control. 2022;69:2115&#x2013;2128. doi: 10.1109/TUFFC.2022.3169684.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TUFFC.2022.3169684</ArticleId><ArticleId IdType="pubmed">35452387</ArticleId></ArticleIdList></Reference><Reference><Citation>Awasthi N., Dayal A., Cenkeramaddi L.R., Yalavarthy P.K. Mini-COVIDNet: Efficient lightweight deep neural network for ultrasound based point-of-care detection of COVID-19. IEEE Trans. Ultrason. Ferroelectr. Freq. Control. 2021;68:2023&#x2013;2037. doi: 10.1109/TUFFC.2021.3068190.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TUFFC.2021.3068190</ArticleId><ArticleId IdType="pmc">PMC8544932</ArticleId><ArticleId IdType="pubmed">33755565</ArticleId></ArticleIdList></Reference><Reference><Citation>Chaudhari A.S., Fang Z., Kogan F., Wood J., Stevens K.J., Gibbons E.K., Lee J.H., Gold G.E., Hargreaves B.A. Super-resolution musculoskeletal MRI using deep learning. Magn. Reson. Med. 2018;80:2139&#x2013;2154. doi: 10.1002/mrm.27178.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.27178</ArticleId><ArticleId IdType="pmc">PMC6107420</ArticleId><ArticleId IdType="pubmed">29582464</ArticleId></ArticleIdList></Reference><Reference><Citation>Chun J., Zhang H., Gach H.M., Olberg S., Mazur T., Green O., Kim T., Kim H., Kim J.S., Mutic S., et al. MRI super-resolution reconstruction for MRI-guided adaptive radiotherapy using cascaded deep learning: In the presence of limited training data and unknown translation model. Med. Phys. 2019;46:4148&#x2013;4164. doi: 10.1002/mp.13717.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mp.13717</ArticleId><ArticleId IdType="pubmed">31309585</ArticleId></ArticleIdList></Reference><Reference><Citation>Perdios D., Vonlanthen M., Martinez F., Arditi M., Thiran J.P. CNN-based image reconstruction method for ultrafast ultrasound imaging. IEEE Trans. Ultrason. Ferroelectr. Freq. Control. 2021;69:1154&#x2013;1168. doi: 10.1109/TUFFC.2021.3131383.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TUFFC.2021.3131383</ArticleId><ArticleId IdType="pubmed">34847025</ArticleId></ArticleIdList></Reference><Reference><Citation>Yoon Y.H., Khan S., Huh J., Ye J.C. Efficient B-Mode Ultrasound Image Reconstruction From Sub-Sampled RF Data Using Deep Learning. IEEE Trans. Med. Imaging. 2019;38:325&#x2013;336. doi: 10.1109/TMI.2018.2864821.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2018.2864821</ArticleId><ArticleId IdType="pubmed">30106712</ArticleId></ArticleIdList></Reference><Reference><Citation>Dong C., Loy C.C., He K., Tang X. Image Super-Resolution Using Deep Convolutional Networks. IEEE Trans. Pattern Anal. Mach. Intell. 2016;38:295&#x2013;307. doi: 10.1109/TPAMI.2015.2439281.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TPAMI.2015.2439281</ArticleId><ArticleId IdType="pubmed">26761735</ArticleId></ArticleIdList></Reference><Reference><Citation>Ronneberger O., Fischer P., Brox T. Medical Image Computing and Computer-Assisted Intervention, Proceedings of the MICCAI 2015&#x2014;18th International Conference, Munich, Germany, 5&#x2013;9 October 2015. Springer; Cham, Switzerland: 2015. U-net: Convolutional networks for biomedical image segmentation.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-319-24574-4_28</ArticleId></ArticleIdList></Reference><Reference><Citation>Gholizadeh-Ansari M., Alirezaie J., Babyn P. Deep learning for low-dose CT denoising using perceptual loss and edge detection layer. J. Digit. Imaging. 2020;33:504&#x2013;515. doi: 10.1007/s10278-019-00274-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10278-019-00274-4</ArticleId><ArticleId IdType="pmc">PMC7165209</ArticleId><ArticleId IdType="pubmed">31515756</ArticleId></ArticleIdList></Reference><Reference><Citation>Heinrich M.P., Stille M., Buzug T.M. Residual U-Net convolutional neural network architecture for low-dose CT denoising. Curr. Dir. Biomed. Eng. 2018;4:297&#x2013;300. doi: 10.1515/cdbme-2018-0072.</Citation><ArticleIdList><ArticleId IdType="doi">10.1515/cdbme-2018-0072</ArticleId></ArticleIdList></Reference><Reference><Citation>Awasthi N., Jain G., Kalva S.K., Pramanik M., Yalavarthy P.K. Deep neural network-based sinogram super-resolution and bandwidth enhancement for limited-data photoacoustic tomography. IEEE Trans. Ultrason. Ferroelectr. Freq. Control. 2020;67:2660&#x2013;2673. doi: 10.1109/TUFFC.2020.2977210.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TUFFC.2020.2977210</ArticleId><ArticleId IdType="pubmed">32142429</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang K., Zuo W., Gu S., Zhang L. Learning deep CNN denoiser prior for image restoration; Proceedings of the 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017; Honolulu, HI, USA. 21&#x2013;26 July 2017;</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/CVPR.2017.300</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang Q., Ma Y., Zhao K., Tian Y. A comprehensive survey of loss functions in machine learning. Ann. Data Sci. 2022;9:187&#x2013;212. doi: 10.1007/s40745-020-00253-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s40745-020-00253-5</ArticleId></ArticleIdList></Reference><Reference><Citation>Micikevicius P., Narang S., Alben J., Diamos G., Elsen E., Garcia D., Ginsburg B., Houston M., Kuchaiev O., Venkatesh G., et al. Mixed precision training. arXiv. 20171710.03740</Citation></Reference><Reference><Citation>Abadi M., Agarwal A., Barham P., Brevdo E., Chen Z., Citro C., Corrado G.S., Davis A., Dean J., Devin M., et al.  TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems, 2015. Software.  [(accessed on 1 March 2021)].  Available online:  https://tensorflow.org.</Citation></Reference><Reference><Citation>Chollet F. Keras. 2015.  [(accessed on 1 March 2021)].  Available online:  https://keras.io.</Citation></Reference><Reference><Citation>Kingma D.P., Ba J. Adam: A method for stochastic optimization. arXiv. 20141412.6980</Citation></Reference><Reference><Citation>Jansen G., Awasthi N., Schwab H.M., Lopata R. Enhanced Radon Domain Beamforming Using Deep-Learning-Based Plane Wave Compounding; Proceedings of the 2021 IEEE International Ultrasonics Symposium (IUS); Xi&#x2019;an, China. 11&#x2013;16 September 2021; pp. 1&#x2013;4.</Citation></Reference><Reference><Citation>Parker N., Povey M. Ultrasonic study of the gelation of gelatin: Phase diagram, hysteresis and kinetics. Food Hydrocoll. 2012;26:99&#x2013;107. doi: 10.1016/j.foodhyd.2011.04.016.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.foodhyd.2011.04.016</ArticleId></ArticleIdList></Reference><Reference><Citation>Li Y., Wang W., Yu D. Application of adaptive histogram equalization to X-ray chest images; Proceedings of the Second International Conference on Optoelectronic Science and Engineering&#x2019;94; Beijing, China. 15&#x2013;18 August 1994; pp. 513&#x2013;514.</Citation></Reference><Reference><Citation>Zimmerman J.B., Pizer S.M., Staab E.V., Perry J.R., McCartney W., Brenton B.C. An evaluation of the effectiveness of adaptive histogram equalization for contrast enhancement. IEEE Trans. Med. Imaging. 1988;7:304&#x2013;312. doi: 10.1109/42.14513.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/42.14513</ArticleId><ArticleId IdType="pubmed">18230483</ArticleId></ArticleIdList></Reference><Reference><Citation>Lim J.S. Two-Dimensional Signal and Image Processing. Prentice Hall; Englewood Cliffs, NJ, USA: 1990.</Citation></Reference><Reference><Citation>Gonzalez R.C., Woods R.E. Digital Image Processing. Pearson Education (Singapore) Pte. Ltd.; Delhi, India: 2002.</Citation></Reference><Reference><Citation>Kim Y.T. Contrast enhancement using brightness preserving bi-histogram equalization. IEEE Trans. Consum. Electron. 1997;43:1&#x2013;8.</Citation></Reference><Reference><Citation>Zuiderveld K. Graphics Gems. Academic Press Professional, Inc.; Cambridge, MA, USA: 1994. Contrast limited adaptive histogram equalization; pp. 474&#x2013;485.</Citation></Reference><Reference><Citation>Pai P.P., De A., Banerjee S. Accuracy enhancement for noninvasive glucose estimation using dual-wavelength photoacoustic measurements and kernel-based calibration. IEEE Trans. Instrum. Meas. 2018;67:126&#x2013;136. doi: 10.1109/TIM.2017.2761237.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TIM.2017.2761237</ArticleId></ArticleIdList></Reference><Reference><Citation>Hor&#xe9; A., Ziou D. Image quality metrics: PSNR vs. SSIM; Proceedings of the 2010 International Conference on Pattern Recognition; Istanbul, Turkey. 23&#x2013;26 August 2010;</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ICPR.2010.579</ArticleId></ArticleIdList></Reference><Reference><Citation>Awasthi N., Kalva S.K., Pramanik M., Yalavarthy P.K. Dimensionality reduced plug and play priors for improving photoacoustic tomographic imaging with limited noisy data. Biomed. Opt. Express. 2021;12:1320&#x2013;1338. doi: 10.1364/BOE.415182.</Citation><ArticleIdList><ArticleId IdType="doi">10.1364/BOE.415182</ArticleId><ArticleId IdType="pmc">PMC7984800</ArticleId><ArticleId IdType="pubmed">33796356</ArticleId></ArticleIdList></Reference><Reference><Citation>Benesty J., Chen J., Huang Y., Cohen I. Noise Reduction in Speech Processing. Springer; Berlin/Heidelberg, Germany: 2009. Pearson correlation coefficient; pp. 1&#x2013;4.</Citation></Reference><Reference><Citation>Yang X., Wang S., Han J., Guo Y., Li T. RSAMSR: A deep neural network based on residual self-encoding and attention mechanism for image super-resolution. Optik. 2021;245:167736. doi: 10.1016/j.ijleo.2021.167736.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ijleo.2021.167736</ArticleId></ArticleIdList></Reference><Reference><Citation>Yang X., Zhu Y., Guo Y., Zhou D. An image super-resolution network based on multi-scale convolution fusion. Vis. Comput. 2022;38:4307&#x2013;4317. doi: 10.1007/s00371-021-02297-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00371-021-02297-x</ArticleId></ArticleIdList></Reference><Reference><Citation>Yang X., Fan J., Wu C., Zhou D., Li T. NasmamSR: A fast image super-resolution network based on neural architecture search and multiple attention mechanism. Multimed. Syst. 2022;28:321&#x2013;334. doi: 10.1007/s00530-021-00841-2.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00530-021-00841-2</ArticleId></ArticleIdList></Reference><Reference><Citation>Xu J., Zhou W., Chen Z., Ling S., Le Callet P. Binocular rivalry oriented predictive autoencoding network for blind stereoscopic image quality measurement. IEEE Trans. Instrum. Meas. 2020;70:1&#x2013;13. doi: 10.1109/TIM.2020.3026443.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TIM.2020.3026443</ArticleId><ArticleId IdType="pubmed">0</ArticleId></ArticleIdList></Reference><Reference><Citation>Mishra D., Singh S.K., Singh R.K. Wavelet-based deep auto encoder-decoder (wdaed)-based image compression. IEEE Trans. Circuits Syst. Video Technol. 2020;31:1452&#x2013;1462. doi: 10.1109/TCSVT.2020.3010627.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TCSVT.2020.3010627</ArticleId></ArticleIdList></Reference><Reference><Citation>Damerjian V., Tankyevych O., Souag N., Petit E. Speckle characterization methods in ultrasound images&#x2014;A review. IRBM. 2014;35:202&#x2013;213. doi: 10.1016/j.irbm.2014.05.003.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.irbm.2014.05.003</ArticleId></ArticleIdList></Reference><Reference><Citation>Sajjadi M.S., Scholkopf B., Hirsch M. EnhanceNet: Single image super-resolution through automated texture synthesis; Proceedings of the 2017 IEEE International Conference on Computer Vision; Venice, Italy. 22&#x2013;29 October 2017;</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ICCV.2017.481</ArticleId></ArticleIdList></Reference><Reference><Citation>Agostinelli F., Anderson M.R., Lee H. Adaptive multi-column deep neural networks with application to robust image denoising; Proceedings of the Advances in Neural Information Processing Systems 2013; Lake Tahoe, NV, USA. 5&#x2013;10 December 2013.</Citation></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36611573</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Print">2227-9032</ISSN><JournalIssue CitedMedium="Print"><Volume>11</Volume><Issue>1</Issue><PubDate><Year>2022</Year><Month>Dec</Month><Day>30</Day></PubDate></JournalIssue><Title>Healthcare (Basel, Switzerland)</Title><ISOAbbreviation>Healthcare (Basel)</ISOAbbreviation></Journal><ArticleTitle>Artificial-Intelligence-Based Decision Making for Oral Potentially Malignant Disorder Diagnosis in Internet of Medical Things Environment.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">113</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/healthcare11010113</ELocationID><Abstract><AbstractText>Oral cancer is considered one of the most common cancer types in several counties. Earlier-stage identification is essential for better prognosis, treatment, and survival. To enhance precision medicine, Internet of Medical Things (IoMT) and deep learning (DL) models can be developed for automated oral cancer classification to improve detection rate and decrease cancer-specific mortality. This article focuses on the design of an optimal Inception-Deep Convolution Neural Network for Oral Potentially Malignant Disorder Detection (OIDCNN-OPMDD) technique in the IoMT environment. The presented OIDCNN-OPMDD technique mainly concentrates on identifying and classifying oral cancer by using an IoMT device-based data collection process. In this study, the feature extraction and classification process are performed using the IDCNN model, which integrates the Inception module with DCNN. To enhance the classification performance of the IDCNN model, the moth flame optimization (MFO) technique can be employed. The experimental results of the OIDCNN-OPMDD technique are investigated, and the results are inspected under specific measures. The experimental outcome pointed out the enhanced performance of the OIDCNN-OPMDD model over other DL models.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Alabdan</LastName><ForeName>Rana</ForeName><Initials>R</Initials><Identifier Source="ORCID">0000-0003-2410-486X</Identifier><AffiliationInfo><Affiliation>Department of Information Systems, College of Computer and Information Science, Majmaah University, Majmaah 11952, Saudi Arabia.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Alruban</LastName><ForeName>Abdulrahman</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Department of Information Technology, College of Computer and Information Sciences, Majmaah University, Majmaah 11952, Saudi Arabia.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Hilal</LastName><ForeName>Anwer Mustafa</ForeName><Initials>AM</Initials><AffiliationInfo><Affiliation>Department of Computer and Self Development, Preparatory Year Deanship, Prince Sattam bin Abdulaziz University, AlKharj 16278, Saudi Arabia.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Motwakel</LastName><ForeName>Abdelwahed</ForeName><Initials>A</Initials><Identifier Source="ORCID">0000-0003-4084-5457</Identifier><AffiliationInfo><Affiliation>Department of Information Systems, College of Business Administration in Hawtat bani Tamim, Prince Sattam bin Abdulaziz University, AlKharj 16278, Saudi Arabia.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>30</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Healthcare (Basel)</MedlineTA><NlmUniqueID>101666525</NlmUniqueID><ISSNLinking>2227-9032</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Inception model</Keyword><Keyword MajorTopicYN="N">Internet of Medical Things</Keyword><Keyword MajorTopicYN="N">artificial intelligence</Keyword><Keyword MajorTopicYN="N">biomedical imaging</Keyword><Keyword MajorTopicYN="N">hybrid deep learning</Keyword><Keyword MajorTopicYN="N">oral cancer</Keyword></KeywordList><CoiStatement>The authors declare that they have no conflict of interest. The manuscript was written through contributions of all authors. All authors have given approval for the final version of the manuscript.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>12</Month><Day>11</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>25</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>26</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>1</Hour><Minute>4</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36611573</ArticleId><ArticleId IdType="pmc">PMC9818760</ArticleId><ArticleId IdType="doi">10.3390/healthcare11010113</ArticleId><ArticleId IdType="pii">healthcare11010113</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Kim Y., Kang J.W., Kang J., Kwon E.J., Ha M., Kim Y.K., Lee H., Rhee J.K., Kim Y.H. Novel deep learning-based survival prediction for oral cancer by analyzing tumor-infiltrating lymphocyte profiles through CIBERSORT. Oncoimmunology. 2021;10:1904573. doi: 10.1080/2162402X.2021.1904573.</Citation><ArticleIdList><ArticleId IdType="doi">10.1080/2162402X.2021.1904573</ArticleId><ArticleId IdType="pmc">PMC8018482</ArticleId><ArticleId IdType="pubmed">33854823</ArticleId></ArticleIdList></Reference><Reference><Citation>Song B., Sunny S., Uthoff R.D., Patrick S., Suresh A., Kolur T., Keerthi G., Anbarani A., Wilder-Smith P., Kuriakose M.A., et al. Automatic classification of dual-modalilty, smartphone-based oral dysplasia and malignancy images using deep learning. Biomed. Opt. Express. 2018;9:5318&#x2013;5329. doi: 10.1364/BOE.9.005318.</Citation><ArticleIdList><ArticleId IdType="doi">10.1364/BOE.9.005318</ArticleId><ArticleId IdType="pmc">PMC6238918</ArticleId><ArticleId IdType="pubmed">30460130</ArticleId></ArticleIdList></Reference><Reference><Citation>Chu C., Lee N., Ho J., Choi S., Thomson P. Deep learning for clinical image analyses in oral squamous cell carcinoma: A review. JAMA Otolaryngol. Head Neck Surg. 2021;147:893&#x2013;900. doi: 10.1001/jamaoto.2021.2028.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jamaoto.2021.2028</ArticleId><ArticleId IdType="pubmed">34410314</ArticleId></ArticleIdList></Reference><Reference><Citation>Alabi R.O., Almangush A., Elmusrati M., Leivo I., M&#xe4;kitie A. Measuring the usability and quality of explanations of a machine learning web-based tool for Oral Tongue Cancer Prognostication. Int. J. Environ. Res. Public Health. 2022;19:8366. doi: 10.3390/ijerph19148366.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/ijerph19148366</ArticleId><ArticleId IdType="pmc">PMC9322510</ArticleId><ArticleId IdType="pubmed">35886221</ArticleId></ArticleIdList></Reference><Reference><Citation>Saraswat N., Pillay R., Prabhu N., Everett B., George A. Perceptions and practices of general practitioners towards oral cancer and emerging risk factors among Indian immigrants in Australia: A qualitative study. Int. J. Environ. Res. Public Health. 2021;18:11111. doi: 10.3390/ijerph182111111.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/ijerph182111111</ArticleId><ArticleId IdType="pmc">PMC8582889</ArticleId><ArticleId IdType="pubmed">34769631</ArticleId></ArticleIdList></Reference><Reference><Citation>Adeoye J., Choi S.W., Thomson P. Bayesian disease mapping and The &#x2018;high-risk&#x2019; oral cancer population in Hong Kong. J. Oral Pathol. Med. 2020;49:907&#x2013;913. doi: 10.1111/jop.13045.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/jop.13045</ArticleId><ArticleId IdType="pubmed">32450000</ArticleId></ArticleIdList></Reference><Reference><Citation>Calado G., Behl I., Daniel A., Byrne H.J., Lyng F.M. Raman spectroscopic analysis of Saliva for the diagnosis of oral cancer: A systematic review. Transl. Biophotonics. 2019;1:e201900001. doi: 10.1002/tbio.201900001.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/tbio.201900001</ArticleId></ArticleIdList></Reference><Reference><Citation>Ariji Y., Fukuda M., Kise Y., Nozawa M., Yanashita Y., Fujita H., Katsumata A., Ariji E. Contrast-enhanced computed tomography image assessment of cervical lymph node metastasis in patients with oral cancer by using a deep learning system of artificial intelligence. Oral Surg. Oral Med. Oral Pathol. Oral Radiol. 2019;127:458&#x2013;463. doi: 10.1016/j.oooo.2018.10.002.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.oooo.2018.10.002</ArticleId><ArticleId IdType="pubmed">30497907</ArticleId></ArticleIdList></Reference><Reference><Citation>Azimi S., Ghorbani Z., Tennant M., Kruger E., Safiaghdam H., Rafieian N. Population survey of knowledge about oral cancer and related factors in the capital of Iran. J. Cancer Educ. 2017;34:116&#x2013;123. doi: 10.1007/s13187-017-1275-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s13187-017-1275-7</ArticleId><ArticleId IdType="pubmed">28840479</ArticleId></ArticleIdList></Reference><Reference><Citation>Jeyaraj P., Nadar E.S. Computer-assisted medical image classification for early diagnosis of oral cancer employing deep learning algorithm. J. Cancer Res. Clin. Oncol. 2019;145:829&#x2013;837. doi: 10.1007/s00432-018-02834-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00432-018-02834-7</ArticleId><ArticleId IdType="pubmed">30603908</ArticleId></ArticleIdList></Reference><Reference><Citation>Rahman A.U., Alqahtani A., Aldhafferi N., Nasir M.U., Khan M.F., Khan M.A., Mosavi A. Histopathologic Oral Cancer Prediction Using Oral Squamous Cell Carcinoma Biopsy Empowered with Transfer Learning. Sensors. 2022;22:3833. doi: 10.3390/s22103833.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s22103833</ArticleId><ArticleId IdType="pmc">PMC9146317</ArticleId><ArticleId IdType="pubmed">35632242</ArticleId></ArticleIdList></Reference><Reference><Citation>Figueroa K.C., Song B., Sunny S., Li S., Gurushanth K., Mendonca P., Mukhia N., Patrick S., Gurudath S., Raghavan S., et al. Interpretable deep learning approach for oral cancer classification using guided attention inference network. J. Biomed. Opt. 2022;27:015001. doi: 10.1117/1.JBO.27.1.015001.</Citation><ArticleIdList><ArticleId IdType="doi">10.1117/1.JBO.27.1.015001</ArticleId><ArticleId IdType="pmc">PMC8754153</ArticleId><ArticleId IdType="pubmed">35023333</ArticleId></ArticleIdList></Reference><Reference><Citation>Jubair F., Al-karadsheh O., Malamos D., Al Mahdi S., Saad Y., Hassona Y. A novel lightweight deep convolutional neural network for early detection of oral cancer. Oral Dis. 2022;28:1123&#x2013;1130. doi: 10.1111/odi.13825.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/odi.13825</ArticleId><ArticleId IdType="pubmed">33636041</ArticleId></ArticleIdList></Reference><Reference><Citation>Song B., Sunny S., Li S., Gurushanth K., Mendonca P., Mukhia N., Patrick S., Gurudath S., Raghavan S., Tsusennaro I., et al. Bayesian deep learning for reliable oral cancer image classification. Biomed. Opt. Express. 2021;12:6422&#x2013;6430. doi: 10.1364/BOE.432365.</Citation><ArticleIdList><ArticleId IdType="doi">10.1364/BOE.432365</ArticleId><ArticleId IdType="pmc">PMC8547976</ArticleId><ArticleId IdType="pubmed">34745746</ArticleId></ArticleIdList></Reference><Reference><Citation>Song B., Li S., Sunny S., Gurushanth K., Mendonca P., Mukhia N., Patrick S., Gurudath S., Raghavan S., Tsusennaro I., et al. Classification of imbalanced oral cancer image data from high-risk population. J. Biomed. Opt. 2021;26:105001. doi: 10.1117/1.JBO.26.10.105001.</Citation><ArticleIdList><ArticleId IdType="doi">10.1117/1.JBO.26.10.105001</ArticleId><ArticleId IdType="pmc">PMC8536945</ArticleId><ArticleId IdType="pubmed">34689442</ArticleId></ArticleIdList></Reference><Reference><Citation>Shamim M.Z., Syed S., Shiblee M., Usman M., Ali S.J., Hussein H.S., Farrag M. Automated detection of oral pre-cancerous tongue lesions using deep learning for early diagnosis of oral cavity cancer. Comput. J. 2022;65:91&#x2013;104. doi: 10.1093/comjnl/bxaa136.</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/comjnl/bxaa136</ArticleId></ArticleIdList></Reference><Reference><Citation>Camalan S., Mahmood H., Binol H., Ara&#xfa;jo A.L., Santos-Silva A.R., Vargas P.A., Lopes M.A., Khurram S.A., Gurcan M.N. Convolutional neural network-based clinical predictors of oral dysplasia: Class activation map analysis of deep learning results. Cancers. 2021;13:1291. doi: 10.3390/cancers13061291.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/cancers13061291</ArticleId><ArticleId IdType="pmc">PMC8001078</ArticleId><ArticleId IdType="pubmed">33799466</ArticleId></ArticleIdList></Reference><Reference><Citation>Rajan J.P., Rajan S.E., Martis R.J., Panigrahi B.K. Fog computing employed computer aided cancer classification system using deep neural network in internet of things based healthcare system. J. Med. Syst. 2020;44:34. doi: 10.1007/s10916-019-1500-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10916-019-1500-5</ArticleId><ArticleId IdType="pubmed">31853735</ArticleId></ArticleIdList></Reference><Reference><Citation>Tanriver G., Tekkesin M.S., Ergen O. Automated detection and classification of oral lesions using deep learning to detect oral potentially malignant disorders. Cancers. 2021;13:2766. doi: 10.3390/cancers13112766.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/cancers13112766</ArticleId><ArticleId IdType="pmc">PMC8199603</ArticleId><ArticleId IdType="pubmed">34199471</ArticleId></ArticleIdList></Reference><Reference><Citation>Bhandari B., Alsadoon A., Prasad P., Abdullah S., Haddad S. Deep learning neural network for texture feature extraction in oral cancer: Enhanced loss function. Multimed. Tools Appl. 2020;79:27867&#x2013;27890. doi: 10.1007/s11042-020-09384-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11042-020-09384-6</ArticleId></ArticleIdList></Reference><Reference><Citation>Chan C.H., Huang T.T., Chen C.Y., Lee C.C., Chan M.Y., Chung P.C. Texture-map-based branch-collaborative network for oral cancer detection. IEEE Trans. Biomed. Circuits Syst. 2019;13:766&#x2013;780. doi: 10.1109/TBCAS.2019.2918244.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TBCAS.2019.2918244</ArticleId><ArticleId IdType="pubmed">31135368</ArticleId></ArticleIdList></Reference><Reference><Citation>Mirzabagherian H., Sardari M.A., Menhaj M.B., Suratgar A.A. Classification of Raw Spinal Cord Injury EEG Data Based on the Temporal-Spatial Inception Deep Convolutional Neural Network; Proceedings of the 9th RSI International Conference on Robotics and Mechatronics (ICRoM); Prague, Czech Republic. 20&#x2013;22 April 2021; pp. 43&#x2013;50.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ICRoM54204.2021.9663528</ArticleId></ArticleIdList></Reference><Reference><Citation>Shehab M., Abualigah L., Al Hamad H., Alabool H., Alshinwan M., Khasawneh A.M. Moth&#x2013;flame optimization algorithm: Variants and applications. Neural Comput. Appl. 2020;32:9859&#x2013;9884. doi: 10.1007/s00521-019-04570-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00521-019-04570-6</ArticleId></ArticleIdList></Reference><Reference><Citation> [(accessed on 12 September 2022)].  Available online:  https://www.kaggle.com/datasets/shivam17299/oral-cancer-lips-and-tongue-images.</Citation></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36611515</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Print">2227-9032</ISSN><JournalIssue CitedMedium="Print"><Volume>11</Volume><Issue>1</Issue><PubDate><Year>2022</Year><Month>Dec</Month><Day>25</Day></PubDate></JournalIssue><Title>Healthcare (Basel, Switzerland)</Title><ISOAbbreviation>Healthcare (Basel)</ISOAbbreviation></Journal><ArticleTitle>Equilibrium Optimization Algorithm with Ensemble Learning Based Cervical Precancerous Lesion Classification Model.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">55</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/healthcare11010055</ELocationID><Abstract><AbstractText>Recently, artificial intelligence (AI) with deep learning (DL) and machine learning (ML) has been extensively used to automate labor-intensive and time-consuming work and to help in prognosis and diagnosis. AI's role in biomedical and biological imaging is an emerging field of research and reveals future trends. Cervical cell (CCL) classification is crucial in screening cervical cancer (CC) at an earlier stage. Unlike the traditional classification method, which depends on hand-engineered or crafted features, convolution neural network (CNN) usually categorizes CCLs through learned features. Moreover, the latent correlation of images might be disregarded in CNN feature learning and thereby influence the representative capability of the CNN feature. This study develops an equilibrium optimizer with ensemble learning-based cervical precancerous lesion classification on colposcopy images (EOEL-PCLCCI) technique. The presented EOEL-PCLCCI technique mainly focuses on identifying and classifying cervical cancer on colposcopy images. In the presented EOEL-PCLCCI technique, the DenseNet-264 architecture is used for the feature extractor, and the EO algorithm is applied as a hyperparameter optimizer. An ensemble of weighted voting classifications, namely long short-term memory (LSTM) and gated recurrent unit (GRU), is used for the classification process. A widespread simulation analysis is performed on a benchmark dataset to depict the superior performance of the EOEL-PCLCCI approach, and the results demonstrated the betterment of the EOEL-PCLCCI algorithm over other DL models.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>A Mansouri</LastName><ForeName>Rasha</ForeName><Initials>R</Initials><AffiliationInfo><Affiliation>Department of Biochemistry, Faculty of Sciences, King Abdulaziz University, Jeddah 21589, Saudi Arabia.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Ragab</LastName><ForeName>Mahmoud</ForeName><Initials>M</Initials><Identifier Source="ORCID">0000-0002-4427-0016</Identifier><AffiliationInfo><Affiliation>Information Technology Department, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah 21589, Saudi Arabia.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Mathematics, Faculty of Science, Al-Azhar University, Naser City, Cairo 11884, Egypt.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>25</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Healthcare (Basel)</MedlineTA><NlmUniqueID>101666525</NlmUniqueID><ISSNLinking>2227-9032</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">cervical cancer</Keyword><Keyword MajorTopicYN="N">decision making</Keyword><Keyword MajorTopicYN="N">ensemble learning</Keyword><Keyword MajorTopicYN="N">healthcare</Keyword><Keyword MajorTopicYN="N">medical imaging</Keyword></KeywordList><CoiStatement>The authors declare that they have no conflict of interest. The manuscript was written through the contributions of all authors. All authors have approved the final version of the manuscript.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>11</Month><Day>11</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>17</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>21</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>1</Hour><Minute>3</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36611515</ArticleId><ArticleId IdType="pmc">PMC9819283</ArticleId><ArticleId IdType="doi">10.3390/healthcare11010055</ArticleId><ArticleId IdType="pii">healthcare11010055</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Kuko M., Pourhomayoun M. Single and clustered cervical cell classification with the ensemble and deep learning methods. Inf. Syst. Front. 2020;22:1039&#x2013;1051. doi: 10.1007/s10796-020-10028-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10796-020-10028-1</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhao S., He Y., Qin J., Wang Z. A Semi-supervised Deep Learning Method for Cervical Cell Classification. Anal. Cell. Pathol. 2022;2022:4376178. doi: 10.1155/2022/4376178.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2022/4376178</ArticleId><ArticleId IdType="pmc">PMC8898884</ArticleId><ArticleId IdType="pubmed">35265455</ArticleId></ArticleIdList></Reference><Reference><Citation>Nirmal Jith O.U., Harinarayanan K.K., Gautam S., Bhavsar A., Sao A.K. Computational Pathology and Ophthalmic Medical Image Analysis. Springer; Cham, Switzerland: 2018. DeepCerv: Deep Neural Network for Segmentation Free Robust Cervical Cell Classification; pp. 86&#x2013;94.</Citation></Reference><Reference><Citation>Sompawong N., Mopan J., Pooprasert P., Himakhun W., Suwannarurk K., Ngamvirojcharoen J., Vachiramon T., Tantibundhit C. Automated Pap Smear Cervical Cancer Screening Using Deep Learning; Proceedings of the 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC); Berlin, Germany. 23&#x2013;27 July 2019; Piscataway, NJ, USA: IEEE; 2019. pp. 7044&#x2013;7048.</Citation><ArticleIdList><ArticleId IdType="pubmed">31947460</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen W., Gao L., Li X., Shen W. Lightweight convolutional neural network with knowledge distillation for cervical cells classification. Biomed. Signal Process. Control. 2022;71:103177. doi: 10.1016/j.bspc.2021.103177.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bspc.2021.103177</ArticleId></ArticleIdList></Reference><Reference><Citation>Rahaman M.M., Li C., Yao Y., Kulwa F., Wu X., Li X., Wang Q. DeepCervix: A deep learning-based framework for the classification of cervical cells using hybrid deep feature fusion techniques. Comput. Biol. Med. 2021;136:104649. doi: 10.1016/j.compbiomed.2021.104649.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2021.104649</ArticleId><ArticleId IdType="pubmed">34332347</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang C., Jia D., Li Z., Wu N. Auxiliary classification of cervical cells based on multi-domain hybrid deep learning framework. Biomed. Signal Process. Control. 2022;77:103739. doi: 10.1016/j.bspc.2022.103739.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bspc.2022.103739</ArticleId></ArticleIdList></Reference><Reference><Citation>Lin H., Hu Y., Chen S., Yao J., Zhang L. Fine-grained classification of cervical cells using morphological and appearance based convolutional neural networks. IEEE Access. 2019;7:71541&#x2013;71549. doi: 10.1109/ACCESS.2019.2919390.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2019.2919390</ArticleId></ArticleIdList></Reference><Reference><Citation>Tripathi A., Arora A., Bhan A. Classification of Cervical Cancer Using Deep Learning Algorithm; Proceedings of the 2021 5th International Conference on Intelligent Computing and Control Systems (ICICCS); Madurai, India. 6&#x2013;8 May 2021; Piscataway, NJ, USA: IEEE; 2021. pp. 1210&#x2013;1218.</Citation></Reference><Reference><Citation>Rahaman M.M., Li C., Wu X., Yao Y., Hu Z., Jiang T., Li X., Qi S. A survey for cervical cytopathology image analysis using deep learning. IEEE Access. 2020;8:61687&#x2013;61710. doi: 10.1109/ACCESS.2020.2983186.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2020.2983186</ArticleId></ArticleIdList></Reference><Reference><Citation>Ragab M., Albukhari A., Alyami J., Mansour R.F. Ensemble deep-learning-enabled clinical decision support system for breast cancer diagnosis and classification on ultrasound images. Biology. 2022;11:439. doi: 10.3390/biology11030439.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/biology11030439</ArticleId><ArticleId IdType="pmc">PMC8945718</ArticleId><ArticleId IdType="pubmed">35336813</ArticleId></ArticleIdList></Reference><Reference><Citation>Alsuhibany S.A., Abdel-Khalek S., Algarni A., Fayomi A., Gupta D., Kumar V., Mansour R.F. Ensemble of Deep Learning Based Clinical Decision Support System for Chronic Kidney Disease Diagnosis in Medical Internet of Things Environment. Comput. Intell. Neurosci. 2021;2021:4931450. doi: 10.1155/2021/4931450.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2021/4931450</ArticleId><ArticleId IdType="pmc">PMC8723860</ArticleId><ArticleId IdType="pubmed">34987566</ArticleId></ArticleIdList></Reference><Reference><Citation>Ragab M., Alshehri S., Alhakamy N.A., Mansour R.F., Koundal D. Multiclass Classification of Chest X-ray Images for the Prediction of COVID-19 Using Capsule Network. Comput. Intell. Neurosci. 2022;2022:6185013. doi: 10.1155/2022/6185013.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2022/6185013</ArticleId><ArticleId IdType="pmc">PMC9135545</ArticleId><ArticleId IdType="pubmed">35634055</ArticleId></ArticleIdList></Reference><Reference><Citation>Khamparia A., Gupta D., de Albuquerque V.H.C., Sangaiah A.K., Jhaveri R.H. Internet of health things-driven deep learning system for detection and classification of cervical cells using transfer learning. J. Supercomput. 2020;76:8590&#x2013;8608. doi: 10.1007/s11227-020-03159-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11227-020-03159-4</ArticleId></ArticleIdList></Reference><Reference><Citation>Shi J., Wang R., Zheng Y., Jiang Z., Zhang H., Yu L. Cervical cell classification with graph convolutional network. Comput. Methods Programs Biomed. 2021;198:105807. doi: 10.1016/j.cmpb.2020.105807.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cmpb.2020.105807</ArticleId><ArticleId IdType="pubmed">33130497</ArticleId></ArticleIdList></Reference><Reference><Citation>Allehaibi K.H.S., Nugroho L.E., Lazuardi L., Prabuwono A.S., Mantoro T. Segmentation and classification of cervical cells using deep learning. IEEE Access. 2019;7:116925&#x2013;116941.</Citation></Reference><Reference><Citation>Chen W., Li X., Gao L., Shen W. Improving computer-aided cervical cells classification using transfer learning based snapshot ensemble. Appl. Sci. 2020;10:7292. doi: 10.3390/app10207292.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/app10207292</ArticleId></ArticleIdList></Reference><Reference><Citation>Archana M.C.P., Panicker J.V. Deep Convolutional Neural Networks for Multiclass Cervical Cell Classification; Proceedings of the 2022 International Conference on Wireless Communications Signal Processing and Networking (WiSPNET); Chennai, India. 24&#x2013;26 March 2022; Piscataway, NJ, USA: IEEE; 2022. pp. 376&#x2013;380.</Citation></Reference><Reference><Citation>Dong N., Zhao L., Wu C.H., Chang J.F. Inception v3 based cervical cell classification combined with artificially extracted features. Appl. Soft Comput. 2020;93:106311. doi: 10.1016/j.asoc.2020.106311.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.asoc.2020.106311</ArticleId></ArticleIdList></Reference><Reference><Citation>Li J., Dou Q., Yang H., Liu J., Fu L., Zhang Y., Zheng L., Zhang D. Cervical cell multi-classification algorithm using global context information and attention mechanism. Tissue Cell. 2022;74:101677. doi: 10.1016/j.tice.2021.101677.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.tice.2021.101677</ArticleId><ArticleId IdType="pubmed">34814053</ArticleId></ArticleIdList></Reference><Reference><Citation>Zheng K., Zha Z.J., Cao Y., Chen X., Wu F. La-Net: Layout-Aware Dense Network for Monocular Depth Estimation; Proceedings of the 26th ACM international conference on Multimedia; Seoul, Republic of Korea. 22&#x2013;26 October 2018; pp. 1381&#x2013;1388.</Citation></Reference><Reference><Citation>Gupta S., Deep K., Mirjalili S. An efficient equilibrium optimizer with mutation strategy for numerical optimization. Appl. Soft Comput. 2020;96:106542. doi: 10.1016/j.asoc.2020.106542.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.asoc.2020.106542</ArticleId></ArticleIdList></Reference><Reference><Citation>Alzubi O.A., Qiqieh I., Alzubi J.A. Fusion of deep learning based cyberattack detection and classification model for intelligent systems. Clust. Comput. 2022:1&#x2013;12. doi: 10.1007/s10586-022-03686-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10586-022-03686-0</ArticleId></ArticleIdList></Reference><Reference><Citation> [(accessed on 10 November 2022)].  Available online:  http://mde-lab.aegean.gr/index.php/downloads.</Citation></Reference><Reference><Citation>Fang S., Yang J., Wang M., Liu C., Liu S. An Improved Image Classification Method for Cervical Precancerous Lesions Based on ShuffleNet. Comput. Intell. Neurosci. 2022;2022:9675628. doi: 10.1155/2022/9675628.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2022/9675628</ArticleId><ArticleId IdType="pmc">PMC9489397</ArticleId><ArticleId IdType="pubmed">36148422</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36611454</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Print">2075-4418</ISSN><JournalIssue CitedMedium="Print"><Volume>13</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>03</Day></PubDate></JournalIssue><Title>Diagnostics (Basel, Switzerland)</Title><ISOAbbreviation>Diagnostics (Basel)</ISOAbbreviation></Journal><ArticleTitle>A Holistic Approach to Identify and Classify COVID-19 from Chest Radiographs, ECG, and CT-Scan Images Using ShuffleNet Convolutional Neural Network.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">162</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/diagnostics13010162</ELocationID><Abstract><AbstractText>Early and precise COVID-19 identification and analysis are pivotal in reducing the spread of COVID-19. Medical imaging techniques, such as chest X-ray or chest radiographs, computed tomography (CT) scan, and electrocardiogram (ECG) trace images are the most widely known for early discovery and analysis of the coronavirus disease (COVID-19). Deep learning (DL) frameworks for identifying COVID-19 positive patients in the literature are limited to one data format, either ECG or chest radiograph images. Moreover, using several data types to recover abnormal patterns caused by COVID-19 could potentially provide more information and restrict the spread of the virus. This study presents an effective COVID-19 detection and classification approach using the Shufflenet CNN by employing three types of images, i.e., chest radiograph, CT-scan, and ECG-trace images. For this purpose, we performed extensive classification experiments with the proposed approach using each type of image. With the chest radiograph dataset, we performed three classification experiments at different levels of granularity, i.e., binary, three-class, and four-class classifications. In addition, we performed a binary classification experiment with the proposed approach by classifying CT-scan images into COVID-positive and normal. Finally, utilizing the ECG-trace images, we conducted three experiments at different levels of granularity, i.e., binary, three-class, and five-class classifications. We evaluated the proposed approach with the baseline COVID-19 Radiography Database, SARS-CoV-2 CT-scan, and ECG images dataset of cardiac and COVID-19 patients. The average accuracy of 99.98% for COVID-19 detection in the three-class classification scheme using chest radiographs, optimal accuracy of 100% for COVID-19 detection using CT scans, and average accuracy of 99.37% for five-class classification scheme using ECG trace images have proved the efficacy of our proposed method over the contemporary methods. The optimal accuracy of 100% for COVID-19 detection using CT scans and the accuracy gain of 1.54% (in the case of five-class classification using ECG trace images) from the previous approach, which utilized ECG images for the first time, has a major contribution to improving the COVID-19 prediction rate in early stages. Experimental findings demonstrate that the proposed framework outperforms contemporary models. For example, the proposed approach outperforms state-of-the-art DL approaches, such as Squeezenet, Alexnet, and Darknet19, by achieving the accuracy of 99.98 (proposed method), 98.29, 98.50, and 99.67, respectively.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Ullah</LastName><ForeName>Naeem</ForeName><Initials>N</Initials><AffiliationInfo><Affiliation>Department of Software Engineering, University of Engineering and Technology Taxila, Taxila 47050, Pakistan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Khan</LastName><ForeName>Javed Ali</ForeName><Initials>JA</Initials><Identifier Source="ORCID">0000-0003-3306-1195</Identifier><AffiliationInfo><Affiliation>Department of Software Engineering, University of Science and Technology Bannu, Bannu 28100, Pakistan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>El-Sappagh</LastName><ForeName>Shaker</ForeName><Initials>S</Initials><Identifier Source="ORCID">0000-0001-9705-1477</Identifier><AffiliationInfo><Affiliation>Faculty of Computer Science and Engineering, Galala University, Suez 435611, Egypt.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Information Systems Department, Faculty of Computers and Artificial Intelligence, Benha University, Banha 13518, Egypt.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>El-Rashidy</LastName><ForeName>Nora</ForeName><Initials>N</Initials><Identifier Source="ORCID">0000-0001-8177-9439</Identifier><AffiliationInfo><Affiliation>Department of Machine Learning and Information Retrieval, Faculty of Artificial Intelligence, Kafrelsheiksh University, Kafr Elsheikh 33516, Egypt.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Khan</LastName><ForeName>Mohammad Sohail</ForeName><Initials>MS</Initials><Identifier Source="ORCID">0000-0003-3622-9010</Identifier><AffiliationInfo><Affiliation>Department of Computer Software Engineering, University of Engineering and Technology Mardan, Mardan 23200, Pakistan.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>03</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Diagnostics (Basel)</MedlineTA><NlmUniqueID>101658402</NlmUniqueID><ISSNLinking>2075-4418</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">COVID-19</Keyword><Keyword MajorTopicYN="N">CT scans</Keyword><Keyword MajorTopicYN="N">ECG Trace Images</Keyword><Keyword MajorTopicYN="N">ShuffleNet</Keyword><Keyword MajorTopicYN="N">chest radiographs</Keyword><Keyword MajorTopicYN="N">classification</Keyword><Keyword MajorTopicYN="N">convolutional neural networks</Keyword><Keyword MajorTopicYN="N">detection</Keyword><Keyword MajorTopicYN="N">medical imaging</Keyword></KeywordList><CoiStatement>The authors declare no conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>11</Month><Day>29</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>21</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>28</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>1</Hour><Minute>3</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36611454</ArticleId><ArticleId IdType="pmc">PMC9818310</ArticleId><ArticleId IdType="doi">10.3390/diagnostics13010162</ArticleId><ArticleId IdType="pii">diagnostics13010162</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Ullah N., Khan J.A., Almakdi S., Khan M.S., Alshehri M., Alboaneen D., Raza A. A novel CovidDetNet deep learning model for effective COVID-19 infection detection using chest radiograph images. Appl. Sci. 2022;12:6269. doi: 10.3390/app12126269.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/app12126269</ArticleId></ArticleIdList></Reference><Reference><Citation>Ullah N., Javed A. Deep Features Comparative Analysis for COVID-19 Detection from the Chest Radiograph Images; Proceedings of the 2021 International Conference on Frontiers of Information Technology (FIT); Islamabad, Pakistan. 13&#x2013;14 December 2021; pp. 258&#x2013;263.</Citation></Reference><Reference><Citation>Peng X., Xu X., Li Y., Cheng L., Zhou X., Ren B. Transmission routes of 2019-ncov and controls in dental practice. Int. J. Oral Sci. 2020;12:9. doi: 10.1038/s41368-020-0075-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41368-020-0075-9</ArticleId><ArticleId IdType="pmc">PMC7054527</ArticleId><ArticleId IdType="pubmed">32127517</ArticleId></ArticleIdList></Reference><Reference><Citation> [(accessed on 12 September 2022)].  Available online:  https://covid19.who.int/</Citation></Reference><Reference><Citation>World Health Organization  Laboratory Testing for Coronavirus Disease 2019 (COVID-19) in Suspected Human Cases March 2, 2020.  [(accessed on 15 May 2020)].  Available online:  https://apps.who.int/iris/bitstream/handle/10665/331329/WHO-COVID-19-laboratory-2020.4-eng.pdf.</Citation></Reference><Reference><Citation>American Society for Microbiology  Supply Shortages Impacting COVID-19 and Non-COVID Testing. 2020.  [(accessed on 14 September 2022)].  Available online:  https://asm.org/Articles/2020/September/Clinical-Microbiology-Supply-Shortage-Collecti-1.</Citation></Reference><Reference><Citation>Nawaz M., Nazir T., Javed A., Malik K.M., Saudagar A.K.J., Khan M.B., Hasanat M.H.A., AlTameem A., AlKhathami M. Efficient-ECGNet framework for COVID-19 classification and correlation prediction with the cardio disease through electrocardiogram medical imaging. Front. Med. 2022;9:1&#x2013;19. doi: 10.3389/fmed.2022.1005920.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fmed.2022.1005920</ArticleId><ArticleId IdType="pmc">PMC9672089</ArticleId><ArticleId IdType="pubmed">36405585</ArticleId></ArticleIdList></Reference><Reference><Citation>Barstugan M., Ozkaya U., Ozturk S. Coronavirus (COVID-19) classification using CT images by machine learning methods. arXiv. 20202003.09424</Citation></Reference><Reference><Citation>Akudjedu T.N., Botwe B.O., Wuni A.R., Mishio N.A. Impact of the COVID-19 pandemic on clinical radiography practice in low resource settings: The Ghanaian radiographers&#x2019; perspective. Radiography. 2021;27:443&#x2013;452. doi: 10.1016/j.radi.2020.10.013.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.radi.2020.10.013</ArticleId><ArticleId IdType="pmc">PMC7590818</ArticleId><ArticleId IdType="pubmed">33168371</ArticleId></ArticleIdList></Reference><Reference><Citation>Ashar H., Singh B., Desai R., Abbas R.A., Raut P. Information and Communication Technology for Cohempetitive Strategies (ICTCS 2020) Springer; Singapore: 2022. A Deep Learning-Based Approach for Detection of Viral and Bacterial Pneumonia from Chest X-rays; pp. 173&#x2013;182.</Citation></Reference><Reference><Citation>Ga&#xe1;l G., Maga B., Luk&#xe1;cs A. Attention U-Net Based Adversarial Architectures for Chest X-ray Lung Segmentation. arXiv. 20202003.10304</Citation></Reference><Reference><Citation>Kumar A., Tripathi A.R., Satapathy S.C., Zhang Y.D. SARS-Net: COVID-19 detection from chest X-rays by combining graph convolutional network and convolutional neural network. Pattern Recognit. 2022;122:108255. doi: 10.1016/j.patcog.2021.108255.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.patcog.2021.108255</ArticleId><ArticleId IdType="pmc">PMC8386119</ArticleId><ArticleId IdType="pubmed">34456369</ArticleId></ArticleIdList></Reference><Reference><Citation>Paul A., Basu A., Mahmud M., Kaiser M.S., Sarkar R. Inverted bell-curve-based ensemble of deep learning models for detection of COVID-19 from chest X-rays. Neural Comput. Appl. 2022 doi: 10.1007/s00521-021-06737-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00521-021-06737-6</ArticleId><ArticleId IdType="pmc">PMC8729326</ArticleId><ArticleId IdType="pubmed">35013650</ArticleId></ArticleIdList></Reference><Reference><Citation>Kumari A., Mehta A.K. Proceedings of the International Conference on Paradigms of Communication, Computing and Data Sciences. Springer; Singapore: 2022. Effective prediction of COVID-19 using supervised machine learning with Ensemble Modeling; pp. 537&#x2013;547.</Citation></Reference><Reference><Citation>Aggarwal P., Mishra N.K., Fatimah B., Singh P., Gupta A., Joshi S.D. COVID-19 image classification using deep learning: Advances, challenges and opportunities. Comput. Biol. Med. 2022;144:105350. doi: 10.1016/j.compbiomed.2022.105350.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2022.105350</ArticleId><ArticleId IdType="pmc">PMC8890789</ArticleId><ArticleId IdType="pubmed">35305501</ArticleId></ArticleIdList></Reference><Reference><Citation>Hassan H., Ren Z., Zhou C., Khan M.A., Pan Y., Zhao J., Huang B. Supervised and Weakly Supervised Deep Learning Models for COVID-19 CT Diagnosis: A Systematic Review. Comput. Methods Programs Biomed. 2022;218:106731. doi: 10.1016/j.cmpb.2022.106731.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cmpb.2022.106731</ArticleId><ArticleId IdType="pmc">PMC8897838</ArticleId><ArticleId IdType="pubmed">35286874</ArticleId></ArticleIdList></Reference><Reference><Citation>Mahdy L.N., Ezzat K.A., Elmousalami H.H., Ella H.A., Hassanien A.E. Automatic X-ray COVID-19 lung image classification system based on multi-level thresholding and support vector machine. medRxiv. 2020 doi: 10.1101/2020.03.30.20047787.</Citation><ArticleIdList><ArticleId IdType="doi">10.1101/2020.03.30.20047787</ArticleId></ArticleIdList></Reference><Reference><Citation>Sethy P.K., Behera S.K. Detection of Coronavirus Disease (COVID-19) Based on Deep Features. 2020.  [(accessed on 19 March 2020)].  Available online:  https://www.preprints.org/manuscript/202003.0300/v1.</Citation></Reference><Reference><Citation>Novitasari D.C.R., Hendradi R., Caraka R.E., Rachmawati Y., Fanani N.Z., Syarifudin A., Toharudin T., Chen R.C. Detection of COVID-19 chest X-ray using support vector machine and convolutional neural network. Commun. Math. Biol. Neurosci. 2020;2020:42.</Citation></Reference><Reference><Citation>Agushaka J.O., Ezugwu A.E., Abualigah L. Dwarf Mongoose Optimization Algorithm. Comput. Methods Appl. Mech. Eng. 2020;391:114570. doi: 10.1016/j.cma.2022.114570.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cma.2022.114570</ArticleId></ArticleIdList></Reference><Reference><Citation>Abualigah L., Diabat A., Mirjalili S., Abd Elaziz M., Gandomi A.H. The arithmetic optimization algorithm. Comput. Methods Appl. Mech. Eng. 2021;376:113609. doi: 10.1016/j.cma.2020.113609.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cma.2020.113609</ArticleId></ArticleIdList></Reference><Reference><Citation>Abualigah L., Yousri D., Abd Elaziz M., Ewees A.A., Al-Qaness M.A., Gandomi A.H. Aquila optimizer: A novel meta-heuristic optimization algorithm. Comput. Ind. Eng. 2021;157:107250. doi: 10.1016/j.cie.2021.107250.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cie.2021.107250</ArticleId></ArticleIdList></Reference><Reference><Citation>Abualigah L., Abd Elaziz M., Sumari P., Geem Z.W., Gandomi A.H. Reptile Search Algorithm (RSA): A nature-inspired meta-heuristic optimizer. Expert Syst. Appl. 2021;191:116158. doi: 10.1016/j.eswa.2021.116158.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.eswa.2021.116158</ArticleId></ArticleIdList></Reference><Reference><Citation>Oyelade O.N., Ezugwu A.E.S., Mohamed T.I., Abualigah L. Ebola Optimization Search Algorithm: A New Nature-Inspired Metaheuristic Optimization Algorithm. IEEE Access. 2022;10:16150&#x2013;16177. doi: 10.1109/ACCESS.2022.3147821.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2022.3147821</ArticleId></ArticleIdList></Reference><Reference><Citation>Das D., Santosh K., Pal U. Truncated inception net: COVID-19 outbreak screening using chest X-rays. Phys. Eng. Sci. Med. 2020;43:915&#x2013;925. doi: 10.1007/s13246-020-00888-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s13246-020-00888-x</ArticleId><ArticleId IdType="pmc">PMC7315909</ArticleId><ArticleId IdType="pubmed">32588200</ArticleId></ArticleIdList></Reference><Reference><Citation>Ozturk T., Talo M., Yildirim E., Baloglu U., Yildirim O. Automated detection of COVID-19 cases using deep neural networks with X-ray images. Comput. Biol. Med. 2020;121:103792. doi: 10.1016/j.compbiomed.2020.103792.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2020.103792</ArticleId><ArticleId IdType="pmc">PMC7187882</ArticleId><ArticleId IdType="pubmed">32568675</ArticleId></ArticleIdList></Reference><Reference><Citation>Khan A., Shah J., Bhat M. Coronet: A deep neural network for detection and diagnosis of COVID-19 from chest X-ray images. Comput. Methods Programs Biomed. 2020;196:105581. doi: 10.1016/j.cmpb.2020.105581.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cmpb.2020.105581</ArticleId><ArticleId IdType="pmc">PMC7274128</ArticleId><ArticleId IdType="pubmed">32534344</ArticleId></ArticleIdList></Reference><Reference><Citation>Apostolopoulos I.D., Aznaouridis S.I., Tzani M.A. Extracting Possibly Representative COVID-19 Biomarkers from X-ray Images with Deep Learning Approach and Image Data Related to Pulmonary Diseases. J. Med. Biol. Eng. 2020;40:462&#x2013;469. doi: 10.1007/s40846-020-00529-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s40846-020-00529-4</ArticleId><ArticleId IdType="pmc">PMC7221329</ArticleId><ArticleId IdType="pubmed">32412551</ArticleId></ArticleIdList></Reference><Reference><Citation>Ucar F., Korkmaz D. COVIDiagnosis-Net: Deep Bayes-SqueezeNet based Diagnostic of the Coronavirus Disease 2019 (COVID-19) from X-ray Images. Med. Hypotheses. 2020;140:109761. doi: 10.1016/j.mehy.2020.109761.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.mehy.2020.109761</ArticleId><ArticleId IdType="pmc">PMC7179515</ArticleId><ArticleId IdType="pubmed">32344309</ArticleId></ArticleIdList></Reference><Reference><Citation>Momani S., Abo-Hammour Z.S., Alsmadi O.M. Solution of inverse kinematics problem using genetic algorithms. Appl. Math. Inf. Sci. 2016;10:225. doi: 10.18576/amis/100122.</Citation><ArticleIdList><ArticleId IdType="doi">10.18576/amis/100122</ArticleId></ArticleIdList></Reference><Reference><Citation>Abo-Hammour Z., Arqub O.A., Alsmadi O., Momani S., Alsaedi A. An optimization algorithm for solving systems of singular boundary value problems. Appl. Math. Inf. Sci. 2014;8:2809. doi: 10.12785/amis/080617.</Citation><ArticleIdList><ArticleId IdType="doi">10.12785/amis/080617</ArticleId></ArticleIdList></Reference><Reference><Citation>Ullah N., Javed A., Ghazanfar M.A., Alsufyani A., Bourouis S. A novel DeepMaskNet model for face mask detection and masked facial recognition. J. King Saud Univ.-Comput. Inf. Sci. 2022;34:9905&#x2013;9914. doi: 10.1016/j.jksuci.2021.12.017.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jksuci.2021.12.017</ArticleId></ArticleIdList></Reference><Reference><Citation>Abu Arqub O., Abo-Hammour Z., Momani S., Shawagfeh N. Solving singular two-point boundary value problems using continuous genetic algorithm. Abstr. Appl. Anal. 2012;2012:205391. doi: 10.1155/2012/205391.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2012/205391</ArticleId></ArticleIdList></Reference><Reference><Citation>Mei X., Lee H.-C., Diao K.Y., Huang M., Lin B., Liu C., Xie Z., Ma Y., Robson P.M., Chung M. Artificial intelligence&#x2013;enabled rapid diagnosis of patients with COVID-19. Nat. Med. 2020;26:1224&#x2013;1228. doi: 10.1038/s41591-020-0931-3.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41591-020-0931-3</ArticleId><ArticleId IdType="pmc">PMC7446729</ArticleId><ArticleId IdType="pubmed">32427924</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang S., Kang B., Ma J., Zeng X., Xiao M., Guo J., Cai M., Yang J., Li Y., Meng X., et al. A deep learning algorithm using ct images to screen for corona virus disease (COVID-19) Eur. Radiol. 2021;31:6096&#x2013;6104. doi: 10.1007/s00330-021-07715-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-021-07715-1</ArticleId><ArticleId IdType="pmc">PMC7904034</ArticleId><ArticleId IdType="pubmed">33629156</ArticleId></ArticleIdList></Reference><Reference><Citation>Aversano L., Bernardi M.L., Cimitile M., Pecori R. Deep neural networks ensemble to detect COVID-19 from CT scans. Pattern Recognit. 2021;120:108135. doi: 10.1016/j.patcog.2021.108135.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.patcog.2021.108135</ArticleId><ArticleId IdType="pmc">PMC8494191</ArticleId><ArticleId IdType="pubmed">34642504</ArticleId></ArticleIdList></Reference><Reference><Citation>Okolo G.I., Katsigiannis S., Althobaiti T., Ramzan N. On the Use of Deep Learning for Imaging-Based COVID-19 Detection Using Chest X-rays. Sensors. 2021;21:5702. doi: 10.3390/s21175702.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s21175702</ArticleId><ArticleId IdType="pmc">PMC8434119</ArticleId><ArticleId IdType="pubmed">34502591</ArticleId></ArticleIdList></Reference><Reference><Citation>Uddin A., Talukder B., Monirujjaman K.M., Zaguia A. Study on convolutional neural network to detect COVID-19 from chest X-rays. Math. Probl. Eng. 2021;2021:3366057. doi: 10.1155/2021/3366057.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2021/3366057</ArticleId></ArticleIdList></Reference><Reference><Citation>Rahman T., Akinbi A., Chowdhury M.E., Rashid T.A., &#x15e;eng&#xfc;r A., Khandakar A., Islam K.R., Ismael A.M. COV-ECGNET: COVID-19 detection using ECG trace images with deep convolutional neural network. arXiv. 2021 doi: 10.1007/s13755-021-00169-1.2106.00436</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s13755-021-00169-1</ArticleId><ArticleId IdType="pmc">PMC8785028</ArticleId><ArticleId IdType="pubmed">35096384</ArticleId></ArticleIdList></Reference><Reference><Citation>Absar N., Mamur B., Mahmud A., Emran T.B., Khandaker M.U., Faruque M.R.I., Osman H., Elzaki A., Elkhader B.A. Development of a computer-aided tool for detection of COVID-19 pneumonia from CXR images using machine learning algorithm. J. Radiat. Res. Appl. Sci. 2022;15:32&#x2013;43. doi: 10.1016/j.jrras.2022.02.002.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jrras.2022.02.002</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang X., Zhou X., Lin M., Sun J. ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices. arXiv. 20171707.01083v2</Citation></Reference><Reference><Citation>Chowdhury M.E., Rahman T., Khandakar A., Mazhar R., Kadir M.A., Mahbub Z.B., Islam K.R., Khan M.S., Iqbal A., Al Emadi N., et al. Can AI help in screening Viral and COVID-19 pneumonia? IEEE Access. 2020;8:132665&#x2013;132676. doi: 10.1109/ACCESS.2020.3010287.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2020.3010287</ArticleId></ArticleIdList></Reference><Reference><Citation>Rahman T., Khandakar A., Qiblawey Y., Tahir A., Kiranyaz S., Kashem S.B.A., Islam M.T., Al Maadeed S., Zughaier S.M., Khan M.S., et al. Exploring the Effect of Image Enhancement Techniques on COVID-19 Detection using Chest X-ray Images. Comput. Biol. Med. 2020;132:104319. doi: 10.1016/j.compbiomed.2021.104319.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2021.104319</ArticleId><ArticleId IdType="pmc">PMC7946571</ArticleId><ArticleId IdType="pubmed">33799220</ArticleId></ArticleIdList></Reference><Reference><Citation>Soares E., Angelov P., Biaso S., Froes M.H., Abe D.K. SARS-CoV-2 CT-scan dataset: A large dataset of real patients CT scans for SARS-CoV-2 identification. medRxiv. 2020 doi: 10.1101/2020.04.24.20078584.</Citation><ArticleIdList><ArticleId IdType="doi">10.1101/2020.04.24.20078584</ArticleId></ArticleIdList></Reference><Reference><Citation>Khan A.H., Hussain M., Malik M.K. ECG Images dataset of Cardiac and COVID-19 Patients. Data Brief. 2021;34:106762. doi: 10.1016/j.dib.2021.106762.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.dib.2021.106762</ArticleId><ArticleId IdType="pmc">PMC7820911</ArticleId><ArticleId IdType="pubmed">33521183</ArticleId></ArticleIdList></Reference><Reference><Citation>Iandola F.N., Han S., Moskewicz M.W., Ashraf K., Dally W.J., Keutzer K. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5 MB model size. arXiv. 20161602.07360</Citation></Reference><Reference><Citation>Krizhevsky A., Sutskever I., Hinton G.E. ImageNet Classification with Deep Convolutional Neural Networks. Commun. ACM. 2017;60:84&#x2013;90. doi: 10.1145/3065386.</Citation><ArticleIdList><ArticleId IdType="doi">10.1145/3065386</ArticleId></ArticleIdList></Reference><Reference><Citation>Redmon J. Darknet: Open Source Neural Networks in C.  [(accessed on 23 May 2021)].  Available online:  https://pjreddie.com/darknet.</Citation></Reference><Reference><Citation>Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A. Going deeper with convolutions; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Boston, MA, USA. 7&#x2013;12 June 2015; pp. 1&#x2013;9.</Citation></Reference><Reference><Citation>Sandler M., Howard A., Zhu M., Zhmoginov A., Chen L.C. MobileNetV2: Inverted Residuals and Linear Bottlenecks; Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition; Salt Lake City, UT, USA. 18&#x2013;23 June 2018; pp. 4510&#x2013;4520.</Citation></Reference><Reference><Citation>Huang G., Liu Z., Van Der Maaten L., Weinberger K.Q. Densely Connected Convolutional Networks; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Honolulu, HI, USA. 21&#x2013;26 July 2017; pp. 4700&#x2013;4708.</Citation></Reference><Reference><Citation>Szegedy C., Vanhoucke V., Ioffe S., Shlens J., Wojna Z. Rethinking the inception architecture for computer vision; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Las Vegas, NV, USA. 27&#x2013;30 June 2016; pp. 2818&#x2013;2826.</Citation></Reference><Reference><Citation>He K., Zhang X., Ren S., Sun J. Deep residual learning for image recognition; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Las Vegas, NV, USA. 27&#x2013;30 June 2016; pp. 770&#x2013;778.</Citation></Reference><Reference><Citation>Sanida T., Sideris A., Tsiktsiris D., Dasygenis M. Lightweight Neural Network for COVID-19 Detection from Chest X-ray Images Implemented on an Embedded System. Technologies. 2022;10:37. doi: 10.3390/technologies10020037.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/technologies10020037</ArticleId></ArticleIdList></Reference><Reference><Citation>Basu A., Sheikh K.H., Cuevas E., Sarkar R. COVID-19 detection from CT scans using a two-stage framework. Expert Syst. Appl. 2022;193:116377. doi: 10.1016/j.eswa.2021.116377.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.eswa.2021.116377</ArticleId><ArticleId IdType="pmc">PMC8720180</ArticleId><ArticleId IdType="pubmed">35002099</ArticleId></ArticleIdList></Reference><Reference><Citation>Alquzi S., Alhichri H., Bazi Y. International Conference on Innovative Computing and Communications. Springer; Singapore: 2022. Detection of COVID-19 Using EfficientNet-B3 CNN and Chest Computed Tomography Images; pp. 365&#x2013;373.</Citation></Reference><Reference><Citation>Dutta A.K., Aljarallah N.A., Abirami T., Sundarrajan M., Kadry S., Nam Y., Jeong C.W. Optimal Deep-Learning-Enabled Intelligent Decision Support System for SARS-CoV-2 Classification. J. Healthc. Eng. 2022;2022:4130674. doi: 10.1155/2022/4130674.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2022/4130674</ArticleId><ArticleId IdType="pmc">PMC8846984</ArticleId><ArticleId IdType="pubmed">35178226</ArticleId></ArticleIdList></Reference><Reference><Citation>Ahmad I., Wang X., Zhu M., Wang C., Pi Y., Khan J.A., Khan S., Samuel O.W., Chen S., Li G. EEG-based epileptic seizure detection via machine/deep learning approaches: A Systematic Review. Comput. Intell. Neurosci. 2022;2022:6486570. doi: 10.1155/2022/6486570.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2022/6486570</ArticleId><ArticleId IdType="pmc">PMC9232335</ArticleId><ArticleId IdType="pubmed">35755757</ArticleId></ArticleIdList></Reference><Reference><Citation>Ullah N., Khan J.A., Khan M.S., Khan W., Hassan I., Obayya M., Negm N., Salama A.S. An Effective Approach to Detect and Identify Brain Tumors Using Transfer Learning. Appl. Sci. 2022;12:5645. doi: 10.3390/app12115645.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/app12115645</ArticleId></ArticleIdList></Reference><Reference><Citation>Ullah N., Khan M.S., Khan J.A., Choi A., Anwar M.S. A Robust End-to-End Deep Learning-Based Approach for Effective and Reliable BTD Using MR Images. Sensors. 2022;22:7575. doi: 10.3390/s22197575.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s22197575</ArticleId><ArticleId IdType="pmc">PMC9570935</ArticleId><ArticleId IdType="pubmed">36236674</ArticleId></ArticleIdList></Reference><Reference><Citation>Raza A., Ayub H., Khan J.A., Ahmad I., Salama A.S., Daradkeh Y.I., Javeed D., Rehman A.U., Hamam H. A Hybrid Deep Learning-Based Approach for Brain Tumor Classification. Electronics. 2022;11:1146. doi: 10.3390/electronics11071146.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/electronics11071146</ArticleId></ArticleIdList></Reference><Reference><Citation>Ullah N., Khan J.A., Alharbi L.A., Raza A., Khan W., Ahmad I. An Efficient Approach for Crops Pests Recognition and Classification Based on Novel DeepPestNet Deep Learning Model. IEEE Access. 2022;10:73019&#x2013;73032. doi: 10.1109/ACCESS.2022.3189676.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2022.3189676</ArticleId></ArticleIdList></Reference><Reference><Citation>Ali L., Niamat A., Khan J.A., Golilarz N.A., Xingzhong X., Noor A., Nour R., Bukhari S.A.C. An optimized stacked support vector machines based expert system for the effective prediction of heart failure. IEEE Access. 2019;7:54007&#x2013;54014. doi: 10.1109/ACCESS.2019.2909969.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2019.2909969</ArticleId></ArticleIdList></Reference><Reference><Citation>Ali L., Rahman A., Khan A., Zhou M., Javeed A., Khan J.A. An automated diagnostic system for heart disease prediction based on X2 statistical model and optimally configured deep neural network. IEEE Access. 2019;7:34938&#x2013;34945. doi: 10.1109/ACCESS.2019.2904800.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2019.2904800</ArticleId></ArticleIdList></Reference><Reference><Citation>Jabbar A., Li X., Assam M., Khan J.A., Obayya M., Alkhonaini M.A., Al-Wesabi F.N., Assad M. AFD-StackGAN: Automatic Mask Generation Network for Face De-Occlusion Using StackGAN. Sensors. 2022;22:1747. doi: 10.3390/s22051747.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s22051747</ArticleId><ArticleId IdType="pmc">PMC8914700</ArticleId><ArticleId IdType="pubmed">35270898</ArticleId></ArticleIdList></Reference><Reference><Citation>El-Rashidy N., ElSayed N.E., El-Ghamry A., Talaat F.M. Utilizing fog computing and explainable deep learning techniques for gestational diabetes prediction. Neural Comput. Appl. 2022 doi: 10.1007/s00521-022-08007-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00521-022-08007-5</ArticleId></ArticleIdList></Reference><Reference><Citation>El-Rashidy N., ElSayed N.E., El-Ghamry A., Talaat F.M. Prediction of gestational diabetes based on explainable deep learning and fog computing. Soft Comput. 2022;26:11435&#x2013;11450. doi: 10.1007/s00500-022-07420-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00500-022-07420-1</ArticleId></ArticleIdList></Reference><Reference><Citation>El-Rashidy N., Abuhmed T., Alarabi L., El-Bakry H.M., Abdelrazek S., Ali F., El-Sappagh S. Sepsis prediction in intensive care unit based on genetic feature optimization and stacked deep ensemble learning. Neural Comput. Appl. 2022;34:3603&#x2013;3632. doi: 10.1007/s00521-021-06631-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00521-021-06631-1</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36611451</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Print">2075-4418</ISSN><JournalIssue CitedMedium="Print"><Volume>13</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>03</Day></PubDate></JournalIssue><Title>Diagnostics (Basel, Switzerland)</Title><ISOAbbreviation>Diagnostics (Basel)</ISOAbbreviation></Journal><ArticleTitle>A Review of Recent Advances in Deep Learning Models for Chest Disease Detection Using Radiography.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">159</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/diagnostics13010159</ELocationID><Abstract><AbstractText>Chest X-ray radiography (CXR) is among the most frequently used medical imaging modalities. It has a preeminent value in the detection of multiple life-threatening diseases. Radiologists can visually inspect CXR images for the presence of diseases. Most thoracic diseases have very similar patterns, which makes diagnosis prone to human error and leads to misdiagnosis. Computer-aided detection (CAD) of lung diseases in CXR images is among the popular topics in medical imaging research. Machine learning (ML) and deep learning (DL) provided techniques to make this task more efficient and faster. Numerous experiments in the diagnosis of various diseases proved the potential of these techniques. In comparison to previous reviews our study describes in detail several publicly available CXR datasets for different diseases. It presents an overview of recent deep learning models using CXR images to detect chest diseases such as VGG, ResNet, DenseNet, Inception, EfficientNet, RetinaNet, and ensemble learning methods that combine multiple models. It summarizes the techniques used for CXR image preprocessing (enhancement, segmentation, bone suppression, and data-augmentation) to improve image quality and address data imbalance issues, as well as the use of DL models to speed-up the diagnosis process. This review also discusses the challenges present in the published literature and highlights the importance of interpretability and explainability to better understand the DL models' detections. In addition, it outlines a direction for researchers to help develop more effective models for early and automatic detection of chest diseases.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Ait Nasser</LastName><ForeName>Adnane</ForeName><Initials>A</Initials><Identifier Source="ORCID">0000-0003-1800-5208</Identifier><AffiliationInfo><Affiliation>Perception, Robotics and Intelligent Machines Research Group (PRIME), Department of Computer Science, Universit&#xe9; de Moncton, Moncton, NB E1C 3E9, Canada.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Akhloufi</LastName><ForeName>Moulay A</ForeName><Initials>MA</Initials><Identifier Source="ORCID">0000-0002-4378-2669</Identifier><AffiliationInfo><Affiliation>Perception, Robotics and Intelligent Machines Research Group (PRIME), Department of Computer Science, Universit&#xe9; de Moncton, Moncton, NB E1C 3E9, Canada.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType><PublicationType UI="D016454">Review</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>03</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Diagnostics (Basel)</MedlineTA><NlmUniqueID>101658402</NlmUniqueID><ISSNLinking>2075-4418</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">chest X-ray</Keyword><Keyword MajorTopicYN="N">computer-aided detection</Keyword><Keyword MajorTopicYN="N">deep convolutional neural networks</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword><Keyword MajorTopicYN="N">machine learning</Keyword><Keyword MajorTopicYN="N">radiography</Keyword></KeywordList><CoiStatement>The authors declare no conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>11</Month><Day>29</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>21</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>26</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>1</Hour><Minute>3</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36611451</ArticleId><ArticleId IdType="pmc">PMC9818166</ArticleId><ArticleId IdType="doi">10.3390/diagnostics13010159</ArticleId><ArticleId IdType="pii">diagnostics13010159</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Abiyev R., Ma&#x2019;aitah M.K.S. Deep Convolutional Neural Networks for Chest Diseases Detection. J. Healthc. Eng. 2018;2018:4168538. doi: 10.1155/2018/4168538.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2018/4168538</ArticleId><ArticleId IdType="pmc">PMC6093039</ArticleId><ArticleId IdType="pubmed">30154989</ArticleId></ArticleIdList></Reference><Reference><Citation>Radiological Society of North America  X-ray Radiography-Chest.  [(accessed on 1 November 2022)].  Available online:  https://www.radiologyinfo.org/en/info.cfm?pg=chestrad.</Citation></Reference><Reference><Citation>US Food and Drugs Administration  Medical X-ray Imaging.  [(accessed on 1 November 2022)]; Available online:  https://www.fda.gov/radiation-emitting-products/medical-imaging/medical-x-ray-imaging.</Citation></Reference><Reference><Citation>Ahmad W.S.H.M.W., Zaki W.M.D.W., Fauzi M.F.A., Tan W.H. Classification of Infection and Fluid Regions in Chest X-ray Images; Proceedings of the International Conference on Digital Image Computing: Techniques and Applications (DICTA); Gold Coast, QLD, Australia. 30 November&#x2013;2 December 2016; pp. 1&#x2013;5.</Citation></Reference><Reference><Citation>Whittaker Brown S.A., Padilla M., Mhango G., Powell C., Salvatore M., Henschke C., Yankelevitz D., Sigel K., de Torres J.P., Wisnivesky J. Interstitial Lung Abnormalities and Lung Cancer Risk in the National Lung Screening Trial. Chest. 2019;156:1195&#x2013;1203. doi: 10.1016/j.chest.2019.06.041.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.chest.2019.06.041</ArticleId><ArticleId IdType="pubmed">31404527</ArticleId></ArticleIdList></Reference><Reference><Citation>Cha M.J., Chung M.J., Lee J.H., Lee K.S. Performance of deep learning model in detecting operable lung cancer with chest radiographs. J. Thorac. Imaging. 2019;34:86&#x2013;91. doi: 10.1097/RTI.0000000000000388.</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/RTI.0000000000000388</ArticleId><ArticleId IdType="pubmed">30802232</ArticleId></ArticleIdList></Reference><Reference><Citation>Marciniuk D., Schraufnagel D., Society E.R. The Global Impact of Respiratory Disease. European Respiratory Society; Lausanne, Switzerland: 2017.</Citation></Reference><Reference><Citation>World Health Organization  Pneumonia Dashboard.  [(accessed on 1 November 2022)].  Available online:  https://www.who.int/news-room/fact-sheets/detail/pneumonia.</Citation></Reference><Reference><Citation>Khoiriyah S.A., Basofi A., Fariza A. Convolutional Neural Network for Automatic Pneumonia Detection in Chest Radiography; Proceedings of the International Electronics Symposium (IES); Surabaya, Indonesia. 29&#x2013;30 September 2020; pp. 476&#x2013;480.</Citation></Reference><Reference><Citation>World Health Organization  Tuberculosis.  [(accessed on 1 November 2022)].  Available online:  https://www.who.int/news-room/fact-sheets/detail/tuberculosis.</Citation></Reference><Reference><Citation>Sathitratanacheewin S., Sunanta P., Pongpirul K. Deep learning for automated classification of tuberculosis-related chest X-ray: Dataset distribution shift limits diagnostic performance generalizability. J. Am. Med. Inform. Assoc. 2020;6:593&#x2013;604. doi: 10.1016/j.heliyon.2020.e04614.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.heliyon.2020.e04614</ArticleId><ArticleId IdType="pmc">PMC7396903</ArticleId><ArticleId IdType="pubmed">32775757</ArticleId></ArticleIdList></Reference><Reference><Citation>Avni U., Greenspan H., Konen E., Sharon M., Goldberger J. X-ray Categorization and Retrieval on the Organ and Pathology Level, Using Patch-Based Visual Words. IEEE Trans. Med. Imaging. 2011;30:733&#x2013;746. doi: 10.1109/TMI.2010.2095026.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2010.2095026</ArticleId><ArticleId IdType="pubmed">21118769</ArticleId></ArticleIdList></Reference><Reference><Citation>Jaeger S., Karargyris A., Candemir S., Folio L., Siegelman J., Callaghan F., Xue Z., Palaniappan K., Singh R., Antani S., et al. Automatic Tuberculosis Screening Using Chest Radiographs. IEEE Trans. Med. Imaging. 2014;33:233&#x2013;245. doi: 10.1109/TMI.2013.2284099.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2013.2284099</ArticleId><ArticleId IdType="pubmed">24108713</ArticleId></ArticleIdList></Reference><Reference><Citation>Pattrapisetwong P., Chiracharit W. Automatic lung segmentation in chest radiographs using shadow filter and multilevel thresholding; Proceedings of the International Computer Science and Engineering Conference (ICSEC); Chiang Mai, Thailand. 14&#x2013;17 December 2016; pp. 1&#x2013;6.</Citation></Reference><Reference><Citation>Rasheed J., Hameed A.A., Djeddi C., Jamil A., Al-Turjman F. A machine learning-based framework for diagnosis of COVID-19 from chest X-ray images. Interdiscip. Sci. Comput. Life Sci. 2021;13:103&#x2013;117. doi: 10.1007/s12539-020-00403-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s12539-020-00403-6</ArticleId><ArticleId IdType="pmc">PMC7776293</ArticleId><ArticleId IdType="pubmed">33387306</ArticleId></ArticleIdList></Reference><Reference><Citation>Elaziz M.A., Hosny K.M., Salah A., Darwish M.M., Lu S., Sahlol A.T. New machine learning method for image-based diagnosis of COVID-19. PLoS ONE. 2020;15:e0235187. doi: 10.1371/journal.pone.0235187.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0235187</ArticleId><ArticleId IdType="pmc">PMC7319603</ArticleId><ArticleId IdType="pubmed">32589673</ArticleId></ArticleIdList></Reference><Reference><Citation>Candemir S., Jaeger S., Lin W., Xue Z., Antani S., Thoma G. Medical Imaging 2016: Computer-Aided Diagnosis 2016. SPIE; Bellingham, WA, USA: 2016. Automatic heart localization and radiographic index computation in chest X-rays; pp. 302&#x2013;309.</Citation></Reference><Reference><Citation>Alslatie M., Alquran H., Mustafa W.A., Abu-Qasmieh I., Alqudah A.M., Alkhayyat A. Automated Diagnosis of Heart-Lung Diseases in Chest X-ray Images; Proceedings of the 5th International Conference on Engineering Technology and its Applications (IICETA); Al-Najaf, Iraq. 31 May&#x2013;1 June 2022; pp. 537&#x2013;541.</Citation></Reference><Reference><Citation>Yee S.L.K., Raymond W.J.K. Pneumonia Diagnosis Using Chest X-ray Images and Machine Learning; Proceedings of the 10th International Conference on Biomedical Engineering and Technology; Tokyo, Japan. 15&#x2013;18 September 2020; pp. 101&#x2013;105.</Citation></Reference><Reference><Citation>Chandra T.B., Verma K. Proceedings of the 3rd International Conference on Computer Vision and Image Processing. Springer; Singapore: 2020. Pneumonia Detection on Chest X-ray Using Machine Learning Paradigm; pp. 21&#x2013;33.</Citation></Reference><Reference><Citation>Sousa R., Marques O., Curado G., da Costa R., Soares A., Soares F.A., de Oliveira L. Evaluation of Classifiers to a Childhood Pneumonia Computer-Aided Diagnosis System; Proceedings of the 27th International Symposium on Computer-Based Medical Systems; New York, NY, USA. 27&#x2013;29 May 2014; pp. 477&#x2013;478.</Citation></Reference><Reference><Citation>Varela-Santos S., Melin P. Intuitionistic and Type-2 Fuzzy Logic Enhancements in Neural and Optimization Algorithms: Theory and Applications. Springer; Cham, Switzerland: 2020. Classification of X-ray images for pneumonia detection using texture features and neural networks; pp. 237&#x2013;253.</Citation></Reference><Reference><Citation>Pavithra R., Pattar S. Detection and classification of lung disease-pneumonia and lung cancer in chest radiology using artificial neural network. Int. J. Sci. Res. Publ. 2015;5:128&#x2013;132.</Citation></Reference><Reference><Citation>Khatri A., Jain R., Vashista H., Mittal N., Ranjan P., Janardhanan R. Pneumonia identification in chest X-ray images using EMD. Trends Commun. Cloud Big Data. 2020;99:87&#x2013;98.</Citation></Reference><Reference><Citation>Das S., Kumar Pradhan S., Mishra S., Pradhan S., Pattnaik P.K. A Machine Learning based Approach for Detection of Pneumonia by Analyzing Chest X-ray Images; Proceedings of the 9th International Conference on Computing for Sustainable Global Development (INDIACom); New Delhi, India. 23&#x2013;25 March 2022; pp. 177&#x2013;183.</Citation></Reference><Reference><Citation>Inbaraj X.A., Villavicencio C., Macrohon J.J., Jeng J.H., Hsieh J.G. A Novel Machine Learning Approach for Tuberculosis Segmentation and Prediction Using Chest-X-ray (CXR) Images. Appl. Sci. 2021;11:9057. doi: 10.3390/app11199057.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/app11199057</ArticleId></ArticleIdList></Reference><Reference><Citation>RAHMAT T., ISMAIL A., ALIMAN S. Chest X-rays Image Classification in Medical Image Analysis. Appl. Med. Inform. 2018;40:63&#x2013;73.</Citation></Reference><Reference><Citation>Piccialli F., Di Somma V., Giampaolo F., Cuomo S., Fortino G. A survey on deep learning in medicine: Why, how and when? Inf. Fusion. 2021;66:111&#x2013;137. doi: 10.1016/j.inffus.2020.09.006.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.inffus.2020.09.006</ArticleId></ArticleIdList></Reference><Reference><Citation>Alghamdi H., Amoudi G., Elhag S., Saeedi K., Nasser J. Deep Learning Approaches for Detecting COVID-19 From Chest X-Ray Images: A Survey. IEEE Access. 2021;9:20235&#x2013;20254. doi: 10.1109/ACCESS.2021.3054484.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2021.3054484</ArticleId><ArticleId IdType="pmc">PMC8545235</ArticleId><ArticleId IdType="pubmed">34786304</ArticleId></ArticleIdList></Reference><Reference><Citation>Chandrasekar S. Exploring the Deep-Learning Techniques in Detecting the Presence of Coronavirus in the Chest X-Ray Images: A Comprehensive Review. Arch. Comput. Methods Eng. 2022;29:5381&#x2013;5395. doi: 10.1007/s11831-022-09768-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11831-022-09768-x</ArticleId><ArticleId IdType="pmc">PMC9126247</ArticleId><ArticleId IdType="pubmed">35645554</ArticleId></ArticleIdList></Reference><Reference><Citation>Mary Shyni H., Chitra E. A comparative study of X-ray and CT images in COVID-19 detection using image processing and deep learning techniques. Comput. Methods Programs Biomed. Update. 2022;2:100054. doi: 10.1016/j.cmpbup.2022.100054.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cmpbup.2022.100054</ArticleId><ArticleId IdType="pmc">PMC8898857</ArticleId><ArticleId IdType="pubmed">35281724</ArticleId></ArticleIdList></Reference><Reference><Citation>Ma J., Song Y., Tian X., Hua Y., Zhang R., Wu J. Survey on deep learning for pulmonary medical imaging. Front. Med. 2020;14:450&#x2013;469. doi: 10.1007/s11684-019-0726-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11684-019-0726-4</ArticleId><ArticleId IdType="pubmed">31840200</ArticleId></ArticleIdList></Reference><Reference><Citation>Elangovan A., Jeyaseelan T. Medical imaging modalities: A survey; Proceedings of the International Conference on Emerging Trends in Engineering, Technology and Science (ICETETS); Pudukkottai, India. 24&#x2013;26 February 2016; pp. 1&#x2013;4.</Citation></Reference><Reference><Citation>Saczynski J., McManus D., Goldberg R. Commonly Used Data-collection Approaches in Clinical Research. Am. J. Med. 2013;126:946&#x2013;950. doi: 10.1016/j.amjmed.2013.04.016.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.amjmed.2013.04.016</ArticleId><ArticleId IdType="pmc">PMC3827694</ArticleId><ArticleId IdType="pubmed">24050485</ArticleId></ArticleIdList></Reference><Reference><Citation>Horng S., Liao R., Wang X., Dalal S., Golland P., Berkowitz S. Deep learning to quantify pulmonary edema in chest radiographs. Radiol. Artif. Intell. 2021;3:e190228. doi: 10.1148/ryai.2021190228.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/ryai.2021190228</ArticleId><ArticleId IdType="pmc">PMC8043362</ArticleId><ArticleId IdType="pubmed">33937857</ArticleId></ArticleIdList></Reference><Reference><Citation>Tolkachev A., Sirazitdinov I., Kholiavchenko M., Mustafaev T., Ibragimov B. Deep learning for diagnosis and segmentation of pneumothorax: The results on the kaggle competition and validation against radiologists. J. Biomed. Health Inform. 2021;25:1660&#x2013;1672. doi: 10.1109/JBHI.2020.3023476.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/JBHI.2020.3023476</ArticleId><ArticleId IdType="pubmed">32956067</ArticleId></ArticleIdList></Reference><Reference><Citation>Schultheiss M., Schober S., Lodde M., Bodden J., Aichele J., M&#xfc;ller-Leisse C., Renger B., Pfeiffer F., Pfeiffer D. A robust convolutional neural network for lung nodule detection in the presence of foreign bodies. Sci. Rep. 2020;10:12987. doi: 10.1038/s41598-020-69789-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-020-69789-z</ArticleId><ArticleId IdType="pmc">PMC7395787</ArticleId><ArticleId IdType="pubmed">32737389</ArticleId></ArticleIdList></Reference><Reference><Citation>Demner-Fushman D., Kohli M., Rosenman M., Shooshan S., Rodriguez L., Antani S., Thoma G., McDonald C. Preparing a collection of radiology examinations for distribution and retrieval. Am. Med. Inform. Assoc. 2016;23:304&#x2013;310. doi: 10.1093/jamia/ocv080.</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/jamia/ocv080</ArticleId><ArticleId IdType="pmc">PMC5009925</ArticleId><ArticleId IdType="pubmed">26133894</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang X., Peng Y., Lu L., Lu Z., Bagheri M., Summers R.M. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases; Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR); Honolulu, HI, USA. 21&#x2013;26 July 2017; pp. 2097&#x2013;2106.</Citation></Reference><Reference><Citation>Ryoo S., Kim H.J. Activities of the Korean Institute of Tuberculosis. Osong Public Health Res. Perspect. 2014;5:S43&#x2013;S49. doi: 10.1016/j.phrp.2014.10.007.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.phrp.2014.10.007</ArticleId><ArticleId IdType="pmc">PMC4301635</ArticleId><ArticleId IdType="pubmed">25861580</ArticleId></ArticleIdList></Reference><Reference><Citation>Jaeger S., Candemir S., Antani S., W&#xe1;ng Y.X.J., Lu P.X., Thoma G. Two public chest X-ray datasets for computer-aided screening of pulmonary diseases. Quant. Imaging Med. Surg. 2014;4:475&#x2013;477.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4256233</ArticleId><ArticleId IdType="pubmed">25525580</ArticleId></ArticleIdList></Reference><Reference><Citation>Van Ginneken B., Stegmann M.B., Loog M. Segmentation of anatomical structures in chest radiographs using supervised methods: A comparative study on a public database. Med. Image Anal. 2006;10:19&#x2013;40. doi: 10.1016/j.media.2005.02.002.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2005.02.002</ArticleId><ArticleId IdType="pubmed">15919232</ArticleId></ArticleIdList></Reference><Reference><Citation>Shiraishi J., Katsuragawa S., Ikezoe J., Matsumoto T., Kobayashi T., Komatsu K.i., Matsui M., Fujita H., Kodera Y., Doi K. Development of a Digital Image Database for Chest Radiographs With and Without a Lung Nodule: Receiver Operating Characteristic Analysis of Radiologists Detection of Pulmonary Nodules. Am. J. Roentgenol. AJR. 2000;174:71&#x2013;74. doi: 10.2214/ajr.174.1.1740071.</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/ajr.174.1.1740071</ArticleId><ArticleId IdType="pubmed">10628457</ArticleId></ArticleIdList></Reference><Reference><Citation>Stanford ML Group  ChexPert a Large Chest X-ray Dataset and Competition.  [(accessed on 1 November 2022)].  Available online:  https://stanfordmlgroup.github.io/competitions/chexpert/</Citation></Reference><Reference><Citation>Bustos A., Pertusa A., Salinas J.M., de la Iglesia-Vay&#xe1; M. PadChest: A large chest X-ray image dataset with multi-label annotated reports. Med. Image Anal. 2020;66:101797. doi: 10.1016/j.media.2020.101797.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2020.101797</ArticleId><ArticleId IdType="pubmed">32877839</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhu C., Pinsky P., Kramer B., Prorok P., Purdue M., Berg C., Gohagan J. The Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial and Its Associated Research Resource. Natl. Cancer Inst. 2013;105:1684&#x2013;1693. doi: 10.1093/jnci/djt281.</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/jnci/djt281</ArticleId><ArticleId IdType="pmc">PMC3888207</ArticleId><ArticleId IdType="pubmed">24115361</ArticleId></ArticleIdList></Reference><Reference><Citation>Johnson A., Pollard T., Berkowitz S., Greenbaum N., Lungren M., Deng C.y., Mark R., Horng S. MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports. Sci. Data. 2019;6:317. doi: 10.1038/s41597-019-0322-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41597-019-0322-0</ArticleId><ArticleId IdType="pmc">PMC6908718</ArticleId><ArticleId IdType="pubmed">31831740</ArticleId></ArticleIdList></Reference><Reference><Citation>Nguyen H., Lam K., Le L., Pham H., Tran D., Nguyen D., Le D., Pham C., Tong H., Dinh D., et al. VinDr-CXR: An open dataset of chest X-rays with radiologist&#x2019;s annotations. arXiv. 2021 doi: 10.1038/s41597-022-01498-w.2012.15029</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41597-022-01498-w</ArticleId><ArticleId IdType="pmc">PMC9300612</ArticleId><ArticleId IdType="pubmed">35858929</ArticleId></ArticleIdList></Reference><Reference><Citation>Vingroup Big Data Institute  VinBigData Chest X-ray Abnormalities Detection.  [(accessed on 1 November 2022)].  Available online:  https://www.kaggle.com/c/vinbigdata-chest-xray-abnormalities-detection.</Citation></Reference><Reference><Citation>Mooney P. Chest X-ray Images (Pneumonia)  [(accessed on 1 November 2022)].  Available online:  https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia.</Citation></Reference><Reference><Citation>The Radiological Society of North America &amp; the Society of Thoracic Radiology  RSNA Pneumonia Detection Challenge.  [(accessed on 1 November 2022)].  Available online:  https://www.kaggle.com/c/rsna-pneumonia-detection-challenge/data.</Citation></Reference><Reference><Citation>Pavlova M., Tuinstra T., Aboutalebi H., Zhao A., Gunraj H., Wong A. COVIDx CXR-3: A Large-Scale, Open-Source Benchmark Dataset of Chest X-ray Images for Computer-Aided COVID-19 Diagnostics. arXiv. 20222206.03671</Citation></Reference><Reference><Citation>Cohen J.P., Morrison P., Dao L., Roth K., Duong T., Ghassemi M. COVID-19 Image Data Collection: Prospective Predictions Are the Future. arXiv. 20202006.11988</Citation></Reference><Reference><Citation>Chung A., Wang L., Wong A., Lin Z.Q., McInnis P., Gunraj H. Figure 1 COVID-19 Chest X-ray.  [(accessed on 1 November 2022)].  Available online:  https://github.com/agchung/Figure1-COVID-chestxray-dataset/tree/master/images.</Citation></Reference><Reference><Citation>Wang L., Wong A., Chung A., Lin Z.Q., McInnis P., Gunraj H. Actualmed COVID Chest X-ray.  [(accessed on 1 November 2022)].  Available online:  https://github.com/agchung/Actualmed-COVID-chestxray-dataset/tree/master/images.</Citation></Reference><Reference><Citation>Tawsifur R. COVID-19 Radiography Database.  [(accessed on 1 November 2022)].  Available online:  https://www.kaggle.com/tawsifurrahman/covid19-radiography-database.</Citation></Reference><Reference><Citation>Tsai E., Simpson S., Lungren M., Hershman M., Roshkovan L., Colak E., Erickson B., Shih G., Stein A., Kalpathy-Cramer J., et al. Data from medical imaging data resource center (MIDRC)-RSNA international covid radiology database (RICORD) release 1C-Chest X-ray, covid+(MIDRC-RICORD-1C) Cancer Imaging Arch. 2021;6:13.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7993245</ArticleId><ArticleId IdType="pubmed">33399506</ArticleId></ArticleIdList></Reference><Reference><Citation>Vay&#xe1; M.d.l.I., Saborit J.M., Montell J.A., Pertusa A., Bustos A., Cazorla M., Galant J., Barber X., Orozco-Beltr&#xe1;n D., Garc&#xed;a-Garc&#xed;a F., et al. BIMCV COVID-19+: A large annotated dataset of RX and CT images from COVID-19 patients. arXiv. 20202006.01174</Citation></Reference><Reference><Citation>Saltz J., Saltz M., Prasanna P., Moffitt R., Hajagos J., Bremer E., Balsamo J., Kurc T. Stony Brook University COVID-19 Positive Cases (Dataset)  [(accessed on 25 December 2022)];Cancer Imaging Arch. 2021  Available online:  https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=89096912.</Citation></Reference><Reference><Citation>Hwang S., Kim H.E., Jeong J., Kim H.J. Medical Imaging 2016: Computer-Aided Diagnosis. SPIE; Bellingham, WA, USA: 2016. A novel approach for tuberculosis screening based on deep convolutional neural networks; pp. 750&#x2013;757.</Citation></Reference><Reference><Citation>Irvin J., Rajpurkar P., Ko M., Yu Y., Ciurea-Ilcus S., Chute C., Marklund H., Haghgoo B., Ball R., Shpanskaya K., et al. CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison. Proc. AAAI Conf. Artif. Intell. 2019;33:590&#x2013;597. doi: 10.1609/aaai.v33i01.3301590.</Citation><ArticleIdList><ArticleId IdType="doi">10.1609/aaai.v33i01.3301590</ArticleId></ArticleIdList></Reference><Reference><Citation>Karargyris A., Kashyap S., Lourentzou I., Wu J., Sharma A., Tong M., Abedin S., Beymer D., Mukherjee V., Krupinski E.A., et al. Creation and validation of a chest X-ray dataset with eye-tracking and report dictation for AI development. Sci. Data. 2021;8:92. doi: 10.1038/s41597-021-00863-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41597-021-00863-5</ArticleId><ArticleId IdType="pmc">PMC7994908</ArticleId><ArticleId IdType="pubmed">33767191</ArticleId></ArticleIdList></Reference><Reference><Citation>Kermany D., Goldbaum M., Cai W., Valentim C., Liang H., Baxter S., McKeown A., Yang G., Wu X., Yan F., et al. Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning. Cell. 2018;172:1122&#x2013;1131. doi: 10.1016/j.cell.2018.02.010.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cell.2018.02.010</ArticleId><ArticleId IdType="pubmed">29474911</ArticleId></ArticleIdList></Reference><Reference><Citation>Ait Nasser A., Akhloufi M.A. Chest Diseases Classification Using CXR and Deep Ensemble Learning; Proceedings of the 19th International Conference on Content-Based Multimedia Indexing; Graz, Austria. 14&#x2013;16 September 2022; pp. 116&#x2013;120.</Citation></Reference><Reference><Citation>Nayak S.R., Nayak D.R., Sinha U., Arora V., Pachori R.B. Application of deep learning techniques for detection of COVID-19 cases using chest X-ray images: A comprehensive study. Biomed. Signal Process. Control. 2021;64:102365. doi: 10.1016/j.bspc.2020.102365.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bspc.2020.102365</ArticleId><ArticleId IdType="pmc">PMC7674150</ArticleId><ArticleId IdType="pubmed">33230398</ArticleId></ArticleIdList></Reference><Reference><Citation>Goodfellow I., Pouget-Abadie J., Mirza M., Xu B., Warde-Farley D., Ozair S., Courville A., Bengio Y. Generative adversarial nets. Adv. Neural Inf. Process. Syst. 2014;27:1&#x2013;9.</Citation></Reference><Reference><Citation>Kora Venu S., Ravula S. Evaluation of Deep Convolutional Generative Adversarial Networks for Data Augmentation of Chest X-ray Images. Future Internet. 2021;13:8. doi: 10.3390/fi13010008.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/fi13010008</ArticleId></ArticleIdList></Reference><Reference><Citation>Chuquicusma M., Hussein S., Burt J., Bagci U. How to fool radiologists with generative adversarial networks? A visual turing test for lung cancer diagnosis; Proceedings of the 15th international symposium on biomedical imaging (ISBI); Washington, DC, USA. 4&#x2013;7 April 2018; pp. 240&#x2013;244.</Citation></Reference><Reference><Citation>Madani A., Moradi M., Karargyris A., Syeda-Mahmood T. Medical Imaging 2018: Image Processing. SPIE; Bellingham, WA, USA: 2018. Chest X-ray generation and data augmentation for cardiovascular abnormality classification; pp. 415&#x2013;420.</Citation></Reference><Reference><Citation>Albahli S., Ahmad Hassan Yar G.N. AI-driven deep convolutional neural networks for chest X-ray pathology identification. J. X-ray Sci. Technol. 2022;30:365&#x2013;376. doi: 10.3233/XST-211082.</Citation><ArticleIdList><ArticleId IdType="doi">10.3233/XST-211082</ArticleId><ArticleId IdType="pubmed">35068415</ArticleId></ArticleIdList></Reference><Reference><Citation>SK S., Naveen N. Algorithm for pre-processing chest-x-ray using multi-level enhancement operation; Proceedings of the International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET); Chennai, India. 23&#x2013;25 March 2016; pp. 2182&#x2013;2186.</Citation></Reference><Reference><Citation>Reza A. Realization of the contrast limited adaptive histogram equalization (CLAHE) for real-time image enhancement. J. VLSI Signal Process. Syst. Signal Image Video Technol. 2004;38:35&#x2013;44. doi: 10.1023/B:VLSI.0000028532.53893.82.</Citation><ArticleIdList><ArticleId IdType="doi">10.1023/B:VLSI.0000028532.53893.82</ArticleId></ArticleIdList></Reference><Reference><Citation>Agaian S., Panetta K., Grigoryan A. Transform-based image enhancement algorithms with performance measure. IEEE Trans. Image Process. 2001;10:367&#x2013;382. doi: 10.1109/83.908502.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/83.908502</ArticleId><ArticleId IdType="pubmed">18249627</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen S., Cai Y. Enhancement of chest radiograph in emergency intensive care unit by means of reverse anisotropic diffusion-based unsharp masking model. Diagnostics. 2019;9:45. doi: 10.3390/diagnostics9020045.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/diagnostics9020045</ArticleId><ArticleId IdType="pmc">PMC6627656</ArticleId><ArticleId IdType="pubmed">31022984</ArticleId></ArticleIdList></Reference><Reference><Citation>Aashiq M., Kumara W., Kumara M., Pushpakumari P., Udhyani H., Rajendran H., Shih T. Image Enhancement Based CNN Approach to Covid-19 Diagnosis Using Chest X-ray Images; Proceedings of the 4th IEEE Eurasia Conference on Biomedical Engineering, Healthcare and Sustainability; Tainan, Taiwan. 27&#x2013;29 May 2022; pp. 1&#x2013;4.</Citation></Reference><Reference><Citation>Munadi K., Muchtar K., Maulina N., Pradhan B. Image Enhancement for Tuberculosis Detection Using Deep Learning. IEEE Access. 2020;8:217897&#x2013;217907. doi: 10.1109/ACCESS.2020.3041867.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2020.3041867</ArticleId></ArticleIdList></Reference><Reference><Citation>Rahman T., Khandakar A., Qiblawey Y., Tahir A., Kiranyaz S., Kashem S.B.A., Islam M.T., Al Maadeed S., Zughaier S.M., Khan M.S., et al. Exploring the effect of image enhancement techniques on COVID-19 detection using chest X-ray images. Comput. Biol. Med. 2021;132:104319. doi: 10.1016/j.compbiomed.2021.104319.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2021.104319</ArticleId><ArticleId IdType="pmc">PMC7946571</ArticleId><ArticleId IdType="pubmed">33799220</ArticleId></ArticleIdList></Reference><Reference><Citation>Nefoussi S., Amamra A., Amarouche I.A. International Conference on Computing Systems and Applications. Springer; Cham, Switzerland: 2020. A Comparative Study of Chest X-ray Image Enhancement Techniques for Pneumonia Recognition; pp. 276&#x2013;288.</Citation></Reference><Reference><Citation>Zhou Y., Shi C., Lai B., Jimenez G. Contrast enhancement of medical images using a new version of the World Cup Optimization algorithm. Quant. Imaging Med. Surg. 2019;9:1528&#x2013;1547. doi: 10.21037/qims.2019.08.19.</Citation><ArticleIdList><ArticleId IdType="doi">10.21037/qims.2019.08.19</ArticleId><ArticleId IdType="pmc">PMC6785505</ArticleId><ArticleId IdType="pubmed">31667139</ArticleId></ArticleIdList></Reference><Reference><Citation>Genc S., Akpinar K.N., Karagol S. Automated Abnormality Classification of Chest Radiographs using MobileNetV2; Proceedings of the International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA); Ankara, Turkey. 26&#x2013;28 June 2020; pp. 1&#x2013;4.</Citation></Reference><Reference><Citation>Koonsanit K., Thongvigitmanee S., Pongnapang N., Thajchayapong P. Image enhancement on digital x-ray images using N-CLAHE; Proceedings of the 10th Biomedical Engineering International Conference (BMEiCON); Hokkaido, Japan. 31 August&#x2013;2 September 2017; pp. 1&#x2013;4.</Citation></Reference><Reference><Citation>Kushol R., Raihan M., Salekin M.S., Rahman A. Contrast enhancement of medical X-ray image using morphological operators with optimal structuring element. arXiv. 20191905.08545</Citation></Reference><Reference><Citation>Kumarasinghe H., Kolonne S., Fernando C., Meedeniya D. U-Net Based Chest X-ray Segmentation with Ensemble Classification for Covid-19 and Pneumonia. Int. J. Online Biomed. Eng. 2022;18:161&#x2013;175. doi: 10.3991/ijoe.v18i07.30807.</Citation><ArticleIdList><ArticleId IdType="doi">10.3991/ijoe.v18i07.30807</ArticleId></ArticleIdList></Reference><Reference><Citation>Gu X., Pan L., Liang H., Yang R. Classification of Bacterial and Viral Childhood Pneumonia Using Deep Learning in Chest Radiography; Proceedings of the 3rd International Conference on Multimedia and Image Processing; Guiyang, China. 16&#x2013;18 March 2018; pp. 88&#x2013;93.</Citation></Reference><Reference><Citation>Sogancioglu E., Murphy K., Calli E., Scholten E., Schalekamp S., Van Ginneken B. Cardiomegaly Detection on Chest Radiographs: Segmentation Versus Classification. IEEE Access. 2020;8:94631&#x2013;94642. doi: 10.1109/ACCESS.2020.2995567.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2020.2995567</ArticleId></ArticleIdList></Reference><Reference><Citation>Eslami M., Tabarestani S., Albarqouni S., Adeli E., Navab N., Adjouadi M. Image-to-Images Translation for Multi-Task Organ Segmentation and Bone Suppression in Chest X-ray. IEEE Trans. Med. Imaging. 2020;39:2553&#x2013;2565. doi: 10.1109/TMI.2020.2974159.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2020.2974159</ArticleId><ArticleId IdType="pubmed">32078541</ArticleId></ArticleIdList></Reference><Reference><Citation>Ghali R., Akhloufi M. ARSeg: An Attention RegSeg Architecture for CXR Lung Segmentation; Proceedings of the 23rd International Conference on Information Reuse and Integration for Data Science (IRI); San Diego, CA, USA. 9&#x2013;11 August 2022; pp. 291&#x2013;296.</Citation></Reference><Reference><Citation>Dai W., Dong N., Wang Z., Liang X., Zhang H., Xing E.P. Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support. Springer; Cham, Switzerland: 2018. Scan: Structure correcting adversarial network for organ segmentation in chest X-rays; pp. 263&#x2013;273.</Citation></Reference><Reference><Citation>Liu W., Luo J., Yang Y., Wang W., Deng J., Yu L. Automatic lung segmentation in chest X-ray images using improved U-Net. Sci. Rep. 2022;12:8649. doi: 10.1038/s41598-022-12743-y.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-022-12743-y</ArticleId><ArticleId IdType="pmc">PMC9127108</ArticleId><ArticleId IdType="pubmed">35606509</ArticleId></ArticleIdList></Reference><Reference><Citation>Ronneberger O., Fischer P., Brox T. International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer; Cham, Switzerland: 2015. U-net: Convolutional networks for biomedical image segmentation; pp. 234&#x2013;241.</Citation></Reference><Reference><Citation>Long J., Shelhamer E., Darrell T. Fully Convolutional Networks for Semantic Segmentation; Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR); Boston, MA, USA. 7&#x2013;12 June 2015.</Citation></Reference><Reference><Citation>Matsubara N., Teramoto A., Saito K., Fujita H. Bone suppression for chest X-ray image using a convolutional neural filter. Phys. Eng. Sci. Med. 2020;43:97&#x2013;108. doi: 10.1007/s13246-019-00822-w.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s13246-019-00822-w</ArticleId><ArticleId IdType="pubmed">31773501</ArticleId></ArticleIdList></Reference><Reference><Citation>Sato K., Kanno N., Ishii T., Saijo Y. Computer-aided Detection of Lung Tumors in Chest X-ray Images Using a Bone Suppression Algorithm and A Deep Learning Framework. J. Phys. Conf. Ser. 2021;2071:012002. doi: 10.1088/1742-6596/2071/1/012002.</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/1742-6596/2071/1/012002</ArticleId></ArticleIdList></Reference><Reference><Citation>Zarshenas A., Liu J., Forti P., Suzuki K. Separation of bones from soft tissue in chest radiographs: Anatomy-specific orientation-frequency-specific deep neural network convolution. Med. Phys. 2019;46:2232&#x2013;2242. doi: 10.1002/mp.13468.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mp.13468</ArticleId><ArticleId IdType="pmc">PMC6510604</ArticleId><ArticleId IdType="pubmed">30848498</ArticleId></ArticleIdList></Reference><Reference><Citation>Rajaraman S., Cohen G., Spear L., Folio L., Antani S. DeBoNet: A deep bone suppression model ensemble to improve disease detection in chest radiographs. PLoS ONE. 2022;17:e0265691. doi: 10.1371/journal.pone.0265691.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0265691</ArticleId><ArticleId IdType="pmc">PMC8970404</ArticleId><ArticleId IdType="pubmed">35358235</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhou Z., Zhou L., Shen K. Dilated conditional GAN for bone suppression in chest radiographs with enforced semantic features. Med. Phys. 2020;47:6207&#x2013;6215. doi: 10.1002/mp.14371.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mp.14371</ArticleId><ArticleId IdType="pubmed">32621786</ArticleId></ArticleIdList></Reference><Reference><Citation>Gordienko Y., Gang P., Hui J., Zeng W., Kochura Y., Alienin O., Rokovyi O., Stirenko S. International Conference on Computer Science, Engineering and Education Applications. Springer; Cham, Switzerland: 2019. Deep Learning with Lung Segmentation and Bone Shadow Exclusion Techniques for Chest X-ray Analysis of Lung Cancer; pp. 638&#x2013;647.</Citation></Reference><Reference><Citation>Siddiqi R. Automated Pneumonia Diagnosis using a Customized Sequential Convolutional Neural Network; Proceedings of the 3rd International Conference on Deep Learning Technologies; Xiamen, China. 5&#x2013;7 July 2019; pp. 64&#x2013;70.</Citation></Reference><Reference><Citation>Ma Y., Lv W. Identification of Pneumonia in Chest X-ray Image Based on Transformer. Int. J. Antennas Propag. 2022;2022:5072666. doi: 10.1155/2022/5072666.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2022/5072666</ArticleId></ArticleIdList></Reference><Reference><Citation>Singh S., Rawat S., Gupta M., Tripathi B., Alanzi F., Majumdar A., Khuwuthyakorn P., Thinnukool O. Deep Attention Network for Pneumonia Detection Using Chest X-ray Images. Comput. Mater. Contin. 2022;74:1673&#x2013;1691. doi: 10.32604/cmc.2023.032364.</Citation><ArticleIdList><ArticleId IdType="doi">10.32604/cmc.2023.032364</ArticleId></ArticleIdList></Reference><Reference><Citation>Darapaneni N., Ranjan A., Bright D., Trivedi D., Kumar K., Kumar V., Paduri A.R. Pneumonia Detection in Chest X-rays using Neural Networks. arXiv. 20222204.03618</Citation></Reference><Reference><Citation>Rajpurkar P., Irvin J., Zhu K., Yang B., Mehta H., Duan T., Ding D., Bagul A., Langlotz C., Shpanskaya K. Chexnet: Radiologist-level pneumonia detection on chest X-rays with deep learning. arXiv. 20171711.05225</Citation></Reference><Reference><Citation>Kundu R., Das R., Geem Z.W., Han G.T., Sarkar R. Pneumonia detection in chest X-ray images using an ensemble of deep learning models. PLoS ONE. 2021;16:e0256630. doi: 10.1371/journal.pone.0256630.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0256630</ArticleId><ArticleId IdType="pmc">PMC8423280</ArticleId><ArticleId IdType="pubmed">34492046</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang J., Xie Y., Pang G., Liao Z., Verjans J., Li W., Sun Z., He J., Li Y., Shen C., et al. Viral Pneumonia Screening on Chest X-rays Using Confidence-Aware Anomaly Detection. IEEE Trans. Med. Imaging. 2020;40:879&#x2013;890. doi: 10.1109/TMI.2020.3040950.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2020.3040950</ArticleId><ArticleId IdType="pmc">PMC8544953</ArticleId><ArticleId IdType="pubmed">33245693</ArticleId></ArticleIdList></Reference><Reference><Citation>Sharma H., Jain J.S., Bansal P., Gupta S. Feature Extraction and Classification of Chest X-ray Images Using CNN to Detect Pneumonia; Proceedings of the 10th International Conference on Cloud Computing, Data Science and Engineering (Confluence); Noida, India. 29&#x2013;31 January 2020; pp. 227&#x2013;231.</Citation></Reference><Reference><Citation>Stephen O., Sain M., Maduh U.J., Jeong D.U. An Efficient Deep Learning Approach to Pneumonia Classification in Healthcare. J. Healthc. Eng. 2019;2019:4180949. doi: 10.1155/2019/4180949.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2019/4180949</ArticleId><ArticleId IdType="pmc">PMC6458916</ArticleId><ArticleId IdType="pubmed">31049186</ArticleId></ArticleIdList></Reference><Reference><Citation>JF Healthcare  World-Class AI.  [(accessed on 1 November 2022)].  Available online:  http://www.jfhealthcare.com/en/</Citation></Reference><Reference><Citation>World Health Organization  World Cancer Report.  [(accessed on 1 November 2022)].  Available online:  https://www.who.int/cancer/publications/WRC_2014/en/</Citation></Reference><Reference><Citation>Sim Y., Chung M.J., Kotter E., Yune S., Kim M., Do S., Han K., Kim H., Yang S., Lee D.J., et al. Deep Convolutional Neural Network&#x2013;based Software Improves Radiologist Detection of Malignant Lung Nodules on Chest Radiographs. Radiology. 2020;294:199&#x2013;209. doi: 10.1148/radiol.2019182465.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2019182465</ArticleId><ArticleId IdType="pubmed">31714194</ArticleId></ArticleIdList></Reference><Reference><Citation>Thamilarasi V., Roselin R. Automatic classification and accuracy by deep learning using cnn methods in lung chest X-ray images. IOP Conf. Ser. Mater. Sci. Eng. 2021;1055:012099. doi: 10.1088/1757-899X/1055/1/012099.</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/1757-899X/1055/1/012099</ArticleId></ArticleIdList></Reference><Reference><Citation>Bush I. Lung nodule detection and classification. Rep. Stanf. Comput. Sci. 2016;20:196&#x2013;209.</Citation></Reference><Reference><Citation>Pesce E., Withey S.J., Ypsilantis P.P., Bakewell R., Goh V., Montana G. Learning to detect chest radiographs containing pulmonary lesions using visual attention networks. Med. Image Anal. 2019;53:26&#x2013;38. doi: 10.1016/j.media.2018.12.007.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2018.12.007</ArticleId><ArticleId IdType="pubmed">30660946</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang X., Yu J., Zhu Q., Li S., Zhao Z., Yang B., Pu J. Potential of deep learning in assessing pneumoconiosis depicted on digital chest radiography. Occup. Environ. Med. 2020;77:597&#x2013;602. doi: 10.1136/oemed-2019-106386.</Citation><ArticleIdList><ArticleId IdType="doi">10.1136/oemed-2019-106386</ArticleId><ArticleId IdType="pubmed">32471837</ArticleId></ArticleIdList></Reference><Reference><Citation>Li X., Shen L., Xie X., Huang S., Xie Z., Hong X., Yu J. Multi-resolution convolutional networks for chest X-ray radiograph based lung nodule detection. Artif. Intell. Med. 2020;103:101744. doi: 10.1016/j.artmed.2019.101744.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.artmed.2019.101744</ArticleId><ArticleId IdType="pubmed">31732411</ArticleId></ArticleIdList></Reference><Reference><Citation>Kim Y.G., Lee S.M., Lee K.H., Jang R., Seo J.B., Kim N. Optimal matrix size of chest radiographs for computer-aided detection on lung nodule or mass with deep learning. Eur. Radiol. 2020;30:4943&#x2013;4951. doi: 10.1007/s00330-020-06892-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-020-06892-9</ArticleId><ArticleId IdType="pubmed">32350657</ArticleId></ArticleIdList></Reference><Reference><Citation>Iqbal A., Usman M., Ahmed Z. An efficient deep learning-based framework for tuberculosis detection using chest X-ray images. Tuberculosis. 2022;136:102234. doi: 10.1016/j.tube.2022.102234.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.tube.2022.102234</ArticleId><ArticleId IdType="pubmed">35872406</ArticleId></ArticleIdList></Reference><Reference><Citation>Xu T., Yuan Z. Convolution Neural Network with Coordinate Attention for the Automatic Detection of Pulmonary Tuberculosis Images on Chest X-rays. IEEE Access. 2022;10:86710&#x2013;86717. doi: 10.1109/ACCESS.2022.3199419.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2022.3199419</ArticleId></ArticleIdList></Reference><Reference><Citation>Showkatian E., Salehi M., Ghaffari H., Reiazi R. Deep learning-based automatic detection of tuberculosis disease in chest X-ray images. Pol. J. Radiol. 2022;87:118&#x2013;124. doi: 10.5114/pjr.2022.113435.</Citation><ArticleIdList><ArticleId IdType="doi">10.5114/pjr.2022.113435</ArticleId><ArticleId IdType="pmc">PMC8906182</ArticleId><ArticleId IdType="pubmed">35280947</ArticleId></ArticleIdList></Reference><Reference><Citation>Lakhani P., Sundaram B. Deep learning at chest radiography: Automated classification of pulmonary tuberculosis by using convolutional neural networks. Radiology. 2017;284:574&#x2013;582. doi: 10.1148/radiol.2017162326.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2017162326</ArticleId><ArticleId IdType="pubmed">28436741</ArticleId></ArticleIdList></Reference><Reference><Citation>Rahman T., Khandakar A., Kadir M.A., Islam K.R., Islam K., Mazhar R., Hamid T., Islam M.T., Kashem S., Mahbub Z.B., et al. Reliable Tuberculosis Detection Using Chest X-Ray With Deep Learning, Segmentation and Visualization. IEEE Access. 2020;8:191586&#x2013;191601. doi: 10.1109/ACCESS.2020.3031384.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2020.3031384</ArticleId></ArticleIdList></Reference><Reference><Citation>Dey S., Roychoudhury R., Malakar S., Sarkar R. An optimized fuzzy ensemble of convolutional neural networks for detecting tuberculosis from Chest X-ray images. Appl. Soft Comput. 2021;114:108094. doi: 10.1016/j.asoc.2021.108094.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.asoc.2021.108094</ArticleId></ArticleIdList></Reference><Reference><Citation>Hooda R., Sofat S., Kaur S., Mittal A., Meriaudeau F. Deep-learning: A potential method for tuberculosis detection using chest radiography; Proceedings of the International Conference on Signal and Image Processing Applications (ICSIPA); Kuching, Malaysia. 12&#x2013;14 September 2017; pp. 497&#x2013;502.</Citation></Reference><Reference><Citation>Nguyen Q., Nguyen B., Dao S., Unnikrishnan B., Dhingra R., Ravichandran S.R., Satpathy S., Raja P.N., Chua M. Deep Learning Models for Tuberculosis Detection from Chest X-ray Images; Proceedings of the 26th International Conference on Telecommunications (ICT); Hanoi, Vietnam. 8&#x2013;10 April 2019; pp. 381&#x2013;385.</Citation></Reference><Reference><Citation>Lopes U., Valiati J.F. Pre-trained convolutional neural networks as feature extractors for tuberculosis detection. Comput. Biol. Med. 2017;89:135&#x2013;143. doi: 10.1016/j.compbiomed.2017.08.001.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2017.08.001</ArticleId><ArticleId IdType="pubmed">28800442</ArticleId></ArticleIdList></Reference><Reference><Citation>Meraj S.S., Yaakob R., Azman A., Rum S., Shahrel A., Nazri A., Zakaria N.F. Detection of pulmonary tuberculosis manifestation in chest X-rays using different convolutional neural network (CNN) models. Int. J. Eng. Adv. Technol. (IJEAT) 2019;9:2270&#x2013;2275. doi: 10.35940/ijeat.A2632.109119.</Citation><ArticleIdList><ArticleId IdType="doi">10.35940/ijeat.A2632.109119</ArticleId></ArticleIdList></Reference><Reference><Citation>Abbas A., Abdelsamea M.M., Gaber M.M. DeTrac: Transfer Learning of Class Decomposed Medical Images in Convolutional Neural Networks. IEEE Access. 2020;8:74901&#x2013;74913. doi: 10.1109/ACCESS.2020.2989273.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2020.2989273</ArticleId></ArticleIdList></Reference><Reference><Citation>Islam A., Stea G., Mahmud S., Rahman M. COVID-19 Cases Detection from Chest X-ray Images using CNN based Deep Learning Model. Int. J. Adv. Comput. Sci. Appl. (IJACSA) 2022;13:960&#x2013;971. doi: 10.14569/IJACSA.2022.01305108.</Citation><ArticleIdList><ArticleId IdType="doi">10.14569/IJACSA.2022.01305108</ArticleId></ArticleIdList></Reference><Reference><Citation>Patel P. Chest X-ray (Covid-19 &amp; Pneumonia)  [(accessed on 1 November 2022)].  Available online:  https://www.kaggle.com/datasets/prashant268/chest-xray-covid19-pneumonia.</Citation></Reference><Reference><Citation>Alqahtani A., Akram S., Ramzan M., Nawaz F., Khan H., Alhashlan E., Alqhtani S.M., Waris A., Ali Z. A Transfer Learning Based Approach for COVID-19 Detection Using Inception-v4 Model. Intell. Autom. Soft Comput. 2022;35:1721&#x2013;1736. doi: 10.32604/iasc.2023.025597.</Citation><ArticleIdList><ArticleId IdType="doi">10.32604/iasc.2023.025597</ArticleId></ArticleIdList></Reference><Reference><Citation>Jawahar M., Anbarasi J., Ravi V., Jayachandran P., Jasmine G., Manikandan R., Sekaran R., Kannan S. CovMnet-Deep Learning Model for classifying Coronavirus (COVID-19) Health Technol. 2022;12:1009&#x2013;1024. doi: 10.1007/s12553-022-00688-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s12553-022-00688-1</ArticleId><ArticleId IdType="pmc">PMC9362573</ArticleId><ArticleId IdType="pubmed">35966170</ArticleId></ArticleIdList></Reference><Reference><Citation>Ismael A., &#x15e;eng&#xfc;r A. Deep learning approaches for COVID-19 detection based on chest X-ray images. Expert Syst. Appl. 2021;164:114054. doi: 10.1016/j.eswa.2020.114054.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.eswa.2020.114054</ArticleId><ArticleId IdType="pmc">PMC7521412</ArticleId><ArticleId IdType="pubmed">33013005</ArticleId></ArticleIdList></Reference><Reference><Citation>Brunese L., Mercaldo F., Reginelli A., Santone A. Explainable Deep Learning for Pulmonary Disease and Coronavirus COVID-19 Detection from X-rays. Comput. Methods Programs Biomed. 2020;196:105608. doi: 10.1016/j.cmpb.2020.105608.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cmpb.2020.105608</ArticleId><ArticleId IdType="pmc">PMC7831868</ArticleId><ArticleId IdType="pubmed">32599338</ArticleId></ArticleIdList></Reference><Reference><Citation>Ahsan M., Gupta K.D., Islam M.M., Sen S., Rahman M., Shakhawat Hossain M. COVID-19 Symptoms Detection Based on NasNetMobile with Explainable AI Using Various Imaging Modalities. Mach. Learn. Knowl. Extr. 2020;2:490&#x2013;504. doi: 10.3390/make2040027.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/make2040027</ArticleId></ArticleIdList></Reference><Reference><Citation>Apostolopoulos I., Mpesiana T. Covid-19: Automatic detection from X-ray images utilizing transfer learning with convolutional neural networks. Phys. Eng. Sci. Med. 2020;43:635&#x2013;640. doi: 10.1007/s13246-020-00865-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s13246-020-00865-4</ArticleId><ArticleId IdType="pmc">PMC7118364</ArticleId><ArticleId IdType="pubmed">32524445</ArticleId></ArticleIdList></Reference><Reference><Citation>Maranh&#xe3;o A. COVID-19 X-rays.  [(accessed on 1 November 2022)].  Available online:  https://www.kaggle.com/andrewmvd/convid19-X-rays.</Citation></Reference><Reference><Citation>Nguyen T., Do T.H., Pham Q.D. A Deep Learning based System for Covid-19 Positive Cases Detection Using Chest X-ray Images; Proceedings of the 2022 13th International Conference on Information and Communication Technology Convergence (ICTC); Jeju Island, Republic of Korea. 19&#x2013;21 October 2022; pp. 1082&#x2013;1087.</Citation></Reference><Reference><Citation>Bekhet S., Hassaballah M., Kenk M., Hameed M.A. An Artificial Intelligence Based Technique for COVID-19 Diagnosis from Chest X-ray; Proceedings of the 2nd Novel Intelligent and Leading Emerging Sciences Conference (NILES); Giza, Egypt. 24&#x2013;26 October 2020; pp. 191&#x2013;195.</Citation></Reference><Reference><Citation>Engstrom L., Tran B., Tsipras D., Schmidt L., Madry A. Exploring the Landscape of Spatial Robustness; Proceedings of the 36th International Conference on Machine Learning; Long Beach, CA, USA. 9&#x2013;15 June 2019; pp. 1802&#x2013;1811.</Citation></Reference><Reference><Citation>Sethy P.K., Behera S.K., Anitha K., Pandey C., Khan M. Computer aid screening of COVID-19 using X-ray and CT scan images: An inner comparison. J. X-ray Sci. Technol. 2021;29:197&#x2013;210. doi: 10.3233/XST-200784.</Citation><ArticleIdList><ArticleId IdType="doi">10.3233/XST-200784</ArticleId><ArticleId IdType="pubmed">33492267</ArticleId></ArticleIdList></Reference><Reference><Citation>Chetoui M., Akhloufi M., Yousefi B., Bouattane E.M. Explainable COVID-19 Detection on Chest X-rays Using an End-to-End Deep Convolutional Neural Network Architecture. Big Data Cogn. Comput. 2021;5:73. doi: 10.3390/bdcc5040073.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/bdcc5040073</ArticleId></ArticleIdList></Reference><Reference><Citation>Selvaraju R., Cogswell M., Das A., Vedantam R., Parikh D., Batra D. Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization; Proceedings of the International Conference on Computer Vision (ICCV); Venice, Italy. 22&#x2013;29 October 2017; pp. 618&#x2013;626.</Citation></Reference><Reference><Citation>Hemdan E.E.D., Shouman M., Karar M.E. COVIDX-Net: A Framework of Deep Learning Classifiers to Diagnose COVID-19 in X-ray Images. arXiv. 20202003.11055</Citation></Reference><Reference><Citation>Khan E., Rehman M., Ahmed F., Alfouzan F., Alzahrani N., Ahmad J. Chest X-ray Classification for the Detection of COVID-19 Using Deep Learning Techniques. Sensors. 2022;22:1211. doi: 10.3390/s22031211.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s22031211</ArticleId><ArticleId IdType="pmc">PMC8838072</ArticleId><ArticleId IdType="pubmed">35161958</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang Z., Zhang K., Wang B. Detection of COVID-19 Cases Based on Deep Learning with X-ray Images. Electronics. 2022;11:3511. doi: 10.3390/electronics11213511.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/electronics11213511</ArticleId></ArticleIdList></Reference><Reference><Citation>Majdi M., Salman K., Morris M., Merchant N., Rodriguez J. Deep learning classification of chest X-ray images; Proceedings of the Southwest Symposium on Image Analysis and Interpretation (SSIAI); Albuquerque, NM, USA. 29&#x2013;31 March 2020; pp. 116&#x2013;119.</Citation></Reference><Reference><Citation>Bar Y., Diamant I., Wolf L., Greenspan H. Medical Imaging 2015: Computer-Aided Diagnosis. SPIE; Bellingham, WA, USA: 2015. Deep learning with non-medical training used for chest pathology identification; pp. 215&#x2013;221.</Citation></Reference><Reference><Citation>Cicero M., Bilbily A., Colak E., Dowdell T., Gray B., Perampaladas K., Barfett J. Training and validating a deep convolutional neural network for computer-aided detection and classification of abnormalities on frontal chest radiographs. Investig. Radiol. 2017;52:281&#x2013;287. doi: 10.1097/RLI.0000000000000341.</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/RLI.0000000000000341</ArticleId><ArticleId IdType="pubmed">27922974</ArticleId></ArticleIdList></Reference><Reference><Citation>Yao L., Poblenz E., Dagunts D., Covington B., Bernard D., Lyman K. Learning to diagnose from scratch by exploiting dependencies among labels. arXiv. 20171710.10501</Citation></Reference><Reference><Citation>Kumar P., Grewal M., Srivastava M.M. International Conference Image Analysis and Recognition. Springer; Cham, Switzerland: 2018. Boosted cascaded convnets for multilabel classification of thoracic diseases in chest radiographs; pp. 546&#x2013;552.</Citation></Reference><Reference><Citation>Zhao J., Li M., Shi W., Miao Y., Jiang Z., Ji B. A deep learning method for classification of chest X-ray images. J. Phys. Conf. Ser. 2021;1848:012030. doi: 10.1088/1742-6596/1848/1/012030.</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/1742-6596/1848/1/012030</ArticleId></ArticleIdList></Reference><Reference><Citation>Kim S., Rim B., Choi S., Lee A., Min S., Hong M. Deep Learning in Multi-Class Lung Diseases&#x2019; Classification on Chest X-ray Images. Diagnostics. 2022;12:915. doi: 10.3390/diagnostics12040915.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/diagnostics12040915</ArticleId><ArticleId IdType="pmc">PMC9025806</ArticleId><ArticleId IdType="pubmed">35453963</ArticleId></ArticleIdList></Reference><Reference><Citation>Hong M., Rim B., Lee H., Jang H., Oh J., Choi S. Multi-class classification of lung diseases using CNN models. Appl. Sci. 2021;11:9289. doi: 10.3390/app11199289.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/app11199289</ArticleId></ArticleIdList></Reference><Reference><Citation>Blais M.A., Akhloufi M. Deep Learning and Binary Relevance Classification of Multiple Diseases using Chest X-ray images; Proceedings of the 43rd Annual International Conference of the IEEE Engineering in Medicine Biology Society (EMBC); Guadalajara, Mexico. 1&#x2013;5 November 2021; pp. 2794&#x2013;2797.</Citation><ArticleIdList><ArticleId IdType="pubmed">34891829</ArticleId></ArticleIdList></Reference><Reference><Citation>Smit A., Jain S., Rajpurkar P., Pareek A., Ng A., Lungren M. CheXbert: Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Labeling Using BERT. arXiv. 20202004.09167</Citation></Reference><Reference><Citation>Calli E., Sogancioglu E., Scholten E., Murphy K., van Ginneken B. Medical Imaging 2019: Computer-Aided Diagnosis. SPIE; Bellingham, WA, USA: 2019. Handling label noise through model confidence and uncertainty: Application to chest radiograph classification; p. 1095016.</Citation></Reference><Reference><Citation>Rolnick D., Veit A., Belongie S., Shavit N. Deep Learning is Robust to Massive Label Noise. arXiv. 20171705.10694</Citation></Reference><Reference><Citation>Hussain Z., Andleeb I., Ansari M.S., Joshi A.M., Kanwal N. Wasserstein GAN based Chest X-ray Dataset Augmentation for Deep Learning Models: COVID-19 Detection Use-Case; Proceedings of the 44th Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC); Glasgow, UK. 11&#x2013;15 July 2022; pp. 2058&#x2013;2061.</Citation><ArticleIdList><ArticleId IdType="pubmed">36085636</ArticleId></ArticleIdList></Reference><Reference><Citation>Buragadda S., Rani K.S., Vasantha S.V., Chakravarthi M.K. HCUGAN: Hybrid Cyclic UNET GAN for Generating Augmented Synthetic Images of Chest X-ray Images for Multi Classification of Lung Diseases. Int. J. Eng. Trends Technol. 2022;70:229&#x2013;238. doi: 10.14445/22315381/IJETT-V70I2P227.</Citation><ArticleIdList><ArticleId IdType="doi">10.14445/22315381/IJETT-V70I2P227</ArticleId></ArticleIdList></Reference><Reference><Citation>Barshooi A.H., Amirkhani A. A novel data augmentation based on Gabor filter and convolutional deep learning for improving the classification of COVID-19 chest X-ray images. Biomed. Signal Process. Control. 2022;72:103326. doi: 10.1016/j.bspc.2021.103326.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bspc.2021.103326</ArticleId><ArticleId IdType="pmc">PMC8576144</ArticleId><ArticleId IdType="pubmed">34777557</ArticleId></ArticleIdList></Reference><Reference><Citation>Raoof S., Feigin D., Sung A., Raoof S., Irugulpati L., Rosenow III E.C. Interpretation of plain chest roentgenogram. Chest. 2012;141:545&#x2013;558. doi: 10.1378/chest.10-1302.</Citation><ArticleIdList><ArticleId IdType="doi">10.1378/chest.10-1302</ArticleId><ArticleId IdType="pubmed">22315122</ArticleId></ArticleIdList></Reference><Reference><Citation>Hwang E.J., Nam J.G., Lim W.H., Park S.J., Jeong Y.S., Kang J.H., Hong E.K., Kim T.M., Goo J.M., Park S., et al. Deep Learning for Chest Radiograph Diagnosis in the Emergency Department. Radiology. 2019;293:573&#x2013;580. doi: 10.1148/radiol.2019191225.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2019191225</ArticleId><ArticleId IdType="pubmed">31638490</ArticleId></ArticleIdList></Reference><Reference><Citation>Feki I., Ammar S., Kessentini Y., Muhammad K. Federated learning for COVID-19 screening from Chest X-ray images. Appl. Soft Comput. 2021;106:107330. doi: 10.1016/j.asoc.2021.107330.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.asoc.2021.107330</ArticleId><ArticleId IdType="pmc">PMC7979273</ArticleId><ArticleId IdType="pubmed">33776607</ArticleId></ArticleIdList></Reference><Reference><Citation>Liu B., Yan B., Zhou Y., Yang Y., Zhang Y. Experiments of federated learning for COVID-19 chest x-ray images. arXiv. 20202007.05592</Citation></Reference><Reference><Citation>Singh A., Sengupta S., Lakshminarayanan V. Explainable deep learning models in medical image analysis. J. Imaging. 2020;6:52. doi: 10.3390/jimaging6060052.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/jimaging6060052</ArticleId><ArticleId IdType="pmc">PMC8321083</ArticleId><ArticleId IdType="pubmed">34460598</ArticleId></ArticleIdList></Reference><Reference><Citation>Suamsung Health Care  Auto Lung Nodule Detection.  [(accessed on 1 November 2022)].  Available online:  https://www.samsunghealthcare.com/en/products/DigitalRadiography/</Citation></Reference><Reference><Citation>Siemense Healthineers  AI-Rad Companion.  [(accessed on 1 November 2022)].  Available online:  https://www.siemens-healthineers.com/digital-health-solutions/digital-solutions-overview/clinical-decision-support/ai-rad-companion.</Citation></Reference><Reference><Citation>Oxipit Company  Chesteye Quality.  [(accessed on 1 November 2022)].  Available online:  https://oxipit.ai/products/chesteye/</Citation></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36611423</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Print">2075-4418</ISSN><JournalIssue CitedMedium="Print"><Volume>13</Volume><Issue>1</Issue><PubDate><Year>2022</Year><Month>Dec</Month><Day>30</Day></PubDate></JournalIssue><Title>Diagnostics (Basel, Switzerland)</Title><ISOAbbreviation>Diagnostics (Basel)</ISOAbbreviation></Journal><ArticleTitle>An Efficient Deep Learning Method for Detection of COVID-19 Infection Using Chest X-ray Images.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">131</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/diagnostics13010131</ELocationID><Abstract><AbstractText>The research community has recently shown significant interest in designing automated systems to detect coronavirus disease 2019 (COVID-19) using deep learning approaches and chest radiography images. However, state-of-the-art deep learning techniques, especially convolutional neural networks (CNNs), demand more learnable parameters and memory. Therefore, they may not be suitable for real-time diagnosis. Thus, the design of a lightweight CNN model for fast and accurate COVID-19 detection is an urgent need. In this paper, a lightweight CNN model called LW-CORONet is proposed that comprises a sequence of convolution, rectified linear unit (ReLU), and pooling layers followed by two fully connected layers. The proposed model facilitates extracting meaningful features from the chest X-ray (CXR) images with only five learnable layers. The proposed model is evaluated using two larger CXR datasets (Dataset-1: 2250 images and Dataset-2: 15,999 images) and the classification accuracy obtained are 98.67% and 99.00% on Dataset-1 and 95.67% and 96.25% on Dataset-2 for multi-class and binary classification cases, respectively. The results are compared with four contemporary pre-trained CNN models as well as state-of-the-art models. The effect of several hyperparameters: different optimization techniques, batch size, and learning rate have also been investigated. The proposed model demands fewer parameters and requires less memory space. Hence, it is effective for COVID-19 detection and can be utilized as a supplementary tool to assist radiologists in their diagnosis.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Nayak</LastName><ForeName>Soumya Ranjan</ForeName><Initials>SR</Initials><Identifier Source="ORCID">0000-0002-4155-884X</Identifier><AffiliationInfo><Affiliation>Amity School of Engineering and Technology, Amity University Uttar Pradesh, Noida 201301, India.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Nayak</LastName><ForeName>Deepak Ranjan</ForeName><Initials>DR</Initials><Identifier Source="ORCID">0000-0002-8929-5778</Identifier><AffiliationInfo><Affiliation>Department of Computer Science and Engineering, Malaviya National Institute of Technology, Jaipur 302017, India.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Sinha</LastName><ForeName>Utkarsh</ForeName><Initials>U</Initials><Identifier Source="ORCID">0000-0001-7312-7625</Identifier><AffiliationInfo><Affiliation>Amity School of Engineering and Technology, Amity University Uttar Pradesh, Noida 201301, India.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Arora</LastName><ForeName>Vaibhav</ForeName><Initials>V</Initials><AffiliationInfo><Affiliation>Amity School of Engineering and Technology, Amity University Uttar Pradesh, Noida 201301, India.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Pachori</LastName><ForeName>Ram Bilas</ForeName><Initials>RB</Initials><Identifier Source="ORCID">0000-0002-6061-4309</Identifier><AffiliationInfo><Affiliation>Department of Electrical Engineering, Indian Institute of Technology Indore, Indore 453552, India.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>30</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Diagnostics (Basel)</MedlineTA><NlmUniqueID>101658402</NlmUniqueID><ISSNLinking>2075-4418</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">CNN</Keyword><Keyword MajorTopicYN="N">COVID-19</Keyword><Keyword MajorTopicYN="N">LW-CORONet</Keyword><Keyword MajorTopicYN="N">chest X-ray</Keyword><Keyword MajorTopicYN="N">transfer learning</Keyword></KeywordList><CoiStatement>The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the results.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>11</Month><Day>22</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>6</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>19</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>1</Hour><Minute>3</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36611423</ArticleId><ArticleId IdType="pmc">PMC9818579</ArticleId><ArticleId IdType="doi">10.3390/diagnostics13010131</ArticleId><ArticleId IdType="pii">diagnostics13010131</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>WHO  Coronavirus Disease 2019 (COVID-19) Situation Report&#x2013;127. 2020.  [(accessed on 21 November 2022)].  Available online:  https://www.who.int/docs/default-source/coronaviruse/situation-reports/20200526-covid-19-sitrep-127.pdf?sfvrsn=7b6655ab_8.</Citation></Reference><Reference><Citation>Huang C., Wang Y., Li X., Ren L., Zhao J., Hu Y., Zhang L., Fan G., Xu J., Gu X., et al. Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China. Lancet. 2020;395:497&#x2013;506. doi: 10.1016/S0140-6736(20)30183-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0140-6736(20)30183-5</ArticleId><ArticleId IdType="pmc">PMC7159299</ArticleId><ArticleId IdType="pubmed">31986264</ArticleId></ArticleIdList></Reference><Reference><Citation>Giacomo G., Antonio P., Maurizio C. Critical Care Utilization for the COVID-19 Outbreak in Lombardy, Italy. JAMA. 2020;323:1545.</Citation><ArticleIdList><ArticleId IdType="pubmed">32167538</ArticleId></ArticleIdList></Reference><Reference><Citation>Jiang X., Coffee M., Bari A., Wang J., Jiang X., Huang J., Shi J., Dai J., Cai J., Zhang T., et al. Towards an artificial intelligence framework for data-driven prediction of coronavirus clinical severity. CMC Comput. Mater. Contin. 2020;63:537&#x2013;551. doi: 10.32604/cmc.2020.010691.</Citation><ArticleIdList><ArticleId IdType="doi">10.32604/cmc.2020.010691</ArticleId></ArticleIdList></Reference><Reference><Citation>Fang Y., Zhang H., Xie J., Lin M., Ying L., Pang P., Ji W. Sensitivity of chest CT for COVID-19: Comparison to RT-PCR. Radiology. 2020;296:200432. doi: 10.1148/radiol.2020200432.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2020200432</ArticleId><ArticleId IdType="pmc">PMC7233365</ArticleId><ArticleId IdType="pubmed">32073353</ArticleId></ArticleIdList></Reference><Reference><Citation>Dong D., Tang Z., Wang S., Hui H., Gong L., Lu Y., Xue Z., Liao H., Chen F., Yang F., et al. The role of imaging in the detection and management of COVID-19: A review. IEEE Rev. Biomed. Eng. 2021;14:16&#x2013;29. doi: 10.1109/RBME.2020.2990959.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/RBME.2020.2990959</ArticleId><ArticleId IdType="pubmed">32356760</ArticleId></ArticleIdList></Reference><Reference><Citation>Kanne J.P., Little B.P., Chung J.H., Elicker B.M., Ketai L.H. Essentials for radiologists on COVID-19: An update radiology scientific expert panel. Radiology. 2020;296:E113&#x2013;E114. doi: 10.1148/radiol.2020200527.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2020200527</ArticleId><ArticleId IdType="pmc">PMC7233379</ArticleId><ArticleId IdType="pubmed">32105562</ArticleId></ArticleIdList></Reference><Reference><Citation>Chung M., Bernheim A., Mei X., Zhang N., Huang M., Zeng X., Cui J., Xu W., Yang Y., Fayad Z.A., et al. CT imaging features of 2019 novel coronavirus (2019-nCoV) Radiology. 2020;295:202&#x2013;207. doi: 10.1148/radiol.2020200230.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2020200230</ArticleId><ArticleId IdType="pmc">PMC7194022</ArticleId><ArticleId IdType="pubmed">32017661</ArticleId></ArticleIdList></Reference><Reference><Citation>Joshi A.M., Nayak D.R. MFL-Net: An Efficient Lightweight Multi-Scale Feature Learning CNN for COVID-19 Diagnosis from CT Images. IEEE J. Biomed. Health Inform. 2022;26:5355&#x2013;5363. doi: 10.1109/JBHI.2022.3196489.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/JBHI.2022.3196489</ArticleId><ArticleId IdType="pubmed">35981061</ArticleId></ArticleIdList></Reference><Reference><Citation>Xie X., Zhong Z., Zhao W., Zheng C., Wang F., Liu J. Chest CT for typical 2019-nCoV pneumonia: Relationship to negative RT-PCR testing. Radiology. 2020;296:200343. doi: 10.1148/radiol.2020200343.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2020200343</ArticleId><ArticleId IdType="pmc">PMC7233363</ArticleId><ArticleId IdType="pubmed">32049601</ArticleId></ArticleIdList></Reference><Reference><Citation>Shi F., Wang J., Shi J., Wu Z., Wang Q., Tang Z., He K., Shi Y., Shen D. Review of artificial intelligence techniques in imaging data acquisition, segmentation and diagnosis for COVID-19. IEEE Rev. Biomed. Eng. 2021;14:4&#x2013;15. doi: 10.1109/RBME.2020.2987975.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/RBME.2020.2987975</ArticleId><ArticleId IdType="pubmed">32305937</ArticleId></ArticleIdList></Reference><Reference><Citation>Gu X., Pan L., Liang H., Yang R. Classification of bacterial and viral childhood pneumonia using deep learning in chest radiography; Proceedings of the 3rd International Conference on Multimedia and Image Processing; Guiyang, China. 16&#x2013;18 March 2018; pp. 88&#x2013;93.</Citation></Reference><Reference><Citation>Chouhan V., Singh S.K., Khamparia A., Gupta D., Tiwari P., Moreira C., Dama&#x161;evi&#x10d;ius R., De Albuquerque V.H.C. A novel transfer learning based approach for pneumonia detection in chest X-ray images. Appl. Sci. 2020;10:559. doi: 10.3390/app10020559.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/app10020559</ArticleId></ArticleIdList></Reference><Reference><Citation>Lakhani P., Sundaram B. Deep learning at chest radiography: Automated classification of pulmonary tuberculosis by using convolutional neural networks. Radiology. 2017;284:574&#x2013;582. doi: 10.1148/radiol.2017162326.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2017162326</ArticleId><ArticleId IdType="pubmed">28436741</ArticleId></ArticleIdList></Reference><Reference><Citation>Rajpurkar P., Irvin J., Ball R.L., Zhu K., Yang B., Mehta H., Duan T., Ding D., Bagul A., Langlotz C.P., et al. Deep learning for chest radiograph diagnosis: A retrospective comparison of the CheXNeXt algorithm to practicing radiologists. PLoS Med. 2018;15:e1002686. doi: 10.1371/journal.pmed.1002686.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pmed.1002686</ArticleId><ArticleId IdType="pmc">PMC6245676</ArticleId><ArticleId IdType="pubmed">30457988</ArticleId></ArticleIdList></Reference><Reference><Citation>Chaudhary P.K., Pachori R.B. Automatic diagnosis of COVID-19 and pneumonia using FBD method; Proceedings of the 2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM); Seoul, Republic of Korea. 16&#x2013;19 December 2020; pp. 2257&#x2013;2263.</Citation></Reference><Reference><Citation>Ozturk T., Talo M., Yildirim E.A., Baloglu U.B., Yildirim O., Acharya U.R. Automated detection of COVID-19 cases using deep neural networks with X-ray images. Comput. Biol. Med. 2020;121:103792. doi: 10.1016/j.compbiomed.2020.103792.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2020.103792</ArticleId><ArticleId IdType="pmc">PMC7187882</ArticleId><ArticleId IdType="pubmed">32568675</ArticleId></ArticleIdList></Reference><Reference><Citation>Hemdan E.E.D., Shouman M.A., Karar M.E. COVIDX-Net: A framework of deep learning classifiers to diagnose covid-19 in X-ray images. arXiv. 20202003.11055</Citation></Reference><Reference><Citation>Narin A., Kaya C., Pamuk Z. Automatic detection of coronavirus disease (COVID-19) using X-ray images and deep convolutional neural networks. arXiv. 2020 doi: 10.1007/s10044-021-00984-y.2003.10849</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10044-021-00984-y</ArticleId><ArticleId IdType="pmc">PMC8106971</ArticleId><ArticleId IdType="pubmed">33994847</ArticleId></ArticleIdList></Reference><Reference><Citation>Ucar F., Korkmaz D. COVIDiagnosis-Net: Deep Bayes-SqueezeNet based Diagnostic of the Coronavirus Disease 2019 (COVID-19) from X-Ray Images. Med. Hypotheses. 2020;140:109761. doi: 10.1016/j.mehy.2020.109761.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.mehy.2020.109761</ArticleId><ArticleId IdType="pmc">PMC7179515</ArticleId><ArticleId IdType="pubmed">32344309</ArticleId></ArticleIdList></Reference><Reference><Citation>Rahimzadeh M., Attar A. A modified deep convolutional neural network for detecting COVID-19 and pneumonia from chest X-ray images based on the concatenation of Xception and ResNet50V2. Inform. Med. Unlocked. 2020;19:100360. doi: 10.1016/j.imu.2020.100360.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.imu.2020.100360</ArticleId><ArticleId IdType="pmc">PMC7255267</ArticleId><ArticleId IdType="pubmed">32501424</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang L., Lin Z.Q., Wong A. COVID-Net: A tailored deep convolutional neural network design for detection of COVID-19 cases from chest X-ray images. Sci. Rep. 2020;10:19549. doi: 10.1038/s41598-020-76550-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-020-76550-z</ArticleId><ArticleId IdType="pmc">PMC7658227</ArticleId><ArticleId IdType="pubmed">33177550</ArticleId></ArticleIdList></Reference><Reference><Citation>To&#x11f;a&#xe7;ar M., Ergen B., C&#xf6;mert Z. COVID-19 detection using deep learning models to exploit Social Mimic Optimization and structured chest X-ray images using fuzzy color and stacking approaches. Comput. Biol. Med. 2020;121:103805. doi: 10.1016/j.compbiomed.2020.103805.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2020.103805</ArticleId><ArticleId IdType="pmc">PMC7202857</ArticleId><ArticleId IdType="pubmed">32568679</ArticleId></ArticleIdList></Reference><Reference><Citation>Toraman S., Alaku&#x15f; T.B., T&#xfc;rko&#x11f;lu &#x130;. Convolutional capsnet: A novel artificial neural network approach to detect COVID-19 disease from X-ray images using capsule networks. Chaos Solitons Fractals. 2020;140:110122. doi: 10.1016/j.chaos.2020.110122.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.chaos.2020.110122</ArticleId><ArticleId IdType="pmc">PMC7357532</ArticleId><ArticleId IdType="pubmed">32834634</ArticleId></ArticleIdList></Reference><Reference><Citation>Han Z., Wei B., Hong Y., Li T., Cong J., Zhu X., Wei H., Zhang W. Accurate Screening of COVID-19 using Attention Based Deep 3D Multiple Instance Learning. IEEE Trans. Med. Imaging. 2020;39:2584&#x2013;2594. doi: 10.1109/TMI.2020.2996256.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2020.2996256</ArticleId><ArticleId IdType="pubmed">32730211</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang Y.D., Satapathy S.C., Zhu L.Y., G&#xf3;rriz J.M., Wang S.H. A seven-layer convolutional neural network for chest CT based COVID-19 diagnosis using stochastic pooling. IEEE Sens. J. 2020;22:17573&#x2013;17582. doi: 10.1109/JSEN.2020.3025855.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/JSEN.2020.3025855</ArticleId><ArticleId IdType="pmc">PMC9564037</ArticleId><ArticleId IdType="pubmed">36346095</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang S.H., Nayak D.R., Guttery D.S., Zhang X., Zhang Y.D. COVID-19 classification by CCSHNet with deep fusion using transfer learning and discriminant correlation analysis. Inf. Fusion. 2021;68:131&#x2013;148. doi: 10.1016/j.inffus.2020.11.005.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.inffus.2020.11.005</ArticleId><ArticleId IdType="pmc">PMC7837204</ArticleId><ArticleId IdType="pubmed">33519321</ArticleId></ArticleIdList></Reference><Reference><Citation>Chaudhary P.K., Pachori R.B. FBSED based automatic diagnosis of COVID-19 using X-ray and CT images. Comput. Biol. Med. 2021;134:104454. doi: 10.1016/j.compbiomed.2021.104454.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2021.104454</ArticleId><ArticleId IdType="pmc">PMC8088544</ArticleId><ArticleId IdType="pubmed">33965836</ArticleId></ArticleIdList></Reference><Reference><Citation>Joshi A.M., Nayak D.R., Das D., Zhang Y.D. LiMS-Net: A Lightweight Multi-Scale CNN for COVID-19 Detection from Chest CT Scans. ACM Trans. Manag. Inf. Syst. (TMIS) 2022 doi: 10.1145/3551647.</Citation><ArticleIdList><ArticleId IdType="doi">10.1145/3551647</ArticleId></ArticleIdList></Reference><Reference><Citation>Bhattacharyya A., Bhaik D., Kumar S., Thakur P., Sharma R., Pachori R.B. A deep learning based approach for automatic detection of COVID-19 cases using chest X-ray images. Biomed. Signal Process. Control. 2022;71:103182. doi: 10.1016/j.bspc.2021.103182.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bspc.2021.103182</ArticleId><ArticleId IdType="pmc">PMC8457928</ArticleId><ArticleId IdType="pubmed">34580596</ArticleId></ArticleIdList></Reference><Reference><Citation>Jyoti K., Sushma S., Kumar P., Yadav S., Pachori R.B., Mukherjee S. Automatic diagnosis of COVID-19 with MCA-inspired TQWT-based classification of chest X-ray images. Comput. Biol. Med. 2022;152:106331. doi: 10.1016/j.compbiomed.2022.106331.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2022.106331</ArticleId><ArticleId IdType="pmc">PMC9683525</ArticleId><ArticleId IdType="pubmed">36502692</ArticleId></ArticleIdList></Reference><Reference><Citation>Nayak S.R., Nayak D.R., Sinha U., Arora V., Pachori R.B. Application of deep learning techniques for detection of COVID-19 cases using chest X-ray images: A comprehensive study. Biomed. Signal Process. Control. 2021;64:102365. doi: 10.1016/j.bspc.2020.102365.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bspc.2020.102365</ArticleId><ArticleId IdType="pmc">PMC7674150</ArticleId><ArticleId IdType="pubmed">33230398</ArticleId></ArticleIdList></Reference><Reference><Citation>Simonyan K., Zisserman A. Very deep convolutional networks for large-scale image recognition. arXiv. 20141409.1556</Citation></Reference><Reference><Citation>He K., Zhang X., Ren S., Sun J. Deep residual learning for image recognition; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Las Vegas, NV, USA. 27&#x2013;30 June 2016; pp. 770&#x2013;778.</Citation></Reference><Reference><Citation>Huang G., Liu Z., Van Der Maaten L., Weinberger K.Q. Densely connected convolutional networks; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Honolulu, HI, USA. 21&#x2013;26 July 2017; pp. 4700&#x2013;4708.</Citation></Reference><Reference><Citation>Chollet F. Xception: Deep learning with depthwise separable convolutions; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Honolulu, HI, USA. 21&#x2013;26 July 2017; pp. 1251&#x2013;1258.</Citation></Reference><Reference><Citation>Haghanifar A., Majdabadi M.M., Choi Y., Deivalakshmi S., Ko S. COVID-CXNet: Detecting COVID-19 in frontal chest X-ray images using deep learning. arXiv. 2020 doi: 10.1007/s11042-022-12156-z.2006.13807</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11042-022-12156-z</ArticleId><ArticleId IdType="pmc">PMC8989406</ArticleId><ArticleId IdType="pubmed">35431611</ArticleId></ArticleIdList></Reference><Reference><Citation>Senthilkumaran N., Thimmiaraja J. Histogram equalization for image enhancement using MRI brain images; Proceedings of the 2014 World Congress on Computing and Communication Technologies; Trichirappalli, India. 27 February&#x2013;1 March 2014; pp. 80&#x2013;83.</Citation></Reference><Reference><Citation>Soomro T.A., Afifi A.J., Shah A.A., Soomro S., Baloch G.A., Zheng L., Yin M., Gao J. Impact of Image Enhancement Technique on CNN Model for Retinal Blood Vessels Segmentation. IEEE Access. 2019;7:158183&#x2013;158197. doi: 10.1109/ACCESS.2019.2950228.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2019.2950228</ArticleId></ArticleIdList></Reference><Reference><Citation>Agrafiotis D. Academic Press Library in Signal Processing. Volume 5. Elsevier; Amsterdam, The Netherlands: 2014. Video Error Concealment; pp. 295&#x2013;321.</Citation></Reference><Reference><Citation>Pisano E.D., Zong S., Hemminger B.M., DeLuca M., Johnston R.E., Muller K., Braeuning M.P., Pizer S.M. Contrast limited adaptive histogram equalization image processing to improve the detection of simulated spiculations in dense mammograms. J. Digit. Imaging. 1998;11:193. doi: 10.1007/BF03178082.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/BF03178082</ArticleId><ArticleId IdType="pmc">PMC3453156</ArticleId><ArticleId IdType="pubmed">9848052</ArticleId></ArticleIdList></Reference><Reference><Citation>Nayak D.R., Dash R., Majhi B. Discrete ripplet-II transform and modified PSO based improved evolutionary extreme learning machine for pathological brain detection. Neurocomputing. 2018;282:232&#x2013;247. doi: 10.1016/j.neucom.2017.12.030.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neucom.2017.12.030</ArticleId></ArticleIdList></Reference><Reference><Citation>Shin H.C., Roth H.R., Gao M., Lu L., Xu Z., Nogues I., Yao J., Mollura D., Summers R.M. Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning. IEEE Trans. Med. Imaging. 2016;35:1285&#x2013;1298. doi: 10.1109/TMI.2016.2528162.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2016.2528162</ArticleId><ArticleId IdType="pmc">PMC4890616</ArticleId><ArticleId IdType="pubmed">26886976</ArticleId></ArticleIdList></Reference><Reference><Citation>Shorten C., Khoshgoftaar T.M. A survey on image data augmentation for deep learning. J. Big Data. 2019;6:60. doi: 10.1186/s40537-019-0197-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s40537-019-0197-0</ArticleId><ArticleId IdType="pmc">PMC8287113</ArticleId><ArticleId IdType="pubmed">34306963</ArticleId></ArticleIdList></Reference><Reference><Citation>Nayak D.R., Dash R., Majhi B. Automated Diagnosis of Multi-class Brain Abnormalities using MRI Images: A Deep Convolutional Neural Network based Method. Pattern Recognit. Lett. 2020;138:385&#x2013;391. doi: 10.1016/j.patrec.2020.04.018.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.patrec.2020.04.018</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang Y.D., Nayak D.R., Zhang X., Wang S.H. Diagnosis of secondary pulmonary tuberculosis by an eight-layer improved convolutional neural network with stochastic pooling and hyperparameter optimization. J. Ambient. Intell. Humaniz. Comput. 2020:1&#x2013;18. doi: 10.1007/s12652-020-02612-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s12652-020-02612-9</ArticleId></ArticleIdList></Reference><Reference><Citation>Swati Z.N.K., Zhao Q., Kabir M., Ali F., Ali Z., Ahmed S., Lu J. Brain tumor classification for MR images using transfer learning and fine-tuning. Comput. Med. Imaging Graph. 2019;75:34&#x2013;46. doi: 10.1016/j.compmedimag.2019.05.001.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compmedimag.2019.05.001</ArticleId><ArticleId IdType="pubmed">31150950</ArticleId></ArticleIdList></Reference><Reference><Citation>Litjens G., Kooi T., Bejnordi B.E., Setio A.A.A., Ciompi F., Ghafoorian M., Van Der Laak J.A., Van Ginneken B., S&#xe1;nchez C.I. A survey on deep learning in medical image analysis. Med. Image Anal. 2017;42:60&#x2013;88. doi: 10.1016/j.media.2017.07.005.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2017.07.005</ArticleId><ArticleId IdType="pubmed">28778026</ArticleId></ArticleIdList></Reference><Reference><Citation>Ioffe S., Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv. 20151502.03167</Citation></Reference><Reference><Citation>Boureau Y.L., Ponce J., LeCun Y. A theoretical analysis of feature pooling in visual recognition; Proceedings of the 27th International Conference on Machine Learning; Haifa, Israel. 21&#x2013;24 June 2010; pp. 111&#x2013;118.</Citation></Reference><Reference><Citation>Selvaraju R.R., Cogswell M., Das A., Vedantam R., Parikh D., Batra D. Grad-CAM: Visual explanations from deep networks via gradient-based localization; Proceedings of the IEEE International Conference on Computer Vision; Venice, Italy. 22&#x2013;29 October 2017; pp. 618&#x2013;626.</Citation></Reference><Reference><Citation>Sutskever I., Martens J., Dahl G., Hinton G. On the importance of initialization and momentum in deep learning; Proceedings of the International Conference on Machine Learning; Atlanta, GA, USA. 16&#x2013;21 June 2013; pp. 1139&#x2013;1147.</Citation></Reference><Reference><Citation>Kingma D.P., Ba J. Adam: A method for stochastic optimization. arXiv. 20141412.6980</Citation></Reference><Reference><Citation>Hinton G., Srivastava N., Swersky K. Neural Networks for Machine Learning. Lecture 6a Overview of Mini-Batch Gradient Descent Course. 2012.  [(accessed on 21 November 2022)].  Available online:  https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf.</Citation></Reference><Reference><Citation>Duchi J., Hazan E., Singer Y. Adaptive subgradient methods for online learning and stochastic optimization. J. Mach. Learn. Res. 2011;12:2121&#x2013;2159.</Citation></Reference><Reference><Citation>Krizhevsky A., Sutskever I., Hinton G.E. Imagenet classification with deep convolutional neural networks; Proceedings of the Advances in Neural Information Processing Systems; Lake Tahoe, NV, USA. 3&#x2013;6 December 2012; pp. 1097&#x2013;1105.</Citation></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36611418</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Print">2075-4418</ISSN><JournalIssue CitedMedium="Print"><Volume>13</Volume><Issue>1</Issue><PubDate><Year>2022</Year><Month>Dec</Month><Day>30</Day></PubDate></JournalIssue><Title>Diagnostics (Basel, Switzerland)</Title><ISOAbbreviation>Diagnostics (Basel)</ISOAbbreviation></Journal><ArticleTitle>A Multi-Stage Approach to Breast Cancer Classification Using Histopathology Images.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">126</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/diagnostics13010126</ELocationID><Abstract><AbstractText>Breast cancer is one of the deadliest diseases worldwide among women. Early diagnosis and proper treatment can save many lives. Breast image analysis is a popular method for detecting breast cancer. Computer-aided diagnosis of breast images helps radiologists do the task more efficiently and appropriately. Histopathological image analysis is an important diagnostic method for breast cancer, which is basically microscopic imaging of breast tissue. In this work, we developed a deep learning-based method to classify breast cancer using histopathological images. We propose a patch-classification model to classify the image patches, where we divide the images into patches and pre-process these patches with stain normalization, regularization, and augmentation methods. We use machine-learning-based classifiers and ensembling methods to classify the image patches into four categories: normal, benign, in situ, and invasive. Next, we use the patch information from this model to classify the images into two classes (cancerous and non-cancerous) and four other classes (normal, benign, in situ, and invasive). We introduce a model to utilize the 2-class classification probabilities and classify the images into a 4-class classification. The proposed method yields promising results and achieves a classification accuracy of 97.50% for 4-class image classification and 98.6% for 2-class image classification on the ICIAR BACH dataset.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Bagchi</LastName><ForeName>Arnab</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Department of Computer Science and Engineering, Jadavpur University, Kolkata 700032, West Bengal, India.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Pramanik</LastName><ForeName>Payel</ForeName><Initials>P</Initials><AffiliationInfo><Affiliation>Department of Computer Science and Engineering, Jadavpur University, Kolkata 700032, West Bengal, India.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Sarkar</LastName><ForeName>Ram</ForeName><Initials>R</Initials><Identifier Source="ORCID">0000-0001-8813-4086</Identifier><AffiliationInfo><Affiliation>Department of Computer Science and Engineering, Jadavpur University, Kolkata 700032, West Bengal, India.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>30</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Diagnostics (Basel)</MedlineTA><NlmUniqueID>101658402</NlmUniqueID><ISSNLinking>2075-4418</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">BACH dataset</Keyword><Keyword MajorTopicYN="N">breast cancer</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword><Keyword MajorTopicYN="N">ensemble learning</Keyword><Keyword MajorTopicYN="N">histopathology images</Keyword></KeywordList><CoiStatement>The authors declare no conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>11</Month><Day>14</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>16</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>25</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>1</Hour><Minute>3</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36611418</ArticleId><ArticleId IdType="pmc">PMC9818545</ArticleId><ArticleId IdType="doi">10.3390/diagnostics13010126</ArticleId><ArticleId IdType="pii">diagnostics13010126</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Malvia S., Bagadi S.A., Dubey U.S., Saxena S. Epidemiology of breast cancer in Indian women. Asia-Pac. J. Clin. Oncol. 2017;13:289&#x2013;295. doi: 10.1111/ajco.12661.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/ajco.12661</ArticleId><ArticleId IdType="pubmed">28181405</ArticleId></ArticleIdList></Reference><Reference><Citation>Siegel R.L., Miller K.D., Jemal A. Cancer statistics, 2018. CA Cancer J. Clin. 2018;68:7&#x2013;30. doi: 10.3322/caac.21442.</Citation><ArticleIdList><ArticleId IdType="doi">10.3322/caac.21442</ArticleId><ArticleId IdType="pubmed">29313949</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang S., Liu Y., Feng Y., Zhang J., Swinnen J., Li Y., Ni Y. A Review on Curability of Cancers: More Efforts for Novel Therapeutic Options Are Needed. Cancers. 2019;11:1782. doi: 10.3390/cancers11111782.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/cancers11111782</ArticleId><ArticleId IdType="pmc">PMC6896199</ArticleId><ArticleId IdType="pubmed">31766180</ArticleId></ArticleIdList></Reference><Reference><Citation>Society A.C. Breast Cancer Facts &amp; Figures 2019&#x2013;2020. American Cancer Society, Inc.; Atlanta, GA, USA: 2019.</Citation></Reference><Reference><Citation>Dheeba J., Singh N.A., Selvi S.T. Computer-aided detection of breast cancer on mammograms: A swarm intelligence optimized wavelet neural network approach. J. Biomed. Inform. 2014;49:45&#x2013;52. doi: 10.1016/j.jbi.2014.01.010.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jbi.2014.01.010</ArticleId><ArticleId IdType="pubmed">24509074</ArticleId></ArticleIdList></Reference><Reference><Citation>Shen R., Yan K., Tian K., Jiang C., Zhou K. Breast mass detection from the digitized X-ray mammograms based on the combination of deep active learning and self-paced learning. Futur. Gener. Comput. Syst. 2019;101:668&#x2013;679. doi: 10.1016/j.future.2019.07.013.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.future.2019.07.013</ArticleId></ArticleIdList></Reference><Reference><Citation>Qi X., Zhang L., Chen Y., Pi Y., Chen Y., Lv Q., Yi Z. Automated diagnosis of breast ultrasonography images using deep neural networks. Med. Image Anal. 2019;52:185&#x2013;198. doi: 10.1016/j.media.2018.12.006.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2018.12.006</ArticleId><ArticleId IdType="pubmed">30594771</ArticleId></ArticleIdList></Reference><Reference><Citation>Houssein E.H., Emam M.M., Ali A.A., Suganthan P.N. Deep and machine learning techniques for medical imaging-based breast cancer: A comprehensive review. Expert Syst. Appl. 2020;167:114161. doi: 10.1016/j.eswa.2020.114161.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.eswa.2020.114161</ArticleId></ArticleIdList></Reference><Reference><Citation>Sudharshan P., Petitjean C., Spanhol F., Oliveira L.E., Heutte L., Honeine P. Multiple instance learning for histopathological breast cancer image classification. Expert Syst. Appl. 2019;117:103&#x2013;111. doi: 10.1016/j.eswa.2018.09.049.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.eswa.2018.09.049</ArticleId></ArticleIdList></Reference><Reference><Citation>Hekler A., Utikal J.S., Enk A.H., Solass W., Schmitt M., Klode J., Schadendorf D., Sondermann W., Franklin C., Bestvater F., et al. Deep learning outperformed 11 pathologists in the classification of histopathological melanoma images. Eur. J. Cancer. 2019;118:91&#x2013;96. doi: 10.1016/j.ejca.2019.06.012.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejca.2019.06.012</ArticleId><ArticleId IdType="pubmed">31325876</ArticleId></ArticleIdList></Reference><Reference><Citation>Comes M.C., Fucci L., Mele F., Bove S., Cristofaro C., De Risi I., Fanizzi A., Milella M., Strippoli S., Zito A., et al. A deep learning model based on whole slide images to predict disease-free survival in cutaneous melanoma patients. Sci. Rep. 2022;12:1&#x2013;10. doi: 10.1038/s41598-022-24315-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-022-24315-1</ArticleId><ArticleId IdType="pmc">PMC9701687</ArticleId><ArticleId IdType="pubmed">36437296</ArticleId></ArticleIdList></Reference><Reference><Citation>Rajathi G.M. Optimized radial basis neural network for classification of breast cancer images. J. Ambient. Intell. Humaniz. Comput. 2020;17:97&#x2013;108. doi: 10.1007/s12652-020-02534-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s12652-020-02534-6</ArticleId><ArticleId IdType="pubmed">32416697</ArticleId></ArticleIdList></Reference><Reference><Citation>Roy S., Das S., Kar D., Schwenker F., Sarkar R. Computer Aided Breast Cancer Detection Using Ensembling of Texture and Statistical Image Features. Sensors. 2021;21:3628. doi: 10.3390/s21113628.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s21113628</ArticleId><ArticleId IdType="pmc">PMC8197148</ArticleId><ArticleId IdType="pubmed">34071029</ArticleId></ArticleIdList></Reference><Reference><Citation>Basavanhally A., Yu E., Xu J., Ganesan S., Feldman M., Tomaszewski J., Madabhushi A. Medical Imaging 2011: Computer- Aided Diagnosis. Volume 7963. International Society for Optics and Photonics; Bellingham, WA, USA: 2011. Incorporating domain knowledge for tubule detection in breast histopathology using O&#x2019;Callaghan neighborhoods; p. 796310.</Citation></Reference><Reference><Citation>Dundar M.M., Badve S., Bilgin G., Raykar V., Jain R., Sertel O., Gurcan M.N. Computerized classification of intraductal breast lesions using histopathological images. IEEE Trans. Biomed. Eng. 2011;58:1977&#x2013;1984. doi: 10.1109/TBME.2011.2110648.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TBME.2011.2110648</ArticleId><ArticleId IdType="pmc">PMC3328096</ArticleId><ArticleId IdType="pubmed">21296703</ArticleId></ArticleIdList></Reference><Reference><Citation>Melekoodappattu J.G., Dhas A.S., Kandathil B.K., Adarsh K.S. Breast cancer detection in mammogram: Combining modified CNN and texture feature based approach. J. Ambient. Intell. Humaniz. Comput. 2022:1&#x2013;10. doi: 10.1007/s12652-022-03713-3.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s12652-022-03713-3</ArticleId></ArticleIdList></Reference><Reference><Citation>Pramanik P., Mukhopadhyay S., Kaplun D., Sarkar R. International Conference on Mathematics and Its Applications in New Computer Systems. Springer; Cham, Switzerland: 2022. A Deep Feature Selection Method for Tumor Classification in Breast Ultrasound Images; pp. 241&#x2013;252.</Citation></Reference><Reference><Citation>Pramanik P., Mukhopadhyay S., Mirjalili S., Sarkar R. Deep feature selection using local search embedded social ski-driver optimization algorithm for breast cancer detection in mammograms. Neural Comput. Appl. 2022:1&#x2013;21. doi: 10.1007/s00521-022-07895-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00521-022-07895-x</ArticleId><ArticleId IdType="pmc">PMC9638217</ArticleId><ArticleId IdType="pubmed">36373132</ArticleId></ArticleIdList></Reference><Reference><Citation>Majumdar S., Pramanik P., Sarkar R. Gamma function based ensemble of CNN models for breast cancer detection in histopathology images. Expert Syst. Appl. 2023;213:119022. doi: 10.1016/j.eswa.2022.119022.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.eswa.2022.119022</ArticleId></ArticleIdList></Reference><Reference><Citation>Sanyal R., Jethanandani M., Sarkar R. Innovations in Computational Intelligence and Computer Vision. Springer; Singapore: 2020. DAN: Breast Cancer Classification from High-Resolution Histology Images Using Deep Attention Network; pp. 319&#x2013;326.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-981-15-6067-5_35</ArticleId></ArticleIdList></Reference><Reference><Citation>Chattopadhyay S., Dey A., Singh P.K., Oliva D., Cuevas E., Sarkar R. MTRRE-Net: A deep learning model for detection of breast cancer from histopathological images. Comput. Biol. Med. 2022;150:106155. doi: 10.1016/j.compbiomed.2022.106155.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2022.106155</ArticleId><ArticleId IdType="pubmed">36240595</ArticleId></ArticleIdList></Reference><Reference><Citation>Chattopadhyay S., Dey A., Singh P.K., Sarkar R. DRDA-Net: Dense residual dual-shuffle attention network for breast cancer classification using histopathological images. Comput. Biol. Med. 2022;145:105437. doi: 10.1016/j.compbiomed.2022.105437.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2022.105437</ArticleId><ArticleId IdType="pubmed">35339096</ArticleId></ArticleIdList></Reference><Reference><Citation>Bhowal P., Sen S., Velasquez J.D., Sarkar R. Fuzzy ensemble of deep learning models using choquet fuzzy integral, coalition game and information theory for breast cancer histology classification. Expert Syst. Appl. 2021;190:116167. doi: 10.1016/j.eswa.2021.116167.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.eswa.2021.116167</ArticleId></ArticleIdList></Reference><Reference><Citation>Melekoodappattu J.G., Subbian P.S. Automated breast cancer detection using hybrid extreme learning machine classifier. J. Ambient. Intell. Humaniz. Comput. 2020:1&#x2013;10. doi: 10.1007/s12652-020-02359-3.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s12652-020-02359-3</ArticleId></ArticleIdList></Reference><Reference><Citation>Nirmala G., Kumar P.S. RETRACTED ARTICLE: A novel bat optimized runlength networks (BORN) for an efficient classification of breast cancer. J. Ambient. Intell. Humaniz. Comput. 2020;12:4797&#x2013;4808. doi: 10.1007/s12652-020-01890-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s12652-020-01890-7</ArticleId></ArticleIdList></Reference><Reference><Citation>Kumar D., Batra U. Breast Cancer Histopathology Image Classification Using Soft Voting Classifier. In: Abraham A., Castillo O., Virmani D., editors. Proceedings of the 3rd International Conference on Computing Informatics and Networks. Lecture Notes in Networks and Systems. Volume 167. Springer; Singapore: 2021.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-981-15-9712-1_53</ArticleId></ArticleIdList></Reference><Reference><Citation>Preetha R., Jinny S.V. Retracted Article: Early diagnose breast cancer with PCA-LDA based FER and neuro-fuzzy classification system. J. Ambient. Intell. Humaniz. Comput. 2020;12:7195&#x2013;7204. doi: 10.1007/s12652-020-02395-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s12652-020-02395-z</ArticleId></ArticleIdList></Reference><Reference><Citation>Elmannai H., Hamdi M., AlGarni A. Deep Learning Models Combining for Breast Cancer Histopathology Image Classification. Int. J. Comput. Intell. Syst. 2021;14:1003&#x2013;1013. doi: 10.2991/ijcis.d.210301.002.</Citation><ArticleIdList><ArticleId IdType="doi">10.2991/ijcis.d.210301.002</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang Y., Sun L., Ma K., Fang J. Breast Cancer Microscope Image Classification Based on CNN with Image Deformation; Proceedings of the International Conference on Image Analysis and Recognition; P&#xf3;voa de Varzim, Portugal. 27&#x2013;29 June 2018; pp. 845&#x2013;852.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-319-93000-8_96</ArticleId></ArticleIdList></Reference><Reference><Citation>Nazeri K., Aminpour A., Ebrahimi M. Two-Stage Convolu- Tional Neural Network for Breast Cancer Histology Image Classifi- Cation, in International Conference Image Analysis and Recognition. Springer; Cham, Switzerland: 2018. pp. 717&#x2013;726.</Citation></Reference><Reference><Citation>Golatkar A., Anand D., Sethi A. Classification of Breast Cancer Histology Using Deep Learning. In: Campilho A., Karray F., ter Haar Romeny B., editors. Image Analysis and Recognition. ICIAR 2018. Lecture Notes in Computer Science. Volume 10882. Springer; Cham, Switzerland: 2018.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-319-93000-8_95</ArticleId></ArticleIdList></Reference><Reference><Citation>Sanyal R., Kar D., Sarkar R. Carcinoma Type Classification From High-Resolution Breast Microscopy Images Using a Hybrid Ensemble of Deep Convolutional Features and Gradient Boosting Trees Classifiers. IEEE/ACM Trans. Comput. Biol. Bioinform. 2021;19:2124&#x2013;2136. doi: 10.1109/TCBB.2021.3071022.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TCBB.2021.3071022</ArticleId><ArticleId IdType="pubmed">33819160</ArticleId></ArticleIdList></Reference><Reference><Citation>Zou Y., Zhang J., Huang S., Liu B. Breast cancer histopathological image classification using attention high-order deep network. Int. J. Imaging Syst. Technol. 2021;32:266&#x2013;279. doi: 10.1002/ima.22628.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/ima.22628</ArticleId></ArticleIdList></Reference><Reference><Citation>Vang Y.S., Chen Z., Xie X. International Conference Image Analysis and Recognition. Springer; Cham, Switzerland: 2018. Deep Learning Framework for Multi-class Breast Cancer Histology Image Classification; pp. 914&#x2013;922.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-319-93000-8_104</ArticleId></ArticleIdList></Reference><Reference><Citation>Mohamed A., Amer E., Eldin S.N., Khaled J., Hossam M., Elmasry N., Adnan G.T. The Impact of Data processing and Ensemble on Breast Cancer Detection Using Deep Learning. J. Comput. Commun. 2022;1:27&#x2013;37. doi: 10.21608/jocc.2022.218453.</Citation><ArticleIdList><ArticleId IdType="doi">10.21608/jocc.2022.218453</ArticleId></ArticleIdList></Reference><Reference><Citation>Awan R., Koohbanani N.A., Shaban M., Lisowska A., Rajpoot N. International Conference Image Analysis and Recognition. Springer; Cham, Switzerland: 2018. Context-Aware Learning Using Transferable Features for Classification of Breast Cancer Histology Images; pp. 788&#x2013;795.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-319-93000-8_89</ArticleId></ArticleIdList></Reference><Reference><Citation>Rakhlin A., Shvets A., Iglovikov V.I., Kalinin A.A. International Conference Image Analysis and Recognition. Springer; Berlin, Germany: 2018. Deep Convolutional Neural Networks for Breast Cancer Histology Image Analysis; pp. 737&#x2013;744.</Citation></Reference><Reference><Citation>Aresta G., Ara&#xfa;jo T., Kwok S., Chennamsetty S.S., Safwan M., Alex V., Marami B., Prastawa M., Chan M., Donovan M., et al. BACH: Grand challenge on breast cancer histology images. Med. Image Anal. 2019;56:122&#x2013;139. doi: 10.1016/j.media.2019.05.010.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2019.05.010</ArticleId><ArticleId IdType="pubmed">31226662</ArticleId></ArticleIdList></Reference><Reference><Citation>Macenko M., Niethammer M., Marron J.S., Borland D., Woosley J.T., Guan X., Schmitt C., Thomas N.E. A method for normalizing histology slides for quantitative analysis; Proceedings of the 2009 IEEE International Symposium on Biomedical Imaging: From Nano to Macro; Boston, MA, USA. 28 June&#x2013;1 July 2009.</Citation></Reference><Reference><Citation>Simonyan K., Zisserman A. Very deep convolutional networks for large-scale image recognition. arXiv. 20141409.1556</Citation></Reference><Reference><Citation>Szegedy C., Ioffe S., Vanhoucke V., Alemi A.A. Inception-v4, inception-resnet and the impact of residual connections on learning; Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence; San Francisco, CA, USA. 4&#x2013;9 February 2017.</Citation></Reference><Reference><Citation>Chollet F. Xception: Deep learning with depthwise separable convolutions; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Honolulu, HI, USA. 21&#x2013;26 July 2017; pp. 1251&#x2013;1258.</Citation></Reference><Reference><Citation>He K., Zhang X., Ren S., Sun J. Deep residual learning for image recognition; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Las Vegas, NV, USA. 27&#x2013;30 June 2016; pp. 770&#x2013;778.</Citation></Reference><Reference><Citation>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aur&#xe9;lien G&#xe9;ron.  [(accessed on 13 November 2022)].  Available online:  https://www.knowledgeisle.com/wp-content/uploads/2019/12/2-Aur%C3%A9lien-G%C3%A9ron-Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-Tensorflow_-Concepts-Tools-and-Techniques-to-Build-Intelligent-Systems-O%E2%80%99Reilly-Media-2019.pdf.</Citation></Reference><Reference><Citation>Guo Y., Dong H., Song F., Zhu C., Liu J. International Conference Image Analysis and Recognition. Springer; Cham, Switzerland: 2018. Breast Cancer Histology Image Classification Based on Deep Neural Networks; pp. 827&#x2013;836.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-319-93000-8_94</ArticleId></ArticleIdList></Reference><Reference><Citation>Ferreira C.A., Melo T., Sousa P., Meyer M.I., Shakibapour E., Costa P., Campilho A. International Conference Image Analysis and Recognition. Springer; Cham, Switzerland: 2018. Classification of Breast Cancer Histology Images Through Transfer Learning Using a Pre-trained Inception Resnet V2; pp. 763&#x2013;770.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-319-93000-8_86</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36611395</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Print">2075-4418</ISSN><JournalIssue CitedMedium="Print"><Volume>13</Volume><Issue>1</Issue><PubDate><Year>2022</Year><Month>Dec</Month><Day>29</Day></PubDate></JournalIssue><Title>Diagnostics (Basel, Switzerland)</Title><ISOAbbreviation>Diagnostics (Basel)</ISOAbbreviation></Journal><ArticleTitle>Adaptive IoU Thresholding for Improving Small Object Detection: A Proof-of-Concept Study of Hand Erosions Classification of Patients with Rheumatic Arthritis on X-ray Images.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">104</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/diagnostics13010104</ELocationID><Abstract><AbstractText>In recent years, much research evaluating the radiographic destruction of finger joints in patients with rheumatoid arthritis (RA) using deep learning models was conducted. Unfortunately, most previous models were not clinically applicable due to the small object regions as well as the close spatial relationship. In recent years, a new network structure called RetinaNets, in combination with the focal loss function, proved reliable for detecting even small objects. Therefore, the study aimed to increase the recognition performance to a clinically valuable level by proposing an innovative approach with adaptive changes in intersection over union (<i>IoU</i>) values during training of Retina Networks using the focal loss error function. To this end, the erosion score was determined using the Sharp van der Heijde (<i>SvH</i>) metric on 300 conventional radiographs from 119 patients with RA. Subsequently, a standard RetinaNet with different <i>IoU</i> values as well as adaptively modified <i>IoU</i> values were trained and compared in terms of accuracy, mean average accuracy (<i>mAP</i>), and <i>IoU</i>. With the proposed approach of adaptive <i>IoU</i> values during training, erosion detection accuracy could be improved to 94% and an <i>mAP</i> of 0.81 &#xb1; 0.18. In contrast Retina networks with static <i>IoU</i> values achieved only an accuracy of 80% and an <i>mAP</i> of 0.43 &#xb1; 0.24. Thus, adaptive adjustment of <i>IoU</i> values during training is a simple and effective method to increase the recognition accuracy of small objects such as finger and wrist joints.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Radke</LastName><ForeName>Karl Ludger</ForeName><Initials>KL</Initials><Identifier Source="ORCID">0000-0003-1095-1283</Identifier><AffiliationInfo><Affiliation>Department of Diagnostic and Interventional Radiology, Medical Faculty, University Dusseldorf, D-40225 Dusseldorf, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kors</LastName><ForeName>Matthias</ForeName><Initials>M</Initials><Identifier Source="ORCID">0000-0003-0291-9814</Identifier><AffiliationInfo><Affiliation>Department of Diagnostic and Interventional Radiology, Medical Faculty, University Dusseldorf, D-40225 Dusseldorf, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>M&#xfc;ller-Lutz</LastName><ForeName>Anja</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Department of Diagnostic and Interventional Radiology, Medical Faculty, University Dusseldorf, D-40225 Dusseldorf, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Frenken</LastName><ForeName>Miriam</ForeName><Initials>M</Initials><Identifier Source="ORCID">0000-0001-5197-6861</Identifier><AffiliationInfo><Affiliation>Department of Diagnostic and Interventional Radiology, Medical Faculty, University Dusseldorf, D-40225 Dusseldorf, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wilms</LastName><ForeName>Lena Marie</ForeName><Initials>LM</Initials><Identifier Source="ORCID">0000-0002-1159-863X</Identifier><AffiliationInfo><Affiliation>Department of Diagnostic and Interventional Radiology, Medical Faculty, University Dusseldorf, D-40225 Dusseldorf, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Baraliakos</LastName><ForeName>Xenofon</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>Rheumazentrum Ruhrgebiet, Ruhr-University Bochum, D-44649 Herne, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wittsack</LastName><ForeName>Hans-J&#xf6;rg</ForeName><Initials>HJ</Initials><AffiliationInfo><Affiliation>Department of Diagnostic and Interventional Radiology, Medical Faculty, University Dusseldorf, D-40225 Dusseldorf, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Distler</LastName><ForeName>J&#xf6;rg H W</ForeName><Initials>JHW</Initials><AffiliationInfo><Affiliation>Department of Rheumatology &amp; Hiller Research Unit, University Hospital Dusseldorf, D-40225 Dusseldorf, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Abrar</LastName><ForeName>Daniel B</ForeName><Initials>DB</Initials><Identifier Source="ORCID">0000-0002-8116-4431</Identifier><AffiliationInfo><Affiliation>Department of Diagnostic and Interventional Radiology, Medical Faculty, University Dusseldorf, D-40225 Dusseldorf, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Antoch</LastName><ForeName>Gerald</ForeName><Initials>G</Initials><AffiliationInfo><Affiliation>Department of Diagnostic and Interventional Radiology, Medical Faculty, University Dusseldorf, D-40225 Dusseldorf, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Sewerin</LastName><ForeName>Philipp</ForeName><Initials>P</Initials><Identifier Source="ORCID">0000-0001-8465-6207</Identifier><AffiliationInfo><Affiliation>Rheumazentrum Ruhrgebiet, Ruhr-University Bochum, D-44649 Herne, Germany.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Rheumatology &amp; Hiller Research Unit, University Hospital Dusseldorf, D-40225 Dusseldorf, Germany.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>29</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Diagnostics (Basel)</MedlineTA><NlmUniqueID>101658402</NlmUniqueID><ISSNLinking>2075-4418</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">AI</Keyword><Keyword MajorTopicYN="N">RetinaNet</Keyword><Keyword MajorTopicYN="N">X-ray</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword><Keyword MajorTopicYN="N">imaging</Keyword><Keyword MajorTopicYN="N">innovative techniques</Keyword><Keyword MajorTopicYN="N">rheumatic arthritis</Keyword></KeywordList><CoiStatement>The authors declare no conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>10</Month><Day>10</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>8</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>28</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>1</Hour><Minute>2</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36611395</ArticleId><ArticleId IdType="pmc">PMC9818241</ArticleId><ArticleId IdType="doi">10.3390/diagnostics13010104</ArticleId><ArticleId IdType="pii">diagnostics13010104</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Wilson R.L. Rheumatoid Arthritis of the Hand. Orthop. Clin. N. Am. 1986;17:313&#x2013;343. doi: 10.1016/S0030-5898(20)30411-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0030-5898(20)30411-9</ArticleId><ArticleId IdType="pubmed">3520431</ArticleId></ArticleIdList></Reference><Reference><Citation>Frenken M., R&#xfc;bsam G., Mewes A., Radke K.L., Li L., Wilms L.M., Nebelung S., Abrar D.B., Sewerin P. To Contrast or Not to Contrast? On the Role of Contrast Enhancement in Hand MRI Studies of Patients with Rheumatoid Arthritis. Diagnostics. 2022;12:465. doi: 10.3390/diagnostics12020465.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/diagnostics12020465</ArticleId><ArticleId IdType="pmc">PMC8871222</ArticleId><ArticleId IdType="pubmed">35204555</ArticleId></ArticleIdList></Reference><Reference><Citation>van der Helm-van Mil A.H.M., Detert J., Le Cessie S., Filer A., Bastian H., Burmester G.R., Huizinga T.W.J., Raza K. Validation of a prediction rule for disease outcome in patients with recent-onset undifferentiated arthritis: Moving toward individualized treatment decision-making. Arthritis Rheum. 2008;58:2241&#x2013;2247. doi: 10.1002/art.23681.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/art.23681</ArticleId><ArticleId IdType="pubmed">18668546</ArticleId></ArticleIdList></Reference><Reference><Citation>Lindegaard H.M., Vall&#xf8; J., H&#xf8;rslev-Petersen K., Junker P., &#xd8;stergaard M. Low-cost, low-field dedicated extremity magnetic resonance imaging in early rheumatoid arthritis: A 1-year follow-up study. Ann. Rheum. Dis. 2006;65:1208&#x2013;1212. doi: 10.1136/ard.2005.049213.</Citation><ArticleIdList><ArticleId IdType="doi">10.1136/ard.2005.049213</ArticleId><ArticleId IdType="pmc">PMC1798294</ArticleId><ArticleId IdType="pubmed">16540550</ArticleId></ArticleIdList></Reference><Reference><Citation>Abrar D.B., Schleich C., Radke K.L., Frenken M., Stabinska J., Ljimani A., Wittsack H.-J., Antoch G., Bittersohl B., Hesper T., et al. Detection of early cartilage degeneration in the tibiotalar joint using 3 T gagCEST imaging: A feasibility study. Magma. 2021;34:249&#x2013;260. doi: 10.1007/s10334-020-00868-y.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10334-020-00868-y</ArticleId><ArticleId IdType="pmc">PMC8018923</ArticleId><ArticleId IdType="pubmed">32725359</ArticleId></ArticleIdList></Reference><Reference><Citation>Radke K.L., Wilms L.M., Frenken M., Stabinska J., Knet M., Kamp B., Thiel T.A., Filler T.J., Nebelung S., Antoch G., et al. Lorentzian-Corrected Apparent Exchange-Dependent Relaxation (LAREX) &#x3a9;-Plot Analysis&#x2014;An Adaptation for qCEST in a Multi-Pool System: Comprehensive In Silico, In Situ, and In Vivo Studies. Int. J. Mol. Sci. 2022;23:6920. doi: 10.3390/ijms23136920.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/ijms23136920</ArticleId><ArticleId IdType="pmc">PMC9266897</ArticleId><ArticleId IdType="pubmed">35805925</ArticleId></ArticleIdList></Reference><Reference><Citation>Radke K.L., Abrar D.B., Frenken M., Wilms L.M., Kamp B., Boschheidgen M., Liebig P., Ljimani A., Filler T.J., Antoch G., et al. Chemical Exchange Saturation Transfer for Lactate-Weighted Imaging at 3 T MRI: Comprehensive In Silico, In Vitro, In Situ, and In Vivo Evaluations. Tomography. 2022;8:1277&#x2013;1292. doi: 10.3390/tomography8030106.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/tomography8030106</ArticleId><ArticleId IdType="pmc">PMC9149919</ArticleId><ArticleId IdType="pubmed">35645392</ArticleId></ArticleIdList></Reference><Reference><Citation>Abrar D.B., Schleich C., M&#xfc;ller-Lutz A., Frenken M., Radke K.L., Vordenb&#xe4;umen S., Schneider M., Ostendorf B., Sewerin P. Cartilage Degradation in Psoriatic Arthritis Is Associated With Increased Synovial Perfusion as Detected by Magnetic Resonance Imaging. Front. Med. 2020;7:539870. doi: 10.3389/fmed.2020.539870.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fmed.2020.539870</ArticleId><ArticleId IdType="pmc">PMC7546830</ArticleId><ArticleId IdType="pubmed">33102496</ArticleId></ArticleIdList></Reference><Reference><Citation>Truhn D., Zwingenberger K.T., Schock J., Abrar D.B., Radke K.L., Post M., Linka K., Knobe M., Kuhl C., Nebelung S. No pressure, no diamonds?&#x2014;Static vs. dynamic compressive in-situ loading to evaluate human articular cartilage functionality by functional MRI. J. Mech. Behav. Biomed. Mater. 2021;120:104558. doi: 10.1016/j.jmbbm.2021.104558.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jmbbm.2021.104558</ArticleId><ArticleId IdType="pubmed">33957568</ArticleId></ArticleIdList></Reference><Reference><Citation>Radke K.L., Wollschl&#xe4;ger L.M., Nebelung S., Abrar D.B., Schleich C., Boschheidgen M., Frenken M., Schock J., Klee D., Frahm J., et al. Deep Learning-Based Post-Processing of Real-Time MRI to Assess and Quantify Dynamic Wrist Movement in Health and Disease. Diagnostics. 2021;11:1077. doi: 10.3390/diagnostics11061077.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/diagnostics11061077</ArticleId><ArticleId IdType="pmc">PMC8231139</ArticleId><ArticleId IdType="pubmed">34208361</ArticleId></ArticleIdList></Reference><Reference><Citation>Schock J., Truhn D., N&#xfc;rnberger D., Conrad S., Huppertz M.S., Keil S., Kuhl C., Merhof D., Nebelung S. Artificial intelligence-based automatic assessment of lower limb torsion on MRI. Sci. Rep. 2021;11:23244. doi: 10.1038/s41598-021-02708-y.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-021-02708-y</ArticleId><ArticleId IdType="pmc">PMC8636587</ArticleId><ArticleId IdType="pubmed">34853401</ArticleId></ArticleIdList></Reference><Reference><Citation>Zeng G., Guo Y., Zhan J., Wang Z., Lai Z., Du X., Qu X., Guo D. A review on deep learning MRI reconstruction without fully sampled k-space. BMC Med. Imaging. 2021;21:195. doi: 10.1186/s12880-021-00727-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s12880-021-00727-9</ArticleId><ArticleId IdType="pmc">PMC8710001</ArticleId><ArticleId IdType="pubmed">34952572</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang H.-J., Su C.-P., Lai C.-C., Chen W.-R., Chen C., Ho L.-Y., Chu W.-C., Lien C.-Y. Deep Learning-Based Computer-Aided Diagnosis of Rheumatoid Arthritis with Hand X-ray Images Conforming to Modified Total Sharp/van der Heijde Score. Biomedicines. 2022;10:1355. doi: 10.3390/biomedicines10061355.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/biomedicines10061355</ArticleId><ArticleId IdType="pmc">PMC9220074</ArticleId><ArticleId IdType="pubmed">35740376</ArticleId></ArticleIdList></Reference><Reference><Citation>Gayathri J.L., Abraham B., Sujarani M.S., Nair M.S. A computer-aided diagnosis system for the classification of COVID-19 and non-COVID-19 pneumonia on chest X-ray images by integrating CNN with sparse autoencoder and feed forward neural network. Comput. Biol. Med. 2022;141:105134.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8668604</ArticleId><ArticleId IdType="pubmed">34971978</ArticleId></ArticleIdList></Reference><Reference><Citation>Bai L., Zhang Y., Wang P., Zhu X., Xiong J.-W., Cui L. Improved diagnosis of rheumatoid arthritis using an artificial neural network. Sci. Rep. 2022;12:9810. doi: 10.1038/s41598-022-13750-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-022-13750-9</ArticleId><ArticleId IdType="pmc">PMC9192742</ArticleId><ArticleId IdType="pubmed">35697754</ArticleId></ArticleIdList></Reference><Reference><Citation>Hirano T., Nishide M., Nonaka N., Seita J., Ebina K., Sakurada K., Kumanogoh A. Development and validation of a deep-learning model for scoring of radiographic finger joint destruction in rheumatoid arthritis. Rheumatol. Adv. Pract. 2019;3:rkz047. doi: 10.1093/rap/rkz047.</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/rap/rkz047</ArticleId><ArticleId IdType="pmc">PMC6921374</ArticleId><ArticleId IdType="pubmed">31872173</ArticleId></ArticleIdList></Reference><Reference><Citation>Del Prete R., Graziano M.D., Renga A. RetinaNet: A deep learning architecture to achieve a robust wake detector in SAR images; Proceedings of the 2021 IEEE 6th International Forum on Research and Technology for Society and Industry (RTSI); Naples, Italy. 6&#x2013;9 September 2021; pp. 171&#x2013;176.</Citation></Reference><Reference><Citation>Tan L., Huangfu T., Wu L., Chen W. Comparison of RetinaNet, SSD, and YOLO v3 for real-time pill identification. BMC Med. Inform. Decis. Mak. 2021;21:324. doi: 10.1186/s12911-021-01691-8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s12911-021-01691-8</ArticleId><ArticleId IdType="pmc">PMC8609721</ArticleId><ArticleId IdType="pubmed">34809632</ArticleId></ArticleIdList></Reference><Reference><Citation>Nobis F., Geisslinger M., Weber M., Betz J., Lienkamp M. A Deep Learning-based Radar and Camera Sensor Fusion Architecture for Object Detection. arXiv. 2020 doi: 10.48550/arXiv.2005.07431.</Citation><ArticleIdList><ArticleId IdType="doi">10.48550/arXiv.2005.07431</ArticleId></ArticleIdList></Reference><Reference><Citation>Zlocha M., Dou Q., Glocker B. Improving RetinaNet for CT Lesion Detection with Dense Masks from Weak RECIST Labels. In: Shen D., Birukou, editors. Medical Image Computing and Computer Assisted Intervention-MICCAI 2019. 1st ed. Springer International Publishing; Berlin/Heidelberg, Germany: 2019. pp. 402&#x2013;410.</Citation></Reference><Reference><Citation>Choi H.-T., Lee H.-J., Kang H., Yu S., Park H.-H. SSD-EMB: An Improved SSD Using Enhanced Feature Map Block for Object Detection. Sensors. 2021;21:2842. doi: 10.3390/s21082842.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s21082842</ArticleId><ArticleId IdType="pmc">PMC8073181</ArticleId><ArticleId IdType="pubmed">33920696</ArticleId></ArticleIdList></Reference><Reference><Citation>Yan J., Wang H., Yan M., Diao W., Sun X., Li H. IoU-Adaptive Deformable R-CNN: Make Full Use of IoU for Multi-Class Object Detection in Remote Sensing Imagery. Remote Sens. 2019;11:286. doi: 10.3390/rs11030286.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/rs11030286</ArticleId></ArticleIdList></Reference><Reference><Citation>van der Heijde D., Dankert T., Nieman F., Rau R., Boers M. Reliability and sensitivity to change of a simplification of the Sharp/van der Heijde radiological assessment in rheumatoid arthritis. Rheumatology. 1999;38:941&#x2013;947. doi: 10.1093/rheumatology/38.10.941.</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/rheumatology/38.10.941</ArticleId><ArticleId IdType="pubmed">10534543</ArticleId></ArticleIdList></Reference><Reference><Citation>van der Heijde D. How to read radiographs according to the Sharp/van der Heijde method. J. Rheumatol. 2000;27:261&#x2013;263.</Citation><ArticleIdList><ArticleId IdType="pubmed">10648051</ArticleId></ArticleIdList></Reference><Reference><Citation>Khan Z., Yahya N., Alsaih K., Ali S.S.A., Meriaudeau F. Evaluation of Deep Neural Networks for Semantic Segmentation of Prostate in T2W MRI. Sensors. 2020;20:3183. doi: 10.3390/s20113183.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s20113183</ArticleId><ArticleId IdType="pmc">PMC7309110</ArticleId><ArticleId IdType="pubmed">32503330</ArticleId></ArticleIdList></Reference><Reference><Citation>Dra&#x142;us G., Mazur D., Czmil A. Automatic Detection and Counting of Blood Cells in Smear Images Using RetinaNet. Entropy. 2021;23:1522. doi: 10.3390/e23111522.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/e23111522</ArticleId><ArticleId IdType="pmc">PMC8618480</ArticleId><ArticleId IdType="pubmed">34828220</ArticleId></ArticleIdList></Reference><Reference><Citation>Santos A., Marcato Junior J., de Andrade Silva J., Pereira R., Matos D., Menezes G., Higa L., Eltner A., Ramos A.P., Osco L., et al. Storm-Drain and Manhole Detection Using the RetinaNet Method. Sensors. 2020;20:4450. doi: 10.3390/s20164450.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s20164450</ArticleId><ArticleId IdType="pmc">PMC7472039</ArticleId><ArticleId IdType="pubmed">32784983</ArticleId></ArticleIdList></Reference><Reference><Citation>Xie J., Stensrud E., Skramstad T. Detection-Based Object Tracking Applied to Remote Ship Inspection. Sensors. 2021;21:761. doi: 10.3390/s21030761.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s21030761</ArticleId><ArticleId IdType="pmc">PMC7865409</ArticleId><ArticleId IdType="pubmed">33498767</ArticleId></ArticleIdList></Reference><Reference><Citation>Kingma D.P., Ba J. Adam: A Method for Stochastic Optimization; Proceedings of the 3rd International Conference for Learning Representations; San Diego, CA, USA. 7&#x2013;9 May 2014.</Citation></Reference><Reference><Citation>Lin T.-Y., Goyal P., Girshick R., He K., Doll&#xe1;r P. Focal Loss for Dense Object Detection. arXiv. 2017 doi: 10.48550/arXiv.1708.02002.</Citation><ArticleIdList><ArticleId IdType="doi">10.48550/arXiv.1708.02002</ArticleId><ArticleId IdType="pubmed">30040631</ArticleId></ArticleIdList></Reference><Reference><Citation>Tan O., Liu L., You Q., Wang J., Chen A., Ing E., Morrison J.C., Jia Y., Huang D. Focal Loss Analysis of Nerve Fiber Layer Reflectance for Glaucoma Diagnosis. Transl. Vis. Sci. Technol. 2021;10:9. doi: 10.1167/tvst.10.6.9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1167/tvst.10.6.9</ArticleId><ArticleId IdType="pmc">PMC8107497</ArticleId><ArticleId IdType="pubmed">34111254</ArticleId></ArticleIdList></Reference><Reference><Citation>Cohen J. Statistical Power Analysis for the Behavioral Sciences. 2nd ed. Lawrence Erlbaum Associates; Hillsdale, NJ, USA: 1988.</Citation></Reference><Reference><Citation>Armstrong R.A. When to use the Bonferroni correction. Ophthalmic Physiol. Opt. J. Br. Coll. Ophthalmic Opt. 2014;34:502&#x2013;508. doi: 10.1111/opo.12131.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/opo.12131</ArticleId><ArticleId IdType="pubmed">24697967</ArticleId></ArticleIdList></Reference><Reference><Citation>Jones R.M., Sharma A., Hotchkiss R., Sperling J.W., Hamburger J., Ledig C., O&#x2019;Toole R., Gardner M., Venkatesh S., Roberts M.M., et al. Assessment of a deep-learning system for fracture detection in musculoskeletal radiographs. NPJ Digit. Med. 2020;3:144. doi: 10.1038/s41746-020-00352-w.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41746-020-00352-w</ArticleId><ArticleId IdType="pmc">PMC7599208</ArticleId><ArticleId IdType="pubmed">33145440</ArticleId></ArticleIdList></Reference><Reference><Citation>Kundu R., Das R., Geem Z.W., Han G.-T., Sarkar R. Pneumonia detection in chest X-ray images using an ensemble of deep learning models. PLoS ONE. 2021;16:e0256630. doi: 10.1371/journal.pone.0256630.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0256630</ArticleId><ArticleId IdType="pmc">PMC8423280</ArticleId><ArticleId IdType="pubmed">34492046</ArticleId></ArticleIdList></Reference><Reference><Citation>Xue Y., Zhang R., Deng Y., Chen K., Jiang T. A preliminary examination of the diagnostic value of deep learning in hip osteoarthritis. PLoS ONE. 2017;12:e0178992. doi: 10.1371/journal.pone.0178992.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0178992</ArticleId><ArticleId IdType="pmc">PMC5456368</ArticleId><ArticleId IdType="pubmed">28575070</ArticleId></ArticleIdList></Reference><Reference><Citation>Liu F., Zhou Z., Samsonov A., Blankenbaker D., Larison W., Kanarek A., Lian K., Kambhampati S., Kijowski R. Deep Learning Approach for Evaluating Knee MR Images: Achieving High Diagnostic Performance for Cartilage Lesion Detection. Radiology. 2018;289:160&#x2013;169. doi: 10.1148/radiol.2018172986.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2018172986</ArticleId><ArticleId IdType="pmc">PMC6166867</ArticleId><ArticleId IdType="pubmed">30063195</ArticleId></ArticleIdList></Reference><Reference><Citation>Spampinato C., Palazzo S., Giordano D., Aldinucci M., Leonardi R. Deep learning for automated skeletal bone age assessment in X-ray images. Med. Image Anal. 2017;36:41&#x2013;51. doi: 10.1016/j.media.2016.10.010.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2016.10.010</ArticleId><ArticleId IdType="pubmed">27816861</ArticleId></ArticleIdList></Reference><Reference><Citation>Rohrbach J., Reinhard T., Sick B., D&#xfc;rr O. Bone erosion scoring for rheumatoid arthritis with deep convolutional neural networks. Comput. Electr. Eng. 2019;78:472&#x2013;481. doi: 10.1016/j.compeleceng.2019.08.003.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compeleceng.2019.08.003</ArticleId></ArticleIdList></Reference><Reference><Citation>Fan J., Huo T., Li X. A Review of One-Stage Detection Algorithms in Autonomous Driving; Proceedings of the 2020 4th CAA International Conference on Vehicular Control and Intelligence (CVCI); Hangzhou, China. 18&#x2013;20 December 2020; pp. 210&#x2013;214.</Citation></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36611394</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Print">2075-4418</ISSN><JournalIssue CitedMedium="Print"><Volume>13</Volume><Issue>1</Issue><PubDate><Year>2022</Year><Month>Dec</Month><Day>29</Day></PubDate></JournalIssue><Title>Diagnostics (Basel, Switzerland)</Title><ISOAbbreviation>Diagnostics (Basel)</ISOAbbreviation></Journal><ArticleTitle>Primary Tumor Radiomic Model for Identifying Extrahepatic Metastasis of Hepatocellular Carcinoma Based on Contrast Enhanced Computed Tomography.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">102</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/diagnostics13010102</ELocationID><Abstract><AbstractText>This study aimed to identify radiomic features of primary tumor and develop a model for indicating extrahepatic metastasis of hepatocellular carcinoma (HCC). Contrast-enhanced computed tomographic (CT) images of 177 HCC cases, including 26 metastatic (MET) and 151 non-metastatic (non-MET), were retrospectively collected and analyzed. For each case, 851 radiomic features, which quantify shape, intensity, texture, and heterogeneity within the segmented volume of the largest HCC tumor in arterial phase, were extracted using Pyradiomics. The dataset was randomly split into training and test sets. Synthetic Minority Oversampling Technique (SMOTE) was performed to augment the training set to 145 MET and 145 non-MET cases. The test set consists of six MET and six non-MET cases. The external validation set is comprised of 20 MET and 25 non-MET cases collected from an independent clinical unit. Logistic regression and support vector machine (SVM) models were identified based on the features selected using the stepwise forward method while the deep convolution neural network, visual geometry group 16 (VGG16), was trained using CT images directly. Grey-level size zone matrix (GLSZM) features constitute four of eight selected predictors of metastasis due to their perceptiveness to the tumor heterogeneity. The radiomic logistic regression model yielded an area under receiver operating characteristic curve (AUROC) of 0.944 on the test set and an AUROC of 0.744 on the external validation set. Logistic regression revealed no significant difference with SVM in the performance and outperformed VGG16 significantly. As extrahepatic metastasis workups, such as chest CT and bone scintigraphy, are standard but exhaustive, radiomic model facilitates a cost-effective method for stratifying HCC patients into eligibility groups of these workups.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Chan</LastName><ForeName>Lawrence Wing Chi</ForeName><Initials>LWC</Initials><Identifier Source="ORCID">0000-0001-6451-2273</Identifier><AffiliationInfo><Affiliation>Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong SAR, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wong</LastName><ForeName>Sze Chuen Cesar</ForeName><Initials>SCC</Initials><AffiliationInfo><Affiliation>Department of Applied Biology and Chemical Technology, The Hong Kong Polytechnic University, Hong Kong SAR, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Cho</LastName><ForeName>William Chi Shing</ForeName><Initials>WCS</Initials><Identifier Source="ORCID">0000-0003-4174-4586</Identifier><AffiliationInfo><Affiliation>Department of Clinical Oncology, Queen Elizabeth Hospital, Hong Kong SAR, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Huang</LastName><ForeName>Mohan</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong SAR, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Fei</ForeName><Initials>F</Initials><AffiliationInfo><Affiliation>Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong SAR, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Chui</LastName><ForeName>Man Lik</ForeName><Initials>ML</Initials><AffiliationInfo><Affiliation>Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong SAR, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lai</LastName><ForeName>Una Ngo Yin</ForeName><Initials>UNY</Initials><AffiliationInfo><Affiliation>Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong SAR, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Chan</LastName><ForeName>Tiffany Yuen Kwan</ForeName><Initials>TYK</Initials><AffiliationInfo><Affiliation>Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong SAR, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Cheung</LastName><ForeName>Zoe Hoi Ching</ForeName><Initials>ZHC</Initials><AffiliationInfo><Affiliation>Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong SAR, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Cheung</LastName><ForeName>Jerry Chun Yin</ForeName><Initials>JCY</Initials><AffiliationInfo><Affiliation>Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong SAR, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Tang</LastName><ForeName>Kin Fu</ForeName><Initials>KF</Initials><AffiliationInfo><Affiliation>Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong SAR, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Tse</LastName><ForeName>Man Long</ForeName><Initials>ML</Initials><AffiliationInfo><Affiliation>Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong SAR, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wong</LastName><ForeName>Hung Kit</ForeName><Initials>HK</Initials><AffiliationInfo><Affiliation>Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong SAR, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kwok</LastName><ForeName>Hugo Man Fung</ForeName><Initials>HMF</Initials><AffiliationInfo><Affiliation>Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Hong Kong SAR, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Shen</LastName><ForeName>Xinping</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>Department of Radiology, The University of Hong Kong-Shenzhen Hospital, Shenzhen 518053, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Sailong</ForeName><Initials>S</Initials><Identifier Source="ORCID">0000-0002-1058-8229</Identifier><AffiliationInfo><Affiliation>Department of Diagnostic Radiology, The University of Hong Kong, Hong Kong SAR, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Chiu</LastName><ForeName>Keith Wan Hang</ForeName><Initials>KWH</Initials><Identifier Source="ORCID">0000-0002-7930-1193</Identifier><AffiliationInfo><Affiliation>Department of Diagnostic Radiology, The University of Hong Kong, Hong Kong SAR, China.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology &amp; Imaging, Queen Elizabeth Hospital, Hong Kong SAR, China.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>29</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Diagnostics (Basel)</MedlineTA><NlmUniqueID>101658402</NlmUniqueID><ISSNLinking>2075-4418</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">clinical decision-making</Keyword><Keyword MajorTopicYN="N">computed tomography</Keyword><Keyword MajorTopicYN="N">extrahepatic metastasis</Keyword><Keyword MajorTopicYN="N">hepatocellular carcinoma</Keyword><Keyword MajorTopicYN="N">machine learning</Keyword><Keyword MajorTopicYN="N">oversampling</Keyword><Keyword MajorTopicYN="N">radiomics</Keyword></KeywordList><CoiStatement>The authors declare no conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>11</Month><Day>6</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>22</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>24</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>1</Hour><Minute>2</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36611394</ArticleId><ArticleId IdType="pmc">PMC9818425</ArticleId><ArticleId IdType="doi">10.3390/diagnostics13010102</ArticleId><ArticleId IdType="pii">diagnostics13010102</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>American Cancer Society . Global Cancer Facts &amp; Figures. 4th ed. American Cancer Society; Atlanta, GA, USA: 2018. pp. 22&#x2013;24.</Citation></Reference><Reference><Citation>Gomaa A.I., Khan S.A., Toledano M.B., Waked I., Taylor-Robinson S.D. Hepatocellular carcinoma: Epidemiology, risk factors and pathogenesis. World J. Gastroenterol. WJG. 2008;14:4300&#x2013;4308. doi: 10.3748/wjg.14.4300.</Citation><ArticleIdList><ArticleId IdType="doi">10.3748/wjg.14.4300</ArticleId><ArticleId IdType="pmc">PMC2731180</ArticleId><ArticleId IdType="pubmed">18666317</ArticleId></ArticleIdList></Reference><Reference><Citation>Sacco R., Bargellini I., Ginanni B., Bertini M., Faggioni L., Federici G., Romano A., Bertoni M., Metrangolo S., Altomare E., et al. Long-term results of sorafenib in advanced-stage hepatocellular carcinoma: What can we learn from routine clinical practice? Expert Rev. Anticancer Ther. 2012;12:869&#x2013;875. doi: 10.1586/era.12.58.</Citation><ArticleIdList><ArticleId IdType="doi">10.1586/era.12.58</ArticleId><ArticleId IdType="pubmed">22845401</ArticleId></ArticleIdList></Reference><Reference><Citation>Li Y., Gao Z.H., Qu X.J. The adverse effects of sorafenib in patients with advanced cancers. Basic Clin. Pharmacol. Toxicol. 2015;116:216&#x2013;221. doi: 10.1111/bcpt.12365.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/bcpt.12365</ArticleId><ArticleId IdType="pubmed">25495944</ArticleId></ArticleIdList></Reference><Reference><Citation>Uchino K., Tateishi R., Shiina S., Kanda M., Masuzaki R., Kondo Y., Goto T., Omata M., Yoshida H., Koike K. Hepatocellular carcinoma with extrahepatic metastasis: Clinical features and prognostic factors. Cancer. 2011;117:4475&#x2013;4483. doi: 10.1002/cncr.25960.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/cncr.25960</ArticleId><ArticleId IdType="pubmed">21437884</ArticleId></ArticleIdList></Reference><Reference><Citation>Natsuizaka M., Omura T., Akaike T., Kuwata Y., Yamazaki K., Sato T., Karino Y., Toyota J., Suga T., Asaka M. Clinical features of hepatocellular carcinoma with extrahepatic metastases. J. Gastroenterol. Hepatol. 2005;20:1781&#x2013;1787. doi: 10.1111/j.1440-1746.2005.03919.x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/j.1440-1746.2005.03919.x</ArticleId><ArticleId IdType="pubmed">16246200</ArticleId></ArticleIdList></Reference><Reference><Citation>Cheung T.T., Ho C.L., Lo C.M., Chen S., Chan S.C., Chok K.S., Fung J.Y., Yan Chan A.C., Sharr W., Yau T., et al. 11C-acetate and 18F-FDG PET/CT for clinical staging and selection of patients with hepatocellular carcinoma for liver transplantation on the basis of Milan criteria: Surgeon&#x2019;s perspective. J. Nucl. Med. 2013;54:192&#x2013;200. doi: 10.2967/jnumed.112.107516.</Citation><ArticleIdList><ArticleId IdType="doi">10.2967/jnumed.112.107516</ArticleId><ArticleId IdType="pubmed">23321459</ArticleId></ArticleIdList></Reference><Reference><Citation>Yokoo T., Patel A.D., Lev-Cohain N., Singal A.G., Yopp A.C., Pedrosa I. Extrahepatic metastasis risk of hepatocellular carcinoma based on &#x3b1;-fetoprotein and tumor staging parameters at cross-sectional imaging. Cancer Manag. Res. 2017;9:503&#x2013;511. doi: 10.2147/CMAR.S147097.</Citation><ArticleIdList><ArticleId IdType="doi">10.2147/CMAR.S147097</ArticleId><ArticleId IdType="pmc">PMC5652898</ArticleId><ArticleId IdType="pubmed">29081671</ArticleId></ArticleIdList></Reference><Reference><Citation>Xiao S., Chang R.M., Yang M.Y., Lei X., Liu X., Gao W.B., Xiao J.L., Yang L.Y. Actin-like 6A predicts poor prognosis of hepatocellular carcinoma and promotes metastasis and epithelial-mesenchymal transition. Hepatology. 2016;63:1256&#x2013;1271. doi: 10.1002/hep.28417.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/hep.28417</ArticleId><ArticleId IdType="pmc">PMC4834727</ArticleId><ArticleId IdType="pubmed">26698646</ArticleId></ArticleIdList></Reference><Reference><Citation>Xiang Z.L., Zeng Z.C., Tang Z.Y., Fan J., Sun H.C., Tan Y.S. Expression of Cytokeratin 19 and Matrix Metalloproteinase 2 predicts lymph node metastasis in hepatocellular carcinoma. Mol. Biol. Rep. 2011;38:3531&#x2013;3539. doi: 10.1007/s11033-010-0463-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11033-010-0463-x</ArticleId><ArticleId IdType="pubmed">21104440</ArticleId></ArticleIdList></Reference><Reference><Citation>Gatenby R.A., Grove O., Gillies R.J. Quantitative imaging in cancer evolution and ecology. Radiology. 2013;269:8&#x2013;15. doi: 10.1148/radiol.13122697.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.13122697</ArticleId><ArticleId IdType="pmc">PMC3781355</ArticleId><ArticleId IdType="pubmed">24062559</ArticleId></ArticleIdList></Reference><Reference><Citation>Kiryu S., Akai H., Nojima M., Hasegawa K., Shinkawa H., Kokudo N., Yasaka K., Ohtomo K. Impact of hepatocellular carcinoma heterogeneity on computed tomography as a prognostic indicator. Sci. Rep. 2017;7:12689. doi: 10.1038/s41598-017-12688-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-017-12688-7</ArticleId><ArticleId IdType="pmc">PMC5627280</ArticleId><ArticleId IdType="pubmed">28978930</ArticleId></ArticleIdList></Reference><Reference><Citation>Zheng J., Chakraborty J., Chapman W.C., Gerst S., Gonen M., Pak L.M., Jarnagin W.R., DeMatteo R.P., Do R.K., Simpson A.L., et al. Preoperative prediction of microvascular invasion in hepatocellular carcinoma using quantitative image analysis. J. Am. Coll. Surg. 2017;225:778&#x2013;788. doi: 10.1016/j.jamcollsurg.2017.09.003.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jamcollsurg.2017.09.003</ArticleId><ArticleId IdType="pmc">PMC5705269</ArticleId><ArticleId IdType="pubmed">28941728</ArticleId></ArticleIdList></Reference><Reference><Citation>Kim J., Choi S.J., Lee S.H., Lee H.Y., Park H. Predicting survival using pretreatment CT for patients with hepatocellular carcinoma treated with transarterial chemoembolization: Comparison of models using radiomics. AJR Am. J. Roentgenol. 2018;211:1026&#x2013;1034. doi: 10.2214/AJR.18.19507.</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/AJR.18.19507</ArticleId><ArticleId IdType="pubmed">30240304</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhou Y., He L., Huang Y., Chen S., Wu P., Ye W., Liu Z., Liang C. CT-based radiomics signature: A potential biomarker for preoperative prediction of early recurrence in hepatocellular carcinoma. Abdom. Radiol. 2017;42:1695&#x2013;1704. doi: 10.1007/s00261-017-1072-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00261-017-1072-0</ArticleId><ArticleId IdType="pubmed">28180924</ArticleId></ArticleIdList></Reference><Reference><Citation>Miranda Magalhaes Santos J.M., Clemente Oliveira B., Araujo-Filho J.D., Assuncao A.N., Jr., de MMachado F.A., Carlos Tavares Rocha C., Horvat J.V., Menezes M.R., Horvat N. State-of-the-art in radiomics of hepatocellular carcinoma: A review of basic principles, applications, and limitations. Abdom. Radiol. 2020;45:342&#x2013;353. doi: 10.1007/s00261-019-02299-3.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00261-019-02299-3</ArticleId><ArticleId IdType="pubmed">31707435</ArticleId></ArticleIdList></Reference><Reference><Citation>Limkin E.J., Sun R., Dercle L., Zacharaki E.I., Robert C., Reuz&#xe9; S., Schernberg A., Paragios N., Deutsch E., Fert&#xe9; C. Promises and challenges for the implementation of computational medical imaging (radiomics) in oncology. Ann. Oncol. 2017;28:1191&#x2013;1206. doi: 10.1093/annonc/mdx034.</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/annonc/mdx034</ArticleId><ArticleId IdType="pubmed">28168275</ArticleId></ArticleIdList></Reference><Reference><Citation>Schraml C., Kaufmann S., Rempp H., Syha R., Ketelsen D., Notohamiprodjo M., Nikolaou K. Imaging of HCC-current state of the art. Diagnostics. 2015;5:513&#x2013;545. doi: 10.3390/diagnostics5040513.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/diagnostics5040513</ArticleId><ArticleId IdType="pmc">PMC4728473</ArticleId><ArticleId IdType="pubmed">26854169</ArticleId></ArticleIdList></Reference><Reference><Citation>Van Griethuysen J.J., Fedorov A., Parmar C., Hosny A., Aucoin N., Narayan V., Beets-Tan R.G., Fillion-Robin J.C., Pieper S., Aerts H.J. Computational radiomics system to decode the radiographic phenotype. Cancer Res. 2017;77:104&#x2013;107. doi: 10.1158/0008-5472.CAN-17-0339.</Citation><ArticleIdList><ArticleId IdType="doi">10.1158/0008-5472.CAN-17-0339</ArticleId><ArticleId IdType="pmc">PMC5672828</ArticleId><ArticleId IdType="pubmed">29092951</ArticleId></ArticleIdList></Reference><Reference><Citation>Chan L.W., Wong S.C., Chiau C.C., Chan T.M., Tao L., Feng J., Chiu K.W. Association patterns of ontological features signify electronic health records in liver cancer. J. Healthc. Eng. 2017;2017:6493016. doi: 10.1155/2017/6493016.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2017/6493016</ArticleId><ArticleId IdType="pmc">PMC5563431</ArticleId><ArticleId IdType="pubmed">29065631</ArticleId></ArticleIdList></Reference><Reference><Citation>McCaw Z.R., Lane J.M., Saxena R., Redline S., Lin X. Operating characteristics of the rank-based inverse normal transformation for quantitative trait analysis in genome-wide association studies. Biometrics. 2020;76:1262&#x2013;1272. doi: 10.1111/biom.13214.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/biom.13214</ArticleId><ArticleId IdType="pmc">PMC8643141</ArticleId><ArticleId IdType="pubmed">31883270</ArticleId></ArticleIdList></Reference><Reference><Citation>Blagus R., Lusa L. SMOTE for high-dimensional class-imbalanced data. BMC Bioinform. 2013;14:106. doi: 10.1186/1471-2105-14-106.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/1471-2105-14-106</ArticleId><ArticleId IdType="pmc">PMC3648438</ArticleId><ArticleId IdType="pubmed">23522326</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhong Y., Yuan M., Zhang T., Zhang Y.D., Li H., Yu T.F. Radiomics approach to prediction of occult mediastinal lymph node metastasis of lung adenocarcinoma. Am. J. Roentgenol. 2018;211:109&#x2013;113. doi: 10.2214/AJR.17.19074.</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/AJR.17.19074</ArticleId><ArticleId IdType="pubmed">29667885</ArticleId></ArticleIdList></Reference><Reference><Citation>Aerts H.J., Velazquez E.R., Leijenaar R.T., Parmar C., Grossmann P., Carvalho S., Bussink J., Monshouwer R., Haibe-Kains B., Rietveld D., et al. Decoding tumour phenotype by noninvasive imaging using a quantitative radiomics approach. Nat. Commun. 2014;5:4006. doi: 10.1038/ncomms5006.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/ncomms5006</ArticleId><ArticleId IdType="pmc">PMC4059926</ArticleId><ArticleId IdType="pubmed">24892406</ArticleId></ArticleIdList></Reference><Reference><Citation>Tokareva A.O., Chagovets V.V., Kononikhin A.S., Starodubtseva N.L., Nikolaev E.N., Frankevich V.E. Comparison of the effectiveness of variable selection method for creating a diagnostic panel of biomarkers for mass spectrometric lipidome analysis. J. Mass Spectrom. 2021;56:4702. doi: 10.1002/jms.4702.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jms.4702</ArticleId><ArticleId IdType="pubmed">33629457</ArticleId></ArticleIdList></Reference><Reference><Citation>Hosmer D.W., Lemeshow S., Sturdivant R.X. Applied Logistic Regression. John Wiley &amp; Sons; Hoboken, NJ, USA: 2013. Model-building strategies and methods for logistic regression; pp. 89&#x2013;151.</Citation></Reference><Reference><Citation>Babyak M.A. What you see may not be what you get: A brief, nontechnical introduction to overfitting in regression-type model. Psychosom. Med. 2004;66:411&#x2013;421.</Citation><ArticleIdList><ArticleId IdType="pubmed">15184705</ArticleId></ArticleIdList></Reference><Reference><Citation>Simonyan K., Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition; Proceedings of the International Conference on Learning Representations; San Diego, CA, USA. 7&#x2013;9 May 2015.</Citation></Reference><Reference><Citation>Wakabayashi T., Ouhmich F., Gonzalez-Cabrera C., Felli E., Saviano A., Agnus V., Savadjiev P., Baumert T.F., Pessaux P., Marescaux J., et al. Radiomics in hepatocellular carcinoma: A quantitative review. Hepatol. Int. 2019;13:546&#x2013;559. doi: 10.1007/s12072-019-09973-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s12072-019-09973-0</ArticleId><ArticleId IdType="pmc">PMC7613479</ArticleId><ArticleId IdType="pubmed">31473947</ArticleId></ArticleIdList></Reference><Reference><Citation>Thibault G., Fertil B., Navarro C., Pereira S., Cau P., Levy N., Sequeira J., Mari J.L. Shape and texture indexes application to cell nuclei classification. Int. J. Pattern Recognit. Artif. Intell. 2013;27:1357002. doi: 10.1142/S0218001413570024.</Citation><ArticleIdList><ArticleId IdType="doi">10.1142/S0218001413570024</ArticleId></ArticleIdList></Reference><Reference><Citation>Ganeshan B., Panayiotou E., Burnand K., Dizdarevic S., Miles K. Tumour heterogeneity in non-small cell lung carcinoma assessed by CT texture analysis: A potential marker of survival. Eur. Radiol. 2012;22:796&#x2013;802. doi: 10.1007/s00330-011-2319-8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-011-2319-8</ArticleId><ArticleId IdType="pubmed">22086561</ArticleId></ArticleIdList></Reference><Reference><Citation>Rios E., Parmar C., Jermoumi M., Aerts H. Robust radiomics feature quantification using semiautomatic volumetric segmentation. Med. Phys. 2014;41:452. doi: 10.1118/1.4889256.</Citation><ArticleIdList><ArticleId IdType="doi">10.1118/1.4889256</ArticleId><ArticleId IdType="pmc">PMC4098900</ArticleId><ArticleId IdType="pubmed">25025374</ArticleId></ArticleIdList></Reference><Reference><Citation>Yu H., Meng X., Chen H., Liu J., Gao W., Du L., Chen Y., Wang Y., Liu X., Liu B., et al. Predicting the level of tumor-infiltrating lymphocytes in patients with breast cancer: Usefulness of mammographic radiomics features. Front. Oncol. 2021;11:393. doi: 10.3389/fonc.2021.628577.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fonc.2021.628577</ArticleId><ArticleId IdType="pmc">PMC7991288</ArticleId><ArticleId IdType="pubmed">33777776</ArticleId></ArticleIdList></Reference><Reference><Citation>Mao B., Zhang L., Ning P., Ding F., Wu F., Lu G., Geng Y., Ma J. Preoperative prediction for pathological grade of hepatocellular carcinoma via machine learning-based radiomics. Eur. Radiol. 2020;30:6924&#x2013;6932. doi: 10.1007/s00330-020-07056-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-020-07056-5</ArticleId><ArticleId IdType="pubmed">32696256</ArticleId></ArticleIdList></Reference><Reference><Citation>Prezzi D., Owczarczyk K., Bassett P., Siddique M., Breen D.J., Cook G.J., Goh V. Adaptive statistical iterative reconstruction (ASIR) affects CT radiomics quantification in primary colorectal cancer. Eur. Radiol. 2019;29:5227&#x2013;5235. doi: 10.1007/s00330-019-06073-3.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-019-06073-3</ArticleId><ArticleId IdType="pmc">PMC6717179</ArticleId><ArticleId IdType="pubmed">30887205</ArticleId></ArticleIdList></Reference><Reference><Citation>Peng J., Zhang J., Zhang Q.F., Xu Y.K., Zhou J., Liu L. A radiomics nomogram for preoperative prediction of microvascular invasion risk in hepatitis B virus-related hepatocellular carcinoma. Diagn. Interv. Radiol. 2018;24:121&#x2013;127. doi: 10.5152/dir.2018.17467.</Citation><ArticleIdList><ArticleId IdType="doi">10.5152/dir.2018.17467</ArticleId><ArticleId IdType="pmc">PMC5951199</ArticleId><ArticleId IdType="pubmed">29770763</ArticleId></ArticleIdList></Reference><Reference><Citation>Ji G.W., Zhu F.P., Xu Q., Wang K., Wu M.Y., Tang W.W., Li X.C., Wang X.H. Machine-learning analysis of contrast-enhanced CT radiomics predicts recurrence of hepatocellular carcinoma after resection: A multi-institutional study. EBioMedicine. 2019;50:156&#x2013;165. doi: 10.1016/j.ebiom.2019.10.057.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ebiom.2019.10.057</ArticleId><ArticleId IdType="pmc">PMC6923482</ArticleId><ArticleId IdType="pubmed">31735556</ArticleId></ArticleIdList></Reference><Reference><Citation>Shan Q.Y., Hu H.T., Feng S.T., Peng Z.P., Chen S.L., Zhou Q., Li X., Xie X.Y., Lu M.D., Wang W., et al. CT-based peritumoral radiomics signatures to predict early recurrence in hepatocellular carcinoma after curative tumour resection or ablation. Cancer Imaging. 2019;19:11. doi: 10.1186/s40644-019-0197-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s40644-019-0197-5</ArticleId><ArticleId IdType="pmc">PMC6391838</ArticleId><ArticleId IdType="pubmed">30813956</ArticleId></ArticleIdList></Reference><Reference><Citation>Coppola F., Giannini V., Gabelloni M., Panic J., Defeudis A., Lo Monaco S., Cattabriga A., Cocozza M.A., Pastore L.V., Polici M., et al. Radiomics and Magnetic Resonance Imaging of Rectal Cancer: From Engineering to Clinical Practice. Diagnostics. 2021;11:756. doi: 10.3390/diagnostics11050756.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/diagnostics11050756</ArticleId><ArticleId IdType="pmc">PMC8146913</ArticleId><ArticleId IdType="pubmed">33922483</ArticleId></ArticleIdList></Reference><Reference><Citation>Park H.J., Kim J.H., Choi S.Y., Lee E.S., Park S.J., Byun J.Y., Choi B.I. Prediction of therapeutic response of hepatocellular carcinoma to transcatheter arterial chemoembolization based on pretherapeutic dynamic CT and textural findings. Am. J. Roentgenol. 2017;209:211&#x2013;220. doi: 10.2214/AJR.16.17398.</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/AJR.16.17398</ArticleId><ArticleId IdType="pubmed">28813195</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36611379</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Print">2075-4418</ISSN><JournalIssue CitedMedium="Print"><Volume>13</Volume><Issue>1</Issue><PubDate><Year>2022</Year><Month>Dec</Month><Day>28</Day></PubDate></JournalIssue><Title>Diagnostics (Basel, Switzerland)</Title><ISOAbbreviation>Diagnostics (Basel)</ISOAbbreviation></Journal><ArticleTitle>An Automatic Premature Ventricular Contraction Recognition System Based on Imbalanced Dataset and Pre-Trained Residual Network Using Transfer Learning on ECG Signal.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">87</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/diagnostics13010087</ELocationID><Abstract><AbstractText>The development of automatic monitoring and diagnosis systems for cardiac patients over the internet has been facilitated by recent advancements in wearable sensor devices from electrocardiographs (ECGs), which need the use of patient-specific approaches. Premature ventricular contraction (PVC) is a common chronic cardiovascular disease that can cause conditions that are potentially fatal. Therefore, for the diagnosis of likely heart failure, precise PVC detection from ECGs is crucial. In the clinical settings, cardiologists typically employ long-term ECGs as a tool to identify PVCs, where a cardiologist must put in a lot of time and effort to appropriately assess the long-term ECGs which is time consuming and cumbersome. By addressing these issues, we have investigated a deep learning method with a pre-trained deep residual network, ResNet-18, to identify PVCs automatically using transfer learning mechanism. Herein, features are extracted by the inner layers of the network automatically compared to hand-crafted feature extraction methods. Transfer learning mechanism handles the difficulties of required large volume of training data for a deep model. The pre-trained model is evaluated on the Massachusetts Institute of Technology-Beth Israel Hospital (MIT-BIH) Arrhythmia and Institute of Cardiological Technics (INCART) datasets. First, we used the Pan-Tompkins algorithm to segment 44,103 normal and 6423 PVC beats, as well as 106,239 normal and 9987 PVC beats from the MIT-BIH Arrhythmia and IN-CART datasets, respectively. The pre-trained model employed the segmented beats as input after being converted into 2D (two-dimensional) images. The method is optimized with the using of weighted random samples, on-the-fly augmentation, Adam optimizer, and call back feature. The results from the proposed method demonstrate the satisfactory findings without the using of any complex pre-processing and feature extraction technique as well as design complexity of model. Using LOSOCV (leave one subject out cross-validation), the received accuracies on MIT-BIH and INCART are 99.93% and 99.77%, respectively, suppressing the state-of-the-art methods for PVC recognition on unseen data. This demonstrates the efficacy and generalizability of the proposed method on the imbalanced datasets. Due to the absence of device-specific (patient-specific) information at the evaluating stage on the target datasets in this study, the method might be used as a general approach to handle the situations in which ECG signals are obtained from different patients utilizing a variety of smart sensor devices.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Ullah</LastName><ForeName>Hadaate</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>State Key Laboratory of Electronic Thin Films and Integrated Devices, School of Materials and Energy, University of Electronic Science and Technology of China, Chengdu 610054, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Heyat</LastName><ForeName>Md Belal Bin</ForeName><Initials>MBB</Initials><Identifier Source="ORCID">0000-0001-5307-9582</Identifier><AffiliationInfo><Affiliation>IoT Research Center, College of Computer Science and Software Engineering, Shenzhen University, Shenzhen 518060, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Akhtar</LastName><ForeName>Faijan</ForeName><Initials>F</Initials><AffiliationInfo><Affiliation>School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu 610054, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Muaad</LastName><ForeName>Abdullah Y</ForeName><Initials>AY</Initials><Identifier Source="ORCID">0000-0001-8304-9261</Identifier><AffiliationInfo><Affiliation>IT Department, Sana'a Community College, Sana'a 5695, Yemen.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Ukwuoma</LastName><ForeName>Chiagoziem C</ForeName><Initials>CC</Initials><Identifier Source="ORCID">0000-0002-4532-6026</Identifier><AffiliationInfo><Affiliation>School of Information and Software Engineering, University of Electronic Science and Technology of China, Chengdu 610054, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Bilal</LastName><ForeName>Muhammad</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>College of Pharmacy, Liaquat University of Medical and Health Sciences, Jamshoro 76090, Pakistan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Miraz</LastName><ForeName>Mahdi H</ForeName><Initials>MH</Initials><AffiliationInfo><Affiliation>School of Computing and Data Science, Xiamen University Malaysia, Bandar Sunsuria, Sepang 43900, Malaysia.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>School of Computing, Glynd&#x175;r University, Wrexham LL11 2AW, UK.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Bhuiyan</LastName><ForeName>Mohammad Arif Sobhan</ForeName><Initials>MAS</Initials><Identifier Source="ORCID">0000-0003-0772-0556</Identifier><AffiliationInfo><Affiliation>School of Computing and Data Science, Xiamen University Malaysia, Bandar Sunsuria, Sepang 43900, Malaysia.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wu</LastName><ForeName>Kaishun</ForeName><Initials>K</Initials><AffiliationInfo><Affiliation>IoT Research Center, College of Computer Science and Software Engineering, Shenzhen University, Shenzhen 518060, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Dama&#x161;evi&#x10d;ius</LastName><ForeName>Robertas</ForeName><Initials>R</Initials><Identifier Source="ORCID">0000-0001-9990-1084</Identifier><AffiliationInfo><Affiliation>Department of Software Engineering, Kaunas University of Technology, 44249 Kaunas, Lithuania.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Pan</LastName><ForeName>Taisong</ForeName><Initials>T</Initials><Identifier Source="ORCID">0000-0003-1576-3409</Identifier><AffiliationInfo><Affiliation>State Key Laboratory of Electronic Thin Films and Integrated Devices, School of Materials and Energy, University of Electronic Science and Technology of China, Chengdu 610054, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Gao</LastName><ForeName>Min</ForeName><Initials>M</Initials><Identifier Source="ORCID">0000-0003-3899-2933</Identifier><AffiliationInfo><Affiliation>State Key Laboratory of Electronic Thin Films and Integrated Devices, School of Materials and Energy, University of Electronic Science and Technology of China, Chengdu 610054, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lin</LastName><ForeName>Yuan</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>State Key Laboratory of Electronic Thin Films and Integrated Devices, School of Materials and Energy, University of Electronic Science and Technology of China, Chengdu 610054, China.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Medico-Engineering Corporation on Applied Medicine Research Center, University of Electronic Science and Technology of China, Chengdu 610054, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lai</LastName><ForeName>Dakun</ForeName><Initials>D</Initials><Identifier Source="ORCID">0000-0001-9070-1721</Identifier><AffiliationInfo><Affiliation>Biomedical Imaging and Electrophysiology Laboratory, School of Electronic Science and Engineering, University of Electronic Science and Technology of China, Chengdu 610054, China.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>28</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Diagnostics (Basel)</MedlineTA><NlmUniqueID>101658402</NlmUniqueID><ISSNLinking>2075-4418</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">electrocardiogram</Keyword><Keyword MajorTopicYN="N">imbalanced datasets</Keyword><Keyword MajorTopicYN="N">patient-specific</Keyword><Keyword MajorTopicYN="N">pre-trained</Keyword><Keyword MajorTopicYN="N">premature ventricular contraction</Keyword><Keyword MajorTopicYN="N">recognition</Keyword><Keyword MajorTopicYN="N">residual network</Keyword><Keyword MajorTopicYN="N">transfer learning</Keyword></KeywordList><CoiStatement>The authors declare no conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>11</Month><Day>6</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>5</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>23</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>1</Hour><Minute>2</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36611379</ArticleId><ArticleId IdType="pmc">PMC9818233</ArticleId><ArticleId IdType="doi">10.3390/diagnostics13010087</ArticleId><ArticleId IdType="pii">diagnostics13010087</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Ullah H., Bin Heyat M.B., Akhtar F., Sumbul, Muaad A.Y., Islam M.S., Abbas Z., Pan T., Gao M., Lin Y., et al. An End-to-End Cardiac Arrhythmia Recognition Method with an Effective DenseNet Model on Imbalanced Datasets Using ECG Signal. Comput. Intell. Neurosci. 2022;2022:9475162. doi: 10.1155/2022/9475162.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2022/9475162</ArticleId><ArticleId IdType="pmc">PMC9536938</ArticleId><ArticleId IdType="pubmed">36210977</ArticleId></ArticleIdList></Reference><Reference><Citation>Bin Heyat M.B., Akhtar F., Ansari M.A., Khan A., Alkahtani F., Khan H., Lai D. Progress in Detection of Insomnia Sleep Disorder: A Comprehensive Review. Curr. Drug Targets. 2020;22:672&#x2013;684. doi: 10.2174/1389450121666201027125828.</Citation><ArticleIdList><ArticleId IdType="doi">10.2174/1389450121666201027125828</ArticleId><ArticleId IdType="pubmed">33109045</ArticleId></ArticleIdList></Reference><Reference><Citation>Bin Heyat M.B., Akhtar F., Khan M.H., Ullah N., Gul I., Khan H., Lai D. Detection, Treatment Planning, and Genetic Predisposition of Bruxism: A Systematic Mapping Process and Network Visualization Technique. CNS Neurol. Disord. Drug Targets. 2020;20:755&#x2013;775. doi: 10.2174/19963181MTExyMzM33.</Citation><ArticleIdList><ArticleId IdType="doi">10.2174/19963181MTExyMzM33</ArticleId><ArticleId IdType="pubmed">33172381</ArticleId></ArticleIdList></Reference><Reference><Citation>Lai D., Bin Heyat M.B., Khan F.I., Zhang Y. Prognosis of Sleep Bruxism Using Power Spectral Density Approach Applied on EEG Signal of Both EMG1-EMG2 and ECG1-ECG2 Channels. IEEE Access. 2019;7:82553&#x2013;82562. doi: 10.1109/ACCESS.2019.2924181.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2019.2924181</ArticleId></ArticleIdList></Reference><Reference><Citation>Sayadi O., Shamsollahi M.B., Clifford G.D. Robust Detection of Premature Ventricular Contractions Using a Wave-Based Bayesian Framework. IEEE Trans. Biomed. Eng. 2010;57:353&#x2013;362. doi: 10.1109/TBME.2009.2031243.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TBME.2009.2031243</ArticleId><ArticleId IdType="pmc">PMC2927513</ArticleId><ArticleId IdType="pubmed">19758851</ArticleId></ArticleIdList></Reference><Reference><Citation>Allami R. Premature Ventricular Contraction Analysis for Real-Time Patient Monitoring. Biomed. Signal Process. Control. 2019;47:358&#x2013;365. doi: 10.1016/j.bspc.2018.08.040.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bspc.2018.08.040</ArticleId></ArticleIdList></Reference><Reference><Citation>De Oliveira B.R., de Abreu C.C.E., Duarte M.A.Q., Vieira Filho J. Geometrical Features for Premature Ventricular Contraction Recognition with Analytic Hierarchy Process Based Machine Learning Algorithms Selection. Comput. Methods Programs Biomed. 2019;169:59&#x2013;69. doi: 10.1016/j.cmpb.2018.12.028.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cmpb.2018.12.028</ArticleId><ArticleId IdType="pubmed">30638592</ArticleId></ArticleIdList></Reference><Reference><Citation>Mazidi M.H., Eshghi M., Raoufy M.R. Detection of Premature Ventricular Contraction (PVC) Using Linear and Nonlinear Techniques: An Experimental Study. Clust. Comput. 2019;23:759&#x2013;774. doi: 10.1007/s10586-019-02953-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10586-019-02953-x</ArticleId></ArticleIdList></Reference><Reference><Citation>Ullah H., Kiber A., Huq A., Arif M., Bhuiyan S. Computing the Performance of FFNN for Classifying Purposes. Malays. J. Appl. Sci. 2018;3:8&#x2013;20.</Citation></Reference><Reference><Citation>Demir N., Kuncan M., Kaya Y., Kuncan F. Multi-Layer Co-Occurrence Matrices for Person Identification from ECG Signals. Trait. Signal. 2022;39:431&#x2013;440. doi: 10.18280/ts.390204.</Citation><ArticleIdList><ArticleId IdType="doi">10.18280/ts.390204</ArticleId></ArticleIdList></Reference><Reference><Citation>Kaplan Berkaya S., Uysal A.K., Sora Gunal E., Ergin S., Gunal S., Gulmezoglu M.B. A Survey on ECG Analysis. Biomed. Signal Process. Control. 2018;43:216&#x2013;235. doi: 10.1016/j.bspc.2018.03.003.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bspc.2018.03.003</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhao W., Zhu R., Zhang J., Mao Y., Chen H., Ju W., Li M., Yang G., Gu K., Wang Z., et al. Machine Learning for Distinguishing Right from Left Premature Ventricular Contraction Origin Using Surface Electrocardiogram Features. Heart Rhythm. 2022;19:1781&#x2013;1789. doi: 10.1016/j.hrthm.2022.07.010.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.hrthm.2022.07.010</ArticleId><ArticleId IdType="pubmed">35843464</ArticleId></ArticleIdList></Reference><Reference><Citation>Sraitih M., Jabrane Y., El Hassani A.H. An Automated System for ECG Arrhythmia Detection Using Machine Learning Techniques. J. Clin. Med. 2021;10:5450. doi: 10.3390/jcm10225450.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/jcm10225450</ArticleId><ArticleId IdType="pmc">PMC8618527</ArticleId><ArticleId IdType="pubmed">34830732</ArticleId></ArticleIdList></Reference><Reference><Citation>Han D., Bashar S.K., Mohagheghian F., Ding E., Whitcomb C., McManus D.D., Chon K.H. Premature Atrial and Ventricular Contraction Detection Using Photoplethysmographic Data from a Smartwatch. Sensors. 2020;20:5683. doi: 10.3390/s20195683.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s20195683</ArticleId><ArticleId IdType="pmc">PMC7582300</ArticleId><ArticleId IdType="pubmed">33028000</ArticleId></ArticleIdList></Reference><Reference><Citation>Nawabi A.K., Jinfang S., Abbasi R., Iqbal M.S., Bin Heyat M.B., Akhtar F., Wu K., Twumasi B.A. Segmentation of Drug-Treated Cell Image and Mitochondrial-Oxidative Stress Using Deep Convolutional Neural Network. Oxid. Med. Cell. Longev. 2022;2022:5641727. doi: 10.1155/2022/5641727.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2022/5641727</ArticleId><ArticleId IdType="pmc">PMC9162846</ArticleId><ArticleId IdType="pubmed">35663204</ArticleId></ArticleIdList></Reference><Reference><Citation>Ullah H., Bin Heyat M.B., Alsalman H., Khan H.M., Akhtar F., Gumaei A., Mehdi A., Muaad A.Y., Islam M.S., Ali A., et al. An Effective and Lightweight Deep Electrocardiography Arrhythmia Recognition Model Using Novel Special and Native Structural Regularization Techniques on Cardiac Signal. J. Healthc. Eng. 2022;2022:3408501. doi: 10.1155/2022/3408501.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2022/3408501</ArticleId><ArticleId IdType="pmc">PMC9018174</ArticleId><ArticleId IdType="pubmed">35449862</ArticleId></ArticleIdList></Reference><Reference><Citation>Ali L., He Z., Cao W., Rauf H.T., Imrana Y., Bin Heyat M.B. MMDD-Ensemble: A Multimodal Data&#x2013;Driven Ensemble Approach for Parkinson&#x2019;s Disease Detection. Front. Neurosci. 2021;15:754058. doi: 10.3389/fnins.2021.754058.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fnins.2021.754058</ArticleId><ArticleId IdType="pmc">PMC8591047</ArticleId><ArticleId IdType="pubmed">34790091</ArticleId></ArticleIdList></Reference><Reference><Citation>Ukwuoma C.C., Qin Z., Belal Bin Heyat M., Akhtar F., Bamisile O., Muaad A.Y., Addo D., Al-antari M.A. A Hybrid Explainable Ensemble Transformer Encoder for Pneumonia Identification from Chest X-Ray Images. J. Adv. Res. 2022. in press .</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jare.2022.08.021</ArticleId><ArticleId IdType="pubmed">36084812</ArticleId></ArticleIdList></Reference><Reference><Citation>Bengio Y., Courville A., Vincent P. Representation Learning: A Review and New Perspectives. IEEE Trans. Pattern Anal. Mach. Intell. 2012;35:1798&#x2013;1828. doi: 10.1109/TPAMI.2013.50.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TPAMI.2013.50</ArticleId><ArticleId IdType="pubmed">23787338</ArticleId></ArticleIdList></Reference><Reference><Citation>Guragai B., Alshorman O., Masadeh M., Heyat M.B. Bin A Survey on Deep Learning Classification Algorithms for Motor Imagery; Proceedings of the 2020 32nd International Conference on Microelectronics (ICM); Aqaba, Jordan. 14&#x2013;17 December 2020;</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ICM50269.2020.9331503</ArticleId></ArticleIdList></Reference><Reference><Citation>Lai D., Zhang X., Zhang Y., Bin Heyat M.B. Convolutional Neural Network Based Detection of Atrial Fibrillation Combing R-R Intervals and F-Wave Frequency Spectrum; Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS; Berlin, Germany. 23&#x2013;27 July 2019; Hoboken, NJ, USA: IEEE; 2019. pp. 4897&#x2013;4900.</Citation><ArticleIdList><ArticleId IdType="pubmed">31946958</ArticleId></ArticleIdList></Reference><Reference><Citation>Acharya U.R., Fujita H., Oh S.L., Raghavendra U., Tan J.H., Adam M., Gertych A., Hagiwara Y. Automated Identification of Shockable and Non-Shockable Life-Threatening Ventricular Arrhythmias Using Convolutional Neural Network. Futur. Gener. Comput. Syst. 2018;79:952&#x2013;959. doi: 10.1016/j.future.2017.08.039.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.future.2017.08.039</ArticleId></ArticleIdList></Reference><Reference><Citation>Hemanth D.J., Deperlioglu O., Kose U. An Enhanced Diabetic Retinopathy Detection and Classification Approach Using Deep Convolutional Neural Network. Neural Comput. Appl. 2019;32:707&#x2013;721. doi: 10.1007/s00521-018-03974-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00521-018-03974-0</ArticleId></ArticleIdList></Reference><Reference><Citation>Naz M., Shah J.H., Khan M.A., Sharif M., Raza M., Dama&#x161;evi&#x10d;ius R. From ECG signals to images: A transformation based approach for deep learning. PeerJ Comput. Sci. 2021;7:e386. doi: 10.7717/peerj-cs.386.</Citation><ArticleIdList><ArticleId IdType="doi">10.7717/peerj-cs.386</ArticleId><ArticleId IdType="pmc">PMC7959637</ArticleId><ArticleId IdType="pubmed">33817032</ArticleId></ArticleIdList></Reference><Reference><Citation>Hannun A.Y., Rajpurkar P., Haghpanahi M., Tison G.H., Bourn C., Turakhia M.P., Ng A.Y. Cardiologist-Level Arrhythmia Detection and Classification in Ambulatory Electrocardiograms Using a Deep Neural Network. Nat. Med. 2019;25:65&#x2013;69. doi: 10.1038/s41591-018-0268-3.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41591-018-0268-3</ArticleId><ArticleId IdType="pmc">PMC6784839</ArticleId><ArticleId IdType="pubmed">30617320</ArticleId></ArticleIdList></Reference><Reference><Citation>Yildirim &#xd6;. A Novel Wavelet Sequences Based on Deep Bidirectional LSTM Network Model for ECG Signal Classification. Comput. Biol. Med. 2018;96:189&#x2013;202. doi: 10.1016/j.compbiomed.2018.03.016.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2018.03.016</ArticleId><ArticleId IdType="pubmed">29614430</ArticleId></ArticleIdList></Reference><Reference><Citation>Awais M., Raza M., Singh N., Bashir K., Manzoor U., Islam S.U., Rodrigues J.J.P.C. LSTM-Based Emotion Detection Using Physiological Signals: IoT Framework for Healthcare and Distance Learning in COVID-19. IEEE Internet Things J. 2021;8:16863&#x2013;16871. doi: 10.1109/JIOT.2020.3044031.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/JIOT.2020.3044031</ArticleId><ArticleId IdType="pmc">PMC8864945</ArticleId><ArticleId IdType="pubmed">35582634</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen C., Hua Z., Zhang R., Liu G., Wen W. Automated Arrhythmia Classification Based on a Combination Network of CNN and LSTM. Biomed. Signal Process. Control. 2020;57:101819. doi: 10.1016/j.bspc.2019.101819.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bspc.2019.101819</ArticleId></ArticleIdList></Reference><Reference><Citation>Khamparia A., Pandey B., Tiwari S., Gupta D., Khanna A., Rodrigues J.J.P.C. An Integrated Hybrid CNN&#x2013;RNN Model for Visual Description and Generation of Captions. Circuits, Syst. Signal Process. 2020;39:776&#x2013;788. doi: 10.1007/s00034-019-01306-8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00034-019-01306-8</ArticleId></ArticleIdList></Reference><Reference><Citation>Akda&#x11f; S., Kuncan F., Kaya Y. A New Approach for Congestive Heart Failure and Arrhythmia Classification Using Downsampling Local Binary Patterns with LSTM. Turkish J. Electr. Eng. Comput. Sci. 2022;30:2145&#x2013;2164. doi: 10.55730/1300-0632.3930.</Citation><ArticleIdList><ArticleId IdType="doi">10.55730/1300-0632.3930</ArticleId></ArticleIdList></Reference><Reference><Citation>Kaya Y., Kuncan F., Tekin R. A New Approach for Congestive Heart Failure and Arrhythmia Classification Using Angle Transformation with LSTM. Arab. J. Sci. Eng. 2022;47:10497&#x2013;10513. doi: 10.1007/s13369-022-06617-8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s13369-022-06617-8</ArticleId></ArticleIdList></Reference><Reference><Citation>Ihsanto E., Ramli K., Sudiana D., Gunawan T.S. Fast and Accurate Algorithm for ECG Authentication Using Residual Depthwise Separable Convolutional Neural Networks. Appl. Sci. 2020;10:3304. doi: 10.3390/app10093304.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/app10093304</ArticleId></ArticleIdList></Reference><Reference><Citation>Yu J., Wang X., Chen X., Guo J. Searching for Premature Ventricular Contraction from Electrocardiogram by Using One-Dimensional Convolutional Neural Network. Electronics. 2020;9:1790. doi: 10.3390/electronics9111790.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/electronics9111790</ArticleId></ArticleIdList></Reference><Reference><Citation>Yu J., Wang X., Chen X., Guo J. Automatic Premature Ventricular Contraction Detection Using Deep Metric Learning and Knn. Biosensors. 2021;11:69. doi: 10.3390/bios11030069.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/bios11030069</ArticleId><ArticleId IdType="pmc">PMC8000997</ArticleId><ArticleId IdType="pubmed">33806367</ArticleId></ArticleIdList></Reference><Reference><Citation>Yildirim O., Talo M., Ay B., Baloglu U.B., Aydin G., Acharya U.R. Automated Detection of Diabetic Subject Using Pre-Trained 2D-CNN Models with Frequency Spectrum Images Extracted from Heart Rate Signals. Comput. Biol. Med. 2019;113:103387. doi: 10.1016/j.compbiomed.2019.103387.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2019.103387</ArticleId><ArticleId IdType="pubmed">31421276</ArticleId></ArticleIdList></Reference><Reference><Citation>He K., Zhang X., Ren S., Sun J. Deep Residual Learning for Image Recognition; Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition; IEEE Computer Society; Las Vegas, NV, USA. 27&#x2013;30 June 2016; pp. 770&#x2013;778.</Citation></Reference><Reference><Citation>Krawczyk B. Learning from Imbalanced Data: Open Challenges and Future Directions. Prog. Artif. Intell. 2016;5:221&#x2013;232. doi: 10.1007/s13748-016-0094-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s13748-016-0094-0</ArticleId></ArticleIdList></Reference><Reference><Citation>Rajesh K.N.V.P.S., Dhuli R. Classification of Imbalanced ECG Beats Using Re-Sampling Techniques and AdaBoost Ensemble Classifier. Biomed. Signal Process. Control. 2018;41:242&#x2013;254. doi: 10.1016/j.bspc.2017.12.004.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bspc.2017.12.004</ArticleId></ArticleIdList></Reference><Reference><Citation>Beritelli F., Capizzi G., Lo Sciuto G., Napoli C., Wo&#x17a;niak M. A novel training method to preserve generalization of RBPNN classifiers applied to ECG signals diagnosis. Neural Netw. 2018;108:331&#x2013;338. doi: 10.1016/j.neunet.2018.08.023.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neunet.2018.08.023</ArticleId><ArticleId IdType="pubmed">30245432</ArticleId></ArticleIdList></Reference><Reference><Citation>Ai M.T., Sumiati S., Rosalina V. A predictive model for heart disease diagnosis based on multinomial logistic regression. Inf. Technol. Control. 2021;50:308&#x2013;318. doi: 10.5755/j01.itc.50.2.27672.</Citation><ArticleIdList><ArticleId IdType="doi">10.5755/j01.itc.50.2.27672</ArticleId></ArticleIdList></Reference><Reference><Citation>Jothi Prakash V., Karthikeyan N.K. Dual-layer deep ensemble techniques for classifying heart disease. Inf. Technol. Control. 2022;51:158&#x2013;179. doi: 10.5755/j01.itc.51.1.30083.</Citation><ArticleIdList><ArticleId IdType="doi">10.5755/j01.itc.51.1.30083</ArticleId></ArticleIdList></Reference><Reference><Citation>Tayyib M., Amir M., Yousufi M., Abdullah S., Maqsood S., Irfan M. Modified block compressed sensing for extraction of fetal electrocardiogram from mother electrocardiogram using block compressed sensing based guided focuss and fast-independent component. Inf. Technol. Control. 2021;50:123&#x2013;137. doi: 10.5755/J01.ITC.50.1.24145.</Citation><ArticleIdList><ArticleId IdType="doi">10.5755/J01.ITC.50.1.24145</ArticleId></ArticleIdList></Reference><Reference><Citation>Jang J.H., Kim T.Y., Yoon D. Effectiveness of Transfer Learning for Deep Learning-Based Electrocardiogram Analysis. Healthc. Inform. Res. 2021;27:19&#x2013;28. doi: 10.4258/hir.2021.27.1.19.</Citation><ArticleIdList><ArticleId IdType="doi">10.4258/hir.2021.27.1.19</ArticleId><ArticleId IdType="pmc">PMC7921576</ArticleId><ArticleId IdType="pubmed">33611873</ArticleId></ArticleIdList></Reference><Reference><Citation>Kim H., Kim S., Van Helleputte N., Artes A., Konijnenburg M., Huisken J., Van Hoof C., Yazicioglu R.F. A Configurable and Low-Power Mixed Signal SoC for Portable ECG Monitoring Applications. IEEE Trans. Biomed. Circuits Syst. 2014;8:257&#x2013;267. doi: 10.1109/TBCAS.2013.2260159.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TBCAS.2013.2260159</ArticleId><ArticleId IdType="pubmed">24875285</ArticleId></ArticleIdList></Reference><Reference><Citation>Ullah H., Wahab M.A., Will G., Karim M.R., Pan T., Gao M., Lai D., Lin Y., Miraz M.H. Recent Advances in Stretchable and Wearable Capacitive Electrophysiological Sensors for Long-Term Health Monitoring. Biosensensors. 2022;12:630. doi: 10.3390/bios12080630.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/bios12080630</ArticleId><ArticleId IdType="pmc">PMC9406032</ArticleId><ArticleId IdType="pubmed">36005025</ArticleId></ArticleIdList></Reference><Reference><Citation>Allam J.P., Samantray S., Ari S. SpEC: A System for Patient Specific ECG Beat Classification Using Deep Residual Network. Biocybern. Biomed. Eng. 2020;40:1446&#x2013;1457. doi: 10.1016/j.bbe.2020.08.001.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bbe.2020.08.001</ArticleId></ArticleIdList></Reference><Reference><Citation>Kiranyaz S., Ince T., Gabbouj M. Personalized Monitoring and Advance Warning System for Cardiac Arrhythmias. Sci. Rep. 2017;7:9270. doi: 10.1038/s41598-017-09544-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-017-09544-z</ArticleId><ArticleId IdType="pmc">PMC5571226</ArticleId><ArticleId IdType="pubmed">28839215</ArticleId></ArticleIdList></Reference><Reference><Citation>Lodwich A., Rangoni Y., Breuel T. Evaluation of Robustness and Performance of Early Stopping Rules with Multi Layer Perceptrons; Proceedings of the 2009 International Joint Conference on Neural Networks; Atlanta, GA, USA. 14&#x2013;19 June 2009; pp. 1877&#x2013;1884.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/IJCNN.2009.5178626</ArticleId></ArticleIdList></Reference><Reference><Citation>Efraimidis P., Spirakis P. Encyclopedia of Algorithms. Springer; Boston, MA, USA: 2008. Weighted Random Sampling; pp. 1024&#x2013;1027.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-0-387-30162-4_478</ArticleId></ArticleIdList></Reference><Reference><Citation>Kingma D.P., Ba J. Adam: A Method for Stochastic Optimization. arXiv. 2014 doi: 10.48550/arXiv.1412.6980.</Citation><ArticleIdList><ArticleId IdType="doi">10.48550/arXiv.1412.6980</ArticleId></ArticleIdList></Reference><Reference><Citation>Pan J., Tompkins W.J. A Real-Time QRS Detection Algorithm. IEEE Trans. Biomed. Eng. 1985;BME-32:230&#x2013;236. doi: 10.1109/TBME.1985.325532.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TBME.1985.325532</ArticleId><ArticleId IdType="pubmed">3997178</ArticleId></ArticleIdList></Reference><Reference><Citation>De Chazal P., O&#x2019;Dwyer M., Reilly R.B. Automatic Classification of Heartbeats Using ECG Morphology and Heartbeat Interval Features. IEEE Trans. Biomed. Eng. 2004;51:1196&#x2013;1206. doi: 10.1109/TBME.2004.827359.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TBME.2004.827359</ArticleId><ArticleId IdType="pubmed">15248536</ArticleId></ArticleIdList></Reference><Reference><Citation>Lam T.K., Ohta M., Schamoni S., Riezler S. On-the-Fly Aligned Data Augmentation for Sequence-to-Sequence ASR; Proceedings of the 22nd Annual Conference of the International Speech Communication Association (INTERSPEECH 2021); Brno, Czech Republic. 30 August&#x2013;3 September 2021; pp. 4481&#x2013;4485.</Citation><ArticleIdList><ArticleId IdType="doi">10.21437/Interspeech.2021-1679</ArticleId></ArticleIdList></Reference><Reference><Citation>Krizhevsky A., Sutskever I., Hinton G.E. ImageNet Classification with Deep Convolutional Neural Networks. Commun. ACM. 2017;60:84&#x2013;90. doi: 10.1145/3065386.</Citation><ArticleIdList><ArticleId IdType="doi">10.1145/3065386</ArticleId></ArticleIdList></Reference><Reference><Citation>Simonyan K., Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition; Proceedings of the 3rd International Conference on Learning Representations, ICLR 2015&#x2014;Conference Track Proceedings, ICLR; San Diego, CA, USA. 7&#x2013;9 May 2015.</Citation></Reference><Reference><Citation>He K., Zhang X., Ren S., Sun J. Computer Vision&#x2014;ECCV 2016. Volume 9908. Springer; Cham, Switzerland: 2016. Identity Mappings in Deep Residual Networks; pp. 630&#x2013;645. Lecture Notes in Computer Science.</Citation></Reference><Reference><Citation>Nair V., Hinton G.E. Rectified Linear Units Improve Restricted Boltzmann Machines.  [(accessed on 5 October 2022)].  Available online:  https://openreview.net/forum?id=rkb15iZdZB.</Citation></Reference><Reference><Citation>Paszke A., Gross S., Massa F., Lerer A., Bradbury Google J., Chanan G., Killeen T., Lin Z., Gimelshein N., Antiga L., et al. PyTorch: An Imperative Style, High-Performance Deep Learning Library. Adv. Neural Process. Syst. 2019;32:1&#x2013;12.</Citation></Reference><Reference><Citation>Chetlur S., Woolley C., Vandermersch P., Cohen J., Tran J., Catanzaro B., Shelhamer E. CuDNN: Efficient Primitives for Deep Learning. arXiv. 2014 doi: 10.48550/arXiv.1410.0759.</Citation><ArticleIdList><ArticleId IdType="doi">10.48550/arXiv.1410.0759</ArticleId></ArticleIdList></Reference><Reference><Citation>Bin Heyat M.B., Akhtar F., Khan A., Noor A., Benjdira B., Qamar Y., Abbas S.J., Lai D. A Novel Hybrid Machine Learning Classification for the Detection of Bruxism Patients Using Physiological Signals. Appl. Sci. 2020;10:7410. doi: 10.3390/app10217410.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/app10217410</ArticleId></ArticleIdList></Reference><Reference><Citation>Ukwuoma C.C., Qin Z., Bin Heyat M.B., Akhtar F., Smahi A., Jackson J.K., Furqan Qadri S., Muaad A.Y., Monday H.N., Nneji G.U. Automated Lung-Related Pneumonia and COVID-19 Detection Based on Novel Feature Extraction Framework and Vision Transformer Approaches Using Chest X-Ray Images. Bioengineering. 2022;9:709. doi: 10.3390/bioengineering9110709.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/bioengineering9110709</ArticleId><ArticleId IdType="pmc">PMC9687434</ArticleId><ArticleId IdType="pubmed">36421110</ArticleId></ArticleIdList></Reference><Reference><Citation>Chola C., Muaad A.Y., Bin Heyat M.B., Benifa J.V.B., Naji W.R., Hemachandran K., Mahmoud N.F., Samee N.A., Al-Antari M.A., Kadah Y.M., et al. BCNet: A Deep Learning Computer-Aided Diagnosis Framework for Human Peripheral Blood Cell Identification. Diagnostics. 2022;12:2815. doi: 10.3390/diagnostics12112815.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/diagnostics12112815</ArticleId><ArticleId IdType="pmc">PMC9689932</ArticleId><ArticleId IdType="pubmed">36428875</ArticleId></ArticleIdList></Reference><Reference><Citation>Tripathi P., Ansari M.A., Gandhi T.K., Mehrotra R., Bin Heyat M.B., Akhtar F., Ukwuoma C.C., Muaad A.Y., Kadah Y.M., Al-Antari M.A., et al. Ensemble Computational Intelligent for Insomnia Sleep Stage Detection via the Sleep ECG Signal. IEEE Access. 2022;10:108710&#x2013;108721. doi: 10.1109/ACCESS.2022.3212120.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2022.3212120</ArticleId></ArticleIdList></Reference><Reference><Citation>Pedregosa F., Michel V., Grisel O., Blondel M., Prettenhofer P., Weiss R., Vanderplas J., Cournapeau D., Pedregosa F., Varoquaux G., et al. Scikit-Learn: Machine Learning in Python. J. Mach. Learn. Res. 2011;12:2825&#x2013;2830.</Citation></Reference><Reference><Citation>Hoekema R., Uijen G.J.H., Van Oosterom A. Geometrical Aspects of the Interindividual Variability of Multilead ECG Recordings. IEEE Trans. Biomed. Eng. 2001;48:551&#x2013;559. doi: 10.1109/10.918594.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/10.918594</ArticleId><ArticleId IdType="pubmed">11341529</ArticleId></ArticleIdList></Reference><Reference><Citation>Talbi M.L., Charef A. PVC Discrimination Using the QRS Power Spectrum and Self-Organizing Maps. Comput. Methods Programs Biomed. 2009;94:223&#x2013;231. doi: 10.1016/j.cmpb.2008.12.009.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cmpb.2008.12.009</ArticleId><ArticleId IdType="pubmed">19215994</ArticleId></ArticleIdList></Reference><Reference><Citation>Walker H.K., Hall W.D., Hurst J.W. Clinical Methods. Geriatr. Psychiatry. 1990:77&#x2013;121.</Citation></Reference><Reference><Citation>Malek A.S., Elnahrawy A., Anwar H., Naeem M. Automated Detection of Premature Ventricular Contraction in ECG Signals Using Enhanced Template Matching Algorithm. Biomed. Phys. Eng. Express. 2020;6:015024. doi: 10.1088/2057-1976/ab6995.</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/2057-1976/ab6995</ArticleId><ArticleId IdType="pubmed">33438612</ArticleId></ArticleIdList></Reference><Reference><Citation>Ge D., Srinivasan N., Krishnan S.M. Cardiac Arrhythmia Classification Using Autoregressive Modeling. Biomed. Eng. Online. 2002;1:5. doi: 10.1186/1475-925X-1-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/1475-925X-1-5</ArticleId><ArticleId IdType="pmc">PMC149374</ArticleId><ArticleId IdType="pubmed">12473180</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhao Z., Wang X., Cai Z., Li J., Liu C. PVC Recognition for Wearable ECGs Using Modified Frequency Slice Wavelet Transform and Convolutional Neural Network; Proceedings of the 2019 Computing in Cardiology (CinC); Singapore. 8&#x2013;11 September 2019;</Citation><ArticleIdList><ArticleId IdType="doi">10.23919/CINC49843.2019.9005872</ArticleId></ArticleIdList></Reference><Reference><Citation>Li Q., Liu C., Li Q., Shashikumar S.P., Nemati S., Shen Z., Clifford G.D. Ventricular Ectopic Beat Detection Using a Wavelet Transform and a Convolutional Neural Network. Physiol. Meas. 2019;40:055002. doi: 10.1088/1361-6579/ab17f0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/1361-6579/ab17f0</ArticleId><ArticleId IdType="pubmed">30970338</ArticleId></ArticleIdList></Reference><Reference><Citation>Hoang T., Fahier N., Fang W.C. Multi-Leads ECG Premature Ventricular Contraction Detection Using Tensor Decomposition and Convolutional Neural Network; Proceedings of the 2019 IEEE Biomedical Circuits and Systems Conference (BioCAS); Nara, Japan. 17&#x2013;19 October 2019;</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/BIOCAS.2019.8919049</ArticleId></ArticleIdList></Reference><Reference><Citation>Jun T.J., Park H.J., Minh N.H., Kim D., Kim Y.-H. Premature Ventricular Contraction Beat Detection with Deep Neural Networks; Proceedings of the 2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA); Anaheim, CA, USA. 18&#x2013;20 December 2017; pp. 859&#x2013;864.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ICMLA.2016.0154</ArticleId></ArticleIdList></Reference><Reference><Citation>Junior E.A., Valentim R.A.D.M., Brand&#xe3;o G.B. Real-Time Premature Ventricular Contractions Detection Based on Redundant Discrete Wavelet Transform. Res. Biomed. Eng. 2018;34:187&#x2013;197. doi: 10.1590/2446-4740.01618.</Citation><ArticleIdList><ArticleId IdType="doi">10.1590/2446-4740.01618</ArticleId></ArticleIdList></Reference><Reference><Citation>Somani S., Russak A.J., Richter F., Zhao S., Vaid A., Chaudhry F., De Freitas J.K., Naik N., Miotto R., Nadkarni G.N., et al. Deep Learning and the Electrocardiogram: Review of the Current State-of-the-Art. EP Eur. 2021;23:1179&#x2013;1191. doi: 10.1093/europace/euaa377.</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/europace/euaa377</ArticleId><ArticleId IdType="pmc">PMC8350862</ArticleId><ArticleId IdType="pubmed">33564873</ArticleId></ArticleIdList></Reference><Reference><Citation>Lecun Y., Bengio Y., Hinton G. Deep Learning. Nature. 2015;521:436&#x2013;444. doi: 10.1038/nature14539.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nature14539</ArticleId><ArticleId IdType="pubmed">26017442</ArticleId></ArticleIdList></Reference><Reference><Citation>Ullah H., Bu Y., Pan T., Gao M., Islam S., Lin Y., Lai D. Cardiac Arrhythmia Recognition Using Transfer Learning with a Pre-Trained DenseNet; Proceedings of the 2021 IEEE 2nd International Conference on Pattern Recognition and Machine Learning (PRML); Chengdu, China. 16&#x2013;18 July 2021; pp. 347&#x2013;353.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/PRML52754.2021.9520710</ArticleId></ArticleIdList></Reference><Reference><Citation>Ullah H., Mahmud S., Chowhury R.H. Identification of Brain disorders by Sub-band Decomposition of EEG signals and Measurement of Signal to Noise Ratio. Indones. J. Electr. Eng. Comput. Sci. 2016;4:568&#x2013;579. doi: 10.11591/IJEECS.V4.I3.PP568-579.</Citation><ArticleIdList><ArticleId IdType="doi">10.11591/IJEECS.V4.I3.PP568-579</ArticleId></ArticleIdList></Reference><Reference><Citation>Komolovait&#x117; D., Maskeli&#x16b;nas R., Dama&#x161;evi&#x10d;ius R. Deep convolutional neural Network-based visual stimuli classification using electroencephalography signals of healthy and Alzheimer&#x2019;s disease subjects. Life. 2022;12:374. doi: 10.3390/life12030374.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/life12030374</ArticleId><ArticleId IdType="pmc">PMC8950142</ArticleId><ArticleId IdType="pubmed">35330125</ArticleId></ArticleIdList></Reference><Reference><Citation>Tamulis &#x17d;., Vasiljevas M., Dama&#x161;evi&#x10d;ius R., Maskeliunas R., Misra S. Affective computing for eHealth using low-cost remote internet of things-based EMG platform. In: Ghosh U., Chakraborty C., Garg L., Srivastava G., editors. Intelligent Internet of Things for Healthcare and Industry: Internet of Things. Springer; Cham, Switzerland: 2022.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-030-81473-1_3</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36611373</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Print">2075-4418</ISSN><JournalIssue CitedMedium="Print"><Volume>13</Volume><Issue>1</Issue><PubDate><Year>2022</Year><Month>Dec</Month><Day>28</Day></PubDate></JournalIssue><Title>Diagnostics (Basel, Switzerland)</Title><ISOAbbreviation>Diagnostics (Basel)</ISOAbbreviation></Journal><ArticleTitle>An Alternative Diagnostic Method for <i>C. neoformans</i>: Preliminary Results of Deep-Learning Based Detection Model.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">81</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/diagnostics13010081</ELocationID><Abstract><AbstractText><i>Cryptococcus neoformans</i> is an opportunistic fungal pathogen with significant medical importance, especially in immunosuppressed patients. It is the causative agent of cryptococcosis. An estimated 220,000 annual cases of cryptococcal meningitis (CM) occur among people with HIV/AIDS globally, resulting in nearly 181,000 deaths. The gold standards for the diagnosis are either direct microscopic identification or fungal cultures. However, these diagnostic methods need special types of equipment and clinical expertise, and relatively low sensitivities have also been reported. This study aims to produce and implement a deep-learning approach to detect <i>C. neoformans</i> in patient samples. Therefore, we adopted the state-of-the-art VGG16 model, which determines the output information from a single image. Images that contain <i>C. neoformans</i> are designated positive, while others are designated negative throughout this section. Model training, validation, testing, and evaluation were conducted using frameworks and libraries. The state-of-the-art VGG16 model produced an accuracy and loss of 86.88% and 0.36203, respectively. Results prove that the deep learning framework VGG16 can be helpful as an alternative diagnostic method for the rapid and accurate identification of the <i>C. neoformans</i>, leading to early diagnosis and subsequent treatment. Further studies should include more and higher quality images to eliminate the limitations of the adopted deep learning model.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Seyer Cagatan</LastName><ForeName>Ayse</ForeName><Initials>A</Initials><Identifier Source="ORCID">0000-0002-5096-898X</Identifier><AffiliationInfo><Affiliation>Department of Medical and Clinical Microbiology, Faculty of Medicine, Cyprus International University, TRNC Mersin 10, Nicosia 99010, Turkey.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Taiwo Mustapha</LastName><ForeName>Mubarak</ForeName><Initials>M</Initials><Identifier Source="ORCID">0000-0001-8653-3809</Identifier><AffiliationInfo><Affiliation>Operational Research Center in Healthcare, Near East University, TRNC Mersin 10, Nicosia 99138, Turkey.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Bagkur</LastName><ForeName>Cemile</ForeName><Initials>C</Initials><Identifier Source="ORCID">0000-0003-0998-4176</Identifier><AffiliationInfo><Affiliation>DESAM Research Institute, Near East University, TRNC Mersin 10, Nicosia 99138, Turkey.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Sanlidag</LastName><ForeName>Tamer</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>DESAM Research Institute, Near East University, TRNC Mersin 10, Nicosia 99138, Turkey.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Ozsahin</LastName><ForeName>Dilber Uzun</ForeName><Initials>DU</Initials><AffiliationInfo><Affiliation>Operational Research Center in Healthcare, Near East University, TRNC Mersin 10, Nicosia 99138, Turkey.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Medical Diagnostic Imaging Department, College of Health Science, University of Sharjah, Sharjah 27272, United Arab Emirates.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>28</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Diagnostics (Basel)</MedlineTA><NlmUniqueID>101658402</NlmUniqueID><ISSNLinking>2075-4418</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">C. neoformans</Keyword><Keyword MajorTopicYN="N">artificial intelligence</Keyword><Keyword MajorTopicYN="N">cryptococcosis</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword><Keyword MajorTopicYN="N">diagnosis</Keyword></KeywordList><CoiStatement>The authors declare no conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>10</Month><Day>17</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>11</Month><Day>23</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>16</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>1</Hour><Minute>2</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36611373</ArticleId><ArticleId IdType="pmc">PMC9818640</ArticleId><ArticleId IdType="doi">10.3390/diagnostics13010081</ArticleId><ArticleId IdType="pii">diagnostics13010081</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Negroni R. Cryptococcosis. Clin. Dermatol. 2012;30:599&#x2013;609. doi: 10.1016/j.clindermatol.2012.01.005.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.clindermatol.2012.01.005</ArticleId><ArticleId IdType="pubmed">23068147</ArticleId></ArticleIdList></Reference><Reference><Citation>Kwon-Chung K.J., Fraser J.A., Doering T.L., Wang Z., Janbon G., Idnurm A., Bahn Y.S. Cryptococcus neoformans and Cryptococcus gattii, the Etiologic Agents of Cryptococcosis. Cold. Spring. Harb. Perspect. Med. 2014;4:a019760. doi: 10.1101/cshperspect.a019760.</Citation><ArticleIdList><ArticleId IdType="doi">10.1101/cshperspect.a019760</ArticleId><ArticleId IdType="pmc">PMC4066639</ArticleId><ArticleId IdType="pubmed">24985132</ArticleId></ArticleIdList></Reference><Reference><Citation>Setianingrum F., Rautemaa-Richardson R., Denning D.W. Pulmonary cryptococcosis: A review of pathobiology and clinical aspects. Med. Mycol. 2019;57:133&#x2013;150. doi: 10.1093/mmy/myy086.</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/mmy/myy086</ArticleId><ArticleId IdType="pubmed">30329097</ArticleId></ArticleIdList></Reference><Reference><Citation>Bahn Y.S., Sun S., Heitman J., Lin X. Microbe Profile: Cryptococcus neoformans species complex. Microbiology. 2020;166:797&#x2013;799. doi: 10.1099/mic.0.000973.</Citation><ArticleIdList><ArticleId IdType="doi">10.1099/mic.0.000973</ArticleId><ArticleId IdType="pmc">PMC7717486</ArticleId><ArticleId IdType="pubmed">32956032</ArticleId></ArticleIdList></Reference><Reference><Citation>Zaragoza O. Basic principles of the virulence of Cryptococcus. Virulence. 2019;10:490&#x2013;501. doi: 10.1080/21505594.2019.1614383.</Citation><ArticleIdList><ArticleId IdType="doi">10.1080/21505594.2019.1614383</ArticleId><ArticleId IdType="pmc">PMC6550552</ArticleId><ArticleId IdType="pubmed">31119976</ArticleId></ArticleIdList></Reference><Reference><Citation>Centers for Disease Control and Prevention.  [(accessed on 1 October 2022)]; Available online:  https://www.cdc.gov/fungal/diseases/cryptococcosis-neoformans/statistics.html.</Citation></Reference><Reference><Citation>Rajasingham R., Govender N.P., Jordan A., Loyse A., Shroufi A., Denning D.W., Meya D.B., Chiller T.M., Boulware D.R. The global burden of HIV-associated cryptococcal infection in adults in 2020: A modelling analysis. Lancet Infect. Dis. 2022;22:1748&#x2013;1755. doi: 10.1016/S1473-3099(22)00499-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S1473-3099(22)00499-6</ArticleId><ArticleId IdType="pmc">PMC9701154</ArticleId><ArticleId IdType="pubmed">36049486</ArticleId></ArticleIdList></Reference><Reference><Citation>Temfack E., Rim J.J.B., Spijker R., Loyse A., Chiller T., Pappas P.G., Perfect J., Sorell T.C., Harrison T.S., Cohen J.F., et al. Cryptococcal Antigen in Serum and Cerebrospinal Fluid for Detecting Cryptococcal Meningitis in Adults Living with Human Immunodeficiency Virus: Systematic Review and Meta-Analysis of Diagnostic Test Accuracy Studies. Clin. Infect. Dis. 2021;72:1268&#x2013;1278. doi: 10.1093/cid/ciaa1243.</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/cid/ciaa1243</ArticleId><ArticleId IdType="pmc">PMC8522332</ArticleId><ArticleId IdType="pubmed">32829406</ArticleId></ArticleIdList></Reference><Reference><Citation>Lakoh S., Rickman H., Sesay M., Kenneh S., Burke R., Baldeh M., Jiba D.F., Tejan Y.S., Boyle S., Koroma C., et al. Prevalence and mortality of cryptococcal disease in adults with advanced HIV in an urban tertiary hospital in Sierra Leone: A prospective study. BMC Infect. Dis. 2020;20:141. doi: 10.1186/s12879-020-4862-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s12879-020-4862-x</ArticleId><ArticleId IdType="pmc">PMC7023785</ArticleId><ArticleId IdType="pubmed">32059703</ArticleId></ArticleIdList></Reference><Reference><Citation>Mustapha M., Ozsahin D., Ozsahin I., Uzun B. Breast Cancer Screening Based on Supervised Learning and Multi-Criteria Decision-Making. Diagnostics. 2022;12:1326. doi: 10.3390/diagnostics12061326.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/diagnostics12061326</ArticleId><ArticleId IdType="pmc">PMC9221649</ArticleId><ArticleId IdType="pubmed">35741136</ArticleId></ArticleIdList></Reference><Reference><Citation>Ozsahin I., Sekeroglu B., Musa M., Mustapha M., Uzun Ozsahin D. Review on Diagnosis of COVID-19 from Chest CT Images Using Artificial Intelligence. Comput. Math. Methods Med. 2020;2020:9756518. doi: 10.1155/2020/9756518.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2020/9756518</ArticleId><ArticleId IdType="pmc">PMC7519983</ArticleId><ArticleId IdType="pubmed">33014121</ArticleId></ArticleIdList></Reference><Reference><Citation>Uzun Ozsahin D., Taiwo Mustapha M., Saleh Mubarak A., Said Ameen Z., Uzun B. Impact of Outliers and Dimensionality Reduction on the Performance of Predictive Models for Medical Disease Diagnosis; Proceedings of the 2022 International Conference on Artificial Intelligence in Everything (AIE); Kitakyushu, Japan. 19&#x2013;22 July 2022;</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/aie57029.2022.00023</ArticleId></ArticleIdList></Reference><Reference><Citation>Ozsahin D., Taiwo Mustapha M., Mubarak A., Said Ameen Z., Uzun B. Impact of feature scaling on machine learning models for the diagnosis of diabetes; Proceedings of the 2022 International Conference on Artificial Intelligence in Everything (AIE); Kitakyushu, Japan. 19&#x2013;22 July 2022;</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/aie57029.2022.00024</ArticleId></ArticleIdList></Reference><Reference><Citation>Uzun Ozsahin D., Mustapha M.T., Bartholomew Duwa B., Ozsahin I. Evaluating the performance of deep learning frameworks for malaria parasite detection using microscopic images of peripheral blood smears. Diagnostics. 2022;12:2702. doi: 10.3390/diagnostics12112702.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/diagnostics12112702</ArticleId><ArticleId IdType="pmc">PMC9689376</ArticleId><ArticleId IdType="pubmed">36359544</ArticleId></ArticleIdList></Reference><Reference><Citation>Jiang Y., Luo J., Huang D., Liu Y., Li D. Machine Learning Advances in Microbiology: A Review of Methods and Applications. Front. Microbiol. 2022;13:925454. doi: 10.3389/fmicb.2022.925454.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fmicb.2022.925454</ArticleId><ArticleId IdType="pmc">PMC9196628</ArticleId><ArticleId IdType="pubmed">35711777</ArticleId></ArticleIdList></Reference><Reference><Citation>Qu K., Guo F., Liu X., Lin Y., Zou Q. Application of Machine Learning in Microbiology. Front. Microbiol. 2019;10:827. doi: 10.3389/fmicb.2019.00827.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fmicb.2019.00827</ArticleId><ArticleId IdType="pmc">PMC6482238</ArticleId><ArticleId IdType="pubmed">31057526</ArticleId></ArticleIdList></Reference><Reference><Citation>Ayala A.P., Recio R. Cryptococcus neoformans Meningoencephalitis. Images in Clinical Medicine. N. Engl. J. Med. 2018;379:281. doi: 10.1056/nejmICM1801051.</Citation><ArticleIdList><ArticleId IdType="doi">10.1056/nejmICM1801051</ArticleId><ArticleId IdType="pubmed">30021095</ArticleId></ArticleIdList></Reference><Reference><Citation>Doi A., Kentaro I., Takegawa H., Miki K., Sono Y., Nishioka H., Takeshita J., Tomii K., Haruta T. Community-acquired pneumonia caused by carbapenem-resistant Streptococcus pneumoniae: Re-examining its prevention and treatment. Int. J. Gen. Med. 2014;7:253&#x2013;257. doi: 10.2147/IJGM.S63744.</Citation><ArticleIdList><ArticleId IdType="doi">10.2147/IJGM.S63744</ArticleId><ArticleId IdType="pmc">PMC4038523</ArticleId><ArticleId IdType="pubmed">24899822</ArticleId></ArticleIdList></Reference><Reference><Citation>Alzubaidi L., Zhang J., Humaidi A., Al-Dujaili A., Duan Y., Al-Shamma O., Santamar&#xed;a J., Fadhel M.A., Al-Amidie M., Farhan L. Review of deep learning: Concepts, CNN architectures, challenges, applications, future directions. J. Big Data. 2021;8:53. doi: 10.1186/s40537-021-00444-8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s40537-021-00444-8</ArticleId><ArticleId IdType="pmc">PMC8010506</ArticleId><ArticleId IdType="pubmed">33816053</ArticleId></ArticleIdList></Reference><Reference><Citation>Yamashita R., Nishio M., Do R., Togashi K. Convolutional neural networks: An overview and application in radiology. Insights Imaging. 2018;9:611&#x2013;629. doi: 10.1007/s13244-018-0639-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s13244-018-0639-9</ArticleId><ArticleId IdType="pmc">PMC6108980</ArticleId><ArticleId IdType="pubmed">29934920</ArticleId></ArticleIdList></Reference><Reference><Citation>Alnussairi M., &#x130;brahim A. Malaria parasite detection using deep learning algorithms based on (CNNs) technique. Comput. Electr. Eng. 2022;103 doi: 10.1016/j.compeleceng.2022.108316.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compeleceng.2022.108316</ArticleId></ArticleIdList></Reference><Reference><Citation>Jeong Y., Woo J., Kang A. Malware Detection on Byte Streams of PDF Files Using Convolutional Neural Networks. Secur. Commun. Netw. 2019;2019:8485365. doi: 10.1155/2019/8485365.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2019/8485365</ArticleId></ArticleIdList></Reference><Reference><Citation>Saha S. A Comprehensive Guide to Convolutional Neural Networks&#x2014;The ELI5 Way. Medium.  [(accessed on 10 October 2022)].  Available online:  https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53.</Citation></Reference><Reference><Citation>Brownlee J.  [(accessed on 10 October 2022)].  Available online:  https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/#:~:text=Two%20common%20pooling%20methods%20are,presence%20of%20a%20feature%20respectively.</Citation></Reference><Reference><Citation>Alhussainy A. A New Pooling Layer based on Wavelet Transform for Convolutional Neural Network. JARDCS. 2020;24:76&#x2013;85. doi: 10.5373/JARDCS/V12I4/20201420.</Citation><ArticleIdList><ArticleId IdType="doi">10.5373/JARDCS/V12I4/20201420</ArticleId></ArticleIdList></Reference><Reference><Citation>Affine Layer  DeepAI.  [(accessed on 10 October 2022)].  Available online:  https://deepai.org/machine-learning-glossary-and-terms/affine-layer.</Citation></Reference><Reference><Citation>Jiang G. How the Convolutional Neural Network Work to Identify Numbers? 2021. [(accessed on 3 October 2022)]. Available online:</Citation><ArticleIdList><ArticleId IdType="doi">10.14293/s2199-1006.1.sor-.ppaiubj.v1</ArticleId></ArticleIdList></Reference><Reference><Citation>Fully Connected Layers in Convolutional Neural Networks. IndianTechWarrior.  [(accessed on 9 October 2022)].  Available online:  https://indiantechwarrior.com/fully-connected-layers-in-convolutional-neural-networks/</Citation></Reference><Reference><Citation>Guan Q., Wang Y., Ping B., Li D., Du J., Qin Y., Lu H., Wan X., Xiang J. Deep convolutional neural network VGG-16 model for differential diagnosing of papillary thyroid carcinomas in cytological images: A pilot study. J. Cancer. 2019;10:4876&#x2013;4882. doi: 10.7150/jca.28769.</Citation><ArticleIdList><ArticleId IdType="doi">10.7150/jca.28769</ArticleId><ArticleId IdType="pmc">PMC6775529</ArticleId><ArticleId IdType="pubmed">31598159</ArticleId></ArticleIdList></Reference><Reference><Citation>Rohini G. Everything You Need to Know about VGG16. Medium.  [(accessed on 10 October 2022)].  Available online:  https://medium.com/@mygreatlearning/everything-you-need-to-know-about-vgg16-7315defb5918#:~:text=VGG16%20is%20object%20detection%20and,to%20use%20with%20transfer%20learning.</Citation></Reference><Reference><Citation>Bansal M., Kumar M., Sachdeva M., Mittal A. Transfer learning for image classification using VGG19: Caltech-101 image data set. J. Ambient. Intell. Humaniz. Comput. 2021 doi: 10.1007/s12652-021-03488-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s12652-021-03488-z</ArticleId><ArticleId IdType="pmc">PMC8446720</ArticleId><ArticleId IdType="pubmed">34548886</ArticleId></ArticleIdList></Reference><Reference><Citation>Hasan M., Fatemi M., Monirujjaman K.M., Kaur M., Zaguia A. Comparative Analysis of Skin Cancer (Benign vs. Malignant) Detection Using Convolutional Neural Networks. J. Healthc. Eng. 2021;2021:5895156. doi: 10.1155/2021/5895156.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2021/5895156</ArticleId><ArticleId IdType="pmc">PMC8684510</ArticleId><ArticleId IdType="pubmed">34931137</ArticleId></ArticleIdList></Reference><Reference><Citation>Narkhede S. Understanding Confusion Matrix. Medium.  [(accessed on 9 October 2022)].  Available online:  https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62.</Citation></Reference><Reference><Citation>Towards Data Science. Feng V. An Overview of ResNet and Its Variants. Medium.  [(accessed on 10 October 2022)].  Available online:  https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035.</Citation></Reference><Reference><Citation>Paperspace Blog. Kurama V. A Guide to ResNet, Inception v3, and SqueezeNet.  [(accessed on 10 October 2022)].  Available online:  https://blog.paperspace.com/popular-deep-learning-architectures-resnet-inceptionv3-squeezenet/</Citation></Reference><Reference><Citation>Huston S.M., Mody C.H. Cryptococcosis: An Emerging Respiratory Mycosis. Clin. Chest. Med. 2009;30:253&#x2013;264. doi: 10.1016/j.ccm.2009.02.006.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ccm.2009.02.006</ArticleId><ArticleId IdType="pubmed">19375632</ArticleId></ArticleIdList></Reference><Reference><Citation>Rathore S.S., Sathiyamoorthy J., Lalitha C., Ramakrishnan J. A holistic review on Cryptococcus neoformans. Microb. Pathog. 2022;166:105521. doi: 10.1016/j.micpath.2022.105521.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.micpath.2022.105521</ArticleId><ArticleId IdType="pubmed">35436563</ArticleId></ArticleIdList></Reference><Reference><Citation>Bermas A. Geddes-McAlister. Combatting the evaluation of antifungal resistance in Cryptococcus neoformans. Mol. Microbiol. 2020;114:721&#x2013;734. doi: 10.1111/mmi.14565.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/mmi.14565</ArticleId><ArticleId IdType="pubmed">32697029</ArticleId></ArticleIdList></Reference><Reference><Citation>Iyer K.R., Revie N.M., Fu C., Robbins N., Cowen L.E. Treatment strategies for cryptococcal infection: Challenges, advances, and future outlook. Nat. Rev. Microbiol. 2021;19:454&#x2013;466. doi: 10.1038/s41579-021-00511-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41579-021-00511-0</ArticleId><ArticleId IdType="pmc">PMC7868659</ArticleId><ArticleId IdType="pubmed">33558691</ArticleId></ArticleIdList></Reference><Reference><Citation>ITPC   [(accessed on 13 October 2022)].  Available online:  https://itpcglobal.org/blog/resource/ending-cryptococcal-meningitis-deaths-by-2030/#:~:text=Treatment%20with%20fluconazole%20alone%2C%20most,end%20all%20HIV%2Drelated%20deaths.</Citation></Reference><Reference><Citation>Zieli&#x144;ski B., Sroka-Oleksiak A., Rymarczyk D., Piekarczyk A., Brzychczy-W&#x142;och M. Deep learning approach to describe and classify fungi microscopic images. PLoS ONE. 2020;15:e0234806. doi: 10.1371/journal.pone.0234806.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0234806</ArticleId><ArticleId IdType="pmc">PMC7326179</ArticleId><ArticleId IdType="pubmed">32603329</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36611360</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Print">2075-4418</ISSN><JournalIssue CitedMedium="Print"><Volume>13</Volume><Issue>1</Issue><PubDate><Year>2022</Year><Month>Dec</Month><Day>26</Day></PubDate></JournalIssue><Title>Diagnostics (Basel, Switzerland)</Title><ISOAbbreviation>Diagnostics (Basel)</ISOAbbreviation></Journal><ArticleTitle>An Overview of Deep-Learning-Based Methods for Cardiovascular Risk Assessment with Retinal Images.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">68</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/diagnostics13010068</ELocationID><Abstract><AbstractText>Cardiovascular diseases (CVDs) are one of the most prevalent causes of premature death. Early detection is crucial to prevent and address CVDs in a timely manner. Recent advances in oculomics show that retina fundus imaging (RFI) can carry relevant information for the early diagnosis of several systemic diseases. There is a large corpus of RFI systematically acquired for diagnosing eye-related diseases that could be used for CVDs prevention. Nevertheless, public health systems cannot afford to dedicate expert physicians to only deal with this data, posing the need for automated diagnosis tools that can raise alarms for patients at risk. Artificial Intelligence (AI) and, particularly, deep learning models, became a strong alternative to provide computerized pre-diagnosis for patient risk retrieval. This paper provides a novel review of the major achievements of the recent state-of-the-art DL approaches to automated CVDs diagnosis. This overview gathers commonly used datasets, pre-processing techniques, evaluation metrics and deep learning approaches used in 30 different studies. Based on the reviewed articles, this work proposes a classification taxonomy depending on the prediction target and summarizes future research challenges that have to be tackled to progress in this line.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Barriada</LastName><ForeName>Rub&#xe9;n G</ForeName><Initials>RG</Initials><Identifier Source="ORCID">0000-0003-0521-4895</Identifier><AffiliationInfo><Affiliation>AIWell Research Group, Faculty of Computer Science, Multimedia and Telecommunications, Universitat Oberta de Catalunya, 08018 Barcelona, Spain.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Masip</LastName><ForeName>David</ForeName><Initials>D</Initials><Identifier Source="ORCID">0000-0001-7898-1847</Identifier><AffiliationInfo><Affiliation>AIWell Research Group, Faculty of Computer Science, Multimedia and Telecommunications, Universitat Oberta de Catalunya, 08018 Barcelona, Spain.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType><PublicationType UI="D016454">Review</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>26</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Diagnostics (Basel)</MedlineTA><NlmUniqueID>101658402</NlmUniqueID><ISSNLinking>2075-4418</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">artificial intelligence</Keyword><Keyword MajorTopicYN="N">cardiovascular diseases</Keyword><Keyword MajorTopicYN="N">convolutional neural networks</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword><Keyword MajorTopicYN="N">healthcare</Keyword><Keyword MajorTopicYN="N">medical imaging</Keyword><Keyword MajorTopicYN="N">oculomics</Keyword><Keyword MajorTopicYN="N">retinal fundus image</Keyword><Keyword MajorTopicYN="N">retinal photography analysis</Keyword></KeywordList><CoiStatement>The authors declare no conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>11</Month><Day>30</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>19</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>21</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>1</Hour><Minute>2</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36611360</ArticleId><ArticleId IdType="pmc">PMC9818382</ArticleId><ArticleId IdType="doi">10.3390/diagnostics13010068</ArticleId><ArticleId IdType="pii">diagnostics13010068</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Tang X. The role of artificial intelligence in medical imaging research. BJR Open. 2019;2:20190031. doi: 10.1259/bjro.20190031.</Citation><ArticleIdList><ArticleId IdType="doi">10.1259/bjro.20190031</ArticleId><ArticleId IdType="pmc">PMC7594889</ArticleId><ArticleId IdType="pubmed">33178962</ArticleId></ArticleIdList></Reference><Reference><Citation>Kooi T., Litjens G., Van Ginneken B., Gubern-M&#xe9;rida A., S&#xe1;nchez C.I., Mann R., den Heeten A., Karssemeijer N. Large scale deep learning for computer aided detection of mammographic lesions. Med. Image Anal. 2017;35:303&#x2013;312. doi: 10.1016/j.media.2016.07.007.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2016.07.007</ArticleId><ArticleId IdType="pubmed">27497072</ArticleId></ArticleIdList></Reference><Reference><Citation>Ghafoorian M., Karssemeijer N., Heskes T., van Uder I.W.M., de Leeuw F.E., Marchiori E., van Ginneken B., Platel B. Non-uniform patch sampling with deep convolutional neural networks for white matter hyperintensity segmentation; Proceedings of the 2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI); Prague, Czech Republic. 13&#x2013;16 April 2016; pp. 1414&#x2013;1417.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ISBI.2016.7493532</ArticleId></ArticleIdList></Reference><Reference><Citation>Esteva A., Kuprel B., Novoa R.A., Ko J., Swetter S.M., Blau H.M., Thrun S. Dermatologist-level classification of skin cancer with deep neural networks. Nature. 2017;542:115&#x2013;118. doi: 10.1038/nature21056.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nature21056</ArticleId><ArticleId IdType="pmc">PMC8382232</ArticleId><ArticleId IdType="pubmed">28117445</ArticleId></ArticleIdList></Reference><Reference><Citation>Ozturk T., Talo M., Yildirim E.A., Baloglu U.B., Yildirim O., Acharya U.R. Automated detection of COVID-19 cases using deep neural networks with X-ray images. Comput. Biol. Med. 2020;121:103792. doi: 10.1016/j.compbiomed.2020.103792.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2020.103792</ArticleId><ArticleId IdType="pmc">PMC7187882</ArticleId><ArticleId IdType="pubmed">32568675</ArticleId></ArticleIdList></Reference><Reference><Citation>Litjens G., Kooi T., Bejnordi B.E., Setio A.A.A., Ciompi F., Ghafoorian M., van der Laak J.A., van Ginneken B., S&#xe1;nchez C.I. A survey on deep learning in medical image analysis. Med. Image Anal. 2017;42:60&#x2013;88. doi: 10.1016/j.media.2017.07.005.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2017.07.005</ArticleId><ArticleId IdType="pubmed">28778026</ArticleId></ArticleIdList></Reference><Reference><Citation>Goutam B., Hashmi M.F., Geem Z.W., Bokde N.D. A Comprehensive Review of Deep Learning Strategies in Retinal Disease Diagnosis Using Fundus Images. IEEE Access. 2022 doi: 10.1109/ACCESS.2022.3178372.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2022.3178372</ArticleId></ArticleIdList></Reference><Reference><Citation>Wagner S.K., Fu D.J., Faes L., Liu X., Huemer J., Khalid H., Ferraz D., Korot E., Kelly C., Balaskas K., et al. Insights into systemic disease through retinal imaging-based oculomics. Transl. Vis. Sci. Technol. 2020;9:6. doi: 10.1167/tvst.9.2.6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1167/tvst.9.2.6</ArticleId><ArticleId IdType="pmc">PMC7343674</ArticleId><ArticleId IdType="pubmed">32704412</ArticleId></ArticleIdList></Reference><Reference><Citation>Kim B.R., Yoo T.K., Kim H.K., Ryu I.H., Kim J.K., Lee I.S., Kim J.S., Shin D.H., Kim Y.S., Kim B.T. Oculomics for sarcopenia prediction: A machine learning approach toward predictive, preventive, and personalized medicine. EPMA J. 2022;13:367&#x2013;382. doi: 10.1007/s13167-022-00292-3.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s13167-022-00292-3</ArticleId><ArticleId IdType="pmc">PMC9437169</ArticleId><ArticleId IdType="pubmed">36061832</ArticleId></ArticleIdList></Reference><Reference><Citation>Harris G., Rickard J.J.S., Butt G., Kelleher L., Blanch R., Cooper J.M., Oppenheimer P.G. Review: Emerging Oculomics based diagnostic technologies for traumatic brain injury. IEEE Rev. Biomed. Eng. 2022 doi: 10.1109/RBME.2022.3161352.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/RBME.2022.3161352</ArticleId><ArticleId IdType="pubmed">35320105</ArticleId></ArticleIdList></Reference><Reference><Citation>Maldonado Garc&#xed;a C., Bonazzola R., Ravikumar N., Frangi A.F. Medical Image Understanding and Analysis. Volume 13413. Springer; Cham, Switzerland: 2022. Predicting Myocardial Infarction Using Retinal OCT Imaging; pp. 787&#x2013;797. Lecture Notes in Computer Science.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-031-12053-4_58</ArticleId></ArticleIdList></Reference><Reference><Citation>Sabanayagam C., Xu D., Ting D.S., Nusinovici S., Banu R., Hamzah H., Lim C., Tham Y.C., Cheung C.Y., Tai E.S., et al. A deep learning algorithm to detect chronic kidney disease from retinal photographs in community-based populations. Lancet Digit. Health. 2020;2:e295&#x2013;e302. doi: 10.1016/S2589-7500(20)30063-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S2589-7500(20)30063-7</ArticleId><ArticleId IdType="pubmed">33328123</ArticleId></ArticleIdList></Reference><Reference><Citation>Cheung C.Y., Ran A.R., Wang S., Chan V.T.T., Sham K., Hilal S., Venketasubramanian N., Cheng C.Y., Sabanayagam C., Tham Y.C., et al. A deep learning model for detection of Alzheimer&#x2019;s disease based on retinal photographs: A retrospective, multicentre case-control study. Lancet Digit. Health. 2022 doi: 10.1016/S2589-7500(22)00169-8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S2589-7500(22)00169-8</ArticleId><ArticleId IdType="pubmed">36192349</ArticleId></ArticleIdList></Reference><Reference><Citation>Mitani A., Huang A., Venugopalan S., Corrado G.S., Peng L., Webster D.R., Hammel N., Liu Y., Varadarajan A.V. Detection of anaemia from retinal fundus images via deep learning. Nat. Biomed. Eng. 2020;4:18&#x2013;27. doi: 10.1038/s41551-019-0487-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41551-019-0487-z</ArticleId><ArticleId IdType="pubmed">31873211</ArticleId></ArticleIdList></Reference><Reference><Citation>WHO  Cardiovascular Diseases (CVDs) 2021.  [(accessed on 20 November 2022)].  Available online:  https://www.who.int/en/news-room/fact-sheets/detail/cardiovascular-diseases-(cvds)</Citation></Reference><Reference><Citation>Wang H., Naghavi M., Allen C., Barber R.M., Bhutta Z.A., Carter A., Casey D.C., Charlson F.J., Chen A.Z., Coates M.M., et al. Global, regional, and national life expectancy, all-cause mortality, and cause-specific mortality for 249 causes of death, 1980&#x2013;2015: A systematic analysis for the Global Burden of Disease Study 2015. Lancet. 2016;388:1459&#x2013;1544. doi: 10.1016/S0140-6736(16)31012-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0140-6736(16)31012-1</ArticleId><ArticleId IdType="pmc">PMC5388903</ArticleId><ArticleId IdType="pubmed">27733281</ArticleId></ArticleIdList></Reference><Reference><Citation>Goff D.C., Jr., Lloyd-Jones D.M., Bennett G., Coady S., D&#x2019;Agostino R.B., Gibbons R., Greenland P., Lackland D.T., Levy D., O&#x2019;Donnell C.J., et al. 2013 ACC/AHA guideline on the assessment of cardiovascular risk: A report of the American College of Cardiology/American Heart Association Task Force on Practice Guidelines. Circulation. 2013;129:S49&#x2013;S73. doi: 10.1161/01.cir.0000437741.48606.98.</Citation><ArticleIdList><ArticleId IdType="doi">10.1161/01.cir.0000437741.48606.98</ArticleId><ArticleId IdType="pubmed">24222018</ArticleId></ArticleIdList></Reference><Reference><Citation>Schmarje L., Santarossa M., Schr&#xf6;der S.M., Koch R. A survey on semi-, self-and unsupervised learning for image classification. IEEE Access. 2021;9:82146&#x2013;82168. doi: 10.1109/ACCESS.2021.3084358.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2021.3084358</ArticleId></ArticleIdList></Reference><Reference><Citation>Van Engelen J.E., Hoos H.H. A survey on semi-supervised learning. Mach. Learn. 2020;109:373&#x2013;440. doi: 10.1007/s10994-019-05855-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10994-019-05855-6</ArticleId></ArticleIdList></Reference><Reference><Citation>Simonyan K., Zisserman A. Very deep convolutional networks for large-scale image recognition. arXiv. 20141409.1556</Citation></Reference><Reference><Citation>Abdullah M., Fraz M.M., Barman S.A. Localization and segmentation of optic disc in retinal images using circular Hough transform and grow-cut algorithm. PeerJ. 2016;4:e2003. doi: 10.7717/peerj.2003.</Citation><ArticleIdList><ArticleId IdType="doi">10.7717/peerj.2003</ArticleId><ArticleId IdType="pmc">PMC4867714</ArticleId><ArticleId IdType="pubmed">27190713</ArticleId></ArticleIdList></Reference><Reference><Citation>Staal J., Abramoff M., Niemeijer M., Viergever M., van Ginneken B. Ridge based vessel segmentation in color images of the retina. IEEE Trans. Med. Imaging. 2004;23:501&#x2013;509. doi: 10.1109/TMI.2004.825627.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2004.825627</ArticleId><ArticleId IdType="pubmed">15084075</ArticleId></ArticleIdList></Reference><Reference><Citation>Decenci&#xe8;re E., Zhang X., Cazuguel G., Lay B., Cochener B., Trone C., Gain P., Ordonez R., Massin P., Erginay A., et al. Feedback on a publicly distributed database: The Messidor database. Image Anal. Stereol. 2014;33:231&#x2013;234. doi: 10.5566/ias.1155.</Citation><ArticleIdList><ArticleId IdType="doi">10.5566/ias.1155</ArticleId></ArticleIdList></Reference><Reference><Citation>Abr&#xe0;moff M.D., Folk J.C., Han D.P., Walker J.D., Williams D.F., Russell S.R., Massin P., Cochener B., Gain P., Tang L., et al. Automated Analysis of Retinal Images for Detection of Referable Diabetic Retinopathy. JAMA Ophthalmol. 2013;131:351&#x2013;357. doi: 10.1001/jamaophthalmol.2013.1743.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jamaophthalmol.2013.1743</ArticleId><ArticleId IdType="pubmed">23494039</ArticleId></ArticleIdList></Reference><Reference><Citation>Hoover A., Kouznetsova V., Goldbaum M. Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response. IEEE Trans. Med. Imaging. 2000;19:203&#x2013;210. doi: 10.1109/42.845178.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/42.845178</ArticleId><ArticleId IdType="pubmed">10875704</ArticleId></ArticleIdList></Reference><Reference><Citation>Budai A., Bock R., Maier A., Hornegger J., Michelson G. Robust vessel segmentation in fundus images. Int. J. Biomed. Imaging. 2013;2013:154860. doi: 10.1155/2013/154860.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2013/154860</ArticleId><ArticleId IdType="pmc">PMC3876700</ArticleId><ArticleId IdType="pubmed">24416040</ArticleId></ArticleIdList></Reference><Reference><Citation>Diabetic Retinopathy Detection | Kaggle.  [(accessed on 29 November 2022)].  Available online:  https://www.kaggle.com/c/diabetic-retinopathy-detection.</Citation></Reference><Reference><Citation>Decenci&#xe8;re E., Cazuguel G., Zhang X., Thibault G., Klein J.C., Meyer F., Marcotegui B., Quellec G., Lamard M., Danno R., et al. TeleOphta: Machine learning and image processing methods for teleophthalmology. IRBM. 2013;34:196&#x2013;203. doi: 10.1016/j.irbm.2013.01.010.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.irbm.2013.01.010</ArticleId></ArticleIdList></Reference><Reference><Citation>Zheng Y., Cheng C.Y., Lamoureux E.L., Chiang P.P.C., Rahman Anuar A., Wang J.J., Mitchell P., Saw S.M., Wong T.Y. How much eye care services do Asian populations need? Projection from the Singapore Epidemiology of Eye Disease (SEED) study. Investig. Ophthalmol. Vis. Sci. 2013;54:2171&#x2013;2177. doi: 10.1167/iovs.12-11393.</Citation><ArticleIdList><ArticleId IdType="doi">10.1167/iovs.12-11393</ArticleId><ArticleId IdType="pubmed">23439593</ArticleId></ArticleIdList></Reference><Reference><Citation>Fu H., Cheng J., Xu Y., Zhang C., Wong D.W.K., Liu J., Cao X. Disc-aware ensemble network for glaucoma screening from fundus image. IEEE Trans. Med. Imaging. 2018;37:2493&#x2013;2501. doi: 10.1109/TMI.2018.2837012.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2018.2837012</ArticleId><ArticleId IdType="pubmed">29994764</ArticleId></ArticleIdList></Reference><Reference><Citation>Majithia S., Tham Y.C., Chee M.L., Teo C.L., Chee M.L., Dai W., Kumari N., Lamoureux E.L., Sabanayagam C., Wong T.Y., et al. Singapore Chinese Eye Study: Key findings from baseline examination and the rationale, methodology of the 6-year follow-up series. Br. J. Ophthalmol. 2020;104:610&#x2013;615. doi: 10.1136/bjophthalmol-2019-314760.</Citation><ArticleIdList><ArticleId IdType="doi">10.1136/bjophthalmol-2019-314760</ArticleId><ArticleId IdType="pubmed">31401553</ArticleId></ArticleIdList></Reference><Reference><Citation>Foong A.W., Saw S.M., Loo J.L., Shen S., Loon S.C., Rosman M., Aung T., Tan D.T., Tai E.S., Wong T.Y. Rationale and methodology for a population-based study of eye diseases in Malay people: The Singapore Malay eye study (SiMES) Ophthalmic Epidemiol. 2007;14:25&#x2013;35. doi: 10.1080/09286580600878844.</Citation><ArticleIdList><ArticleId IdType="doi">10.1080/09286580600878844</ArticleId><ArticleId IdType="pubmed">17365815</ArticleId></ArticleIdList></Reference><Reference><Citation>Jonas J.B., Xu L., Wang Y.X. The Beijing Eye Study. Acta Ophthalmol. 2009;87:247&#x2013;261. doi: 10.1111/j.1755-3768.2008.01385.x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/j.1755-3768.2008.01385.x</ArticleId><ArticleId IdType="pubmed">19426355</ArticleId></ArticleIdList></Reference><Reference><Citation>Zuiderveld K. Academic Press Graphics Gems Series: Graphics Gems IV. Academic Press; London, UK: 1994. Contrast limited adaptive histogram equalization; pp. 474&#x2013;485.</Citation></Reference><Reference><Citation>Shorten C., Khoshgoftaar T.M. A survey on image data augmentation for deep learning. J. Big Data. 2019;6:1&#x2013;48. doi: 10.1186/s40537-019-0197-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s40537-019-0197-0</ArticleId><ArticleId IdType="pmc">PMC8287113</ArticleId><ArticleId IdType="pubmed">34306963</ArticleId></ArticleIdList></Reference><Reference><Citation>Cheung C.Y.L., Ikram M.K., Sabanayagam C., Wong T.Y. Retinal microvasculature as a model to study the manifestations of hypertension. Hypertension. 2012;60:1094&#x2013;1103. doi: 10.1161/HYPERTENSIONAHA.111.189142.</Citation><ArticleIdList><ArticleId IdType="doi">10.1161/HYPERTENSIONAHA.111.189142</ArticleId><ArticleId IdType="pubmed">23045470</ArticleId></ArticleIdList></Reference><Reference><Citation>Shi D., Lin Z., Wang W., Tan Z., Shang X., Zhang X., Meng W., Ge Z., He M. A Deep Learning System for Fully Automated Retinal Vessel Measurement in High Throughput Image Analysis. Front. Cardiovasc. Med. 2022;9:823436. doi: 10.3389/fcvm.2022.823436.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fcvm.2022.823436</ArticleId><ArticleId IdType="pmc">PMC8980780</ArticleId><ArticleId IdType="pubmed">35391847</ArticleId></ArticleIdList></Reference><Reference><Citation>Ronneberger O., Fischer P., Brox T. U-net: Convolutional networks for biomedical image segmentation; Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention; Munich, Germany. 5&#x2013;9 October 2015; Berlin/Heidelberg, Germany: Springer; 2015. pp. 234&#x2013;241.</Citation></Reference><Reference><Citation>Yu F., Zhao J., Gong Y., Wang Z., Li Y., Yang F., Dong B., Li Q., Zhang L. Annotation-Free Cardiac Vessel Segmentation via Knowledge Transfer from Retinal Images. arXiv. 20191907.11483</Citation></Reference><Reference><Citation>Goodfellow I., Pouget-Abadie J., Mirza M., Xu B., Warde-Farley D., Ozair S., Courville A., Bengio Y. Generative adversarial networks. Commun. ACM. 2020;63:139&#x2013;144. doi: 10.1145/3422622.</Citation><ArticleIdList><ArticleId IdType="doi">10.1145/3422622</ArticleId></ArticleIdList></Reference><Reference><Citation>Yin X. Prediction Algorithm of Young Students&#x2019; Physical Health Risk Factors Based on Deep Learning. J. Healthc. Eng. 2021;2021:9049266. doi: 10.1155/2021/9049266.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2021/9049266</ArticleId><ArticleId IdType="pmc">PMC8390172</ArticleId><ArticleId IdType="pubmed">34457224</ArticleId></ArticleIdList></Reference><Reference><Citation>Zekavat S.M., Raghu V.K., Trinder M., Ye Y., Koyama S., Honigberg M.C., Yu Z., Pampana A., Urbut S., Haidermota S., et al. Deep Learning of the Retina Enables Phenome- and Genome-Wide Analyses of the Microvasculature. Circulation. 2022;145:134&#x2013;150. doi: 10.1161/CIRCULATIONAHA.121.057709.</Citation><ArticleIdList><ArticleId IdType="doi">10.1161/CIRCULATIONAHA.121.057709</ArticleId><ArticleId IdType="pmc">PMC8746912</ArticleId><ArticleId IdType="pubmed">34743558</ArticleId></ArticleIdList></Reference><Reference><Citation>Hoque M.E., Kipli K. Deep Learning in Retinal Image Segmentation and Feature Extraction: A Review. Int. J. Online Biomed. Eng. 2021;17:103&#x2013;118. doi: 10.3991/ijoe.v17i14.24819.</Citation><ArticleIdList><ArticleId IdType="doi">10.3991/ijoe.v17i14.24819</ArticleId></ArticleIdList></Reference><Reference><Citation>Budoff M.J., Raggi P., Beller G.A., Berman D.S., Druz R.S., Malik S., Rigolin V.H., Weigold W.G., Soman P., the Imaging Council of the American College of Cardiology Noninvasive cardiovascular risk assessment of the asymptomatic diabetic patient: The Imaging Council of the American College of Cardiology. JACC Cardiovasc. Imaging. 2016;9:176&#x2013;192. doi: 10.1016/j.jcmg.2015.11.011.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jcmg.2015.11.011</ArticleId><ArticleId IdType="pmc">PMC5371352</ArticleId><ArticleId IdType="pubmed">26846937</ArticleId></ArticleIdList></Reference><Reference><Citation>Sim&#xf3; R., Ba&#xf1;eras J., Hern&#xe1;ndez C., Rodr&#xed;guez-Palomares J., Valente F., Gutierrez L., Gonz&#xe1;lez-Alujas T., Ferreira I., Aguad&#xe9;-Bruix S., Montaner J., et al. Diabetic retinopathy as an independent predictor of subclinical cardiovascular disease: Baseline results of the PRECISED study. BMJ Open Diabetes Res. Care. 2019;7:e000845. doi: 10.1136/bmjdrc-2019-000845.</Citation><ArticleIdList><ArticleId IdType="doi">10.1136/bmjdrc-2019-000845</ArticleId><ArticleId IdType="pmc">PMC6936469</ArticleId><ArticleId IdType="pubmed">31908800</ArticleId></ArticleIdList></Reference><Reference><Citation>Gulshan V., Peng L., Coram M., Stumpe M.C., Wu D., Narayanaswamy A., Venugopalan S., Widner K., Madams T., Cuadros J., et al. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. JAMA J. Am. Med. Assoc. 2016;316:2402&#x2013;2410. doi: 10.1001/jama.2016.17216.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jama.2016.17216</ArticleId><ArticleId IdType="pubmed">27898976</ArticleId></ArticleIdList></Reference><Reference><Citation>Szegedy C., Vanhoucke V., Ioffe S., Shlens J., Wojna Z. Rethinking the inception architecture for computer vision; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Las Vegas, NV, USA. 26 June&#x2013;1 July 2016; pp. 2818&#x2013;2826.</Citation></Reference><Reference><Citation>Gargeya R., Leng T. Automated Identification of Diabetic Retinopathy Using Deep Learning. Ophthalmology. 2017;124:962&#x2013;969. doi: 10.1016/j.ophtha.2017.02.008.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ophtha.2017.02.008</ArticleId><ArticleId IdType="pubmed">28359545</ArticleId></ArticleIdList></Reference><Reference><Citation>Breiman L., Friedman J.H., Olshen R.A., Stone C.J. Classification and Regression Trees. Routledge; London, UK: 2017.</Citation></Reference><Reference><Citation>Shetkar A., Mai C.K., Yamini C. Machine Learning Technologies and Applications. Springer; Berlin/Heidelberg, Germany: 2021. Diabetic symptoms prediction through retinopathy; pp. 13&#x2013;20.</Citation></Reference><Reference><Citation>Ting D.S.W., Cheung C.Y.L., Lim G., Tan G.S.W., Quang N.D., Gan A., Hamzah H., Garcia-Franco R., Yeo I.Y.S., Lee S.Y., et al. Development and validation of a deep learning system for diabetic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes. JAMA J. Am. Med. Assoc. 2017;318:2211&#x2013;2223. doi: 10.1001/jama.2017.18152.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jama.2017.18152</ArticleId><ArticleId IdType="pmc">PMC5820739</ArticleId><ArticleId IdType="pubmed">29234807</ArticleId></ArticleIdList></Reference><Reference><Citation>Wong D.C.S., Kiew G., Jeon S., Ting D. Singapore Eye Lesions Analyzer (SELENA): The Deep Learning System for Retinal Diseases. In: Grzybowski A., editor. Artificial Intelligence in Ophthalmology. Springer International Publishing; Cham, Switzerland: 2021. pp. 177&#x2013;185.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-030-78601-4_13</ArticleId></ArticleIdList></Reference><Reference><Citation>Trivedi A., Desbiens J., Gross R., Gupta S., Dodhia R., Ferres J.L. Retinal Microvasculature as Biomarker for Diabetes and Cardiovascular Diseases. arXiv. 20212107.13157</Citation></Reference><Reference><Citation>He K., Zhang X., Ren S., Sun J. Deep residual learning for image recognition; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Las Vegas, NV, USA. 26 June&#x2013;1 July 2016; pp. 770&#x2013;778.</Citation></Reference><Reference><Citation>Redd T.K., Campbell J.P., Brown J.M., Kim S.J., Ostmo S., Chan R.V.P., Dy J., Erdogmus D., Ioannidis S., Kalpathy-Cramer J., et al. Evaluation of a deep learning image assessment system for detecting severe retinopathy of prematurity. Br. J. Ophthalmol. 2019;103:580&#x2013;584. doi: 10.1136/bjophthalmol-2018-313156.</Citation><ArticleIdList><ArticleId IdType="doi">10.1136/bjophthalmol-2018-313156</ArticleId><ArticleId IdType="pmc">PMC7880608</ArticleId><ArticleId IdType="pubmed">30470715</ArticleId></ArticleIdList></Reference><Reference><Citation>Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A. Going deeper with convolutions; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Boston, MA, USA. 7&#x2013;12 June 2015; pp. 1&#x2013;9.</Citation></Reference><Reference><Citation>Poplin R., Varadarajan A.V., Blumer K., Liu Y., McConnell M.V., Corrado G.S., Peng L., Webster D.R. Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning. Nat. Biomed. Eng. 2018;2:158&#x2013;164. doi: 10.1038/s41551-018-0195-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41551-018-0195-0</ArticleId><ArticleId IdType="pubmed">31015713</ArticleId></ArticleIdList></Reference><Reference><Citation>Gerrits N., Elen B., Craenendonck T.V., Triantafyllidou D., Petropoulos I.N., Malik R.A., Boever P.D. Age and sex affect deep learning prediction of cardiometabolic risk factors from retinal images. Sci. Rep. 2020;10:9432. doi: 10.1038/s41598-020-65794-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-020-65794-4</ArticleId><ArticleId IdType="pmc">PMC7287116</ArticleId><ArticleId IdType="pubmed">32523046</ArticleId></ArticleIdList></Reference><Reference><Citation>Sandler M., Howard A., Zhu M., Zhmoginov A., Chen L.C. Mobilenetv2: Inverted residuals and linear bottlenecks; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Salt Lake City, UT, USA. 18&#x2013;23 June 2018; pp. 4510&#x2013;4520.</Citation></Reference><Reference><Citation>Deng J., Dong W., Socher R., Li L.J., Li K., Fei-Fei L. Imagenet: A large-scale hierarchical image database; Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition; Miami, FL, USA. 20&#x2013;25 June 2009; pp. 248&#x2013;255.</Citation></Reference><Reference><Citation>Cheung C.Y., Xu D., Cheng C.Y., Sabanayagam C., Tham Y.C., Yu M., Rim T.H., Chai C.Y., Gopinath B., Mitchell P., et al. A deep-learning system for the assessment of cardiovascular disease risk via the measurement of retinal-vessel calibre. Nat. Biomed. Eng. 2021;5:498&#x2013;508. doi: 10.1038/s41551-020-00626-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41551-020-00626-4</ArticleId><ArticleId IdType="pubmed">33046867</ArticleId></ArticleIdList></Reference><Reference><Citation>Rim T.H., Lee G., Kim Y., Tham Y.C., Lee C.J., Baik S.J., Kim Y.A., Yu M., Deshmukh M., Lee B.K., et al. Prediction of systemic biomarkers from retinal photographs: Development and validation of deep-learning algorithms. Lancet Digit. Health. 2020;2:e526&#x2013;e536. doi: 10.1016/S2589-7500(20)30216-8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S2589-7500(20)30216-8</ArticleId><ArticleId IdType="pubmed">33328047</ArticleId></ArticleIdList></Reference><Reference><Citation>Nusinovici S., Rim T.H., Yu M., Lee G., Tham Y.C., Cheung N., Chong C.C.Y., Soh Z.D., Thakur S., Lee C.J., et al. Retinal photograph-based deep learning predicts biological age, and stratifies morbidity and mortality risk. Age Ageing. 2022;51 doi: 10.1093/ageing/afac065.</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/ageing/afac065</ArticleId><ArticleId IdType="pmc">PMC8973000</ArticleId><ArticleId IdType="pubmed">35363255</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhu Z., Chen Y., Wang W., Wang Y., Hu W., Shang X., Liao H., Shi D., Huang Y., Ha J., et al. Association of Retinal Age Gap with Arterial Stiffness and Incident Cardiovascular Disease. medRxiv. 2022 doi: 10.1161/STROKEAHA.122.038809.</Citation><ArticleIdList><ArticleId IdType="doi">10.1161/STROKEAHA.122.038809</ArticleId><ArticleId IdType="pubmed">35880520</ArticleId></ArticleIdList></Reference><Reference><Citation>Chollet F. Xception: Deep learning with depthwise separable convolutions; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Honolulu, HI, USA. 21&#x2013;26 July 2017; pp. 1251&#x2013;1258.</Citation></Reference><Reference><Citation>Diederichsen A., Mickley H. Coronary Artery Calcium Score and Cardiovascular Event Prediction. JAMA. 2010;304:741&#x2013;742. doi: 10.1001/jama.2010.1142.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jama.2010.1142</ArticleId><ArticleId IdType="pubmed">20716733</ArticleId></ArticleIdList></Reference><Reference><Citation>Greenland P., Blaha M.J., Budoff M.J., Erbel R., Watson K.E. Coronary calcium score and cardiovascular risk. J. Am. Coll. Cardiol. 2018;72:434&#x2013;447. doi: 10.1016/j.jacc.2018.05.027.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jacc.2018.05.027</ArticleId><ArticleId IdType="pmc">PMC6056023</ArticleId><ArticleId IdType="pubmed">30025580</ArticleId></ArticleIdList></Reference><Reference><Citation>Son J., Shin J.Y., Chun E.J., Jung K.H., Park K.H., Park S.J. Predicting high coronary artery calcium score from retinal fundus images with deep learning algorithms. Transl. Vis. Sci. Technol. 2020;9:1&#x2013;9. doi: 10.1167/tvst.9.2.28.</Citation><ArticleIdList><ArticleId IdType="doi">10.1167/tvst.9.2.28</ArticleId><ArticleId IdType="pmc">PMC7410115</ArticleId><ArticleId IdType="pubmed">33184590</ArticleId></ArticleIdList></Reference><Reference><Citation>Rim T.H., Lee C.J., Tham Y.C., Cheung N., Yu M., Lee G., Kim Y., Ting D.S., Chong C.C.Y., Choi Y.S., et al. Deep-learning-based cardiovascular risk stratification using coronary artery calcium scores predicted from retinal photographs. Lancet Digit. Health. 2021;3:e306&#x2013;e316. doi: 10.1016/S2589-7500(21)00043-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S2589-7500(21)00043-1</ArticleId><ArticleId IdType="pubmed">33890578</ArticleId></ArticleIdList></Reference><Reference><Citation>Barriada R.G., Sim&#xf3;-Servat O., Planas A., Hern&#xe1;ndez C., Sim&#xf3; R., Masip D. Deep Learning of Retinal Imaging: A Useful Tool for Coronary Artery Calcium Score Prediction in Diabetic Patients. Appl. Sci. 2022;12:1401. doi: 10.3390/app12031401.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/app12031401</ArticleId></ArticleIdList></Reference><Reference><Citation>Hubbard L.D., Brothers R.J., King W.N., Clegg L.X., Klein R., Cooper L.S., Sharrett A.R., Davis M.D., Cai J., Atherosclerosis Risk in Communities Study Group et al. Methods for evaluation of retinal microvascular abnormalities associated with hypertension/sclerosis in the Atherosclerosis Risk in Communities Study. Ophthalmology. 1999;106:2269&#x2013;2280. doi: 10.1016/S0161-6420(99)90525-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0161-6420(99)90525-0</ArticleId><ArticleId IdType="pubmed">10599656</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang L., Yuan M., An Z., Zhao X., Wu H., Li H., Wang Y., Sun B., Li H., Ding S., et al. Prediction of hypertension, hyperglycemia and dyslipidemia from retinal fundus photographs via deep learning: A cross-sectional study of chronic diseases in central China. PLoS ONE. 2020;15:e0233166. doi: 10.1371/journal.pone.0233166.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0233166</ArticleId><ArticleId IdType="pmc">PMC7224473</ArticleId><ArticleId IdType="pubmed">32407418</ArticleId></ArticleIdList></Reference><Reference><Citation>Srilakshmi V., Anuradha K., Shoba Bindu C. Intelligent decision support system for cardiovascular risk prediction using hybrid loss deep joint segmentation and optimized deep learning. Adv. Eng. Softw. 2022;173:103198. doi: 10.1016/j.advengsoft.2022.103198.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.advengsoft.2022.103198</ArticleId></ArticleIdList></Reference><Reference><Citation>Chang J., Ko A., Park S.M., Choi S., Kim K., Kim S.M., Yun J.M., Kang U., Shin I.H., Shin J.Y., et al. Association of Cardiovascular Mortality and Deep Learning-Funduscopic Atherosclerosis Score derived from Retinal Fundus Images. Am. J. Ophthalmol. 2020;217:121&#x2013;130. doi: 10.1016/j.ajo.2020.03.027.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ajo.2020.03.027</ArticleId><ArticleId IdType="pubmed">32222370</ArticleId></ArticleIdList></Reference><Reference><Citation>Huang F., Lian J., Ng K.S., Shih K.C., Vardhanabhuti V. Predicting CT-Based Coronary Artery Disease Using Vascular Biomarkers Derived from Fundus Photographs with a Graph Convolutional Neural Network. Diagnostics. 2022;12:1390. doi: 10.3390/diagnostics12061390.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/diagnostics12061390</ArticleId><ArticleId IdType="pmc">PMC9221688</ArticleId><ArticleId IdType="pubmed">35741200</ArticleId></ArticleIdList></Reference><Reference><Citation>Lim G., Lim Z.W., Xu D., Ting D.S., Wong T.Y., Lee M.L., Hsu W. Feature isolation for hypothesis testing in retinal imaging: An ischemic stroke prediction case study; Proceedings of the AAAI Conference on Artificial Intelligence; Honolulu, HI, USA. 27 January 2019; pp. 9510&#x2013;9515.</Citation></Reference><Reference><Citation>Ma Y., Xiong J., Zhu Y., Ge Z., Hua R., Fu M., Li C., Wang B., Dong L., Zhao X., et al. Development and validation of a deep learning algorithm using fundus photographs to predict 10-year risk of ischemic cardiovascular diseases among Chinese population. medRxiv. 2021 doi: 10.1101/2021.04.15.21255176.</Citation><ArticleIdList><ArticleId IdType="doi">10.1101/2021.04.15.21255176</ArticleId></ArticleIdList></Reference><Reference><Citation>Szegedy C., Ioffe S., Vanhoucke V., Alemi A.A. Inception-v4, inception-resnet and the impact of residual connections on learning; Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence; San Francisco, CA, USA. 4&#x2013;9 February 2017.</Citation></Reference><Reference><Citation>Revathi T.K., Sathiyabhama B., Sankar S. Diagnosing Cardio Vascular Disease (CVD) using Generative Adversarial Network (GAN) in Retinal Fundus Images. Ann. Rom. Soc. Cell Biol. 2021;25:2563&#x2013;2572.</Citation></Reference><Reference><Citation>Al-Absi H.R.H., Islam M.T., Refaee M.A., Chowdhury M.E.H., Alam T. Cardiovascular Disease Diagnosis from DXA Scan and Retinal Images Using Deep Learning. Sensors. 2022;22:4310. doi: 10.3390/s22124310.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s22124310</ArticleId><ArticleId IdType="pmc">PMC9228833</ArticleId><ArticleId IdType="pubmed">35746092</ArticleId></ArticleIdList></Reference><Reference><Citation>Diaz-Pinto A., Ravikumar N., Attar R., Suinesiaputra A., Zhao Y., Levelt E., Dall&#x2019;Armellina E., Lorenzi M., Chen Q., Keenan T.D., et al. Predicting myocardial infarction through retinal scans and minimal personal information. Nat. Mach. Intell. 2022;4:55&#x2013;61. doi: 10.1038/s42256-021-00427-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s42256-021-00427-7</ArticleId></ArticleIdList></Reference><Reference><Citation>Antelmi L., Ayache N., Robert P., Lorenzi M. Sparse multi-channel variational autoencoder for the joint analysis of heterogeneous data; Proceedings of the International Conference on Machine Learning; Long Beach, CA, USA. 9&#x2013;15 June 2019; pp. 302&#x2013;311.</Citation></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36611358</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Print">2075-4418</ISSN><JournalIssue CitedMedium="Print"><Volume>13</Volume><Issue>1</Issue><PubDate><Year>2022</Year><Month>Dec</Month><Day>26</Day></PubDate></JournalIssue><Title>Diagnostics (Basel, Switzerland)</Title><ISOAbbreviation>Diagnostics (Basel)</ISOAbbreviation></Journal><ArticleTitle>A Comparison of Techniques for Class Imbalance in Deep Learning Classification of Breast Cancer.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">67</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/diagnostics13010067</ELocationID><Abstract><AbstractText>Tools based on deep learning models have been created in recent years to aid radiologists in the diagnosis of breast cancer from mammograms. However, the datasets used to train these models may suffer from class imbalance, i.e., there are often fewer malignant samples than benign or healthy cases, which can bias the model towards the healthy class. In this study, we systematically evaluate several popular techniques to deal with this class imbalance, namely, class weighting, over-sampling, and under-sampling, as well as a synthetic lesion generation approach to increase the number of malignant samples. These techniques are applied when training on three diverse Full-Field Digital Mammography datasets, and tested on in-distribution and out-of-distribution samples. The experiments show that a greater imbalance is associated with a greater bias towards the majority class, which can be counteracted by any of the standard class imbalance techniques. On the other hand, these methods provide no benefit to model performance with respect to Area Under the Curve of the Recall Operating Characteristic (AUC-ROC), and indeed under-sampling leads to a reduction of 0.066 in AUC in the case of a 19:1 benign to malignant imbalance. Our synthetic lesion methodology leads to better performance in most cases, with increases of up to 0.07 in AUC on out-of-distribution test sets over the next best experiment.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Walsh</LastName><ForeName>Ricky</ForeName><Initials>R</Initials><Identifier Source="ORCID">0000-0002-2363-5506</Identifier><AffiliationInfo><Affiliation>ISTIC, Campus Beaulieu, Universit&#xe9; de Rennes 1, 35700 Rennes, France.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Hera-MI SAS, 44800 Saint-Herblain, France.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Tardy</LastName><ForeName>Mickael</ForeName><Initials>M</Initials><Identifier Source="ORCID">0000-0003-4069-9517</Identifier><AffiliationInfo><Affiliation>Hera-MI SAS, 44800 Saint-Herblain, France.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Ecole Centrale Nantes, CNRS, LS2N, UMR 6004, 44000 Nantes, France.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>26</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Diagnostics (Basel)</MedlineTA><NlmUniqueID>101658402</NlmUniqueID><ISSNLinking>2075-4418</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">breast cancer</Keyword><Keyword MajorTopicYN="N">class imbalance</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword><Keyword MajorTopicYN="N">mammography</Keyword><Keyword MajorTopicYN="N">medical imaging</Keyword><Keyword MajorTopicYN="N">synthetic data</Keyword></KeywordList><CoiStatement>The authors declare no conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>11</Month><Day>29</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>19</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>20</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>1</Hour><Minute>2</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36611358</ArticleId><ArticleId IdType="pmc">PMC9818528</ArticleId><ArticleId IdType="doi">10.3390/diagnostics13010067</ArticleId><ArticleId IdType="pii">diagnostics13010067</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Sung H., Ferlay J., Siegel R.L., Laversanne M., Soerjomataram I., Jemal A., Bray F. Global Cancer Statistics 2020: GLOBOCAN Estimates of Incidence and Mortality Worldwide for 36 Cancers in 185 Countries. CA Cancer J. Clin. 2021;71:209&#x2013;249. doi: 10.3322/caac.21660.</Citation><ArticleIdList><ArticleId IdType="doi">10.3322/caac.21660</ArticleId><ArticleId IdType="pubmed">33538338</ArticleId></ArticleIdList></Reference><Reference><Citation>Mandelblatt J.S., Stout N.K., Schechter C.B., Broek J.J.V.D., Miglioretti D.L., Krapcho M., Trentham-Dietz A., Munoz D., Lee S.J., Berry D.A., et al. Collaborative modeling of the benefits and harms associated with different U.S. Breast cancer screening strategies. Ann. Intern. Med. 2016;164:215&#x2013;225. doi: 10.7326/M15-1536.</Citation><ArticleIdList><ArticleId IdType="doi">10.7326/M15-1536</ArticleId><ArticleId IdType="pmc">PMC5079106</ArticleId><ArticleId IdType="pubmed">26756606</ArticleId></ArticleIdList></Reference><Reference><Citation>Geller B.M., Bowles E.J.A., Sohng H.Y., Brenner R.J., Miglioretti D.L., Carney P.A., Elmore J.G. Radiologists&#x2019; Performance and Their Enjoyment of Interpreting Screening Mammograms. AJR Am. J. Roentgenol. 2009;192:361. doi: 10.2214/AJR.08.1647.</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/AJR.08.1647</ArticleId><ArticleId IdType="pmc">PMC2824325</ArticleId><ArticleId IdType="pubmed">19155395</ArticleId></ArticleIdList></Reference><Reference><Citation>Lehman C.D., Wellman R.D., Buist D.S., Kerlikowske K., Tosteson A.N., Miglioretti D.L. Diagnostic accuracy of digital screening mammography with and without computer-aided detection. JAMA Intern. Med. 2015;175:1828&#x2013;1837. doi: 10.1001/jamainternmed.2015.5231.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jamainternmed.2015.5231</ArticleId><ArticleId IdType="pmc">PMC4836172</ArticleId><ArticleId IdType="pubmed">26414882</ArticleId></ArticleIdList></Reference><Reference><Citation>McKinney S.M., Sieniek M., Godbole V., Godwin J., Antropova N., Ashrafian H., Back T., Chesus M., Corrado G.C., Darzi A., et al. International evaluation of an AI system for breast cancer screening. Nature. 2020;577:89&#x2013;94. doi: 10.1038/s41586-019-1799-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41586-019-1799-6</ArticleId><ArticleId IdType="pubmed">31894144</ArticleId></ArticleIdList></Reference><Reference><Citation>Schaffter T., Buist D.S., Lee C.I., Nikulin Y., Ribli D., Guan Y., Lotter W., Jie Z., Du H., Wang S., et al. Evaluation of Combined Artificial Intelligence and Radiologist Assessment to Interpret Screening Mammograms. JAMA Netw. Open. 2020;3:e200265. doi: 10.1001/jamanetworkopen.2020.0265.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jamanetworkopen.2020.0265</ArticleId><ArticleId IdType="pmc">PMC7052735</ArticleId><ArticleId IdType="pubmed">32119094</ArticleId></ArticleIdList></Reference><Reference><Citation>Buda M., Maki A., Mazurowski M.A. A systematic study of the class imbalance problem in convolutional neural networks. Neural Netw. 2018;106:249&#x2013;259. doi: 10.1016/j.neunet.2018.07.011.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neunet.2018.07.011</ArticleId><ArticleId IdType="pubmed">30092410</ArticleId></ArticleIdList></Reference><Reference><Citation>Japkowicz N., Stephen S. The class imbalance problem: A systematic study. Intell. Data Anal. 2002;6:429&#x2013;449. doi: 10.3233/IDA-2002-6504.</Citation><ArticleIdList><ArticleId IdType="doi">10.3233/IDA-2002-6504</ArticleId></ArticleIdList></Reference><Reference><Citation>Wu E., Wu K., Lotter W. Synthesizing lesions using contextual GANs improves breast cancer classification on mammograms. arXiv. 20202006.00086</Citation></Reference><Reference><Citation>Cui C., Li L., Cai H., Fan Z., Zhang L., Dan T., Li J., Wang J. The Chinese Mammography Database (CMMD): An online mammography database with biopsy confirmed types for machine diagnosis of breast. Data Cancer Imaging Arch. 2021 doi: 10.7937/tcia.eqde-4b16.</Citation><ArticleIdList><ArticleId IdType="doi">10.7937/tcia.eqde-4b16</ArticleId></ArticleIdList></Reference><Reference><Citation>Nguyen H.T., Nguyen H.Q., Pham H.H., Lam K., Le L.T., Dao M., Vu V. VinDr-Mammo: A large-scale benchmark dataset for computer-aided diagnosis in full-field digital mammography. arXiv. 2022 doi: 10.48550/arxiv.2203.11205.2203.11205</Citation><ArticleIdList><ArticleId IdType="doi">10.48550/arxiv.2203.11205</ArticleId></ArticleIdList></Reference><Reference><Citation>Anand R., Mehrotra K.G., Mohan C.K., Ranka S. An Improved Algorithm for Neural Network Classification of Imbalanced Training Sets. IEEE Trans. Neural Netw. 1993;4:962&#x2013;969. doi: 10.1109/72.286891.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/72.286891</ArticleId><ArticleId IdType="pubmed">18276526</ArticleId></ArticleIdList></Reference><Reference><Citation>Chawla N.V., Japkowicz N., Kotcz A. Editorial: Special Issue on Learning from Imbalanced Data Sets. ACM SIGKDD Explor. Newsl. 2004;6:1&#x2013;6. doi: 10.1145/1007730.1007733.</Citation><ArticleIdList><ArticleId IdType="doi">10.1145/1007730.1007733</ArticleId></ArticleIdList></Reference><Reference><Citation>Li Z., Kamnitsas K., Glocker B. Analyzing Overfitting under Class Imbalance in Neural Networks for Image Segmentation. IEEE Trans. Med. Imaging. 2021;40:1065&#x2013;1077. doi: 10.1109/TMI.2020.3046692.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2020.3046692</ArticleId><ArticleId IdType="pubmed">33351758</ArticleId></ArticleIdList></Reference><Reference><Citation>Mazurowski M.A., Habas P.A., Zurada J.M., Lo J.Y., Baker J.A., Tourassi G.D. Training neural network classifiers for medical decision making: The effects of imbalanced datasets on classification performance. Neural Netw. 2008;21:427&#x2013;436. doi: 10.1016/j.neunet.2007.12.031.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neunet.2007.12.031</ArticleId><ArticleId IdType="pmc">PMC2346433</ArticleId><ArticleId IdType="pubmed">18272329</ArticleId></ArticleIdList></Reference><Reference><Citation>Bria A., Marrocco C., Tortorella F. Addressing class imbalance in deep learning for small lesion detection on medical images. Comput. Biol. Med. 2020;120:103735. doi: 10.1016/j.compbiomed.2020.103735.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2020.103735</ArticleId><ArticleId IdType="pubmed">32250861</ArticleId></ArticleIdList></Reference><Reference><Citation>Moreira I.C., Amaral I., Domingues I., Cardoso A., Cardoso M.J., Cardoso J.S. INbreast: Toward a Full-field Digital Mammographic Database. Acad. Radiol. 2012;19:236&#x2013;248. doi: 10.1016/j.acra.2011.09.014.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.acra.2011.09.014</ArticleId><ArticleId IdType="pubmed">22078258</ArticleId></ArticleIdList></Reference><Reference><Citation>Johnson J.M., Khoshgoftaar T.M. Survey on deep learning with class imbalance. J. Big Data. 2019;6:1&#x2013;54. doi: 10.1186/s40537-019-0192-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s40537-019-0192-5</ArticleId></ArticleIdList></Reference><Reference><Citation>Shen L., Margolies L.R., Rothstein J.H., Fluder E., McBride R., Sieh W. Deep Learning to Improve Breast Cancer Detection on Screening Mammography. Sci. Rep. 2019;9:12495. doi: 10.1038/s41598-019-48995-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-019-48995-4</ArticleId><ArticleId IdType="pmc">PMC6715802</ArticleId><ArticleId IdType="pubmed">31467326</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhu W., Lou Q., Vang Y.S., Xie X. International Conference on Medical Image Computing and Computer-Assisted Intervention. Volume 10435 LNCS. Springer; Cham, Switzerland: 2017. Deep multi-instance networks with sparse label assignment for whole mammogram classification; pp. 603&#x2013;611. (Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)).</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-319-66179-7_69</ArticleId></ArticleIdList></Reference><Reference><Citation>Abdelhafiz D., Yang C., Ammar R., Nabavi S. Deep convolutional neural networks for mammography: Advances, challenges and applications. BMC Bioinform. 2019;20:281. doi: 10.1186/s12859-019-2823-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s12859-019-2823-4</ArticleId><ArticleId IdType="pmc">PMC6551243</ArticleId><ArticleId IdType="pubmed">31167642</ArticleId></ArticleIdList></Reference><Reference><Citation>Qu W., Balki I., Mendez M., Valen J., Levman J., Tyrrell P.N. Assessing and mitigating the effects of class imbalance in machine learning with application to X-ray imaging. Int. J. Comput. Assist. Radiol. Surg. 2020;15:2041&#x2013;2048. doi: 10.1007/s11548-020-02260-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11548-020-02260-6</ArticleId><ArticleId IdType="pubmed">32965624</ArticleId></ArticleIdList></Reference><Reference><Citation>Wu N., Phang J., Park J., Shen Y., Huang Z., Zorin M., Jastrzebski S., Fevry T., Katsnelson J., Kim E., et al. Deep Neural Networks Improve Radiologists&#x2019; Performance in Breast Cancer Screening. IEEE Trans. Med. Imaging. 2019;39:1184&#x2013;1194. doi: 10.1109/TMI.2019.2945514.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2019.2945514</ArticleId><ArticleId IdType="pmc">PMC7427471</ArticleId><ArticleId IdType="pubmed">31603772</ArticleId></ArticleIdList></Reference><Reference><Citation>Shen Y., Wu N., Phang J., Park J., Liu K., Tyagi S., Heacock L., Kim S.G., Moy L., Cho K., et al. An interpretable classifier for high-resolution breast cancer screening images utilizing weakly supervised localization. Med. Image Anal. 2021;68:101908. doi: 10.1016/j.media.2020.101908.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2020.101908</ArticleId><ArticleId IdType="pmc">PMC7828643</ArticleId><ArticleId IdType="pubmed">33383334</ArticleId></ArticleIdList></Reference><Reference><Citation>Parmar C., Barry J.D., Hosny A., Quackenbush J., Aerts H.J.W.L. Data Analysis Strategies in Medical Imaging. Clin. Cancer Res. Off. J. Am. Assoc. Cancer Res. 2018;24:3492&#x2013;3499. doi: 10.1158/1078-0432.CCR-18-0385.</Citation><ArticleIdList><ArticleId IdType="doi">10.1158/1078-0432.CCR-18-0385</ArticleId><ArticleId IdType="pmc">PMC6082690</ArticleId><ArticleId IdType="pubmed">29581134</ArticleId></ArticleIdList></Reference><Reference><Citation>Frid-Adar M., Klang E., Amitai M., Goldberger J., Greenspan H. Synthetic data augmentation using GAN for improved liver lesion classification; Proceedings of the 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018); Washington, DC, USA. 4&#x2013;7 April 2018; pp. 289&#x2013;293.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ISBI.2018.8363576</ArticleId></ArticleIdList></Reference><Reference><Citation>Madani T.A., Moradi M., Karargyris A., Syeda-Mahmood T., Madani A. Chest X-ray generation and data augmentation for cardiovascular abnormality classification. Image Process. 2018;10574:415&#x2013;420. doi: 10.1117/12.2293971.</Citation><ArticleIdList><ArticleId IdType="doi">10.1117/12.2293971</ArticleId></ArticleIdList></Reference><Reference><Citation>Alyafi B., Diaz O., Mart&#xed; R. DCGANs for realistic breast mass augmentation in x-ray mammography. In: Hahn H.K., Mazurowski M.A., editors. Medical Imaging 2020: Computer-Aided Diagnosis. SPIE; Bellingham, WA, USA: 2020. p. 68.</Citation><ArticleIdList><ArticleId IdType="doi">10.1117/12.2543506</ArticleId></ArticleIdList></Reference><Reference><Citation>Korkinof D., Rijken T., O&#x2019;Neill M., Yearsley J., Harvey H., Glocker B. High-Resolution Mammogram Synthesis using Progressive Generative Adversarial Networks. arXiv. 20181807.03401</Citation></Reference><Reference><Citation>Tremblay J., Prakash A., Acuna D., Brophy M., Jampani V., Anil C., To T., Cameracci E., Boochoon S., Birchfield S. Training deep networks with synthetic data: Bridging the reality gap by domain randomization; Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops; Salt Lake City, UT, USA. 18&#x2013;22 June 2018; pp. 1082&#x2013;1090.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/CVPRW.2018.00143</ArticleId></ArticleIdList></Reference><Reference><Citation>de Sisternes L., Brankov J.G., Zysk A.M., Schmidt R.A., Nishikawa R.M., Wernick M.N. A computational model to generate simulated three-dimensional breast masses. Med. Phys. 2015;42:1098&#x2013;1118. doi: 10.1118/1.4905232.</Citation><ArticleIdList><ArticleId IdType="doi">10.1118/1.4905232</ArticleId><ArticleId IdType="pmc">PMC4320152</ArticleId><ArticleId IdType="pubmed">25652522</ArticleId></ArticleIdList></Reference><Reference><Citation>Cha K.H., Petrick N., Pezeshk A., Graff C.G., Sharma D., Badal A., Sahiner B. Evaluation of data augmentation via synthetic images for improved breast mass detection on mammograms using deep learning. J. Med. Imaging. 2019;7:1. doi: 10.1117/1.JMI.7.1.012703.</Citation><ArticleIdList><ArticleId IdType="doi">10.1117/1.JMI.7.1.012703</ArticleId><ArticleId IdType="pmc">PMC6872953</ArticleId><ArticleId IdType="pubmed">31763356</ArticleId></ArticleIdList></Reference><Reference><Citation>Tardy M., Mateus D. Looking for abnormalities in mammograms with self-and weakly supervised reconstruction. IEEE Trans. Med. Imaging. 2021;40:2711&#x2013;2722. doi: 10.1109/TMI.2021.3050040.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2021.3050040</ArticleId><ArticleId IdType="pubmed">33417539</ArticleId></ArticleIdList></Reference><Reference><Citation>Tardy M. Ph.D. Thesis. Ecole centrale de Nantes; Nantes, France: 2021. Deep Learning for Computer-Aided Early Diagnosis of Breast Cancer.</Citation></Reference><Reference><Citation>Badano A., Graff C.G., Badal A., Sharma D., Zeng R., Samuelson F.W., Glick S.J., Myers K.J. Evaluation of Digital Breast Tomosynthesis as Replacement of Full-Field Digital Mammography Using an In Silico Imaging Trial. JAMA Netw. Open. 2018;1:e185474. doi: 10.1001/jamanetworkopen.2018.5474.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jamanetworkopen.2018.5474</ArticleId><ArticleId IdType="pmc">PMC6324392</ArticleId><ArticleId IdType="pubmed">30646401</ArticleId></ArticleIdList></Reference><Reference><Citation>Muttarak M., Kongmebhol P., Sukhamwang N. Breast calcifications: Which are malignant. Singap. Med. J. 2009;50:907&#x2013;914.</Citation><ArticleIdList><ArticleId IdType="pubmed">19787181</ArticleId></ArticleIdList></Reference><Reference><Citation>Bahl M., Baker J.A., Kinsey E.N., Ghate S.V. Architectural Distortion on Mammography: Correlation With Pathologic Outcomes and Predictors of Malignancy. Am. J. Roentgenol. 2015;205:1339&#x2013;1345. doi: 10.2214/AJR.15.14628.</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/AJR.15.14628</ArticleId><ArticleId IdType="pubmed">26587943</ArticleId></ArticleIdList></Reference><Reference><Citation>Van der Walt S., Sch&#xf6;nberger J.L., Nunez-Iglesias J., Boulogne F., Warner J.D., Yager N., Gouillart E., Yu T. scikit-image: Image processing in Python. PeerJ. 2014;2:e453. doi: 10.7717/peerj.453.</Citation><ArticleIdList><ArticleId IdType="doi">10.7717/peerj.453</ArticleId><ArticleId IdType="pmc">PMC4081273</ArticleId><ArticleId IdType="pubmed">25024921</ArticleId></ArticleIdList></Reference><Reference><Citation>Zack G.W., Rogers W.E., Latt S.A. Automatic measurement of sister chromatid exchange frequency. J. Histochem. Cytochem. Off. J. Histochem. Soc. 1977;25:741&#x2013;753. doi: 10.1177/25.7.70454.</Citation><ArticleIdList><ArticleId IdType="doi">10.1177/25.7.70454</ArticleId><ArticleId IdType="pubmed">70454</ArticleId></ArticleIdList></Reference><Reference><Citation>Tardy M., Mateus D. Leveraging Multi-Task Learning to Cope with Poor and Missing Labels of Mammograms. Front. Radiol. 2022;1:19. doi: 10.3389/fradi.2021.796078.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fradi.2021.796078</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhou Z., Sodha V., Rahman Siddiquee M.M., Feng R., Tajbakhsh N., Gotway M.B., Liang J. International Conference on Medical Image Computing and Computer-Assisted Intervention. Volume 11767 LNCS. Springer; Cham, Switzerland: 2019. Models genesis: Generic autodidactic models for 3d medical image analysis; pp. 384&#x2013;393.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-030-32251-9_42</ArticleId><ArticleId IdType="pmc">PMC7405596</ArticleId><ArticleId IdType="pubmed">32766570</ArticleId></ArticleIdList></Reference><Reference><Citation>Stadnick B., Witowski J., Rajiv V., Ch&#x142;&#x229;dowski J., Shamout F.E., Cho K., Geras K.J. Meta-repository of screening mammography classifiers. arXiv. 2021 doi: 10.48550/arxiv.2108.04800.2108.04800</Citation><ArticleIdList><ArticleId IdType="doi">10.48550/arxiv.2108.04800</ArticleId></ArticleIdList></Reference><Reference><Citation>Ozenne B., Subtil F., Maucort-Boulch D. The precision&#x2013;recall curve overcame the optimism of the receiver operating characteristic curve in rare diseases. J. Clin. Epidemiol. 2015;68:855&#x2013;859. doi: 10.1016/j.jclinepi.2015.02.010.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jclinepi.2015.02.010</ArticleId><ArticleId IdType="pubmed">25881487</ArticleId></ArticleIdList></Reference><Reference><Citation>Saito T., Rehmsmeier M. The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets. PLoS ONE. 2015;10:e0118432. doi: 10.1371/journal.pone.0118432.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0118432</ArticleId><ArticleId IdType="pmc">PMC4349800</ArticleId><ArticleId IdType="pubmed">25738806</ArticleId></ArticleIdList></Reference><Reference><Citation>Bradley A.P. The use of the area under the ROC curve in the evaluation of machine learning algorithms. Pattern Recognit. 1997;30:1145&#x2013;1159. doi: 10.1016/S0031-3203(96)00142-2.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0031-3203(96)00142-2</ArticleId></ArticleIdList></Reference><Reference><Citation>Boughorbel S., Jarray F., El-Anbari M. Optimal classifier for imbalanced data using Matthews Correlation Coefficient metric. PLoS ONE. 2017;12:e0177678. doi: 10.1371/journal.pone.0177678.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0177678</ArticleId><ArticleId IdType="pmc">PMC5456046</ArticleId><ArticleId IdType="pubmed">28574989</ArticleId></ArticleIdList></Reference><Reference><Citation>Choukroun Y., Bakalo R., Ben-Ari R., Akselrod-Ballin A., Barkan E., Kisilev P. Mammogram Classification and Abnormality Detection from Nonlocal Labels using Deep Multiple Instance Neural Network. In: Bruckner S., Hennemuth A., Kainz B., Hotz I., Merhof D., Rieder C., editors. Eurographics Workshop on Visual Computing for Biology and Medicine. The Eurographics Association; Munich, Germany: 2017.</Citation><ArticleIdList><ArticleId IdType="doi">10.2312/vcbm.20171232</ArticleId></ArticleIdList></Reference><Reference><Citation>Chicco D., Jurman G. The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation. BMC Genom. 2020;21:6. doi: 10.1186/s12864-019-6413-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s12864-019-6413-7</ArticleId><ArticleId IdType="pmc">PMC6941312</ArticleId><ArticleId IdType="pubmed">31898477</ArticleId></ArticleIdList></Reference><Reference><Citation>Efron B., Tibshirani R.J. An Introduction to the Bootstrap. Chapman and Hall; London, UK: 1993. pp. 45&#x2013;82.</Citation></Reference><Reference><Citation>D&#x2019;Orsi C.J. 2013 ACR BI-RADS Atlas: Breast Imaging Reporting and Data System&#x2014;Acr. American College of Radiology; Reston, VA, USA: 2014.</Citation></Reference><Reference><Citation>Van Rossum G., Drake F.L. Python 3 Reference Manual. CreateSpace; Scotts Valley, CA, USA: 2009.</Citation></Reference><Reference><Citation>Bradski G. The OpenCV Library.  [(accessed on 18 December 2022)];Dr. Dobb&#x2019;s J. Softw. Tools. 2000  Available online:  https://github.com/opencv/opencv/wiki/CiteOpenCV.</Citation></Reference><Reference><Citation>Chollet F. Keras. 2015.  [(accessed on 18 December 2022)].  Available online:  https://keras.io.</Citation></Reference><Reference><Citation>Abadi M., Agarwal A., Barham P., Brevdo E., Chen Z., Citro C., Corrado G.S., Davis A., Dean J., Devin M., et al.  TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. Software. 2015.  [(accessed on 18 December 2022)].  Available online:  tensorflow.org.</Citation></Reference><Reference><Citation>Hunter J.D. Matplotlib: A 2D graphics environment. Comput. Sci. Eng. 2007;9:90&#x2013;95. doi: 10.1109/MCSE.2007.55.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/MCSE.2007.55</ArticleId></ArticleIdList></Reference><Reference><Citation>Waskom M.L. seaborn: Statistical data visualization. J. Open Source Softw. 2021;6:3021. doi: 10.21105/joss.03021.</Citation><ArticleIdList><ArticleId IdType="doi">10.21105/joss.03021</ArticleId></ArticleIdList></Reference><Reference><Citation>Tardy M., Scheffer B., Mateus D. International Conference on Medical Image Computing and Computer-Assisted Intervention. Volume 11769 LNCS. Springer; Cham, Switzerland: 2019. Uncertainty Measurements for the Reliable Classification of Mammograms; pp. 495&#x2013;503.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-030-32226-7_55</ArticleId></ArticleIdList></Reference><Reference><Citation>Ribli D., Horv&#xe1;th A., Unger Z., Pollner P., Csabai I. Detecting and classifying lesions in mammograms with Deep Learning. Sci. Rep. 2018;8:4165. doi: 10.1038/s41598-018-22437-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-018-22437-z</ArticleId><ArticleId IdType="pmc">PMC5854668</ArticleId><ArticleId IdType="pubmed">29545529</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang X., Liang G., Zhang Y., Blanton H., Bessinger Z., Jacobs N. Inconsistent Performance of Deep Learning Models on Mammogram Classification. J. Am. Coll. Radiol. 2020;17:796&#x2013;803. doi: 10.1016/j.jacr.2020.01.006.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jacr.2020.01.006</ArticleId><ArticleId IdType="pubmed">32068005</ArticleId></ArticleIdList></Reference><Reference><Citation>Chawla N.V., Bowyer K.W., Hall L.O., Kegelmeyer W.P. SMOTE: Synthetic Minority Over-sampling Technique. J. Artif. Intell. Res. 2002;16:321&#x2013;357. doi: 10.1613/jair.953.</Citation><ArticleIdList><ArticleId IdType="doi">10.1613/jair.953</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle>
<PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36611350</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Print">2075-4418</ISSN><JournalIssue CitedMedium="Print"><Volume>13</Volume><Issue>1</Issue><PubDate><Year>2022</Year><Month>Dec</Month><Day>26</Day></PubDate></JournalIssue><Title>Diagnostics (Basel, Switzerland)</Title><ISOAbbreviation>Diagnostics (Basel)</ISOAbbreviation></Journal><ArticleTitle>Artificial Intelligence in Breast Ultrasound: From Diagnosis to Prognosis-A Rapid Review.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">58</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/diagnostics13010058</ELocationID><Abstract><AbstractText Label="BACKGROUND" NlmCategory="BACKGROUND">Ultrasound (US) is a fundamental diagnostic tool in breast imaging. However, US remains an operator-dependent examination. Research into and the application of artificial intelligence (AI) in breast US are increasing. The aim of this rapid review was to assess the current development of US-based artificial intelligence in the field of breast cancer.</AbstractText><AbstractText Label="METHODS" NlmCategory="METHODS">Two investigators with experience in medical research performed literature searching and data extraction on PubMed. The studies included in this rapid review evaluated the role of artificial intelligence concerning BC diagnosis, prognosis, molecular subtypes of breast cancer, axillary lymph node status, and the response to neoadjuvant chemotherapy. The mean values of sensitivity, specificity, and AUC were calculated for the main study categories with a meta-analytical approach.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">A total of 58 main studies, all published after 2017, were included. Only 9/58 studies were prospective (15.5%); 13/58 studies (22.4%) used an ML approach. The vast majority (77.6%) used DL systems. Most studies were conducted for the diagnosis or classification of BC (55.1%). At present, all the included studies showed that AI has excellent performance in breast cancer diagnosis, prognosis, and treatment strategy.</AbstractText><AbstractText Label="CONCLUSIONS" NlmCategory="CONCLUSIONS">US-based AI has great potential and research value in the field of breast cancer diagnosis, treatment, and prognosis. More prospective and multicenter studies are needed to assess the potential impact of AI in breast ultrasound.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Brunetti</LastName><ForeName>Nicole</ForeName><Initials>N</Initials><Identifier Source="ORCID">0000-0003-2487-248X</Identifier><AffiliationInfo><Affiliation>Department of Experimental Medicine (DIMES), University of Genova, Via L.B. Alberti 2, 16132 Genoa, Italy.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Calabrese</LastName><ForeName>Massimo</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Radiology, IRCCS-Ospedale Policlinico San Martino, Largo Rosanna Benzi 10, 16132 Genoa, Italy.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Martinoli</LastName><ForeName>Carlo</ForeName><Initials>C</Initials><AffiliationInfo><Affiliation>Department of Radiology, IRCCS-Ospedale Policlinico San Martino, Largo Rosanna Benzi 10, 16132 Genoa, Italy.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Health Sciences (DISSAL), University of Genova, Via L.B. Alberti 2, 16132 Genoa, Italy.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Tagliafico</LastName><ForeName>Alberto Stefano</ForeName><Initials>AS</Initials><Identifier Source="ORCID">0000-0003-1736-0697</Identifier><AffiliationInfo><Affiliation>Department of Radiology, IRCCS-Ospedale Policlinico San Martino, Largo Rosanna Benzi 10, 16132 Genoa, Italy.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Health Sciences (DISSAL), University of Genova, Via L.B. Alberti 2, 16132 Genoa, Italy.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType><PublicationType UI="D016454">Review</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>26</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Diagnostics (Basel)</MedlineTA><NlmUniqueID>101658402</NlmUniqueID><ISSNLinking>2075-4418</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">artificial intelligence</Keyword><Keyword MajorTopicYN="N">breast cancer</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword><Keyword MajorTopicYN="N">machine learning</Keyword><Keyword MajorTopicYN="N">ultrasound</Keyword></KeywordList><CoiStatement>The authors declare no conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>12</Month><Day>3</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>19</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>20</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>1</Hour><Minute>2</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>9</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36611350</ArticleId><ArticleId IdType="pmc">PMC9818181</ArticleId><ArticleId IdType="doi">10.3390/diagnostics13010058</ArticleId><ArticleId IdType="pii">diagnostics13010058</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Ferlay J., Steliarova-Foucher E., Lortet-Tieulent J., Rosso S., Coebergh J.W., Comber H., Forman D., Bray F. Cancer incidence and mortality patterns in Europe: Estimates for 40 countries in 2012. Eur. J. Cancer. 2013;49:1374&#x2013;1403. doi: 10.1016/j.ejca.2012.12.027.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejca.2012.12.027</ArticleId><ArticleId IdType="pubmed">23485231</ArticleId></ArticleIdList></Reference><Reference><Citation>Yang L., Wang S., Zhang L., Sheng C., Song F., Wang P., Huang Y. Performance of ultrasonography screening for breast cancer: A systematic review and meta-analysis. BMC Cancer. 2020;20:499. doi: 10.1186/s12885-020-06992-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s12885-020-06992-1</ArticleId><ArticleId IdType="pmc">PMC7268243</ArticleId><ArticleId IdType="pubmed">32487106</ArticleId></ArticleIdList></Reference><Reference><Citation>Litjens G., Kooi T., Bejnordi B.E., Setio A.A.A., Ciompi F., Ghafoorian M., Van Der Laak J.A., Van Ginneken B., S&#xe1;nchez C.I. A survey on deep learning in medical image analysis. Med. Image Anal. 2017;42:60&#x2013;88. doi: 10.1016/j.media.2017.07.005.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2017.07.005</ArticleId><ArticleId IdType="pubmed">28778026</ArticleId></ArticleIdList></Reference><Reference><Citation>LeCun Y., Bengio Y., Hinton G. Deep learning. Nature. 2015;521:436&#x2013;444. doi: 10.1038/nature14539.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nature14539</ArticleId><ArticleId IdType="pubmed">26017442</ArticleId></ArticleIdList></Reference><Reference><Citation>Niu S., Huang J., Li J., Liu X., Wang D., Zhang R., Wang Y., Shen H., Qi M., Xiao Y., et al. Application of ultrasound artificial intelligence in the differential diagnosis between benign and malignant breast lesions of BI-RADS 4A. BMC Cancer. 2020;20:959. doi: 10.1186/s12885-020-07413-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s12885-020-07413-z</ArticleId><ArticleId IdType="pmc">PMC7532640</ArticleId><ArticleId IdType="pubmed">33008320</ArticleId></ArticleIdList></Reference><Reference><Citation>Fleury E., Marcomini K. Performance of machine learning software to classify breast lesions using BI-RADS radiomic features on ultrasound images. Eur. Radiol. Exp. 2019;3:34. doi: 10.1186/s41747-019-0112-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s41747-019-0112-7</ArticleId><ArticleId IdType="pmc">PMC6682836</ArticleId><ArticleId IdType="pubmed">31385114</ArticleId></ArticleIdList></Reference><Reference><Citation>Romeo V., Cuocolo R., Apolito R., Stanzione A., Ventimiglia A., Vitale A., Verde F., Accurso A., Amitrano M., Insabato L., et al. Clinical value of radiomics and machine learning in breast ultrasound: A multicenter study for differential diagnosis of benign and malignant lesions. Eur. Radiol. 2021;31:9511&#x2013;9519. doi: 10.1007/s00330-021-08009-2.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-021-08009-2</ArticleId><ArticleId IdType="pmc">PMC8589755</ArticleId><ArticleId IdType="pubmed">34018057</ArticleId></ArticleIdList></Reference><Reference><Citation>Lyu S.-Y., Zhang Y., Zhang M.-W., Zhang B.-S., Gao L.-B., Bai L.-T., Wang J. Diagnostic value of artificial intelligence automatic detection systems for breast BI-RADS 4 nodules. World J. Clin. Cases. 2022;10:518&#x2013;527. doi: 10.12998/wjcc.v10.i2.518.</Citation><ArticleIdList><ArticleId IdType="doi">10.12998/wjcc.v10.i2.518</ArticleId><ArticleId IdType="pmc">PMC8771370</ArticleId><ArticleId IdType="pubmed">35097077</ArticleId></ArticleIdList></Reference><Reference><Citation>Hayashida T., Odani E., Kikuchi M., Nagayama A., Seki T., Takahashi M., Futatsugi N., Matsumoto A., Murata T., Watanuki R., et al. Establishment of a deep-learning system to diagnose BI-RADS4a or higher using breast ultrasound for clinical application. Cancer Sci. 2022;113:3528. doi: 10.1111/cas.15511.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/cas.15511</ArticleId><ArticleId IdType="pmc">PMC9530860</ArticleId><ArticleId IdType="pubmed">35880248</ArticleId></ArticleIdList></Reference><Reference><Citation>Fujioka T., Kubota K., Mori M., Kikuchi Y., Katsuta L., Kasahara M., Oda G., Ishiba T., Nakagawa T., Tateishi U. Distinction between benign and malignant breast masses at breast ultrasound using deep learning method with convolutional neural network. Jpn. J. Radiol. 2019;37:466&#x2013;472. doi: 10.1007/s11604-019-00831-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11604-019-00831-5</ArticleId><ArticleId IdType="pubmed">30888570</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang X.Y., Cui L.G., Feng J., Chen W. Artificial intelligence for breast ultrasound: An adjunct tool to reduce excessive lesion biopsy. Eur. J. Radiol. 2021;138:109624. doi: 10.1016/j.ejrad.2021.109624.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejrad.2021.109624</ArticleId><ArticleId IdType="pubmed">33706046</ArticleId></ArticleIdList></Reference><Reference><Citation>Shen Y., Shamout F.E., Oliver J.R., Witowski J., Kannan K., Park J., Wu N., Huddleston C., Wolfson S., Millet A., et al. Artificial intelligence system reduces pre-positive findings in the interpretation of breast ultrasound exams. Nat. Commun. 2021;12:5645. doi: 10.1038/s41467-021-26023-2.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41467-021-26023-2</ArticleId><ArticleId IdType="pmc">PMC8463596</ArticleId><ArticleId IdType="pubmed">34561440</ArticleId></ArticleIdList></Reference><Reference><Citation>Huo L., Tan Y., Wang S., Geng C., Li Y., Ma X., Wang B., He Y., Yao C., Ouyang T. Machine Learning Models to Improve the Differentiation Between Benign and Malignant Breast Lesions on Ultrasound: A Multicenter External Validation Study. Cancer Manag. Res. 2021;13:3367&#x2013;3379. doi: 10.2147/CMAR.S297794.</Citation><ArticleIdList><ArticleId IdType="doi">10.2147/CMAR.S297794</ArticleId><ArticleId IdType="pmc">PMC8057795</ArticleId><ArticleId IdType="pubmed">33889025</ArticleId></ArticleIdList></Reference><Reference><Citation>Gao Y., Liu B., Zhu Y., Chen L., Tan M., Xiao X., Yu G., Guo Y. Detection and recognition of ultrasound breast nodules based on semi-supervised deep learning: A powerful alternative strategy. Quant. Imaging Med. Surg. 2021;11:2265&#x2013;2278. doi: 10.21037/qims-20-12B.</Citation><ArticleIdList><ArticleId IdType="doi">10.21037/qims-20-12B</ArticleId><ArticleId IdType="pmc">PMC8107344</ArticleId><ArticleId IdType="pubmed">34079700</ArticleId></ArticleIdList></Reference><Reference><Citation>Du R., Chen Y., Li T., Shi L., Fei Z., Li Y. Discrimination of Breast Cancer Based on Ultrasound Images and Convolutional Neural Network. J. Oncol. 2022;2022:7733583. doi: 10.1155/2022/7733583.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2022/7733583</ArticleId><ArticleId IdType="pmc">PMC8957444</ArticleId><ArticleId IdType="pubmed">35345516</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang N., Li X.T., Ma L., Fan Z.Q., Sun Y.S. Application of deep learning to establish a diagnostic model of breast lesions using two-dimensional grayscale ultrasound imaging. Clin. Imaging. 2021;79:56&#x2013;63. doi: 10.1016/j.clinimag.2021.03.024.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.clinimag.2021.03.024</ArticleId><ArticleId IdType="pubmed">33887507</ArticleId></ArticleIdList></Reference><Reference><Citation>Wan K.W., Wong C.H., Ip H.F., Fan D., Yuen P.L., Fong H.Y., Ying M. Evaluation of the performance of traditional machine learning algorithms, convolutional neural network and AutoML Vision in ultrasound breast lesions classification: A comparative study. Quant. Imaging Med. Surg. 2021;11:1381&#x2013;1393. doi: 10.21037/qims-20-922.</Citation><ArticleIdList><ArticleId IdType="doi">10.21037/qims-20-922</ArticleId><ArticleId IdType="pmc">PMC7930687</ArticleId><ArticleId IdType="pubmed">33816176</ArticleId></ArticleIdList></Reference><Reference><Citation>Shia W.C., Lin L.S., Chen D.R. Classification of malignant tumours in breast ultrasound using unsupervised machine learning approaches. Sci. Rep. 2021;11:1418. doi: 10.1038/s41598-021-81008-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-021-81008-x</ArticleId><ArticleId IdType="pmc">PMC7809485</ArticleId><ArticleId IdType="pubmed">33446841</ArticleId></ArticleIdList></Reference><Reference><Citation>Li C., Li J., Tan T., Chen K., Xu Y., Wu R. Application of ultrasonic dual-mode artificially intelligent architecture in assisting radiologists with different diagnostic levels on breast masses classification. Diagn. Interv. Radiol. 2021;27:315&#x2013;322. doi: 10.5152/dir.2021.20018.</Citation><ArticleIdList><ArticleId IdType="doi">10.5152/dir.2021.20018</ArticleId><ArticleId IdType="pmc">PMC8136533</ArticleId><ArticleId IdType="pubmed">34003119</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang X., Liang M., Yang Z., Zheng C., Wu J., Ou B., Li H., Wu X., Luo B., Shen J. Deep Learning-Based Radiomics of B-Mode Ultrasonography and Shear-Wave Elastography: Improved Performance in Breast Mass Classification. Front. Oncol. 2020;10:1621. doi: 10.3389/fonc.2020.01621.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fonc.2020.01621</ArticleId><ArticleId IdType="pmc">PMC7485397</ArticleId><ArticleId IdType="pubmed">32984032</ArticleId></ArticleIdList></Reference><Reference><Citation>Tanaka H., Chiu S.W., Watanabe T., Kaoku S., Yamaguchi T. Computer-aided diagnosis system for breast ultrasound images using deep learning. Phys. Med. Biol. 2019;64:235013. doi: 10.1088/1361-6560/ab5093.</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/1361-6560/ab5093</ArticleId><ArticleId IdType="pubmed">31645021</ArticleId></ArticleIdList></Reference><Reference><Citation>Qi X., Zhang L., Chen Y., Pi Y., Chen Y., Lv Q., Yi Z. Automated diagnosis of breast ultrasonography images using deep neural networks. Med. Image Anal. 2019;52:185&#x2013;198. doi: 10.1016/j.media.2018.12.006.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2018.12.006</ArticleId><ArticleId IdType="pubmed">30594771</ArticleId></ArticleIdList></Reference><Reference><Citation>Byra M., Galperin M., Ojeda-Fournier H., Olson L., O&#x2019;Boyle M., Comstock C., Andre M. Breast mass classification in sonography with transfer learning using a deep convolutional neural network and color conversion. Med. Phys. 2019;46:746&#x2013;755. doi: 10.1002/mp.13361.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mp.13361</ArticleId><ArticleId IdType="pmc">PMC8544811</ArticleId><ArticleId IdType="pubmed">30589947</ArticleId></ArticleIdList></Reference><Reference><Citation>Xiao T., Liu L., Li K., Qin W., Yu S., Li Z. Comparison of Transferred Deep Neural Networks in Ultrasonic Breast Masses Discrimination. BioMed Res. Int. 2018;2018:4605191. doi: 10.1155/2018/4605191.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2018/4605191</ArticleId><ArticleId IdType="pmc">PMC6033250</ArticleId><ArticleId IdType="pubmed">30035122</ArticleId></ArticleIdList></Reference><Reference><Citation>Sultan L.R., Schultz S.M., Cary T.W., Sehgal C.M. Machine learning to improve breast cancer diagnosis by multimodal ultrasound; Proceedings of the 2018 IEEE International Ultrasonics Symposium (IUS); Kobe, Japan. 22&#x2013;25 October 2018.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8293293</ArticleId><ArticleId IdType="pubmed">34295453</ArticleId></ArticleIdList></Reference><Reference><Citation>Becker A.S., Mueller M., Stoffel E., Marcon M., Ghafoor S., Boss A. Classification of breast cancer in ultrasound imaging using a generic deep learning analysis software: A pilot study. Br. J. Radiol. 2018;91:20170576. doi: 10.1259/bjr.20170576.</Citation><ArticleIdList><ArticleId IdType="doi">10.1259/bjr.20170576</ArticleId><ArticleId IdType="pmc">PMC5965470</ArticleId><ArticleId IdType="pubmed">29215311</ArticleId></ArticleIdList></Reference><Reference><Citation>Gu Y., Xu W., Bin Lin B., An X., Tian J., Ran H., Ren W., Chang C., Yuan J., Kang C., et al. Deep learning based on ultrasound images assists breast lesion diagnosis in China: A multicenter diagnostic study. Insights Imaging. 2022;13:124. doi: 10.1186/s13244-022-01259-8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s13244-022-01259-8</ArticleId><ArticleId IdType="pmc">PMC9334487</ArticleId><ArticleId IdType="pubmed">35900608</ArticleId></ArticleIdList></Reference><Reference><Citation>Wei Q., Yan Y.-J., Wu G.-G., Ye X.-R., Jiang F., Liu J., Wang G., Wang Y., Song J., Pan Z.-P., et al. The diagnostic performance of ultrasound computer-aided diagnosis system for distinguishing breast masses: A prospective multicenter study. Eur. Radiol. 2022;32:4046&#x2013;4055. doi: 10.1007/s00330-021-08452-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-021-08452-1</ArticleId><ArticleId IdType="pubmed">35066633</ArticleId></ArticleIdList></Reference><Reference><Citation>Wilding R.C., Sheraton V.M., Soto L., Chotai N., Tan E.Y. Deep learning applied to breast imaging classification and segmentation with human expert intervention. J. Ultrasound. 2022;25:659&#x2013;666. doi: 10.1007/s40477-021-00642-3.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s40477-021-00642-3</ArticleId><ArticleId IdType="pmc">PMC9402837</ArticleId><ArticleId IdType="pubmed">35000127</ArticleId></ArticleIdList></Reference><Reference><Citation>O&#x2019;Connell A.M., Bartolotta T.V., Orlando A., Jung S.H., Baek J., Parker K.J.J. Diagnostic Performance of an Artificial Intelligence System in Breast Ultrasound. Ultrasound Med. 2022;41:97&#x2013;105. doi: 10.1002/jum.15684.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jum.15684</ArticleId><ArticleId IdType="pubmed">33665833</ArticleId></ArticleIdList></Reference><Reference><Citation>Ma H., Tian R., Li H., Sun H., Lu G., Liu R., Wang Z. Fus2Net: A novel Convolutional Neural Network for classification of benign and malignant breast tumor in ultrasound images. Biomed. Eng. Online. 2021;20:112. doi: 10.1186/s12938-021-00950-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s12938-021-00950-z</ArticleId><ArticleId IdType="pmc">PMC8600702</ArticleId><ArticleId IdType="pubmed">34794443</ArticleId></ArticleIdList></Reference><Reference><Citation>Chowdhury A., Razzaque R.R., Muhtadi S. Ultrasound classification of breast masses using a comprehensive Nakagami imaging and machine learning framework. Ultrasonics. 2022;124:106744. doi: 10.1016/j.ultras.2022.106744.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ultras.2022.106744</ArticleId><ArticleId IdType="pubmed">35390626</ArticleId></ArticleIdList></Reference><Reference><Citation>Li J., Bu Y., Lu S., Pang H., Luo C., Liu Y., Qian L.J. Development of a Deep Learning-Based Model for Diagnosing Breast Nodules with Ultrasound. Ultrasound Med. 2021;40:513&#x2013;520. doi: 10.1002/jum.15427.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jum.15427</ArticleId><ArticleId IdType="pubmed">32770574</ArticleId></ArticleIdList></Reference><Reference><Citation>Lai Y.C., Chen H.H., Hsu J.F., Hong Y.J., Chiu T.T., Chiou H.J. Evaluation of physician performance using a concurrent-read artificial intelligence system to support breast ultrasound interpretation. Breast. 2022;65:124&#x2013;135. doi: 10.1016/j.breast.2022.07.009.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.breast.2022.07.009</ArticleId><ArticleId IdType="pmc">PMC9379669</ArticleId><ArticleId IdType="pubmed">35944352</ArticleId></ArticleIdList></Reference><Reference><Citation>Li Y., Gu H., Wang H., Qin P., Wang J. BUSnet: A Deep Learning Model of Breast Tumor Lesion Detection for Ultrasound Images. Front. Oncol. 2022;12:848271. doi: 10.3389/fonc.2022.848271.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fonc.2022.848271</ArticleId><ArticleId IdType="pmc">PMC8989926</ArticleId><ArticleId IdType="pubmed">35402269</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang H., Han L., Chen K., Peng Y., Lin J. Diagnostic Efficiency of the Breast Ultrasound Computer-Aided Prediction Model Based on Convolutional Neural Network in Breast Cancer. J. Digit. Imaging. 2020;33:1218&#x2013;1223. doi: 10.1007/s10278-020-00357-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10278-020-00357-7</ArticleId><ArticleId IdType="pmc">PMC7572988</ArticleId><ArticleId IdType="pubmed">32519253</ArticleId></ArticleIdList></Reference><Reference><Citation>Ye H., Hang J., Zhang M. Automatic identification of triple negative breast cancer in ultrasonography using a deep convolutional neural network. Sci. Rep. 2021;11:20474. doi: 10.1038/s41598-021-00018-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-021-00018-x</ArticleId><ArticleId IdType="pmc">PMC8517009</ArticleId><ArticleId IdType="pubmed">34650065</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang X., Li H., Wang C., Cheng W., Zhu Y., Li D., Jing H., Li S., Hou J., Li J., et al. Evaluating the Accuracy of Breast Cancer and Molecular Subtype Diagnosis by Ultrasound Image Deep Learning Model. Front. Oncol. 2021;11:623506. doi: 10.3389/fonc.2021.623506.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fonc.2021.623506</ArticleId><ArticleId IdType="pmc">PMC7973262</ArticleId><ArticleId IdType="pubmed">33747937</ArticleId></ArticleIdList></Reference><Reference><Citation>Xu Z., Yang Q., Li M., Gu J., Du C., Chen Y., Li B. Predicting HER2 Status in Breast Cancer on Ultrasound Images Using Deep Learning Method. Front. Oncol. 2022;12:829041. doi: 10.3389/fonc.2022.829041.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fonc.2022.829041</ArticleId><ArticleId IdType="pmc">PMC8889619</ArticleId><ArticleId IdType="pubmed">35251999</ArticleId></ArticleIdList></Reference><Reference><Citation>Wu T., Sultan L.R., Tian J., Cary T.W., Sehgal C.M. Machine learning for diagnostic ultrasound of triple-negative breast cancer. Breast Cancer Res. Treat. 2019;173:365&#x2013;373. doi: 10.1007/s10549-018-4984-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10549-018-4984-7</ArticleId><ArticleId IdType="pubmed">30343454</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhou B.-Y., Wang L.-F., Yin H.-H., Wu T.-F., Ren T.-T., Peng C., Li D.-X., Shi H., Sun L.-P., Zhao C.-K., et al. Decoding the molecular subtypes of breast cancer seen on multimodal ultrasound images using an assembled convolutional neural network model: A prospective and multicentre study. EBioMedicine. 2021;74:103684. doi: 10.1016/j.ebiom.2021.103684.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ebiom.2021.103684</ArticleId><ArticleId IdType="pmc">PMC8599999</ArticleId><ArticleId IdType="pubmed">34773890</ArticleId></ArticleIdList></Reference><Reference><Citation>Ma M., Liu R., Wen C., Xu W., Xu Z., Wang S., Wu J., Pan D., Zheng B., Qin G., et al. Predicting the molecular subtype of breast cancer and identifying interpretable imaging features using machine learning algorithms. Eur. Radiol. 2022;32:1652&#x2013;1662. doi: 10.1007/s00330-021-08271-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-021-08271-4</ArticleId><ArticleId IdType="pubmed">34647174</ArticleId></ArticleIdList></Reference><Reference><Citation>Guo Y., Hu Y., Qiao M., Wang Y., Yu J., Li J., Chang C. Radiomics Analysis on Ultrasound for Prediction of Biologic Behavior in Breast Invasive Ductal Carcinoma. Clin. Breast Cancer. 2018;18:e335&#x2013;e344. doi: 10.1016/j.clbc.2017.08.002.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.clbc.2017.08.002</ArticleId><ArticleId IdType="pubmed">28890183</ArticleId></ArticleIdList></Reference><Reference><Citation>Jiang M., Zhang D., Tang S.C., Luo X.M., Chuan Z.R., Lv W.Z., Jiang F., Ni X.J., Cui X.W., Dietrich C.F. Deep learning with convolutional neural network in the assessment of breast cancer molecular subtypes based on US images: A multicenter retrospective study. Eur. Radiol. 2021;31:3673&#x2013;3682. doi: 10.1007/s00330-020-07544-8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-020-07544-8</ArticleId><ArticleId IdType="pubmed">33226454</ArticleId></ArticleIdList></Reference><Reference><Citation>Zheng X., Yao Z., Huang Y., Yu Y., Wang Y., Liu Y., Mao R., Li F., Xiao Y., Wang Y., et al. Deep learning radiomics can predict axillary lymph node status in early-stage breast cancer. Nat. Commun. 2020;11:1236. doi: 10.1038/s41467-020-15027-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41467-020-15027-z</ArticleId><ArticleId IdType="pmc">PMC7060275</ArticleId><ArticleId IdType="pubmed">32144248</ArticleId></ArticleIdList></Reference><Reference><Citation>Jiang M., Li C.-L., Luo X.-M., Chuan Z.-R., Chen R.-X., Tang S.-C., Lv W.-Z., Cui X.-W., Dietrich C.F. Radiomics model based on shear-wave elastography in the assessment of axillary lymph node status in early-stage breast cancer. Eur. Radiol. 2022;32:2313&#x2013;2325. doi: 10.1007/s00330-021-08330-w.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-021-08330-w</ArticleId><ArticleId IdType="pubmed">34671832</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhou L.-Q., Wu X.-L., Huang S.-Y., Wu G.-G., Ye H.-R., Wei Q., Bao L.-Y., Deng Y.-B., Li X.-R., Cui X.-W., et al. Lymph Node Metastasis Prediction from Primary Breast Cancer US Images Using Deep Learning. Radiology. 2020;294:19&#x2013;28. doi: 10.1148/radiol.2019190372.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2019190372</ArticleId><ArticleId IdType="pubmed">31746687</ArticleId></ArticleIdList></Reference><Reference><Citation>Tahmasebi A., Qu E., Sevrukov A., Liu J.-B., Wang S., Lyshchik A., Yu J., Eisenbrey J.R. Assessment of Axillary Lymph Nodes for Metastasis on Ultrasound Using Artificial Intelligence. Ultrason. Imaging. 2021;43:329&#x2013;336. doi: 10.1177/01617346211035315.</Citation><ArticleIdList><ArticleId IdType="doi">10.1177/01617346211035315</ArticleId><ArticleId IdType="pubmed">34416827</ArticleId></ArticleIdList></Reference><Reference><Citation>Ozaki J., Fujioka T., Yamaga E., Hayashi A., Kujiraoka Y., Imokawa T., Takahashi K., Okawa S., Yashima Y., Mori M., et al. Deep learning method with a convolutional neural network for image classification of normal and metastatic axillary lymph nodes on breast ultrasonography. Jpn. J. Radiol. 2022;40:814&#x2013;822. doi: 10.1007/s11604-022-01261-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11604-022-01261-6</ArticleId><ArticleId IdType="pubmed">35284996</ArticleId></ArticleIdList></Reference><Reference><Citation>Guo X., Liu Z., Sun C., Zhang L., Wang Y., Li Z., Shi J., Wu T., Cui H., Zhang J., et al. Deep learning radiomics of ultrasonography: Identifying the risk of axillary non-sentinel lymph node involvement in primary breast cancer. EBioMedicine. 2020;60:103018. doi: 10.1016/j.ebiom.2020.103018.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ebiom.2020.103018</ArticleId><ArticleId IdType="pmc">PMC7519251</ArticleId><ArticleId IdType="pubmed">32980697</ArticleId></ArticleIdList></Reference><Reference><Citation>Lee Y.W., Huang C.S., Shih C.C., Chang R.F. Axillary lymph node metastasis status prediction of early-stage breast cancer using convolutional neural networks. Comput. Biol. Med. 2021;130:104206. doi: 10.1016/j.compbiomed.2020.104206.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2020.104206</ArticleId><ArticleId IdType="pubmed">33421823</ArticleId></ArticleIdList></Reference><Reference><Citation>Sun S., Mutasa S., Liu M.Z., Nemer J., Sun M., Siddique M., Desperito E., Jambawalikar S., Ha R.S. Deep learning prediction of axillary lymph node status using ultrasound images. Comput. Biol. Med. 2022;143:105250. doi: 10.1016/j.compbiomed.2022.105250.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2022.105250</ArticleId><ArticleId IdType="pubmed">35114444</ArticleId></ArticleIdList></Reference><Reference><Citation>DiCenzo D., Quiaoit K., Fatima K., Bhardwaj D., Sannachi L., Gangeh M., Sadeghi-Naini A., Dasgupta A., Kolios M.C., Trudeau M., et al. Quantitative ultrasound radiomics in predicting response to neoadjuvant chemotherapy in patients with locally advanced breast cancer: Results from multi-institutional study. Cancer Med. 2020;9:5798&#x2013;5806. doi: 10.1002/cam4.3255.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/cam4.3255</ArticleId><ArticleId IdType="pmc">PMC7433820</ArticleId><ArticleId IdType="pubmed">32602222</ArticleId></ArticleIdList></Reference><Reference><Citation>Jiang M., Li C.-L., Luo X.-M., Chuan Z.-R., Lv W.-Z., Li X., Cui X.-W., Dietrich C.F. Ultrasound-based deep learning radiomics in the assessment of pathological complete response to neoadjuvant chemotherapy in locally advanced breast cancer. Eur. J. Cancer. 2021;147:95&#x2013;105. doi: 10.1016/j.ejca.2021.01.028.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejca.2021.01.028</ArticleId><ArticleId IdType="pubmed">33639324</ArticleId></ArticleIdList></Reference><Reference><Citation>Gu J., Tong T., He C., Xu M., Yang X., Tian J., Jiang T., Wang K. Deep learning radiomics of ultrasonography can predict response to neoadjuvant chemotherapy in breast cancer at an early stage of treatment: A prospective study. Eur. Radiol. 2022;32:2099&#x2013;2109. doi: 10.1007/s00330-021-08293-y.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-021-08293-y</ArticleId><ArticleId IdType="pubmed">34654965</ArticleId></ArticleIdList></Reference><Reference><Citation>Taleghamar H., Jalalifar S.A., Czarnota G.J., Sadeghi-Naini A. Deep learning of quantitative ultrasound multi-parametric images at pre-treatment to predict breast cancer response to chemotherapy. Sci. Rep. 2022;12:2244. doi: 10.1038/s41598-022-06100-2.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-022-06100-2</ArticleId><ArticleId IdType="pmc">PMC8831592</ArticleId><ArticleId IdType="pubmed">35145158</ArticleId></ArticleIdList></Reference><Reference><Citation>Xie J., Shi H., Du C., Song X., Wei J., Dong Q., Wan C. Dual-Branch Convolutional Neural Network Based on Ultrasound Imaging in the Early Prediction of Neoadjuvant Chemotherapy Response in Patients with Locally Advanced Breast Cancer. Front. Oncol. 2022;12:812463. doi: 10.3389/fonc.2022.812463.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fonc.2022.812463</ArticleId><ArticleId IdType="pmc">PMC9026439</ArticleId><ArticleId IdType="pubmed">35463368</ArticleId></ArticleIdList></Reference><Reference><Citation>Byra M., Dobruch-Sobczak K., Klimonda Z., Piotrzkowska-Wroblewska H., Litniewski J. Early Prediction of Response to Neoadjuvant Chemotherapy in Breast Cancer Sonography Using Siamese Convolutional Neural Networks. IEEE J. Biomed. Health Inform. 2021;25:797&#x2013;805. doi: 10.1109/JBHI.2020.3008040.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/JBHI.2020.3008040</ArticleId><ArticleId IdType="pubmed">32749986</ArticleId></ArticleIdList></Reference><Reference><Citation>D&#x2019;Orsi C., Sickles E., Mendelson E., Morris E. ACR BI-RADS&#xae; Atlas, Breast Imaging Reporting and Data System. 5th ed. American College of Radiology; Reston, VA, USA: 2013.</Citation></Reference><Reference><Citation>Hosny A., Parmar C., Quackenbush J., Schwartz L.H., Aerts H. Artificial intelligence in radiology. Nat. Rev. Cancer. 2018;18:500&#x2013;510. doi: 10.1038/s41568-018-0016-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41568-018-0016-5</ArticleId><ArticleId IdType="pmc">PMC6268174</ArticleId><ArticleId IdType="pubmed">29777175</ArticleId></ArticleIdList></Reference><Reference><Citation>Fujioka T., Mori M., Kubota K., Oyama J., Yamaga E., Yashima Y., Katsuta L., Nomura K., Nara M., Oda G., et al. The Utility of Deep Learning in Breast Ultrasonic Imaging: A Review. Diagnostics. 2020;10:1055. doi: 10.3390/diagnostics10121055.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/diagnostics10121055</ArticleId><ArticleId IdType="pmc">PMC7762151</ArticleId><ArticleId IdType="pubmed">33291266</ArticleId></ArticleIdList></Reference><Reference><Citation>Wu G.-G., Zhou L.-Q., Xu J.-W., Wang J.-Y., Wei Q., Deng Y.-B., Cui X.-W., Dietrich C.F. Artificial intelligence in breast ultrasound. World J. Radiol. 2019;11:19&#x2013;26. doi: 10.4329/wjr.v11.i2.19.</Citation><ArticleIdList><ArticleId IdType="doi">10.4329/wjr.v11.i2.19</ArticleId><ArticleId IdType="pmc">PMC6403465</ArticleId><ArticleId IdType="pubmed">30858931</ArticleId></ArticleIdList></Reference><Reference><Citation>Prat A., Pineda E., Adamo B., Galv&#xe1;n P., Fern&#xe1;ndez A., Gaba L., D&#xed;ez M., Viladot M., Arance A., Mu&#xf1;oz M. Clinical implications of the intrinsic molecular subtypes of breast cancer. Breast. 2015;24((Suppl. 2)):S26&#x2013;S35. doi: 10.1016/j.breast.2015.07.008.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.breast.2015.07.008</ArticleId><ArticleId IdType="pubmed">26253814</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen X.S., Wu J.Y., Huang O.U., Chen C.M., Wu J., Lu J.S., Shao Z.M., Shen Z.Z., Shen K.W. Molecular subtype can predict the response and outcome of Chinese locally advanced breast cancer patients treated with preoperative therapy. Oncol. Rep. 2010;23:1213&#x2013;1220.</Citation><ArticleIdList><ArticleId IdType="pubmed">20372832</ArticleId></ArticleIdList></Reference><Reference><Citation>Schueller G., Jaromi S., Ponhold L., Fuchsjaeger M., Memarsadeghi M., Rudas M., Weber M., Liberman L., Helbich T.H. US-guided 14-gauge core-needle breast biopsy: Results of a validation study in 1352 cases. Radiology. 2008;248:406&#x2013;413. doi: 10.1148/radiol.2482071994.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2482071994</ArticleId><ArticleId IdType="pubmed">18641246</ArticleId></ArticleIdList></Reference><Reference><Citation>Costantini M., Belli P., Bufi E., Asunis A.M., Ferra E., Bitti G.T. Association between sonographic appearances of breast cancers and their histopathologic features and biomarkers. J. Clin. Ultrasound. 2016;44:26&#x2013;33. doi: 10.1002/jcu.22312.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jcu.22312</ArticleId><ArticleId IdType="pubmed">26402438</ArticleId></ArticleIdList></Reference><Reference><Citation>Fujii T., Yajima R., Tatsuki H., Suto T., Morita H., Tsutsumi S., Kuwano H. Significance of lymphatic invasion combined with size of primary tumor for predicting sentinel lymph node metastasis in patients with breast cancer. Anticancer Res. 2015;35:3581&#x2013;3584.</Citation><ArticleIdList><ArticleId IdType="pubmed">26026130</ArticleId></ArticleIdList></Reference><Reference><Citation>De Boer M., Van Deurzen C.H., Van Dijck J.A., Borm G.F., Van Diest P.J., Adang E.M., Nortier J.W., Rutgers E.J., Seynaeve C., Menke-Pluymers M.B., et al. Micrometastases or isolated tumor cells and the outcome of breast cancer. N. Engl. J. Med. 2009;361:653&#x2013;663. doi: 10.1056/NEJMoa0904832.</Citation><ArticleIdList><ArticleId IdType="doi">10.1056/NEJMoa0904832</ArticleId><ArticleId IdType="pubmed">19675329</ArticleId></ArticleIdList></Reference><Reference><Citation> [(accessed on 29 November 2022)].  Available online:  https://old-prod.asco.org/practice-patients/guidelines/breast-cancer.</Citation></Reference><Reference><Citation> [(accessed on 29 November 2022)].  Available online:  https://www.esmo.org/guidelines/guidelines-by-topic/breast-cancer.</Citation></Reference><Reference><Citation>Hortobagyi G.N. Comprehensive management of locally advanced breast cancer. Cancer. 1990;66((Suppl. 6)):1387&#x2013;1391. doi: 10.1002/1097-0142(19900915)66:14+&lt;1387::AID-CNCR2820661414&gt;3.0.CO;2-I.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/1097-0142(19900915)66:14+&lt;1387::AID-CNCR2820661414&gt;3.0.CO;2-I</ArticleId><ArticleId IdType="pubmed">2205369</ArticleId></ArticleIdList></Reference><Reference><Citation>Lyman G.H., Somerfield M.R., Giuliano A.E. Sentinel Lymph Node Biopsy for Patients with Early-Stage Breast Cancer: 2016 American Society of Clinical Oncology Clinical Practice Guideline Update Summary. J. Oncol. Pract. 2017;13:196&#x2013;198. doi: 10.1200/JOP.2016.019992.</Citation><ArticleIdList><ArticleId IdType="doi">10.1200/JOP.2016.019992</ArticleId><ArticleId IdType="pubmed">28118104</ArticleId></ArticleIdList></Reference><Reference><Citation>Qian L., Lv Z., Zhang K., Wang K., Zhu Q., Zhou S., Chang C., Tian J. Application of deep learning to predict underestimation in ductal carcinoma in situ of the breast with ultrasound. Ann. Transl. Med. 2021;9:295. doi: 10.21037/atm-20-3981.</Citation><ArticleIdList><ArticleId IdType="doi">10.21037/atm-20-3981</ArticleId><ArticleId IdType="pmc">PMC7944276</ArticleId><ArticleId IdType="pubmed">33708922</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang H., Li X., Yuan Y., Tong Y., Zhu S., Huang R., Shen K., Guo Y., Wang Y., Chen X. Association of machine learning ultrasound radiomics and disease outcome in triple negative breast cancer. Am. J. Cancer Res. 2022;12:152&#x2013;164.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8822271</ArticleId><ArticleId IdType="pubmed">35141010</ArticleId></ArticleIdList></Reference><Reference><Citation>Yu F., Hang J., Deng J., Yang B., Wang J., Ye X., Liu Y. Radiomics features on ultrasound imaging for the prediction of disease-free survival in triple negative breast cancer: A multi-institutional study. Br. J. Radiol. 2021;94:20210188. doi: 10.1259/bjr.20210188.</Citation><ArticleIdList><ArticleId IdType="doi">10.1259/bjr.20210188</ArticleId><ArticleId IdType="pmc">PMC9328043</ArticleId><ArticleId IdType="pubmed">34478336</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36610215</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>07</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1879-0534</ISSN><JournalIssue CitedMedium="Internet"><Volume>153</Volume><PubDate><Year>2023</Year><Month>Jan</Month><Day>02</Day></PubDate></JournalIssue><Title>Computers in biology and medicine</Title><ISOAbbreviation>Comput Biol Med</ISOAbbreviation></Journal><ArticleTitle>3D carotid artery segmentation using shape-constrained active contours.</ArticleTitle><Pagination><StartPage>106530</StartPage><MedlinePgn>106530</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1016/j.compbiomed.2022.106530</ELocationID><ELocationID EIdType="pii" ValidYN="Y">S0010-4825(22)01238-0</ELocationID><Abstract><AbstractText>Reconstruction of the carotid artery is demanded in the detection and characterization of atherosclerosis. This study proposes a shape-constrained active contour model for segmenting the carotid artery from MR images, which embeds the output of the deep learning network into the active contour. First the centerline of the carotid artery is localized and then modified active contour initialized from the centerline is used to extract the vessel lumen, finally the probability atlas generated by the deep learning network in polar representation domain is integrated into the active contour as a prior information to detect the outer wall. The results showed that the proposed active contour model was efficient and comparable to manual segmentation.</AbstractText><CopyrightInformation>Copyright &#xa9; 2023 Elsevier Ltd. All rights reserved.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Huang</LastName><ForeName>Xianjue</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>School of Biological Science and Medical Engineering, Southeast University, Nanjing, 210096, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wang</LastName><ForeName>Jun</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>First Affiliated Hospital, Nanjing Medical University, Nanjing, 210029, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Li</LastName><ForeName>Zhiyong</ForeName><Initials>Z</Initials><AffiliationInfo><Affiliation>School of Biological Science and Medical Engineering, Southeast University, Nanjing, 210096, China; School of Mechanical, Medical and Process Engineering, Queensland University of Technology, Brisbane, 4000, Australia; Faculty of Sports Science, Ningbo University, Ningbo, 315211, China. Electronic address: zylicam@gmail.com.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>02</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>Comput Biol Med</MedlineTA><NlmUniqueID>1250250</NlmUniqueID><ISSNLinking>0010-4825</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Active contours</Keyword><Keyword MajorTopicYN="N">Carotid artery segmentation</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Level set method</Keyword><Keyword MajorTopicYN="N">Magnetic resonance imaging</Keyword><Keyword MajorTopicYN="N">Vessel segmentation</Keyword></KeywordList><CoiStatement>Declaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>8</Month><Day>11</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>12</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>31</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>18</Hour><Minute>8</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36610215</ArticleId><ArticleId IdType="doi">10.1016/j.compbiomed.2022.106530</ArticleId><ArticleId IdType="pii">S0010-4825(22)01238-0</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36609662</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>07</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1826-6983</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>07</Day></PubDate></JournalIssue><Title>La Radiologia medica</Title><ISOAbbreviation>Radiol Med</ISOAbbreviation></Journal><ArticleTitle>Application of deep learning-based super-resolution to T1-weighted postcontrast gradient echo imaging of the chest.</ArticleTitle><ELocationID EIdType="doi" ValidYN="Y">10.1007/s11547-022-01587-1</ELocationID><Abstract><AbstractText Label="OBJECTIVES" NlmCategory="OBJECTIVE">A deep learning-based super-resolution for postcontrast volume-interpolated breath-hold examination (VIBE) of the chest was investigated in this study. Aim was to improve image quality, noise, artifacts and diagnostic confidence without change of acquisition parameters.</AbstractText><AbstractText Label="MATERIALS AND METHODS" NlmCategory="METHODS">Fifty patients who received VIBE postcontrast imaging of the chest at 1.5&#xa0;T were included in this retrospective study. After acquisition of the standard VIBE (VIBE<sub>S</sub>), a novel deep learning-based algorithm and a denoising algorithm were applied, resulting in enhanced images (VIBE<sub>DL</sub>). Two radiologists qualitatively evaluated both datasets independently, rating sharpness of soft tissue, vessels, bronchial structures, lymph nodes, artifacts, cardiac motion artifacts, noise levels and overall diagnostic confidence, using a Likert scale ranging from 1 to 4. In the presence of lung lesions, the largest lesion was rated regarding sharpness and diagnostic confidence using the same Likert scale as mentioned above. Additionally, the largest diameter of the lesion was measured.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">The sharpness of soft tissue, vessels, bronchial structures and lymph nodes as well as the diagnostic confidence, the extent of artifacts, the extent of cardiac motion artifacts and noise levels were rated superior in VIBE<sub>DL</sub> (all P&#x2009;&lt;&#x2009;0.001). There was no significant difference in the diameter or the localization of the largest lung lesion in VIBE<sub>DL</sub> compared to VIBE<sub>S</sub>. Lesion sharpness as well as detectability was rated significantly better by both readers with VIBE<sub>DL</sub> (both P&#x2009;&lt;&#x2009;0.001).</AbstractText><AbstractText Label="CONCLUSION" NlmCategory="CONCLUSIONS">The application of a novel deep learning-based super-resolution approach in T1-weighted VIBE postcontrast imaging resulted in an improvement in image quality, noise levels and diagnostic confidence as well as in a shortened acquisition time.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Maennlin</LastName><ForeName>Simon</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Diagnostic and Interventional Radiology, University Hospital Tuebingen, Hoppe- Seyler- Str. 3, 72076, T&#xfc;bingen, Germany. Simon.Maennlin@med.uni-tuebingen.de.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wessling</LastName><ForeName>Daniel</ForeName><Initials>D</Initials><AffiliationInfo><Affiliation>Diagnostic and Interventional Radiology, University Hospital Tuebingen, Hoppe- Seyler- Str. 3, 72076, T&#xfc;bingen, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Herrmann</LastName><ForeName>Judith</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Diagnostic and Interventional Radiology, University Hospital Tuebingen, Hoppe- Seyler- Str. 3, 72076, T&#xfc;bingen, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Almansour</LastName><ForeName>Haidara</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>Diagnostic and Interventional Radiology, University Hospital Tuebingen, Hoppe- Seyler- Str. 3, 72076, T&#xfc;bingen, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Nickel</LastName><ForeName>Dominik</ForeName><Initials>D</Initials><AffiliationInfo><Affiliation>MR Applications Predevelopment, Siemens Healthcare GmbH, Allee Am Roethelheimpark 2, 91052, Erlangen, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kannengiesser</LastName><ForeName>Stephan</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>MR Applications Predevelopment, Siemens Healthcare GmbH, Allee Am Roethelheimpark 2, 91052, Erlangen, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Afat</LastName><ForeName>Saif</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Diagnostic and Interventional Radiology, University Hospital Tuebingen, Hoppe- Seyler- Str. 3, 72076, T&#xfc;bingen, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Gassenmaier</LastName><ForeName>Sebastian</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Diagnostic and Interventional Radiology, University Hospital Tuebingen, Hoppe- Seyler- Str. 3, 72076, T&#xfc;bingen, Germany.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>07</Day></ArticleDate></Article><MedlineJournalInfo><Country>Italy</Country><MedlineTA>Radiol Med</MedlineTA><NlmUniqueID>0177625</NlmUniqueID><ISSNLinking>0033-8362</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Chest imaging</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Iterative denoising</Keyword><Keyword MajorTopicYN="N">MRI</Keyword><Keyword MajorTopicYN="N">VIBE</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>7</Month><Day>31</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>30</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>16</Hour><Minute>44</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36609662</ArticleId><ArticleId IdType="doi">10.1007/s11547-022-01587-1</ArticleId><ArticleId IdType="pii">10.1007/s11547-022-01587-1</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Yu N, Yang C, Ma G, Dang S, Ren Z, Wang S et al (2020) Feasibility of pulmonary MRI for nodule detection in comparison to computed tomography. BMC Med Imaging 20(1):53</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s12880-020-00451-w</ArticleId></ArticleIdList></Reference><Reference><Citation>Busse A, Rajagopal R, Yucel S, Beller E, Oner A, Streckenbach F et al (2020) Cardiac MRI-update 2020. Radiologe 60(Suppl 1):33&#x2013;40</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00117-020-00687-1</ArticleId></ArticleIdList></Reference><Reference><Citation>Hallifax RJ, Talwar A, Wrightson JM, Edey A, Gleeson FV (2017) State-of-the-art: radiological investigation of pleural disease. Respir Med 124:88&#x2013;99</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.rmed.2017.02.013</ArticleId></ArticleIdList></Reference><Reference><Citation>Huang YS, Niisato E, Su MM, Benkert T, Hsu HH, Shih JY et al (2021) Detecting small pulmonary nodules with spiral ultrashort echo time sequences in 1.5 T MRI. MAGMA 34(3):399&#x2013;409</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10334-020-00885-x</ArticleId></ArticleIdList></Reference><Reference><Citation>Hargreaves BA (2012) Rapid gradient-echo imaging. J Magn Reson Imaging 36(6):1300&#x2013;1313</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmri.23742</ArticleId></ArticleIdList></Reference><Reference><Citation>Frericks BB, Meyer BC, Martus P, Wendt M, Wolf KJ, Wacker F (2008) MRI of the thorax during whole-body MRI: evaluation of different MR sequences and comparison to thoracic multidetector computed tomography (MDCT). J Magn Reson Imaging 27(3):538&#x2013;545</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmri.21218</ArticleId></ArticleIdList></Reference><Reference><Citation>Scholz O, Denecke T, Bottcher J, Schwarz C, Mentzel HJ, Streitparth F et al (2017) MRI of cystic fibrosis lung manifestations: sequence evaluation and clinical outcome analysis. Clin Radiol 72(9):754&#x2013;763</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.crad.2017.03.017</ArticleId></ArticleIdList></Reference><Reference><Citation>Dang S, Gao X, Ma G, Yu N, Han D, Yang Q et al (2019) Combination of free-breathing radial 3D fat-suppressed T1-weighted gradient-echo sequence with diffusion weighted images: potential for differentiating malignant from benign peripheral solid pulmonary masses. Magn Reson Imaging 57:271&#x2013;276</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.mri.2018.12.004</ArticleId></ArticleIdList></Reference><Reference><Citation>Chandarana H, Block TK, Rosenkrantz AB, Lim RP, Kiefer B, Lee VS (2011) Free-breathing radial 3D fat-suppressed T1-weighted gradient echo sequence. Invest Radiol 46:648&#x2013;653</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/RLI.0b013e31821eea45</ArticleId></ArticleIdList></Reference><Reference><Citation>Yang RK, Roth CG, Ward RJ, deJesus JO, Mitchell DG (2010) Optimizing abdominal MR imaging: approaches to common problems. Radiographics 30:185&#x2013;199</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/rg.301095076</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang Y (2000) Description of parallel imaging in MRI using multiple coils. Magn Reson in Med 44:495&#x2013;499</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/1522-2594(200009)44:3&lt;495::AID-MRM23&gt;3.0.CO;2-S</ArticleId></ArticleIdList></Reference><Reference><Citation>Afat S, Wessling D, Afat C, Nickel D, Arberet S, Herrmann J et al (2022) Analysis of a deep learning-based superresolution algorithm tailored to partial fourier gradient echo sequences of the abdomen at 1.5 T: reduction of breath-hold time and improvement of image quality. Invest Radiol 57(3):157&#x2013;62</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/RLI.0000000000000825</ArticleId></ArticleIdList></Reference><Reference><Citation>Gassenmaier S, Afat S, Nickel D, Kannengiesser S, Herrmann J, Hoffmann R et al (2021) Application of a novel iterative denoising and image enhancement technique in T1-weighted precontrast and postcontrast gradient echo imaging of the abdomen: improvement of image quality and diagnostic confidence. Invest Radiol 56(5):328&#x2013;334</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/RLI.0000000000000746</ArticleId></ArticleIdList></Reference><Reference><Citation>Gassenmaier S, Herrmann J, Nickel D, Kannengiesser S, Afat S, Seith F et al (2021) Image quality improvement of dynamic contrast-enhanced gradient echo magnetic resonance imaging by iterative denoising and edge enhancement. Invest Radiol 56(7):465&#x2013;470</Citation></Reference><Reference><Citation>Lee KH, Park CM, Lee SM, Lee JM, Cho JY, Paeng JC et al (2015) Pulmonary nodule detection in patients with a primary malignancy using hybrid PET/MRI: is there value in adding contrast-enhanced MR imaging? PLoS ONE 10(6):e0129660</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0129660</ArticleId></ArticleIdList></Reference><Reference><Citation>Markl M, Leupold J (2012) Radient echo imaging. J Magn Reson Imaging 35:1274&#x2013;1289</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmri.23638</ArticleId></ArticleIdList></Reference><Reference><Citation>Olthof SC, Reinert C, Nikolaou K, Pfannenberg C, Gatidis S, Benkert T et al (2021) Detection of lung lesions in breath-hold VIBE and free-breathing spiral VIBE MRI compared to CT. Insights Imaging 12(1):175</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s13244-021-01124-0</ArticleId></ArticleIdList></Reference><Reference><Citation>Kim KW, Lee JM, Jeon YS, Kang SE, Baek JH, Han JK et al (2013) Free-breathing dynamic contrast-enhanced MRI of the abdomen and chest using a radial gradient echo sequence with K-space weighted image contrast (KWIC). Eur Radiol 23(5):1352&#x2013;1360</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-012-2699-4</ArticleId></ArticleIdList></Reference><Reference><Citation>Feng L, Grimm R, Block KT, Chandarana H, Kim S, Xu J et al (2014) Golden-angle radial sparse parallel MRI: combination of compressed sensing, parallel imaging, and golden-angle radial sampling for fast and flexible dynamic volumetric MRI. Magn Reson Med 72(3):707&#x2013;717</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.24980</ArticleId></ArticleIdList></Reference><Reference><Citation>Richter JAJ, Wech T, Weng AM, Stich M, Weick S, Breuer K et al (2020) Free-breathing self-gated 4D lung MRI using wave-CAIPI. Magn Reson Med 84(6):3223&#x2013;3233</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.28383</ArticleId></ArticleIdList></Reference><Reference><Citation>Gassenmaier S, Afat S, Nickel D, Mostapha M, Herrmann J, Othman AE (2021) Deep learning-accelerated T2-weighted imaging of the prostate: reduction of acquisition time and improvement of image quality. Eur J Radiol 137:109600</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejrad.2021.109600</ArticleId></ArticleIdList></Reference><Reference><Citation>Gassenmaier S, Afat S, Nickel MD, Mostapha M, Herrmann J, Almansour H et al (2021) Accelerated T2-weighted TSE imaging of the prostate using deep learning image reconstruction: a prospective comparison with standard T2-weighted TSE imaging. Cancers (Basel) 13(14):3593</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/cancers13143593</ArticleId></ArticleIdList></Reference><Reference><Citation>Gassenmaier S, Kustner T, Nickel D, Herrmann J, Hoffmann R, Almansour H et al (2021) Deep learning applications in magnetic resonance imaging: has the future become present? Diagnostics (Basel). 11(12):2181</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/diagnostics11122181</ArticleId></ArticleIdList></Reference><Reference><Citation>Herrmann J, Gassenmaier S, Nickel D, Arberet S, Afat S, Lingg A et al (2021) Diagnostic confidence and feasibility of a deep learning accelerated HASTE sequence of the abdomen in a single breath-hold. Invest Radiol 56(5):313&#x2013;319</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/RLI.0000000000000743</ArticleId></ArticleIdList></Reference><Reference><Citation>Koktzoglou I, Huang R, Ankenbrandt WJ, Walker MT, Edelman RR (2021) Super-resolution head and neck MRA using deep machine learning. Magn Reson Med 86(1):335&#x2013;345</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.28738</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang Q, Shen F, Shen L, Huang J, Sheng W (2019) Lung nodule detection in CT images using a raw patch-based convolutional neural network. J Digit Imaging 32(6):971&#x2013;979</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10278-019-00221-3</ArticleId></ArticleIdList></Reference><Reference><Citation>Avendi MR, Kheradvar A, Jafarkhani H (2016) A combined deep-learning and deformable-model approach to fully automatic segmentation of the left ventricle in cardiac MRI. Med Image Anal 30:108&#x2013;119</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2016.01.005</ArticleId></ArticleIdList></Reference><Reference><Citation>Park S, Lee SM, Kim W, Park H, Jung KH, Do KH et al (2021) Computer-aided detection of subsolid nodules at chest CT: improved performance with deep learning-based CT section thickness reduction. Radiology 299(1):211&#x2013;219</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2021203387</ArticleId></ArticleIdList></Reference><Reference><Citation>Kustner T, Munoz C, Psenicny A, Bustin A, Fuin N, Qi H et al (2021) Deep-learning based super-resolution for 3D isotropic coronary MR angiography in less than a minute. Magn Reson Med 86(5):2837&#x2013;2852</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.28911</ArticleId></ArticleIdList></Reference><Reference><Citation>Ciet P, Bertolo S, Ros M, Casciaro R, Cipolli M, Colagrande S et al (2022) State-of-the-art review of lung imaging in cystic fibrosis with recommendations for pulmonologists and radiologists from the "iMAging managEment of cySTic fibROsis" (MAESTRO) consortium. Eur Respir Rev 31(163):210173. https://doi.org/10.1183/16000617.0173-2021</Citation><ArticleIdList><ArticleId IdType="doi">10.1183/16000617.0173-2021</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36609379</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>10</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">1472-6947</ISSN><JournalIssue CitedMedium="Internet"><Volume>23</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>06</Day></PubDate></JournalIssue><Title>BMC medical informatics and decision making</Title><ISOAbbreviation>BMC Med Inform Decis Mak</ISOAbbreviation></Journal><ArticleTitle>Predicting decompression surgery by applying&#xa0;multimodal deep learning to patients' structured and unstructured health data.</ArticleTitle><Pagination><StartPage>2</StartPage><MedlinePgn>2</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">2</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1186/s12911-022-02096-x</ELocationID><Abstract><AbstractText Label="BACKGROUND" NlmCategory="BACKGROUND">Low back pain (LBP) is a common condition made up of a variety of anatomic and clinical subtypes. Lumbar disc herniation (LDH) and lumbar spinal stenosis (LSS) are two subtypes highly associated with LBP. Patients with LDH/LSS are often started with non-surgical treatments and if those are not effective then go on to have decompression surgery. However, recommendation of surgery is complicated as the outcome may depend on the patient's health characteristics. We developed a deep learning (DL) model to predict decompression surgery for patients with LDH/LSS.</AbstractText><AbstractText Label="MATERIALS AND METHOD" NlmCategory="METHODS">We used datasets of 8387 and 8620 patients from a prospective study that collected data from four healthcare systems to predict early (within 2&#xa0;months) and late surgery (within 12&#xa0;months after a 2&#xa0;month gap), respectively. We developed a DL model to use patients' demographics, diagnosis and procedure codes, drug names, and diagnostic imaging reports to predict surgery. For each prediction task, we evaluated the model's performance using classical and generalizability evaluation. For classical evaluation, we split the data into training (80%) and testing (20%). For generalizability evaluation, we split the data based on the healthcare system. We used the area under the curve (AUC) to assess performance for each evaluation. We compared results to a benchmark model (i.e. LASSO logistic regression).</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">For classical performance, the DL model outperformed the benchmark model for early surgery with an AUC of 0.725 compared to 0.597. For late surgery, the DL model outperformed the benchmark model with an AUC of 0.655 compared to 0.635. For generalizability performance, the DL model outperformed the benchmark model for early surgery. For late surgery, the benchmark model outperformed the DL model.</AbstractText><AbstractText Label="CONCLUSIONS" NlmCategory="CONCLUSIONS">For early surgery, the DL model was preferred for classical and generalizability evaluation. However, for late surgery, the benchmark and DL model had comparable performance. Depending on the prediction task, the balance of performance may shift between DL and a conventional ML method. As a result, thorough assessment is needed to quantify the value of DL, a relatively computationally expensive, time-consuming and less interpretable method.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Jujjavarapu</LastName><ForeName>Chethan</ForeName><Initials>C</Initials><AffiliationInfo><Affiliation>Department of Biomedical Informatics and Medical Education, School of Medicine, University of Washington, Box 358047, Seattle, WA, 98195, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Suri</LastName><ForeName>Pradeep</ForeName><Initials>P</Initials><AffiliationInfo><Affiliation>Clinical Learning, Evidence and Research Center, University of Washington, 4333 Brooklyn Ave NE, Seattle, WA, 98105, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Rehabilitation Medicine, University of Washington, 1959 NE Pacific St, Seattle, WA, 98195, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Pejaver</LastName><ForeName>Vikas</ForeName><Initials>V</Initials><AffiliationInfo><Affiliation>Institute for Genomic Health, Icahn School of Medicine at Mount Sinai, New York, NY, 10029, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Genetics and Genomic Sciences, Icahn School of Medicine at Mount Sinai, New York, NY, 10029, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Friedly</LastName><ForeName>Janna</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Clinical Learning, Evidence and Research Center, University of Washington, 4333 Brooklyn Ave NE, Seattle, WA, 98105, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Rehabilitation Medicine, University of Washington, 1959 NE Pacific St, Seattle, WA, 98195, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Gold</LastName><ForeName>Laura S</ForeName><Initials>LS</Initials><AffiliationInfo><Affiliation>Clinical Learning, Evidence and Research Center, University of Washington, 4333 Brooklyn Ave NE, Seattle, WA, 98105, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, University of Washington, 1959 NE Pacific Street, Seattle, WA, 98195, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Meier</LastName><ForeName>Eric</ForeName><Initials>E</Initials><AffiliationInfo><Affiliation>Clinical Learning, Evidence and Research Center, University of Washington, 4333 Brooklyn Ave NE, Seattle, WA, 98105, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Biostatistics, University of Washington, Box 357232, Seattle, WA, 98195-7232, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Center for Biomedical Statistics, University of Washington, Seattle, WA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Cohen</LastName><ForeName>Trevor</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>Department of Biomedical Informatics and Medical Education, School of Medicine, University of Washington, Box 358047, Seattle, WA, 98195, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Mooney</LastName><ForeName>Sean D</ForeName><Initials>SD</Initials><AffiliationInfo><Affiliation>Department of Biomedical Informatics and Medical Education, School of Medicine, University of Washington, Box 358047, Seattle, WA, 98195, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Heagerty</LastName><ForeName>Patrick J</ForeName><Initials>PJ</Initials><AffiliationInfo><Affiliation>Department of Biostatistics, University of Washington, Box 357232, Seattle, WA, 98195-7232, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Center for Biomedical Statistics, University of Washington, Seattle, WA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Jarvik</LastName><ForeName>Jeffrey G</ForeName><Initials>JG</Initials><AffiliationInfo><Affiliation>Clinical Learning, Evidence and Research Center, University of Washington, 4333 Brooklyn Ave NE, Seattle, WA, 98105, USA. jarvikj@uw.edu.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, University of Washington, 1959 NE Pacific Street, Seattle, WA, 98195, USA. jarvikj@uw.edu.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Neurological Surgery, University of Washington, 1959 NE Pacific Street, Seattle, WA, 98195, USA. jarvikj@uw.edu.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Health Services, University of Washington, Box 357660, Seattle, WA, 98195-7660, USA. jarvikj@uw.edu.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><GrantList CompleteYN="Y"><Grant><GrantID>U24AT009676</GrantID><Acronym>NH</Acronym><Agency>NIH HHS</Agency><Country>United States</Country></Grant><Grant><GrantID>UH2AT007766</GrantID><Acronym>AR</Acronym><Agency>NIAMS NIH HHS</Agency><Country>United States</Country></Grant><Grant><GrantID>5UH3AR06679</GrantID><Acronym>NR</Acronym><Agency>NINR NIH HHS</Agency><Country>United States</Country></Grant></GrantList><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>06</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>BMC Med Inform Decis Mak</MedlineTA><NlmUniqueID>101088682</NlmUniqueID><ISSNLinking>1472-6947</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D019299" MajorTopicYN="N">Decompression, Surgical</DescriptorName><QualifierName UI="Q000009" MajorTopicYN="N">adverse effects</QualifierName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D011446" MajorTopicYN="N">Prospective Studies</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D008159" MajorTopicYN="N">Lumbar Vertebrae</DescriptorName><QualifierName UI="Q000601" MajorTopicYN="N">surgery</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D017116" MajorTopicYN="Y">Low Back Pain</DescriptorName><QualifierName UI="Q000175" MajorTopicYN="N">diagnosis</QualifierName><QualifierName UI="Q000601" MajorTopicYN="N">surgery</QualifierName><QualifierName UI="Q000150" MajorTopicYN="N">complications</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D007405" MajorTopicYN="Y">Intervertebral Disc Displacement</DescriptorName><QualifierName UI="Q000601" MajorTopicYN="N">surgery</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D013130" MajorTopicYN="Y">Spinal Stenosis</DescriptorName><QualifierName UI="Q000601" MajorTopicYN="N">surgery</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D016896" MajorTopicYN="N">Treatment Outcome</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D012189" MajorTopicYN="N">Retrospective Studies</DescriptorName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Classification</Keyword><Keyword MajorTopicYN="N">Decompression surgery</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Generalizability</Keyword><Keyword MajorTopicYN="N">Lower back pain</Keyword><Keyword MajorTopicYN="N">Lumbar disc herniation</Keyword><Keyword MajorTopicYN="N">Lumbar spinal stenosis</Keyword><Keyword MajorTopicYN="N">Machine learning</Keyword><Keyword MajorTopicYN="N">Multimodal</Keyword><Keyword MajorTopicYN="N">Prediction</Keyword></KeywordList><CoiStatement>The authors declare that they have no competing interests.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>5</Month><Day>30</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>29</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>16</Hour><Minute>27</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>8</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36609379</ArticleId><ArticleId IdType="pmc">PMC9824905</ArticleId><ArticleId IdType="doi">10.1186/s12911-022-02096-x</ArticleId><ArticleId IdType="pii">10.1186/s12911-022-02096-x</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Wu A, March L, Zheng X, Huang J, Wang X, Zhao J, et al. Global low back pain prevalence and years lived with disability from 1990 to 2017: estimates from the Global Burden of Disease Study 2017. Ann Transl Medicine. 2020;8(6):299.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7186678</ArticleId><ArticleId IdType="pubmed">32355743</ArticleId></ArticleIdList></Reference><Reference><Citation>Martin BI, Deyo RA, Mirza SK, Turner JA, Comstock BA, Hollingworth W, et al. Expenditures and health status among adults with back and neck problems. JAMA. 2008;299(6):656&#x2013;664.</Citation><ArticleIdList><ArticleId IdType="pubmed">18270354</ArticleId></ArticleIdList></Reference><Reference><Citation>Andersson GB. Epidemiological features of chronic low-back pain. Lancet. 1999;354(9178):581&#x2013;585.</Citation><ArticleIdList><ArticleId IdType="pubmed">10470716</ArticleId></ArticleIdList></Reference><Reference><Citation>Urits I, Burshtein A, Sharma M, Testa L, Gold PA, Orhurhu V, et al. Low back pain, a comprehensive review: pathophysiology, diagnosis, and treatment. Curr Pain Headache R. 2019;23(3):23.</Citation><ArticleIdList><ArticleId IdType="pubmed">30854609</ArticleId></ArticleIdList></Reference><Reference><Citation>Deyo RA, Dworkin SF, Amtmann D, Andersson G, Borenstein D, Carragee E, et al. Report of the NIH task force on research standards for chronic low back pain. J Pain. 2014;15(6):569&#x2013;585.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4128347</ArticleId><ArticleId IdType="pubmed">24787228</ArticleId></ArticleIdList></Reference><Reference><Citation>Dunne L, Murphy E, Rutledge R. &#x201c;Semenly&#x201d; harmless back pain: An unusual presentation of a subcutaneous abscess. Irish Med J. 2019;112(1):857.</Citation><ArticleIdList><ArticleId IdType="pubmed">30719898</ArticleId></ArticleIdList></Reference><Reference><Citation>Amin RM, Andrade NS, Neuman BJ. Lumbar disc herniation. Curr Rev Musculoskelet Medicine. 2017;10(4):507&#x2013;516.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5685963</ArticleId><ArticleId IdType="pubmed">28980275</ArticleId></ArticleIdList></Reference><Reference><Citation>Jarvik JJ, Hollingworth W, Heagerty P, Haynor DR, Deyo RA. The longitudinal assessment of imaging and disability of the back (LAIDBack) study: baseline data. Spine. 2001;26(10):1158&#x2013;1166.</Citation><ArticleIdList><ArticleId IdType="pubmed">11413431</ArticleId></ArticleIdList></Reference><Reference><Citation>Deyo RA, Mirza SK. Herniated lumbar intervertebral disk. New Engl J Medicine. 2016;374(18):1763&#x2013;1772.</Citation><ArticleIdList><ArticleId IdType="pubmed">27144851</ArticleId></ArticleIdList></Reference><Reference><Citation>Genevay S, Atlas SJ. Lumbar spinal stenosis. Best Pract Res Clin Rheumatology. 2010;24(2):253&#x2013;265.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC2841052</ArticleId><ArticleId IdType="pubmed">20227646</ArticleId></ArticleIdList></Reference><Reference><Citation>Katz JN, Harris MB. Lumbar spinal stenosis. New Engl J Med. 2008;358(8):818&#x2013;825.</Citation><ArticleIdList><ArticleId IdType="pubmed">18287604</ArticleId></ArticleIdList></Reference><Reference><Citation>Mannion AF, Dvorak J, M&#xfc;ntener M, Grob D. A prospective study of the interrelationship between subjective and objective measures of disability before and 2 months after lumbar decompression surgery for disc herniation. Eur Spine J. 2005;14(5):454&#x2013;465.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC3454659</ArticleId><ArticleId IdType="pubmed">15830214</ArticleId></ArticleIdList></Reference><Reference><Citation>Machado GC, Ferreira PH, Harris IA, Pinheiro MB, Koes BW, van Tulder M, et al. Effectiveness of surgery for lumbar spinal stenosis: a systematic review and meta-analysis. PLoS ONE. 2015;10(3):e0122800.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4378944</ArticleId><ArticleId IdType="pubmed">25822730</ArticleId></ArticleIdList></Reference><Reference><Citation>Peul WC, van Houwelingen HC, van den Hout WB, Brand R, Eekhof JAH, Tans JTJ, et al. Surgery versus prolonged conservative treatment for sciatica. New Engl J Medicine. 2007;356(22):2245&#x2013;2256.</Citation><ArticleIdList><ArticleId IdType="pubmed">17538084</ArticleId></ArticleIdList></Reference><Reference><Citation>Peul WC, Hout WB van den, Brand R, Thomeer RTWM, Koes BW, Group LTHSIPS. Prolonged conservative care versus early surgery in patients with sciatica caused by lumbar disc herniation: two year results of a randomised controlled trial. Bmj. 2008;336(7657):1355&#x2013;8.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC2427077</ArticleId><ArticleId IdType="pubmed">18502911</ArticleId></ArticleIdList></Reference><Reference><Citation>Malmivaara A, Sl&#xe4;tis P, Heli&#xf6;vaara M, Sainio P, Kinnunen H, Kankare J, et al. Surgical or nonoperative treatment for lumbar spinal stenosis? Spine. 2007;32(1):1&#x2013;8.</Citation><ArticleIdList><ArticleId IdType="pubmed">17202885</ArticleId></ArticleIdList></Reference><Reference><Citation>Weinstein JN, Lurie JD, Tosteson TD, Tosteson ANA, Blood EA, Abdu WA, et al. Surgical versus nonoperative treatment for lumbar disc herniation. Spine. 2008;33(25):2789&#x2013;2800.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC2756172</ArticleId><ArticleId IdType="pubmed">19018250</ArticleId></ArticleIdList></Reference><Reference><Citation>Kovacs FM, Urr&#xfa;tia G, Alarc&#xf3;n JD. Surgery versus conservative treatment for symptomatic lumbar spinal stenosis. Spine. 2011;36(20):E1335&#x2013;E1351.</Citation><ArticleIdList><ArticleId IdType="pubmed">21311394</ArticleId></ArticleIdList></Reference><Reference><Citation>Nerland US, Jakola AS, Giannadakis C, Solheim O, Weber C, Nygaard &#xd8;P, et al. The risk of getting worse: predictors of deterioration after decompressive surgery for lumbar spinal stenosis: a multicenter observational study. World Neurosurg. 2015;84(4):1095&#x2013;1102.</Citation><ArticleIdList><ArticleId IdType="pubmed">26049114</ArticleId></ArticleIdList></Reference><Reference><Citation>Suri P, Hunter DJ, Jouve C, Hartigan C, Limke J, Pena E, et al. Nonsurgical treatment of lumbar disk herniation: are outcomes different in older adults? J Am Geriatr Soc. 2011;59(3):423&#x2013;429.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC3102576</ArticleId><ArticleId IdType="pubmed">21391933</ArticleId></ArticleIdList></Reference><Reference><Citation>Steinmetz MP, Mroz T. Value of adding predictive clinical decision tools to spine surgery. Jama Surg. 2018;153(7):643.</Citation><ArticleIdList><ArticleId IdType="pubmed">29516083</ArticleId></ArticleIdList></Reference><Reference><Citation>Galbusera F, Casaroli G, Bassani T. Artificial intelligence and machine learning in spine research. Jor Spine. 2019;2(1):e1044.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6686793</ArticleId><ArticleId IdType="pubmed">31463458</ArticleId></ArticleIdList></Reference><Reference><Citation>Joshi RS, Lau D, Ames CP. Machine learning in spine surgery: Predictive analytics, imaging applications and next steps. Seminars Spine Surg. 2021;33(2):100878.</Citation></Reference><Reference><Citation>Wiens J, Shenoy ES. Machine learning for healthcare: on the verge of a major shift in healthcare epidemiology. Clin Infect Dis. 2017;66(1):149&#x2013;153.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5850539</ArticleId><ArticleId IdType="pubmed">29020316</ArticleId></ArticleIdList></Reference><Reference><Citation>Miotto R, Wang F, Wang S, Jiang X, Dudley JT. Deep learning for healthcare: review, opportunities and challenges. Brief Bioinform. 2017;19(6):1236&#x2013;1246.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6455466</ArticleId><ArticleId IdType="pubmed">28481991</ArticleId></ArticleIdList></Reference><Reference><Citation>LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015;521(7553):436&#x2013;444.</Citation><ArticleIdList><ArticleId IdType="pubmed">26017442</ArticleId></ArticleIdList></Reference><Reference><Citation>Norgeot B, Glicksberg BS, Trupin L, Lituiev D, Gianfrancesco M, Oskotsky B, et al. Assessment of a deep learning model based on electronic health record data to forecast clinical outcomes in patients with rheumatoid arthritis. Jama Netw Open. 2019;2(3):e190606.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6484652</ArticleId><ArticleId IdType="pubmed">30874779</ArticleId></ArticleIdList></Reference><Reference><Citation>Choi E, Schuetz A, Stewart WF, Sun J. Using recurrent neural network models for early detection of heart failure onset. J Am Med Inform Assn. 2017;24(2):361&#x2013;370.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5391725</ArticleId><ArticleId IdType="pubmed">27521897</ArticleId></ArticleIdList></Reference><Reference><Citation>Huang SC, Pareek A, Seyyedi S, Banerjee I, Lungren MP. Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines. NPJ Digit Med. 2020;3(1):136.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7567861</ArticleId><ArticleId IdType="pubmed">33083571</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang D, Yin C, Zeng J, Yuan X, Zhang P. Combining structured and unstructured data for predictive models: a deep learning approach. Bmc Med Inform Decis. 2020;20(1):280.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7596962</ArticleId><ArticleId IdType="pubmed">33121479</ArticleId></ArticleIdList></Reference><Reference><Citation>Rajkomar A, Oren E, Chen K, Dai AM, Hajaj N, Hardt M, et al. Scalable and accurate deep learning with electronic health records. NPJ Digit Med. 2018;1(1):18.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6550175</ArticleId><ArticleId IdType="pubmed">31304302</ArticleId></ArticleIdList></Reference><Reference><Citation>Miotto R, Li L, Kidd BA, Dudley JT. Deep patient: an unsupervised representation to predict the future of patients from the electronic health records. Sci Rep. 2016;6(1):26094.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4869115</ArticleId><ArticleId IdType="pubmed">27185194</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen D, Liu S, Kingsbury P, Sohn S, Storlie CB, Habermann EB, et al. Deep learning and alternative learning strategies for retrospective real-world clinical data. NPJ Digit Med. 2019;2(1):43.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6550223</ArticleId><ArticleId IdType="pubmed">31304389</ArticleId></ArticleIdList></Reference><Reference><Citation>Jarvik JG, Comstock BA, James KT, Avins AL, Bresnahan BW, Deyo RA, et al. Lumbar imaging with reporting of epidemiology (LIRE)&#x2014;protocol for a pragmatic cluster randomized trial. Contemp Clin Trials. 2015;45(Pt B):157&#x2013;163.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4674321</ArticleId><ArticleId IdType="pubmed">26493088</ArticleId></ArticleIdList></Reference><Reference><Citation>Hebbring SJ. The challenges, advantages and future of phenome-wide association studies. Immunology. 2014;141(2):157&#x2013;165.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC3904236</ArticleId><ArticleId IdType="pubmed">24147732</ArticleId></ArticleIdList></Reference><Reference><Citation>Suri P, Stanaway IB, Zhang Y, Freidin MB, Tsepilov YA, Carrell DS, et al. Genome-wide association studies of low back pain and lumbar spinal disorders using electronic health record data identify a locus associated with lumbar spinal stenosis. Pain. 2021;162(8):2263&#x2013;2272.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8277660</ArticleId><ArticleId IdType="pubmed">33729212</ArticleId></ArticleIdList></Reference><Reference><Citation>Martin BI, Lurie JD, Tosteson ANA, Deyo RA, Tosteson TD, Weinstein JN, et al. Indications for spine surgery. Spine. 2014;39(9):769&#x2013;779.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4018409</ArticleId><ArticleId IdType="pubmed">24525995</ArticleId></ArticleIdList></Reference><Reference><Citation>Deyo RA, Bryan M, Comstock BA, Turner JA, Heagerty P, Friedly J, et al. Trajectories of symptoms and function in older adults with low back disorders. Spine. 2015;40(17):1352&#x2013;1362.</Citation><ArticleIdList><ArticleId IdType="pubmed">25996537</ArticleId></ArticleIdList></Reference><Reference><Citation>Kneeman J, Battalio SL, Korpak A, Cherkin DC, Luo G, Rundell SD, et al. Predicting persistent disabling low back pain in veterans affairs primary care using the STarT back tool. PM R. 2021;13:241&#x2013;249.</Citation><ArticleIdList><ArticleId IdType="pubmed">32902134</ArticleId></ArticleIdList></Reference><Reference><Citation>Friedly J, Chan L, Deyo R. Increases in lumbosacral injections in the medicare population. Spine. 2007;32(16):1754&#x2013;1760.</Citation><ArticleIdList><ArticleId IdType="pubmed">17632396</ArticleId></ArticleIdList></Reference><Reference><Citation>Friedly J, Nishio I, Bishop MJ, Maynard C. The relationship between repeated epidural steroid injections and subsequent opioid use and lumbar surgery. Arch Phys Med Rehab. 2008;89(6):1011&#x2013;1015.</Citation><ArticleIdList><ArticleId IdType="pubmed">18503793</ArticleId></ArticleIdList></Reference><Reference><Citation>Cartwright DJ. ICD-9-CM to ICD-10-CM codes: What? Why? How? Adv Wound Care. 2013;2(10):588&#x2013;592.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC3865615</ArticleId><ArticleId IdType="pubmed">24761333</ArticleId></ArticleIdList></Reference><Reference><Citation>Bird S, Klein E, Loper E. Natural language processing with Python. Sebastopol: O&#x2019;Reilly Media, Inc.; 2009.</Citation></Reference><Reference><Citation>Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, et al. Scikit-learn: machine learning in Python. arXiv. 2012. arXiv:1201.0490.</Citation></Reference><Reference><Citation>&#x158;eh&#x16f;&#x159;ek R, Sojka P. Software framework for topic modelling with large corpora. In: Proceedings of LREC 2010 workshop new challenges for NLP frameworks. 2010; p. 45&#x2013;50.</Citation></Reference><Reference><Citation>Banerjee I, Chen MC, Lungren MP, Rubin DL. Radiology report annotation using intelligent word embeddings: applied to multi-institutional chest CT cohort. J Biomed Inform. 2018;77:11&#x2013;20.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5771955</ArticleId><ArticleId IdType="pubmed">29175548</ArticleId></ArticleIdList></Reference><Reference><Citation>Mikolov T, Chen K, Corrado G, Dean J. Efficient estimation of word representations in vector space. 2013.</Citation></Reference><Reference><Citation>Friedman P. Radiologic reporting: structure. Am J Roentgenol. 1983;140(1):171&#x2013;172.</Citation><ArticleIdList><ArticleId IdType="pubmed">6600314</ArticleId></ArticleIdList></Reference><Reference><Citation>Tibshirani R. Regression shrinkage and selection via the lasso. J Royal Statistical Soc Ser B Methodol. 1996;58(1):267&#x2013;288.</Citation></Reference><Reference><Citation>Bovelstad HM, Nygard S, Storvold HL, Aldrin M, Borgan O, Frigessi A, et al. Predicting survival from microarray data a comparative study. Bioinformatics. 2007;23(16):2080&#x2013;2087.</Citation><ArticleIdList><ArticleId IdType="pubmed">17553857</ArticleId></ArticleIdList></Reference><Reference><Citation>Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, et al. PyTorch: an imperative style, high-performance deep learning library. arXiv. 2019. arXiv:1912.01703.</Citation></Reference><Reference><Citation>Choi E, Bahadori MT, Schuetz A, Stewart WF, Sun J. Doctor AI: predicting clinical events via recurrent neural networks. arXiv. 2015. arXiv:1511.05942.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5341604</ArticleId><ArticleId IdType="pubmed">28286600</ArticleId></ArticleIdList></Reference><Reference><Citation>Chung J, Gulcehre C, Cho K, Bengio Y. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. arXiv. 2014. arXiv:1412.3555.</Citation></Reference><Reference><Citation>Choi E, Xiao C, Stewart WF, Sun J. MiME: multilevel medical embedding of electronic health records for predictive healthcare. arXiv. 2018. arXiv:1810.09593.</Citation></Reference><Reference><Citation>Wang Y, Xu X, Jin T, Li X, Xie G, Wang J. Inpatient2Vec: Medical Representation Learning for Inpatients. In: 2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM) 2019; p. 1113&#x2013;7.</Citation></Reference><Reference><Citation>Steinberg E, Jung K, Fries JA, Corbin CK, Pfohl SR, Shah NH. Language models are an effective representation learning technique for electronic health record data. J Biomed Inform. 2021;113:103637.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7863633</ArticleId><ArticleId IdType="pubmed">33290879</ArticleId></ArticleIdList></Reference><Reference><Citation>King G, Zeng L. Logistic regression in rare events data. Polit Anal. 2001;9(2):137&#x2013;163.</Citation></Reference><Reference><Citation>Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout: a simple way to prevent neural networks from overfitting. J Mach Learn Res. 2014;15:1929&#x2013;1958.</Citation></Reference><Reference><Citation>Zhang Y, Wallace B. A sensitivity analysis of (and practitioners&#x2019; guide to) convolutional neural networks for sentence classification. 2015.</Citation></Reference><Reference><Citation>Andr&#xe9; A, Peyrou B, Carpentier A, Vignaux JJ. Feasibility and assessment of a machine learning-based predictive model of outcome after lumbar decompression surgery. Global Spine J. 2022;12:894&#x2013;908.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC9344503</ArticleId><ArticleId IdType="pubmed">33207969</ArticleId></ArticleIdList></Reference><Reference><Citation>Wilson B, Gaonkar B, Yoo B, Salehi B, Attiah M, Villaroman D, et al. Predicting spinal surgery candidacy from imaging data using machine learning. Neurosurgery. 2021;89(1):116&#x2013;121.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8203423</ArticleId><ArticleId IdType="pubmed">33826737</ArticleId></ArticleIdList></Reference><Reference><Citation>Keeney BJ, Fulton-Kehoe D, Turner JA, Wickizer TM, Chan KCG, Franklin GM. Early predictors of lumbar spine surgery after occupational back injury. Spine. 2013;38(11):953&#x2013;964.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4258106</ArticleId><ArticleId IdType="pubmed">23238486</ArticleId></ArticleIdList></Reference><Reference><Citation>Cherkin DC, Deyo RA, Wheeler K, Ciol MA. Physician views about treating low back pain: the results of a national survey. Spine. 1995;20(1):1&#x2013;8.</Citation><ArticleIdList><ArticleId IdType="pubmed">7709266</ArticleId></ArticleIdList></Reference><Reference><Citation>Cherkin DC, Deyo RA, Wheeler K, Ciol MA. Physician variation in diagnostic testing for low back pain. Who you see is what you get. Arthr Rheum. 1994;37(1):15&#x2013;22.</Citation><ArticleIdList><ArticleId IdType="pubmed">8129759</ArticleId></ArticleIdList></Reference><Reference><Citation>Azad TD, Ehresman J, Ahmed AK, Staartjes VE, Lubelski D, Stienen MN, et al. Fostering reproducibility and generalizability in machine learning for clinical prediction modeling in spine surgery. Spine J. 2021;21(10):1610&#x2013;1616.</Citation><ArticleIdList><ArticleId IdType="pubmed">33065274</ArticleId></ArticleIdList></Reference><Reference><Citation>Kwon O, Sim JM. Effects of data set features on the performances of classification algorithms. Expert Syst Appl. 2013;40(5):1847&#x2013;1857.</Citation></Reference><Reference><Citation>Milani CJ, Rundell SD, Jarvik JG, Friedly J, Heagerty PJ, Avins A, et al. Associations of race and ethnicity with patient-reported outcomes and health care utilization among older adults initiating a new episode of care for back pain. Spine. 2018;43(14):1007&#x2013;1017.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5972040</ArticleId><ArticleId IdType="pubmed">29189640</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen Y, Campbell P, Strauss VY, Foster NE, Jordan KP, Dunn KM. Trajectories and predictors of the long-term course of low back pain: cohort study with 5-year follow-up. Pain. 2018;159(2):252&#x2013;260.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5771685</ArticleId><ArticleId IdType="pubmed">29112007</ArticleId></ArticleIdList></Reference><Reference><Citation>Harris A, Guadix SW, Riley LH, Jain A, Kebaish KM, Skolasky RL. Changes in racial and ethnic disparities in lumbar spinal surgery associated with the passage of the Affordable Care Act, 2006&#x2013;2014. Spine J. 2021;21(1):64&#x2013;70.</Citation><ArticleIdList><ArticleId IdType="pubmed">32768655</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36607999</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>10</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic-eCollection"><Journal><ISSN IssnType="Electronic">1932-6203</ISSN><JournalIssue CitedMedium="Internet"><Volume>18</Volume><Issue>1</Issue><PubDate><Year>2023</Year></PubDate></JournalIssue><Title>PloS one</Title><ISOAbbreviation>PLoS One</ISOAbbreviation></Journal><ArticleTitle>Prostatic urinary tract visualization with super-resolution deep learning models.</ArticleTitle><Pagination><StartPage>e0280076</StartPage><MedlinePgn>e0280076</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">e0280076</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1371/journal.pone.0280076</ELocationID><Abstract><AbstractText>In urethra-sparing radiation therapy, prostatic urinary tract visualization is important in decreasing the urinary side effect. A methodology has been developed to visualize the prostatic urinary tract using post-urination magnetic resonance imaging (PU-MRI) without a urethral catheter. This study investigated whether the combination of PU-MRI and super-resolution (SR) deep learning models improves the visibility of the prostatic urinary tract. We enrolled 30 patients who had previously undergone real-time-image-gated spot scanning proton therapy by insertion of fiducial markers. PU-MRI was performed using a non-contrast high-resolution two-dimensional T2-weighted turbo spin-echo imaging sequence. Four different SR deep learning models were used: the enhanced deep SR network (EDSR), widely activated SR network (WDSR), SR generative adversarial network (SRGAN), and residual dense network (RDN). The complex wavelet structural similarity index measure (CW-SSIM) was used to quantitatively assess the performance of the proposed SR images compared to PU-MRI. Two radiation oncologists used a 1-to-5 scale to subjectively evaluate the visibility of the prostatic urinary tract. Cohen's weighted kappa (k) was used as a measure of agreement of inter-operator reliability. The mean CW-SSIM in EDSR, WDSR, SRGAN, and RDN was 99.86%, 99.89%, 99.30%, and 99.67%, respectively. The mean prostatic urinary tract visibility scores of the radiation oncologists were 3.70 and 3.53 for PU-MRI (k = 0.93), 3.67 and 2.70 for EDSR (k = 0.89), 3.70 and 2.73 for WDSR (k = 0.88), 3.67 and 2.73 for SRGAN (k = 0.88), and 4.37 and 3.73 for RDN (k = 0.93), respectively. The results suggest that SR images using RDN are similar to the original images, and the SR deep learning models subjectively improve the visibility of the prostatic urinary tract.</AbstractText><CopyrightInformation>Copyright: &#xa9; 2023 Yoshimura et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Yoshimura</LastName><ForeName>Takaaki</ForeName><Initials>T</Initials><Identifier Source="ORCID">0000-0002-6663-4335</Identifier><AffiliationInfo><Affiliation>Department of Health Sciences and Technology, Faculty of Health Sciences, Hokkaido University, Sapporo, Japan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Medical Physics, Hokkaido University Hospital, Sapporo, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Nishioka</LastName><ForeName>Kentaro</ForeName><Initials>K</Initials><Identifier Source="ORCID">0000-0002-8272-7603</Identifier><AffiliationInfo><Affiliation>Department of Radiation Medical Science and Engineering, Faculty of Medicine, Hokkaido University, Sapporo, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Hashimoto</LastName><ForeName>Takayuki</ForeName><Initials>T</Initials><Identifier Source="ORCID">0000-0001-5276-3665</Identifier><AffiliationInfo><Affiliation>Department of Radiation Medical Science and Engineering, Faculty of Medicine, Hokkaido University, Sapporo, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Mori</LastName><ForeName>Takashi</ForeName><Initials>T</Initials><Identifier Source="ORCID">0000-0001-5246-215X</Identifier><AffiliationInfo><Affiliation>Department of Radiation Oncology, Hokkaido University Hospital, Department of Radiation Oncology, Hokkaido University Hospital, Sapporo, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kogame</LastName><ForeName>Shoki</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Division of Radiological Science and Technology, Department of Health Sciences, School of Medicine, Hokkaido University, Sapporo, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Seki</LastName><ForeName>Kazuya</ForeName><Initials>K</Initials><AffiliationInfo><Affiliation>Division of Radiological Science and Technology, Department of Health Sciences, School of Medicine, Hokkaido University, Sapporo, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Sugimori</LastName><ForeName>Hiroyuki</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>Department of Biomedical Science and Engineering, Faculty of Health Sciences, Hokkaido University, Sapporo, Japan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Clinical AI Human Resources Development Program, Faculty of Medicine, Hokkaido University, Sapporo, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yamashina</LastName><ForeName>Hiroko</ForeName><Initials>H</Initials><Identifier Source="ORCID">0000-0003-1357-0873</Identifier><AffiliationInfo><Affiliation>Department of Biomedical Science and Engineering, Faculty of Health Sciences, Hokkaido University, Sapporo, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Nomura</LastName><ForeName>Yusuke</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Department of Radiation oncology, Stanford University, Stanford, CA, United States of America.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Global Center for Biomedical Science and Engineering, Faculty of Medicine, Hokkaido University, Sapporo, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kato</LastName><ForeName>Fumi</ForeName><Initials>F</Initials><AffiliationInfo><Affiliation>Department of Diagnostic and Interventional Radiology, Hokkaido University Hospital, Sapporo, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kudo</LastName><ForeName>Kohsuke</ForeName><Initials>K</Initials><Identifier Source="ORCID">0000-0001-5351-9242</Identifier><AffiliationInfo><Affiliation>Department of Diagnostic Imaging, Faculty of Medicine, Hokkaido University, Sapporo, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Shimizu</LastName><ForeName>Shinichi</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Department of Medical Physics, Hokkaido University Hospital, Sapporo, Japan.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiation Medical Science and Engineering, Faculty of Medicine, Hokkaido University, Sapporo, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Aoyama</LastName><ForeName>Hidefumi</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>Department of Radiation Oncology, Faculty of Medicine, Hokkaido University, Sapporo, Japan.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>06</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>PLoS One</MedlineTA><NlmUniqueID>101285081</NlmUniqueID><ISSNLinking>1932-6203</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D008297" MajorTopicYN="N">Male</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D015203" MajorTopicYN="N">Reproducibility of Results</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D008279" MajorTopicYN="N">Magnetic Resonance Imaging</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D011467" MajorTopicYN="N">Prostate</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D014521" MajorTopicYN="N">Urethra</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D007091" MajorTopicYN="N">Image Processing, Computer-Assisted</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading></MeshHeadingList><CoiStatement>The authors have declared that no competing interests exist.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>4</Month><Day>11</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>20</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>13</Hour><Minute>44</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36607999</ArticleId><ArticleId IdType="pmc">PMC9821403</ArticleId><ArticleId IdType="doi">10.1371/journal.pone.0280076</ArticleId><ArticleId IdType="pii">PONE-D-22-10629</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Vainshtein J, Abu-Isa E, Olson KB, Ray ME, Sandler HM, Normolle D, et al.. Randomized phase II trial of urethral sparing intensity modulated radiation therapy in low-risk prostate cancer: implications for focal therapy. Radiat Oncol. 2012;7(82). doi: 10.1186/1748-717X-7-82</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/1748-717X-7-82</ArticleId><ArticleId IdType="pmc">PMC3408353</ArticleId><ArticleId IdType="pubmed">22681643</ArticleId></ArticleIdList></Reference><Reference><Citation>Shimizu S, Nishioka K, Suzuki R, Shinohara N, Maruyama S, Abe T, et al.. Early results of urethral dose reduction and small safety margin in intensity-modulated radiation therapy (IMRT) for localized prostate cancer using a real-time tumor-tracking radiotherapy (RTRT) system. Radiat Oncol. 2014;9(1): 1&#x2013;8. doi: 10.1186/1748-717X-9-118</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/1748-717X-9-118</ArticleId><ArticleId IdType="pmc">PMC4035733</ArticleId><ArticleId IdType="pubmed">24884868</ArticleId></ArticleIdList></Reference><Reference><Citation>Thomsen JB, Arp DT, Carl J. Urethra sparing&#x2014;potential of combined Nickel-Titanium stent and intensity modulated radiation therapy in prostate cancer. Radiother Oncol. 2012;103(2): 256&#x2013;260. doi: 10.1016/j.radonc.2011.11.015</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.radonc.2011.11.015</ArticleId><ArticleId IdType="pubmed">22197354</ArticleId></ArticleIdList></Reference><Reference><Citation>Dekura Y, Nishioka K, Hashimoto T, Miyamoto N, Suzuki R, Yoshimura T, et al.. The urethral position may shift due to urethral catheter placement in the treatment planning for prostate radiation therapy. Radiat Oncol. 2019;14(1): 226. doi: 10.1186/s13014-019-1424-8</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s13014-019-1424-8</ArticleId><ArticleId IdType="pmc">PMC6909476</ArticleId><ArticleId IdType="pubmed">31831045</ArticleId></ArticleIdList></Reference><Reference><Citation>Kataria T, Gupta D, Goyal S, Bisht SS, Chaudhary R, Narang K, et al.. Simple diagrammatic method to delineate male urethra in prostate cancer radiotherapy: an MRI based approach. Br J Radiol. 2016;89(1068): 20160348. doi: 10.1259/bjr.20160348</Citation><ArticleIdList><ArticleId IdType="doi">10.1259/bjr.20160348</ArticleId><ArticleId IdType="pmc">PMC5604912</ArticleId><ArticleId IdType="pubmed">27748126</ArticleId></ArticleIdList></Reference><Reference><Citation>Rai R, Sidhom M, Lim K, Ohanessian L, Liney GP. MRI micturating urethrography for improved urethral delineation in prostate radiotherapy planning: a case study. Phys Med Biol. 2017;62(8): 3003&#x2013;3010. doi: 10.1088/1361-6560/62/8/3003</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/1361-6560/62/8/3003</ArticleId><ArticleId IdType="pubmed">28306557</ArticleId></ArticleIdList></Reference><Reference><Citation>Zakian KL, Wibmer A, Vargas HA, Alberts E, Kadbi M, Mychalczak B, et al.. Comparison of Motion-Insensitive T2-Weighted MRI Pulse Sequences for Visualization of the Prostatic Urethra During MR Simulation. Pract Radiat Oncol. 2019;9(6): e534&#x2013;e40. doi: 10.1016/j.prro.2019.06.009</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.prro.2019.06.009</ArticleId><ArticleId IdType="pmc">PMC6832802</ArticleId><ArticleId IdType="pubmed">31252087</ArticleId></ArticleIdList></Reference><Reference><Citation>Yoshimura T, Nishioka K, Hashimoto T, Fujiwara T, Ishizaka K, Sugimori H, et al.. Visualizing the urethra by magnetic resonance imaging without usage of a catheter for radiotherapy of prostate cancer. Phys Imaging Radiat Oncol. 2021;18: 1&#x2013;4. doi: 10.1016/j.phro.2021.03.002</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.phro.2021.03.002</ArticleId><ArticleId IdType="pmc">PMC8254197</ArticleId><ArticleId IdType="pubmed">34258400</ArticleId></ArticleIdList></Reference><Reference><Citation>Agustsson E, Timofte R. NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study. 2017: 1122&#x2013;1131.</Citation></Reference><Reference><Citation>Dong C, Loy CC, He K, Tang X, editors. Learning a Deep Convolutional Network for Image Super-Resolution 2014; Cham: Springer International Publishing.</Citation></Reference><Reference><Citation>Shi J, Liu Q, Wang C, Zhang Q, Ying S, Xu H. Super-resolution reconstruction of MR image with a novel residual learning network algorithm. Phys Med Biol. 2018;63(8): 085011. doi: 10.1088/1361-6560/aab9e9</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/1361-6560/aab9e9</ArticleId><ArticleId IdType="pubmed">29583134</ArticleId></ArticleIdList></Reference><Reference><Citation>Sood R, Rusu M, editors. Anisotropic Super Resolution In Prostate Mri Using Super Resolution Generative Adversarial Networks. 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019); 2019: 8&#x2013;11.</Citation></Reference><Reference><Citation>Chen Y, Christodoulou A, Zhou Z, Shi F, Xie Y, Li D. MRI Super-Resolution with GAN and 3D Multi-Level DenseNet: Smaller, Faster, and Better. arXiv:2003.01217v2 [Preprint]. 2020 [cited 2022 Nov 16]. doi: 10.48550/arXiv.2003.01217</Citation><ArticleIdList><ArticleId IdType="doi">10.48550/arXiv.2003.01217</ArticleId></ArticleIdList></Reference><Reference><Citation>Pham CH, Tor-Diez C, Meunier H, Bednarek N, Fablet R, Passat N, et al.. Multiscale brain MRI super-resolution using deep 3D convolutional networks. Comput Med Imaging Graph. 2019;77: 101647. doi: 10.1016/j.compmedimag.2019.101647</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compmedimag.2019.101647</ArticleId><ArticleId IdType="pubmed">31493703</ArticleId></ArticleIdList></Reference><Reference><Citation>Kustner T, Munoz C, Psenicny A, Bustin A, Fuin N, Qi H, et al.. Deep-learning based super-resolution for 3D isotropic coronary MR angiography in less than a minute. Magn Reson Med. 2021;86(5): 2837&#x2013;2852. doi: 10.1002/mrm.28911</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.28911</ArticleId><ArticleId IdType="pubmed">34240753</ArticleId></ArticleIdList></Reference><Reference><Citation>Ishida M, Nakayama R, Uno M, Ito T, Goto Y, Ichikawa Y, et al.. Learning-based super-resolution technique significantly improves detection of coronary artery stenoses on 1.5T whole-heart coronary MRA. J Cardiovasc Magn Reson. 2014;16(S1).</Citation></Reference><Reference><Citation>Elguindi S, Zelefsky MJ, Jiang J, Veeraraghavan H, Deasy JO, Hunt MA, et al.. Deep learning-based auto-segmentation of targets and organs-at-risk for magnetic resonance imaging only planning of prostate radiotherapy. Phys Imaging Radiat Oncol. 2019;12: 80&#x2013;86. doi: 10.1016/j.phro.2019.11.006</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.phro.2019.11.006</ArticleId><ArticleId IdType="pmc">PMC7192345</ArticleId><ArticleId IdType="pubmed">32355894</ArticleId></ArticleIdList></Reference><Reference><Citation>Mohler JL, Antonarakis ES, Armstrong AJ, D&#x2019;Amico AV, Davis BJ, Dorff T, et al.. Prostate Cancer, Version 2.2019, NCCN Clinical Practice Guidelines in Oncology. J Natl Compr Canc Netw. 2019;17(5): 479&#x2013;505. doi: 10.6004/jnccn.2019.0023</Citation><ArticleIdList><ArticleId IdType="doi">10.6004/jnccn.2019.0023</ArticleId><ArticleId IdType="pubmed">31085757</ArticleId></ArticleIdList></Reference><Reference><Citation>Weber DC, Zilli T, Vallee JP, Rouzaud M, Miralbell R, Cozzi L. Intensity modulated proton and photon therapy for early prostate cancer with or without transperineal injection of a polyethylen glycol spacer: a treatment planning comparison study. Int J Radiat Oncol Biol Phys. 2012;84(3): e311&#x2013;8. doi: 10.1016/j.ijrobp.2012.03.028</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ijrobp.2012.03.028</ArticleId><ArticleId IdType="pubmed">22999271</ArticleId></ArticleIdList></Reference><Reference><Citation>Ruggieri R, Naccarato S, Stavrev P, Stavreva N, Fersino S, Giaj Levra N, et al.. Volumetric-modulated arc stereotactic body radiotherapy for prostate cancer: dosimetric impact of an increased near-maximum target dose and of a rectal spacer. Br J Radiol. 2015;88(1054): 20140736. doi: 10.1259/bjr.20140736</Citation><ArticleIdList><ArticleId IdType="doi">10.1259/bjr.20140736</ArticleId><ArticleId IdType="pmc">PMC4738106</ArticleId><ArticleId IdType="pubmed">26235142</ArticleId></ArticleIdList></Reference><Reference><Citation>Lim B, Son S, Kim H, Nah S, Mu Lee K. Enhanced Deep Residual Networks for Single Image Super-Resolution. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017: 136&#x2013;144.</Citation></Reference><Reference><Citation>Timofte R, Agustsson E, Gool LV, Yang M-H, Zhang L, Lim B, et al.. NTIRE 2017 Challenge on Single Image Super-Resolution: Methods and Results. 2017:1110&#x2013;1121.</Citation></Reference><Reference><Citation>Yu J, Fan Y, Yang J, Xu N, Wang X, Huang TS. Wide Activation for Efficient and Accurate Image Super-Resolution. arXiv:180808718 [Preprint]. 2018. [cited 16 Nov 2022]. doi: 10.48550/arXiv.1808.08718</Citation><ArticleIdList><ArticleId IdType="doi">10.48550/arXiv.1808.08718</ArticleId></ArticleIdList></Reference><Reference><Citation>Timofte R, Gu S, Wu J, Gool LV, Zhang L, Yang MH, et al.., editors. NTIRE 2018 Challenge on Single Image Super-Resolution: Methods and Results. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). 2018: 18&#x2013;22.</Citation></Reference><Reference><Citation>Ledig C, Theis L, Huszar F, Caballero J, Cunningham A, Acosta A, et al.. Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017:4681&#x2013;4690.</Citation></Reference><Reference><Citation>Zhang Y, Tian Y, Kong Y, Zhong B, Fu Y. Residual Dense Network for Image Super-Resolution. arXiv:180208797v2 [Preprint]. 2018. [cited 16 Nov 2022]. doi: 10.48550/arXiv.1802.08797</Citation><ArticleIdList><ArticleId IdType="doi">10.48550/arXiv.1802.08797</ArticleId></ArticleIdList></Reference><Reference><Citation>Kingma D, Ba J. Adam: A Method for Stochastic Optimization. Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015. 2014.</Citation></Reference><Reference><Citation>Sampat MP, Wang Z, Gupta S, Bovik AC, Markey MK. Complex wavelet structural similarity: a new image similarity index. IEEE Trans Image Process. 2009;18(11): 2385&#x2013;2401. doi: 10.1109/TIP.2009.2025923</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TIP.2009.2025923</ArticleId><ArticleId IdType="pubmed">19556195</ArticleId></ArticleIdList></Reference><Reference><Citation>Fleiss JL, Cohen J. The Equivalence of Weighted Kappa and the Intraclass Correlation Coefficient as Measures of Reliability. Educational and Psychological Measurement. 1973;33(3): 613&#x2013;619.</Citation></Reference><Reference><Citation>Landis JR, Koch GG. The Measurement of Observer Agreement for Categorical Data. Biometrics. 1977;33(1): 159&#x2013;174.</Citation><ArticleIdList><ArticleId IdType="pubmed">843571</ArticleId></ArticleIdList></Reference><Reference><Citation>Cai J, Gu S, Radu T, Lei Z. NTIRE 2019 Challenge on Real Image Super-Resolution: Methods and Results. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. 2019.</Citation></Reference><Reference><Citation>Andreas L, Martin D, Radu T. NTIRE 2020 Challenge on Real-World Image Super-Resolution: Methods and Results. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. 2020.</Citation></Reference><Reference><Citation>Cai J, Zeng H, Yong H, Cao Z, Zhang L. Toward Real-World Single Image Super-Resolution: A New Benchmark and a New Model. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 2019.</Citation></Reference><Reference><Citation>Knoll F, Hammernik K, Kobler E, Pock T, Recht MP, Sodickson DK. Assessment of the generalization of learned image reconstruction and the potential for transfer learning. Magn Reson Med. 2019;81(1): 116&#x2013;128. doi: 10.1002/mrm.27355</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.27355</ArticleId><ArticleId IdType="pmc">PMC6240410</ArticleId><ArticleId IdType="pubmed">29774597</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36607982</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>10</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic-eCollection"><Journal><ISSN IssnType="Electronic">1932-6203</ISSN><JournalIssue CitedMedium="Internet"><Volume>18</Volume><Issue>1</Issue><PubDate><Year>2023</Year></PubDate></JournalIssue><Title>PloS one</Title><ISOAbbreviation>PLoS One</ISOAbbreviation></Journal><ArticleTitle>Deep learning prediction of pathological complete response, residual cancer burden, and progression-free survival in breast cancer patients.</ArticleTitle><Pagination><StartPage>e0280148</StartPage><MedlinePgn>e0280148</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">e0280148</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1371/journal.pone.0280148</ELocationID><Abstract><AbstractText>The goal of this study was to employ novel deep-learning convolutional-neural-network (CNN) to predict pathological complete response (PCR), residual cancer burden (RCB), and progression-free survival (PFS) in breast cancer patients treated with neoadjuvant chemotherapy using longitudinal multiparametric MRI, demographics, and molecular subtypes as inputs. In the I-SPY-1 TRIAL, 155 patients with stage 2 or 3 breast cancer with breast tumors underwent neoadjuvant chemotherapy met the inclusion/exclusion criteria. The inputs were dynamic-contrast-enhanced (DCE) MRI, and T2- weighted MRI as three-dimensional whole-images without the tumor segmentation, as well as molecular subtypes and demographics. The outcomes were PCR, RCB, and PFS. Three ("Integrated", "Stack" and "Concatenation") CNN were evaluated using receiver-operating characteristics and mean absolute errors. The Integrated approach outperformed the "Stack" or "Concatenation" CNN. Inclusion of both MRI and non-MRI data outperformed either alone. The combined pre- and post-neoadjuvant chemotherapy data outperformed either alone. Using the best model and data combination, PCR prediction yielded an accuracy of 0.81&#xb1;0.03 and AUC of 0.83&#xb1;0.03; RCB prediction yielded an accuracy of 0.80&#xb1;0.02 and Cohen's &#x3ba; of 0.73&#xb1;0.03; PFS prediction yielded a mean absolute error of 24.6&#xb1;0.7 months (survival ranged from 6.6 to 127.5 months). Deep learning using longitudinal multiparametric MRI, demographics, and molecular subtypes accurately predicts PCR, RCB, and PFS in breast cancer patients. This approach may prove useful for treatment selection, planning, execution, and mid-treatment adjustment.</AbstractText><CopyrightInformation>Copyright: &#xa9; 2023 Dammu et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Dammu</LastName><ForeName>Hongyi</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>Department of Radiology, Montefiore Medical Center and Albert Einstein College of Medicine, Bronx, New York, United States of America.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Ren</LastName><ForeName>Thomas</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>Department of Radiology, Montefiore Medical Center and Albert Einstein College of Medicine, Bronx, New York, United States of America.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Duong</LastName><ForeName>Tim Q</ForeName><Initials>TQ</Initials><Identifier Source="ORCID">0000-0001-6403-2827</Identifier><AffiliationInfo><Affiliation>Department of Radiology, Montefiore Medical Center and Albert Einstein College of Medicine, Bronx, New York, United States of America.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>06</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>PLoS One</MedlineTA><NlmUniqueID>101285081</NlmUniqueID><ISSNLinking>1932-6203</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D005260" MajorTopicYN="N">Female</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D001943" MajorTopicYN="Y">Breast Neoplasms</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName><QualifierName UI="Q000188" MajorTopicYN="N">drug therapy</QualifierName><QualifierName UI="Q000473" MajorTopicYN="N">pathology</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000077982" MajorTopicYN="N">Progression-Free Survival</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D018365" MajorTopicYN="N">Neoplasm, Residual</DescriptorName><QualifierName UI="Q000209" MajorTopicYN="N">etiology</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D008279" MajorTopicYN="N">Magnetic Resonance Imaging</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D020360" MajorTopicYN="N">Neoadjuvant Therapy</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D012189" MajorTopicYN="N">Retrospective Studies</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D016896" MajorTopicYN="N">Treatment Outcome</DescriptorName></MeshHeading></MeshHeadingList><CoiStatement>The authors have declared that no competing interests exist.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>3</Month><Day>30</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>20</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>13</Hour><Minute>43</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36607982</ArticleId><ArticleId IdType="pmc">PMC9821469</ArticleId><ArticleId IdType="doi">10.1371/journal.pone.0280148</ArticleId><ArticleId IdType="pii">PONE-D-22-09310</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Curigliano G, Burstein HJ, Winer EP, Gnant M, Dubsky P, Loibl S, et al.. De-escalating and escalating treatments for early-stage breast cancer: the St. Gallen International Expert Consensus Conference on the Primary Therapy of Early Breast Cancer 2017. Ann Oncol. 2018;29(10):2153. Epub 2018/05/08. doi: 10.1093/annonc/mdx806 ; PubMed Central PMCID: PMC6887963.</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/annonc/mdx806</ArticleId><ArticleId IdType="pmc">PMC6887963</ArticleId><ArticleId IdType="pubmed">29733336</ArticleId></ArticleIdList></Reference><Reference><Citation>Cortazar P, Zhang L, Untch M, Mehta K, Costantino JP, Wolmark N, et al.. Pathological complete response and long-term clinical benefit in breast cancer: the CTNeoBC pooled analysis. Lancet. 2014;384(9938):164&#x2013;72. Epub 2014/02/18. doi: 10.1016/S0140-6736(13)62422-8 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0140-6736(13)62422-8</ArticleId><ArticleId IdType="pubmed">24529560</ArticleId></ArticleIdList></Reference><Reference><Citation>Cortazar P, Geyer CE Jr. Pathological complete response in neoadjuvant treatment of breast cancer. Ann Surg Oncol. 2015;22(5):1441&#x2013;6. Epub 2015/03/03. doi: 10.1245/s10434-015-4404-8 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1245/s10434-015-4404-8</ArticleId><ArticleId IdType="pubmed">25727556</ArticleId></ArticleIdList></Reference><Reference><Citation>Symmans WF, Peintinger F, Hatzis C, Rajan R, Kuerer H, Valero V, et al.. Measurement of residual breast cancer burden to predict survival after neoadjuvant chemotherapy. J Clin Oncol. 2007;25(28):4414&#x2013;22. Epub 2007/09/06. doi: 10.1200/JCO.2007.10.6823 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1200/JCO.2007.10.6823</ArticleId><ArticleId IdType="pubmed">17785706</ArticleId></ArticleIdList></Reference><Reference><Citation>Li H, Yao L, Jin P, Hu L, Li X, Guo T, et al.. MRI and PET/CT for evaluation of the pathological response to neoadjuvant chemotherapy in breast cancer: A systematic review and meta-analysis. Breast. 2018;40:106&#x2013;15. Epub 2018/05/15. doi: 10.1016/j.breast.2018.04.018 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.breast.2018.04.018</ArticleId><ArticleId IdType="pubmed">29758503</ArticleId></ArticleIdList></Reference><Reference><Citation>Price ER, Wong J, Mukhtar R, Hylton N, Esserman LJ. How to use magnetic resonance imaging following neoadjuvant chemotherapy in locally advanced breast cancer. World J Clin Cases. 2015;3(7):607&#x2013;13. Epub 2015/08/06. doi: 10.12998/wjcc.v3.i7.607 ; PubMed Central PMCID: PMC4517335.</Citation><ArticleIdList><ArticleId IdType="doi">10.12998/wjcc.v3.i7.607</ArticleId><ArticleId IdType="pmc">PMC4517335</ArticleId><ArticleId IdType="pubmed">26244152</ArticleId></ArticleIdList></Reference><Reference><Citation>Lo Gullo R, Eskreis-Winkler S, Morris EA, Pinker K. Machine learning with multiparametric magnetic resonance imaging of the breast for early prediction of response to neoadjuvant chemotherapy. Breast. 2020;49:115&#x2013;22. Epub 2019/12/02. doi: 10.1016/j.breast.2019.11.009 ; PubMed Central PMCID: PMC7375548.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.breast.2019.11.009</ArticleId><ArticleId IdType="pmc">PMC7375548</ArticleId><ArticleId IdType="pubmed">31786416</ArticleId></ArticleIdList></Reference><Reference><Citation>Houssein E, Emam MM, Ali AA, Suganthan PN. Deep and machine learning techniques for medical imaging-based breast cancer: A comprehensive review. Expert Systems With Applications. 2021;167:114161.</Citation></Reference><Reference><Citation>LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015;521(7553):436&#x2013;44. Epub 2015/05/29. doi: 10.1038/nature14539 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nature14539</ArticleId><ArticleId IdType="pubmed">26017442</ArticleId></ArticleIdList></Reference><Reference><Citation>Chougrad H, Zouaki H, Alheyane O. Deep Convolutional Neural Networks for breast cancer screening. Comput Methods Programs Biomed. 2018;157:19&#x2013;30. Epub 2018/02/27. doi: 10.1016/j.cmpb.2018.01.011 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cmpb.2018.01.011</ArticleId><ArticleId IdType="pubmed">29477427</ArticleId></ArticleIdList></Reference><Reference><Citation>Yamashita R, Nishio M, Do RKG, Togashi K. Convolutional neural networks: an overview and application in radiology. Insights Imaging. 2018;9(4):611&#x2013;29. Epub 2018/06/24. doi: 10.1007/s13244-018-0639-9 ; PubMed Central PMCID: PMC6108980.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s13244-018-0639-9</ArticleId><ArticleId IdType="pmc">PMC6108980</ArticleId><ArticleId IdType="pubmed">29934920</ArticleId></ArticleIdList></Reference><Reference><Citation>Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems. 2012:1097&#x2013;105.</Citation></Reference><Reference><Citation>Mani S, Chen Y, Li X, Arlinghaus L, Chakravarthy AB, Abramson V, et al.. Machine learning for predicting the response of breast cancer to neoadjuvant chemotherapy. J Am Med Inform Assoc. 2013;20(4):688&#x2013;95. Epub 2013/04/26. doi: 10.1136/amiajnl-2012-001332 ; PubMed Central PMCID: PMC3721158.</Citation><ArticleIdList><ArticleId IdType="doi">10.1136/amiajnl-2012-001332</ArticleId><ArticleId IdType="pmc">PMC3721158</ArticleId><ArticleId IdType="pubmed">23616206</ArticleId></ArticleIdList></Reference><Reference><Citation>Tahmassebi A, Gandomi AH, Fong S, Meyer-Baese A, Foo SY. Multi-stage optimization of a deep model: A case study on ground motion modeling. PLoS One. 2018;13(9):e0203829. Epub 2018/09/20. doi: 10.1371/journal.pone.0203829 ; PubMed Central PMCID: PMC6145533.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0203829</ArticleId><ArticleId IdType="pmc">PMC6145533</ArticleId><ArticleId IdType="pubmed">30231077</ArticleId></ArticleIdList></Reference><Reference><Citation>Cain EH, Saha A, Harowicz MR, Marks JR, Marcom PK, Mazurowski MA. Multivariate machine learning models for prediction of pathologic response to neoadjuvant therapy in breast cancer using MRI features: a study using an independent validation set. Breast Cancer Res Treat. 2019;173(2):455&#x2013;63. Epub 2018/10/18. doi: 10.1007/s10549-018-4990-9 ; PubMed Central PMCID: PMC6483397.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10549-018-4990-9</ArticleId><ArticleId IdType="pmc">PMC6483397</ArticleId><ArticleId IdType="pubmed">30328048</ArticleId></ArticleIdList></Reference><Reference><Citation>Hussain L, Huang P, Nguyen T, Lone KJ, Ali A, Khan MS, et al.. Machine learning classification of texture features of MRI breast tumor and peri-tumor of combined pre- and early treatment predicts pathologic complete response. Biomed Eng Online. 2021;20(1):63. Epub 20210628. doi: 10.1186/s12938-021-00899-z ; PubMed Central PMCID: PMC8240261.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s12938-021-00899-z</ArticleId><ArticleId IdType="pmc">PMC8240261</ArticleId><ArticleId IdType="pubmed">34183038</ArticleId></ArticleIdList></Reference><Reference><Citation>Ha R, Chin C, Karcich J, Liu MZ, Chang P, Mutasa S, et al.. Prior to Initiation of Chemotherapy, Can We Predict Breast Tumor Response? Deep Learning Convolutional Neural Networks Approach Using a Breast MRI Tumor Dataset. J Digit Imaging. 2019;32(5):693&#x2013;701. Epub 2018/10/27. doi: 10.1007/s10278-018-0144-1 ; PubMed Central PMCID: PMC6737125.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10278-018-0144-1</ArticleId><ArticleId IdType="pmc">PMC6737125</ArticleId><ArticleId IdType="pubmed">30361936</ArticleId></ArticleIdList></Reference><Reference><Citation>El Adoui M, Drisis S, Benjelloun M. A PRM approach for early prediction of breast cancer response to chemotherapy based on registered MR images. Int J Comput Assist Radiol Surg. 2018;13(8):1233&#x2013;43. Epub 2018/05/24. doi: 10.1007/s11548-018-1790-y .</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11548-018-1790-y</ArticleId><ArticleId IdType="pubmed">29790078</ArticleId></ArticleIdList></Reference><Reference><Citation>Syed A, Adam R, Ren T, Lu J, Maldjian T, Duong TQ. Machine learning with textural analysis of longitudinal multiparametric MRI and molecular subtypes accurately predicts Pathologic Complete Response in patients with invasive breast cancer. PLoS One. 2022, in press.</Citation></Reference><Reference><Citation>Ravichandran K, Braman N, Janowczyk A, Madabhushib A. A deep learning classifer for prediction of pathological complete response to neoadjuvant chemotherapy from baseline breast DCE-MRI. SPIE Medical Imaging. 2018;10575:105750C&#x2013;1.</Citation></Reference><Reference><Citation>Schettini F, Pascual T, Conte B, Chic N, Braso-Maristany F, Galvan P, et al.. HER2-enriched subtype and pathological complete response in HER2-positive breast cancer: A systematic review and meta-analysis. Cancer Treat Rev. 2020;84:101965. Epub 2020/01/31. doi: 10.1016/j.ctrv.2020.101965 ; PubMed Central PMCID: PMC7230134.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ctrv.2020.101965</ArticleId><ArticleId IdType="pmc">PMC7230134</ArticleId><ArticleId IdType="pubmed">32000054</ArticleId></ArticleIdList></Reference><Reference><Citation>Kalinowski L, Saunus JM, McCart Reed AE, Lakhani SR. Breast Cancer Heterogeneity in Primary and Metastatic Disease. Adv Exp Med Biol. 2019;1152:75&#x2013;104. Epub 2019/08/29. doi: 10.1007/978-3-030-20301-6_6 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-030-20301-6_6</ArticleId><ArticleId IdType="pubmed">31456181</ArticleId></ArticleIdList></Reference><Reference><Citation>Hylton NM, Blume JD, Bernreuter WK, Pisano ED, Rosen MA, Morris EA, et al.. Locally advanced breast cancer: MR imaging for prediction of response to neoadjuvant chemotherapy&#x2014;results from ACRIN 6657/I-SPY TRIAL. Radiology. 2012;263(3):663&#x2013;72. Epub 2012/05/25. doi: 10.1148/radiol.12110748 ; PubMed Central PMCID: PMC3359517.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.12110748</ArticleId><ArticleId IdType="pmc">PMC3359517</ArticleId><ArticleId IdType="pubmed">22623692</ArticleId></ArticleIdList></Reference><Reference><Citation>Hylton NM, Gatsonis CA, Rosen MA, Lehman CD, Newitt DC, Partridge SC, et al.. Neoadjuvant Chemotherapy for Breast Cancer: Functional Tumor Volume by MR Imaging Predicts Recurrence-free Survival-Results from the ACRIN 6657/CALGB 150007 I-SPY 1 TRIAL. Radiology. 2016;279(1):44&#x2013;55. Epub 2015/12/02. doi: 10.1148/radiol.2015150013 ; PubMed Central PMCID: PMC4819899.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2015150013</ArticleId><ArticleId IdType="pmc">PMC4819899</ArticleId><ArticleId IdType="pubmed">26624971</ArticleId></ArticleIdList></Reference><Reference><Citation>He K, Zhang X, Ren S, Sun J, editors. Deep residual learning for image recognition. Proceedings of the IEEE conference on computer vision and pattern recognition; 2016.</Citation></Reference><Reference><Citation>Duanmu H, Huang PB, Brahmava S, Lin S, Ren T, Kong RJ, et al.. Prediction of Pathological Complete Response to Neoadjuvant Chemotherapy in Breast Cancer Using Deep Learning with Integrative Imaging, Molecular and Demographic Data. MICCAI 2020: Medical Image Computing and Computer Assisted Intervention 2020. p. 242&#x2013;52.</Citation></Reference><Reference><Citation>Qu YH, Zhu HT, Cao K, Li XT, Ye M, Sun YS. Prediction of pathological complete response to neoadjuvant chemotherapy in breast cancer using a deep learning (DL) method. Thorac Cancer. 2020;11(3):651&#x2013;8. Epub 2020/01/17. doi: 10.1111/1759-7714.13309 ; PubMed Central PMCID: PMC7049483.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/1759-7714.13309</ArticleId><ArticleId IdType="pmc">PMC7049483</ArticleId><ArticleId IdType="pubmed">31944571</ArticleId></ArticleIdList></Reference><Reference><Citation>Liu MZ, Mutasa S, Chang P, Siddique M, Jambawalikar S, Ha R. A novel CNN algorithm for pathological complete response prediction using an I-SPY TRIAL breast MRI database. Magn Reson Imaging. 2020;73:148&#x2013;51. Epub 2020/09/06. doi: 10.1016/j.mri.2020.08.021 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.mri.2020.08.021</ArticleId><ArticleId IdType="pmc">PMC8111786</ArticleId><ArticleId IdType="pubmed">32889091</ArticleId></ArticleIdList></Reference><Reference><Citation>Huynh BQ, Antropova N, Giger ML. Comparison of breast DCE-MRI contrast time points for predicting response to neoadjuvant chemotherapy using deep convolutional neural network features with transfer learning. SPIE Medical Imaging. 2017;10134. doi: 10.1117/12.2255316</Citation><ArticleIdList><ArticleId IdType="doi">10.1117/12.2255316</ArticleId></ArticleIdList></Reference><Reference><Citation>El Adoui M, Drisis S, Benjelloun M. Predict Breast Tumor Response to Chemotherapy Using a 3D Deep Learning Architecture Applied to DCE-MRI Data. IWBBIO 2019: Bioinformatics and Biomedical Engineering. 2019;11466:8.</Citation></Reference><Reference><Citation>Symmans WF, Wei C, Gould R, Yu X, Zhang Y, Liu M, et al.. Long-Term Prognostic Risk After Neoadjuvant Chemotherapy Associated With Residual Cancer Burden and Breast Cancer Subtype. J Clin Oncol. 2017;35(10):1049&#x2013;60. Epub 2017/01/31. doi: 10.1200/JCO.2015.63.1010 ; PubMed Central PMCID: PMC5455352.</Citation><ArticleIdList><ArticleId IdType="doi">10.1200/JCO.2015.63.1010</ArticleId><ArticleId IdType="pmc">PMC5455352</ArticleId><ArticleId IdType="pubmed">28135148</ArticleId></ArticleIdList></Reference><Reference><Citation>Tahmassebi A, Wengert GJ, Helbich TH, Bago-Horvath Z, Alaei S, Bartsch R, et al.. Impact of Machine Learning With Multiparametric Magnetic Resonance Imaging of the Breast for Early Prediction of Response to Neoadjuvant Chemotherapy and Survival Outcomes in Breast Cancer Patients. Invest Radiol. 2019;54(2):110&#x2013;7. Epub 2018/10/26. doi: 10.1097/RLI.0000000000000518 ; PubMed Central PMCID: PMC6310100.</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/RLI.0000000000000518</ArticleId><ArticleId IdType="pmc">PMC6310100</ArticleId><ArticleId IdType="pubmed">30358693</ArticleId></ArticleIdList></Reference><Reference><Citation>Li J, Zhou Z, Dong J, Fu Y, Li Y, Luan Z, et al.. Predicting breast cancer 5-year survival using machine learning: A systematic review. J PloS one. 2021;16(4):e0250370. doi: 10.1371/journal.pone.0250370</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0250370</ArticleId><ArticleId IdType="pmc">PMC8051758</ArticleId><ArticleId IdType="pubmed">33861809</ArticleId></ArticleIdList></Reference><Reference><Citation>Shouket T, Mahmood S, Hassan MT, Iftikhar A, editors. Overall and Disease-Free Survival Prediction of Postoperative Breast Cancer Patients using Machine Learning Techniques. 2019 22nd International Multitopic Conference (INMIC); 2019: IEEE.</Citation></Reference><Reference><Citation>Ren T, Cattell R, Duanmu H, Huang P, Li H, Vanguri R, et al.. Convolutional Neural Network Detection of Axillary Lymph Node Metastasis Using Standard Clinical Breast MRI. Clin Breast Cancer. 2020;20(3):e301&#x2013;e8. Epub 2020/03/07. doi: 10.1016/j.clbc.2019.11.009 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.clbc.2019.11.009</ArticleId><ArticleId IdType="pubmed">32139272</ArticleId></ArticleIdList></Reference><Reference><Citation>Ren T, Lin S, Huang P, Duong TQ. Convolutional Neural Network of Multiparametric MRI Accurately Detects Axillary Lymph Node Metastasis in Breast Cancer Patients With Pre Neoadjuvant Chemotherapy. Clin Breast Cancer. 2022;22(2):170&#x2013;7. Epub 20210713. doi: 10.1016/j.clbc.2021.07.002 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.clbc.2021.07.002</ArticleId><ArticleId IdType="pubmed">34384696</ArticleId></ArticleIdList></Reference><Reference><Citation>Cattell RF, Kang JJ, Ren T, Huang PB, Muttreja A, Dacosta S, et al.. MRI Volume Changes of Axillary Lymph Nodes as Predictor of Pathologic Complete Responses to Neoadjuvant Chemotherapy in Breast Cancer. Clin Breast Cancer. 2020;20(1):68&#x2013;79 e1. Epub 20190626. doi: 10.1016/j.clbc.2019.06.006 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.clbc.2019.06.006</ArticleId><ArticleId IdType="pubmed">31327729</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36607506</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>06</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1861-6429</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>06</Day></PubDate></JournalIssue><Title>International journal of computer assisted radiology and surgery</Title><ISOAbbreviation>Int J Comput Assist Radiol Surg</ISOAbbreviation></Journal><ArticleTitle>Lesion-preserving unpaired image-to-image translation between MRI and CT from ischemic stroke patients.</ArticleTitle><ELocationID EIdType="doi" ValidYN="Y">10.1007/s11548-022-02828-4</ELocationID><Abstract><AbstractText Label="PURPOSE" NlmCategory="OBJECTIVE">Multiple medical imaging modalities are used for clinical follow-up ischemic stroke analysis. Mixed-modality datasets are challenging, both for clinical rating purposes and for training machine learning models. While image-to-image translation methods have been applied to harmonize stroke patient images to a single modality, they have only been used for paired data so far. In the more common unpaired scenario, the standard cycle-consistent generative adversarial network (CycleGAN) method is not able to translate the stroke lesions properly. Thus, the aim of this work was to develop and evaluate a novel image-to-image translation regularization approach for unpaired 3D follow-up stroke patient datasets.</AbstractText><AbstractText Label="METHODS" NlmCategory="METHODS">A modified CycleGAN was used to translate images between 238 non-contrast computed tomography (NCCT) and 244 fluid-attenuated inversion recovery (FLAIR) MRI datasets, two of the most relevant follow-up modalities in clinical practice. We introduced an additional attention-guided mechanism to encourage an improved translation of the lesion and a gradient-consistency loss to preserve structural brain morphology.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">The proposed modifications were able to preserve the overall quality provided by the CycleGAN translation. This was confirmed by the FID score and gradient correlation results. Furthermore, the lesion preservation was significantly improved compared to a standard CycleGAN. This was evaluated for location and volume with segmentation models, which were trained on real datasets and applied to the translated test images. Here, the Dice score coefficient resulted in 0.81 and 0.62 for datasets translated to FLAIR and NCCT, respectively, compared to 0.57 and 0.50 for the corresponding datasets translated using a standard CycleGAN. Finally, an analysis of the distribution of mean lesion intensities showed substantial improvements.</AbstractText><AbstractText Label="CONCLUSION" NlmCategory="CONCLUSIONS">The results of this work show that the proposed image-to-image translation method is effective at preserving stroke lesions in unpaired modality translation, supporting its potential as a tool for stroke image analysis in real-life scenarios.</AbstractText><CopyrightInformation>&#xa9; 2023. CARS.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Gutierrez</LastName><ForeName>Alejandro</ForeName><Initials>A</Initials><Identifier Source="ORCID">0000-0003-1224-7169</Identifier><AffiliationInfo><Affiliation>Department of Radiology, University of Calgary, 3330 Hospital Drive NW, Calgary, AB, T2N 4N1, Canada. alejandro.gutierrez@ucalgary.ca.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Hotchkiss Brain Institute, University of Calgary, Calgary, AB, Canada. alejandro.gutierrez@ucalgary.ca.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Biomedical Engineering Program, University of Calgary, Calgary, AB, Canada. alejandro.gutierrez@ucalgary.ca.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Alberta Children's Hospital Research Institute, University of Calgary, Calgary, AB, Canada. alejandro.gutierrez@ucalgary.ca.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Tuladhar</LastName><ForeName>Anup</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Department of Radiology, University of Calgary, 3330 Hospital Drive NW, Calgary, AB, T2N 4N1, Canada.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Hotchkiss Brain Institute, University of Calgary, Calgary, AB, Canada.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Alberta Children's Hospital Research Institute, University of Calgary, Calgary, AB, Canada.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wilms</LastName><ForeName>Matthias</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Radiology, University of Calgary, 3330 Hospital Drive NW, Calgary, AB, T2N 4N1, Canada.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Hotchkiss Brain Institute, University of Calgary, Calgary, AB, Canada.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Alberta Children's Hospital Research Institute, University of Calgary, Calgary, AB, Canada.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Rajashekar</LastName><ForeName>Deepthi</ForeName><Initials>D</Initials><AffiliationInfo><Affiliation>Department of Radiology, University of Calgary, 3330 Hospital Drive NW, Calgary, AB, T2N 4N1, Canada.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Hotchkiss Brain Institute, University of Calgary, Calgary, AB, Canada.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Alberta Children's Hospital Research Institute, University of Calgary, Calgary, AB, Canada.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Hill</LastName><ForeName>Michael D</ForeName><Initials>MD</Initials><AffiliationInfo><Affiliation>Department of Radiology, University of Calgary, 3330 Hospital Drive NW, Calgary, AB, T2N 4N1, Canada.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Hotchkiss Brain Institute, University of Calgary, Calgary, AB, Canada.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Clinical Neurosciences, University of Calgary, Calgary, AB, Canada.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Community Health Sciences, Cumming School of Medicine, University of Calgary, Calgary, AB, Canada.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Medicine, Cumming School of Medicine, University of Calgary, Calgary, AB, Canada.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Demchuk</LastName><ForeName>Andrew</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Department of Radiology, University of Calgary, 3330 Hospital Drive NW, Calgary, AB, T2N 4N1, Canada.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Medicine, Cumming School of Medicine, University of Calgary, Calgary, AB, Canada.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Goyal</LastName><ForeName>Mayank</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Radiology, University of Calgary, 3330 Hospital Drive NW, Calgary, AB, T2N 4N1, Canada.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Medicine, Cumming School of Medicine, University of Calgary, Calgary, AB, Canada.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Fiehler</LastName><ForeName>Jens</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Department of Diagnostic and Interventional Neuroradiology, University Medical Center Hamburg-Eppendorf, Martinistr. 52, 20251, Hamburg, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Forkert</LastName><ForeName>Nils D</ForeName><Initials>ND</Initials><AffiliationInfo><Affiliation>Department of Radiology, University of Calgary, 3330 Hospital Drive NW, Calgary, AB, T2N 4N1, Canada.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Hotchkiss Brain Institute, University of Calgary, Calgary, AB, Canada.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Alberta Children's Hospital Research Institute, University of Calgary, Calgary, AB, Canada.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Clinical Neurosciences, University of Calgary, Calgary, AB, Canada.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>06</Day></ArticleDate></Article><MedlineJournalInfo><Country>Germany</Country><MedlineTA>Int J Comput Assist Radiol Surg</MedlineTA><NlmUniqueID>101499225</NlmUniqueID><ISSNLinking>1861-6410</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Computed tomography</Keyword><Keyword MajorTopicYN="N">Data harmonization</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Ischemic stroke</Keyword><Keyword MajorTopicYN="N">Magnetic resonance imaging</Keyword><Keyword MajorTopicYN="N">Modality translation</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>5</Month><Day>13</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>22</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>11</Hour><Minute>19</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36607506</ArticleId><ArticleId IdType="doi">10.1007/s11548-022-02828-4</ArticleId><ArticleId IdType="pii">10.1007/s11548-022-02828-4</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Fiehler J, Thomalla G, Bernhardt M, Kniep H, Berlis A, Dorn F, Eckert B, Kemmling A, Langner S, Remonda L, Reith W, Rohde S, M&#xf6;hlenbruch M, Bendszus M, Forkert ND, Gellissen S (2019) ERASER: a thrombectomy study with predictive analytics end point. Stroke 50(5):1275&#x2013;1278. https://doi.org/10.1161/STROKEAHA.119.024858</Citation><ArticleIdList><ArticleId IdType="doi">10.1161/STROKEAHA.119.024858</ArticleId></ArticleIdList></Reference><Reference><Citation>Makkat S, Vandevenne JE, Verswijvel G, Ijsewijn T, Grieten M, Palmers Y, De Schepper AM, Parizel PM (2002) Signs of acute stroke seen on fluid-attenuated inversion recovery MR imaging. Am J Roentgenol 179(1):237&#x2013;243</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/ajr.179.1.1790237</ArticleId></ArticleIdList></Reference><Reference><Citation>Peultier A-C, Redekop WK, Dippel DW, Bereczki D, Si-Mohamed S, Douek PC, Severens JL (2019) What stroke image do we want? European survey on acute stroke imaging and revascularisation treatment. Health Policy Technol 8(3):261&#x2013;267. https://doi.org/10.1016/j.hlpt.2019.08.005</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.hlpt.2019.08.005</ArticleId></ArticleIdList></Reference><Reference><Citation>Cl&#xe8;rigues A, Valverde S, Bernal J, Freixenet J, Oliver A, Llad&#xf3; X (2020) Acute and sub-acute stroke lesion segmentation from multimodal MRI. Comput Methods Progr Biomed 194:105521. https://doi.org/10.1016/j.cmpb.2020.105521</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cmpb.2020.105521</ArticleId></ArticleIdList></Reference><Reference><Citation>Gillmann C, Peter L, Schmidt C, Saur D, Scheuermann G (2021) Visualizing multimodal deep learning for lesion prediction. IEEE Computer springer nature 2021 LATEX template 14 Lesion-preserving image-to-image translation graphics and applications 41(5), 90&#x2013;98. https://doi.org/10.1109/MCG.2021.3099881</Citation></Reference><Reference><Citation>Lo Vercio L, Amador K, Bannister J, Crites S, Gutierrez A, MacDonald ME, Moore J, Mouches P, Rajasheka D, Schimert S, Subbanna N, Tuladhar A, Wang N, Wilms M, Winder A, Forkert ND (2020) Supervised machine learning tools: a tutorial for clinicians. J Neural Eng 17(6):062001. https://doi.org/10.1088/1741-2552/abbff2</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/1741-2552/abbff2</ArticleId></ArticleIdList></Reference><Reference><Citation>MacEachern SJ, Forkert ND (2021) Machine learning for precision medicine. Genome 64(4):416&#x2013;425. https://doi.org/10.1139/gen-2020-0131</Citation><ArticleIdList><ArticleId IdType="doi">10.1139/gen-2020-0131</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhu J-Y, Park T, Isola P, Efros AA (2017) Unpaired image-to-image translation using cycle-consistent adversarial networks. In: Proceedings of the IEEE international conference on computer vision. pp. 2223&#x2013;2232</Citation></Reference><Reference><Citation>Hiasa Y, Otak Y, Takao M, Matsuoka T, Takashima K, Carass A, Prince J, Sugano N, Sato Y (2018) Cross-modality image synthesis from unpaired data using CycleGAN. In: International workshop on simulation and synthesis in medical imaging. Springer, pp. 31&#x2013;41. https://doi.org/10.1007/978-3-030-00536-84</Citation></Reference><Reference><Citation>Cohen JP, Luck M, Honari S (2018) Distribution matching losses can hallucinate features in medical image translation. In: International conference on medical image computing and computer-assisted intervention. Springer, pp. 529&#x2013;536. https://doi.org/10.1007/978-3-030-00928-160</Citation></Reference><Reference><Citation>Alami Mejjati Y, Richardt C, Tompkin J, Cosker D, Kim KI (2018) Unsupervised attention&#x2013;guided image-to-image translation. Advances in neural information processing systems, 31</Citation></Reference><Reference><Citation>Emami H, DongM, Glide&#x2013;Hurst, CK. (2020) Attention-guided generative adversarial network to address atypical anatomy in synthetic CT generation. In: 2020 IEEE 21st International conference on information reuse and integration for data science (IRI), pp. 188&#x2013;193 IEEE https://doi.org/10.1109/IRI49571.2020.00034</Citation></Reference><Reference><Citation>Abu-Srhan A, Almallahi I, Abushariah MA, Mahafza W, Al-Kadi OS (2021) Paired-unpaired unsupervised attention guided GAN with transfer learning for bidirectional brain MR-CT synthesis. Comput Biol Med 136:104763. https://doi.org/10.1016/j.compbiomed.2021.104763</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2021.104763</ArticleId></ArticleIdList></Reference><Reference><Citation>Gutierrez A, Tuladhar A, Rajashekar D, Forkert ND (2022) Lesion-preserving unpaired image-to-image translation between MRI and CT from ischemic stroke patients. In: Medical imaging 2022: computer-aided diagnosis, vol. 12033, pp. 308&#x2013;314 https://doi.org/10.1117/12.2613203</Citation></Reference><Reference><Citation>Demchuk A, Goyal M, Menon B, Eesa M, Ryckborst K, Kamal N, Patil S, Mishra S, Almekhlafi M, Randhawa P, Roy D, Willinsky R, Montanera W, Silver F, Shuaib A, Rempel J, Jovin T, Frei D, Sapkota B, Hill M (2015) Endovascular treatment for small core and anterior circulation proximal occlusion with emphasis on minimizing CT to recanalization times (ESCAPE) trial: methodology. Int J Stroke 10(3):429&#x2013;438. https://doi.org/10.1111/ijs.12424</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/ijs.12424</ArticleId></ArticleIdList></Reference><Reference><Citation>Cheng B, Forkert ND, Zavaglia M, Hilgetag C, Golsari A, Siemonsen S, Fiehler J, Pedraza S, Puig J, Cho T-H, Alawneh J, Baron J-C, Ostergaard L, Gerloff C, Thomalla G (2014) Influence of stroke infarct location on functional outcome measured by the modified rankin scale. Stroke 45(6):1695&#x2013;1702. https://doi.org/10.1161/STROKEAHA.114.005152</Citation><ArticleIdList><ArticleId IdType="doi">10.1161/STROKEAHA.114.005152</ArticleId></ArticleIdList></Reference><Reference><Citation>Menon B, Al-Ajlan F, Najm M, Puig J, Castellanos M, Dowlatshahi D, Calleja A, Sohn S-I, Ahn SH, Poppe A, Mikul&#xed;k R, Asdaghi N, Field T, Jin A, Asil T, Boulanger J-M, Smith E, Coutts S, Barber P, Demchuk A (2018) Association of clinical, imaging, and thrombus characteristics with recanalization of visible intracranial occlusion in patients with acute ischemic stroke. JAMA 320(10):1017&#x2013;1026. https://doi.org/10.1001/jama.2018.12498</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jama.2018.12498</ArticleId></ArticleIdList></Reference><Reference><Citation>Avants BB, Tustison NJ, Song G, Cook PA, Klein A, Gee JC (2011) A reproducible evaluation of ANTs similarity metric performance in brain image registration. Neuroimage 54(3):2033&#x2013;2044. https://doi.org/10.1016/j.neuroimage.2010.09.025</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2010.09.025</ArticleId></ArticleIdList></Reference><Reference><Citation>Muschelli J, Ullman NL, Mould WA, Vespa P, Hanley DF, Crainiceanu CM (2015) Validated automatic brain extraction of head CT images. Neuroimage 114:379&#x2013;385. https://doi.org/10.1016/j.neuroimage.2015.03.074</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2015.03.074</ArticleId></ArticleIdList></Reference><Reference><Citation>Rajashekar D, Wilms M, MacDonald ME, Ehrhardt J, Mouches P, Frayne R, Hill MD, Forkert ND (2020) High-resolution T2-FLAIR and non-contrast CT brain atlas of the elderly. Sci Data 7(1):1&#x2013;7. https://doi.org/10.1038/s41597-020-0379-9</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41597-020-0379-9</ArticleId></ArticleIdList></Reference><Reference><Citation>Ehrhardt J, S&#xe4;ring D, Handels H (2007) Structure-preserving interpolation of temporal and spatial image sequences using an optical flow-based method. Methods Inf Med 46(03):300&#x2013;307. https://doi.org/10.1160/ME9047</Citation><ArticleIdList><ArticleId IdType="doi">10.1160/ME9047</ArticleId></ArticleIdList></Reference><Reference><Citation>Heusel M, Ramsauer H, Unterthiner T, Nessler B, &amp; Hochreiter S (2017) Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30</Citation></Reference><Reference><Citation>Zhang Y, Liu S, Li C, Wang J (2021) Rethinking the dice loss for deep learning lesion segmentation in medical images. J Shanghai Jiaotong Univ (Sci) 26:93&#x2013;102. https://doi.org/10.1007/s12204-021-2264-x</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s12204-021-2264-x</ArticleId></ArticleIdList></Reference><Reference><Citation>Thiyagarajan SK, Murugan K (2021) A systematic review on techniques adapted for segmentation and classification of ischemic stroke lesions from brain MR images. Wireless Pers Commun 118(2):1225&#x2013;1244. https://doi.org/10.1007/s11277-021-08069-z</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11277-021-08069-z</ArticleId></ArticleIdList></Reference><Reference><Citation>Broocks G, Leischner H, Hanning U, Flottmann F, Faizy T, Sch&#xf6;n G, Sporns P, Thomalla G, Kamalian S, Lev M, Fiehler J, Kemmling A (2020) Lesion age imaging in acute stroke: water uptake in CT versus DWI-FLAIR mismatch. Ann Neurol 88(6):1144&#x2013;1152</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/ana.25903</ArticleId></ArticleIdList></Reference><Reference><Citation>Qazi E, Al-Ajlan F, Mahajan A, Sohn S-I, Mishra SChang H, Najm M, d&#x2019;Esterre C, Demchuk A, Goyal M, Lee T, Hill M, Menon B (2016) Non-contrast CT in place of MRI mismatch in the imaging triage of acute ischemic stroke patients. Med Res Arch 4(6)</Citation></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36607083</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>10</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Print"><Journal><ISSN IssnType="Electronic">1520-8532</ISSN><JournalIssue CitedMedium="Internet"><Volume>40</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>01</Day></PubDate></JournalIssue><Title>Journal of the Optical Society of America. A, Optics, image science, and vision</Title><ISOAbbreviation>J Opt Soc Am A Opt Image Sci Vis</ISOAbbreviation></Journal><ArticleTitle>Image restoration for blurry optical images caused by photon diffusion with deep learning.</ArticleTitle><Pagination><StartPage>96</StartPage><EndPage>107</EndPage><MedlinePgn>96-107</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1364/JOSAA.475890</ELocationID><Abstract><AbstractText>Optical macroscopic imaging techniques have shown great significance in the investigations of biomedical issues by revealing structural or functional information of living bodies through the detection of visible or near-infrared light derived from different mechanisms. However, optical macroscopic imaging techniques suffer from poor spatial resolution due to photon diffusion in biological tissues. This dramatically restricts the application of optical imaging techniques in numerous situations. In this paper, an image restoration method based on deep learning is proposed to eliminate the blur caused by photon diffusion in optical macroscopic imaging. Two blurry images captured at orthogonal angles are used as the additional information to ensure the uniqueness of the solution and restore the small targets at deep locations. Then a fully convolutional neural network is proposed to accomplish the image restoration, which consists of three sectors: V-shaped network for central view, V-shaped network for side views, and synthetical path. The two V-shaped networks are concatenated to the synthetical path with skip connections to generate the output image. Simulations as well as phantom and mouse experiments are implemented. Results indicate the effectiveness of the proposed method.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Xuanxuan</ForeName><Initials>X</Initials></Author><Author ValidYN="Y"><LastName>Cui</LastName><ForeName>Jiapei</ForeName><Initials>J</Initials></Author><Author ValidYN="Y"><LastName>Jia</LastName><ForeName>Yunfei</ForeName><Initials>Y</Initials></Author><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Peng</ForeName><Initials>P</Initials></Author><Author ValidYN="Y"><LastName>Song</LastName><ForeName>Fan</ForeName><Initials>F</Initials></Author><Author ValidYN="Y"><LastName>Cao</LastName><ForeName>Xu</ForeName><Initials>X</Initials></Author><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Jiulou</ForeName><Initials>J</Initials></Author><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Lin</ForeName><Initials>L</Initials></Author><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Guanglei</ForeName><Initials>G</Initials></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>J Opt Soc Am A Opt Image Sci Vis</MedlineTA><NlmUniqueID>9800943</NlmUniqueID><ISSNLinking>1084-7529</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D000818" MajorTopicYN="N">Animals</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D051379" MajorTopicYN="N">Mice</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D007091" MajorTopicYN="N">Image Processing, Computer-Assisted</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D016571" MajorTopicYN="N">Neural Networks, Computer</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D019047" MajorTopicYN="N">Phantoms, Imaging</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D061848" MajorTopicYN="N">Optical Imaging</DescriptorName></MeshHeading></MeshHeadingList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>9</Hour><Minute>6</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>ppublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36607083</ArticleId><ArticleId IdType="doi">10.1364/JOSAA.475890</ArticleId><ArticleId IdType="pii">524422</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36606774</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>10</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Print"><Journal><ISSN IssnType="Electronic">1539-4522</ISSN><JournalIssue CitedMedium="Internet"><Volume>61</Volume><Issue>34</Issue><PubDate><Year>2022</Year><Month>Dec</Month><Day>01</Day></PubDate></JournalIssue><Title>Applied optics</Title><ISOAbbreviation>Appl Opt</ISOAbbreviation></Journal><ArticleTitle>Noise-robust deep learning ghost imaging using a non-overlapping pattern for defect position mapping.</ArticleTitle><Pagination><StartPage>10126</StartPage><EndPage>10133</EndPage><MedlinePgn>10126-10133</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1364/AO.470770</ELocationID><Abstract><AbstractText>Defect detection requires highly sensitive and robust inspection methods. This study shows that non-overlapping illumination patterns can improve the noise robustness of deep learning ghost imaging (DLGI) without modifying the convolutional neural network (CNN). Ghost imaging (GI) can be accelerated by combining GI and deep learning. However, the robustness of DLGI decreases in exchange for higher speed. Using non-overlapping patterns can decrease the noise effects in the input data to the CNN. This study evaluates the DLGI robustness by using non-overlapping patterns generated based on binary notation. The results show that non-overlapping patterns improve the position accuracy by up to 51%, enabling the detection of defect positions with higher accuracy in noisy environments.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Kataoka</LastName><ForeName>Shoma</ForeName><Initials>S</Initials></Author><Author ValidYN="Y"><LastName>Mizutani</LastName><ForeName>Yasuhiro</ForeName><Initials>Y</Initials></Author><Author ValidYN="Y"><LastName>Uenohara</LastName><ForeName>Tsutomu</ForeName><Initials>T</Initials></Author><Author ValidYN="Y"><LastName>Takaya</LastName><ForeName>Yasuhiro</ForeName><Initials>Y</Initials></Author><Author ValidYN="Y"><LastName>Matoba</LastName><ForeName>Osamu</ForeName><Initials>O</Initials></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>Appl Opt</MedlineTA><NlmUniqueID>0247660</NlmUniqueID><ISSNLinking>1559-128X</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D016571" MajorTopicYN="N">Neural Networks, Computer</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D003952" MajorTopicYN="N">Diagnostic Imaging</DescriptorName></MeshHeading></MeshHeadingList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>9</Hour><Minute>3</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>11</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>ppublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36606774</ArticleId><ArticleId IdType="doi">10.1364/AO.470770</ArticleId><ArticleId IdType="pii">522016</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36605945</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>09</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>09</Day></DateRevised><Article PubModel="Electronic-eCollection"><Journal><ISSN IssnType="Print">1664-2392</ISSN><JournalIssue CitedMedium="Print"><Volume>13</Volume><PubDate><Year>2022</Year></PubDate></JournalIssue><Title>Frontiers in endocrinology</Title><ISOAbbreviation>Front Endocrinol (Lausanne)</ISOAbbreviation></Journal><ArticleTitle>Ultrasound images-based deep learning radiomics nomogram for preoperative prediction of <i>RET</i> rearrangement in papillary thyroid carcinoma.</ArticleTitle><Pagination><StartPage>1062571</StartPage><MedlinePgn>1062571</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">1062571</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3389/fendo.2022.1062571</ELocationID><Abstract><AbstractText Label="PURPOSE" NlmCategory="UNASSIGNED">To create an ultrasound -based deep learning radiomics nomogram (DLRN) for preoperatively predicting the presence of <i>RET</i> rearrangement among patients with papillary thyroid carcinoma (PTC).</AbstractText><AbstractText Label="METHODS" NlmCategory="UNASSIGNED">We retrospectively enrolled 650 patients with PTC. Patients were divided into the <i>RET</i>/PTC rearrangement group (n = 103) and the non-<i>RET</i>/PTC rearrangement group (n = 547). Radiomics features were extracted based on hand-crafted features from the ultrasound images, and deep learning networks were used to extract deep transfer learning features. The least absolute shrinkage and selection operator regression was applied to select the features of nonzero coefficients from radiomics and deep transfer learning features; then, we established the deep learning radiomics signature. DLRN was constructed using a logistic regression algorithm by combining clinical and deep learning radiomics signatures. The prediction performance was evaluated using the receiver operating characteristic curve, calibration curve, and decision curve analysis.</AbstractText><AbstractText Label="RESULTS" NlmCategory="UNASSIGNED">Comparing the effectiveness of the models by linking the area under the receiver operating characteristic curve of each model, we found that the area under the curve of DLRN could reach 0.9545 (95% confidence interval: 0.9133-0.9558) in the test cohort and 0.9396 (95% confidence interval: 0.9185-0.9607) in the training cohort, indicating that the model has an excellent performance in predicting <i>RET</i> rearrangement in PTC. The decision curve analysis demonstrated that the combined model was clinically useful.</AbstractText><AbstractText Label="CONCLUSION" NlmCategory="UNASSIGNED">The novel ultrasonic-based DLRN has an important clinical value for predicting <i>RET</i> rearrangement in PTC. It can provide physicians with a preoperative non-invasive primary screening method for <i>RET</i> rearrangement diagnosis, thus facilitating targeted patients with purposeful molecular sequencing to avoid unnecessary medical investment and improve treatment outcomes.</AbstractText><CopyrightInformation>Copyright &#xa9; 2022 Yu, Zhang, Zheng, Jia and Lu.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Yu</LastName><ForeName>Jialong</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Department of Thyroid Surgery, The First Affiliated Hospital of Zhengzhou University, Henan, &#xa0;China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Yihan</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Department of Ophthalmology, The First Affiliated Hospital of Zhengzhou University, Henan, &#xa0;China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zheng</LastName><ForeName>Jian</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Department of Thyroid Surgery, The First Affiliated Hospital of Zhengzhou University, Henan, &#xa0;China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Jia</LastName><ForeName>Meng</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Thyroid Surgery, The First Affiliated Hospital of Zhengzhou University, Henan, &#xa0;China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lu</LastName><ForeName>Xiubo</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>Department of Thyroid Surgery, The First Affiliated Hospital of Zhengzhou University, Henan, &#xa0;China.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>20</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Front Endocrinol (Lausanne)</MedlineTA><NlmUniqueID>101555782</NlmUniqueID><ISSNLinking>1664-2392</ISSNLinking></MedlineJournalInfo><ChemicalList><Chemical><RegistryNumber>EC 2.7.10.1</RegistryNumber><NameOfSubstance UI="C099282">RET protein, human</NameOfSubstance></Chemical><Chemical><RegistryNumber>EC 2.7.10.1</RegistryNumber><NameOfSubstance UI="D051096">Proto-Oncogene Proteins c-ret</NameOfSubstance></Chemical></ChemicalList><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D049451" MajorTopicYN="N">Nomograms</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000077273" MajorTopicYN="N">Thyroid Cancer, Papillary</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName><QualifierName UI="Q000235" MajorTopicYN="N">genetics</QualifierName><QualifierName UI="Q000601" MajorTopicYN="N">surgery</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D012189" MajorTopicYN="N">Retrospective Studies</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D002869" MajorTopicYN="N">Chromosome Aberrations</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D013964" MajorTopicYN="Y">Thyroid Neoplasms</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName><QualifierName UI="Q000235" MajorTopicYN="N">genetics</QualifierName><QualifierName UI="Q000601" MajorTopicYN="N">surgery</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D051096" MajorTopicYN="N">Proto-Oncogene Proteins c-ret</DescriptorName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">RET rearrangement</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword><Keyword MajorTopicYN="N">nomogram</Keyword><Keyword MajorTopicYN="N">papillary thyroid carcinoma</Keyword><Keyword MajorTopicYN="N">prediction</Keyword><Keyword MajorTopicYN="N">radiomics</Keyword></KeywordList><CoiStatement>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>10</Month><Day>6</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>12</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>2</Hour><Minute>44</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36605945</ArticleId><ArticleId IdType="pmc">PMC9807879</ArticleId><ArticleId IdType="doi">10.3389/fendo.2022.1062571</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Siegel RL, Miller KD, Fuchs HE, Jemal A. Cancer statistics, 2021. CA Cancer J Clin (2021) 71(1):7&#x2013;33. doi:&#xa0;10.3322/caac.21654</Citation><ArticleIdList><ArticleId IdType="doi">10.3322/caac.21654</ArticleId><ArticleId IdType="pubmed">33433946</ArticleId></ArticleIdList></Reference><Reference><Citation>Ito Y, Miyauchi A, Kihara M, Fukushima M, Higashiyama T, Miya A. Overall survival of papillary thyroid carcinoma patients: A single-institution long-term follow-up of 5897 patients. World J Surg (2018) 42(3):615&#x2013;22. doi:&#xa0;10.1007/s00268-018-4479-z</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00268-018-4479-z</ArticleId><ArticleId IdType="pmc">PMC5801380</ArticleId><ArticleId IdType="pubmed">29349484</ArticleId></ArticleIdList></Reference><Reference><Citation>Grogan RH, Kaplan SP, Cao H, Weiss RE, Degroot LJ, Simon CA, et al. . A study of recurrence and death from papillary thyroid cancer with 27 years of median follow-up. Surgery (2013) 154(6):1436&#x2013;46;discussion 46-7. doi:&#xa0;10.1016/j.surg.2013.07.008</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.surg.2013.07.008</ArticleId><ArticleId IdType="pubmed">24075674</ArticleId></ArticleIdList></Reference><Reference><Citation>Gan T, Huang B, Chen Q, Sinner HF, Lee CY, Sloan DA, et al. . Risk of recurrence in differentiated thyroid cancer: A population-based comparison of the 7th and 8th editions of the American joint committee on cancer staging systems. Ann Surg Oncol (2019) 26(9):2703&#x2013;10. doi:&#xa0;10.1245/s10434-019-07275-1</Citation><ArticleIdList><ArticleId IdType="doi">10.1245/s10434-019-07275-1</ArticleId><ArticleId IdType="pmc">PMC6684465</ArticleId><ArticleId IdType="pubmed">30830539</ArticleId></ArticleIdList></Reference><Reference><Citation>Xing M. Molecular pathogenesis and mechanisms of thyroid cancer. Nat Rev Cancer (2013) 13(3):184&#x2013;99. doi:&#xa0;10.1038/nrc3431</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nrc3431</ArticleId><ArticleId IdType="pmc">PMC3791171</ArticleId><ArticleId IdType="pubmed">23429735</ArticleId></ArticleIdList></Reference><Reference><Citation>Chu YH, Wirth LJ, Farahani AA, Nose V, Faquin WC, Dias-Santagata D, et al. . Clinicopathologic features of kinase fusion-related thyroid carcinomas: An integrative analysis with molecular characterization. Mod Pathol (2020) 33(12):2458&#x2013;72. doi:&#xa0;10.1038/s41379-020-0638-5</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41379-020-0638-5</ArticleId><ArticleId IdType="pmc">PMC7688509</ArticleId><ArticleId IdType="pubmed">32737449</ArticleId></ArticleIdList></Reference><Reference><Citation>Li AY, McCusker MG, Russo A, Scilla KA, Gittens A, Arensmeyer K, et al. . Ret fusions in solid tumors. Cancer Treat Rev (2019) 81:101911. doi:&#xa0;10.1016/j.ctrv.2019.101911</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ctrv.2019.101911</ArticleId><ArticleId IdType="pubmed">31715421</ArticleId></ArticleIdList></Reference><Reference><Citation>Fagin JA, Mitsiades N. Molecular pathology of thyroid cancer: Diagnostic and clinical implications. Best Pract Res Clin Endocrinol Metab (2008) 22(6):955&#x2013;69. doi:&#xa0;10.1016/j.beem.2008.09.017</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.beem.2008.09.017</ArticleId><ArticleId IdType="pmc">PMC2615540</ArticleId><ArticleId IdType="pubmed">19041825</ArticleId></ArticleIdList></Reference><Reference><Citation>Romei C, Elisei R. Ret/Ptc translocations and clinico-pathological features in human papillary thyroid carcinoma. Front Endocrinol (Lausanne) (2012) 3:54. doi:&#xa0;10.3389/fendo.2012.00054</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fendo.2012.00054</ArticleId><ArticleId IdType="pmc">PMC3356050</ArticleId><ArticleId IdType="pubmed">22654872</ArticleId></ArticleIdList></Reference><Reference><Citation>Zafon C, Obiols G, Castellv&#xed; J, Tallada N, Baena JA, Sim&#xf3; R, et al. . Clinical significance of Ret/Ptc and P53 protein expression in sporadic papillary thyroid carcinoma. Histopathology (2007) 50(2):225&#x2013;31. doi:&#xa0;10.1111/j.1365-2559.2006.02555.x</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/j.1365-2559.2006.02555.x</ArticleId><ArticleId IdType="pubmed">17222251</ArticleId></ArticleIdList></Reference><Reference><Citation>Fisher SB, Cote GJ, Bui-Griffith JH, Lu W, Tang X, Hai T, et al. . Genetic characterization of medullary thyroid cancer in childhood survivors of the Chernobyl accident. Surgery (2019) 165(1):58&#x2013;63. doi:&#xa0;10.1016/j.surg.2018.08.029</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.surg.2018.08.029</ArticleId><ArticleId IdType="pubmed">30392857</ArticleId></ArticleIdList></Reference><Reference><Citation>Adeniran AJ, Zhu Z, Gandhi M, Steward DL, Fidler JP, Giordano TJ, et al. . Correlation between genetic alterations and microscopic features, clinical manifestations, and prognostic characteristics of thyroid papillary carcinomas. Am J Surg Pathol (2006) 30(2):216&#x2013;22. doi:&#xa0;10.1097/01.pas.0000176432.73455.1b</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/01.pas.0000176432.73455.1b</ArticleId><ArticleId IdType="pubmed">16434896</ArticleId></ArticleIdList></Reference><Reference><Citation>Ullmann TM, Thiesmeyer JW, Lee YJ, Beg S, Mosquera JM, Elemento O, et al. . Ret fusion-positive papillary thyroid cancers are associated with a more aggressive phenotype. Ann Surg Oncol (2022) 29(7):4266&#x2013;73. doi:&#xa0;10.1245/s10434-022-11418-2</Citation><ArticleIdList><ArticleId IdType="doi">10.1245/s10434-022-11418-2</ArticleId><ArticleId IdType="pubmed">35230579</ArticleId></ArticleIdList></Reference><Reference><Citation>Cancer Genome Atlas Research N . Integrated genomic characterization of papillary thyroid carcinoma. Cell (2014) 159(3):676&#x2013;90. doi:&#xa0;10.1016/j.cell.2014.09.050</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cell.2014.09.050</ArticleId><ArticleId IdType="pmc">PMC4243044</ArticleId><ArticleId IdType="pubmed">25417114</ArticleId></ArticleIdList></Reference><Reference><Citation>Di Cristofaro J, Vasko V, Savchenko V, Cherenko S, Larin A, Ringel MD, et al. . Ret/Ptc1 and Ret/Ptc3 in thyroid tumors from Chernobyl liquidators: Comparison with sporadic tumors from Ukrainian and French patients. Endocr Relat Cancer (2005) 12(1):173&#x2013;83. doi:&#xa0;10.1677/erc.1.00884</Citation><ArticleIdList><ArticleId IdType="doi">10.1677/erc.1.00884</ArticleId><ArticleId IdType="pubmed">15788648</ArticleId></ArticleIdList></Reference><Reference><Citation>Lewinski A, Adamczewski Z, Zygmunt A, Markuszewski L, Karbownik-Lewinska M, Stasiak M. Correlations between molecular landscape and sonographic image of different variants of papillary thyroid carcinoma. J Clin Med (2019) 8(11):1916. doi:&#xa0;10.3390/jcm8111916</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/jcm8111916</ArticleId><ArticleId IdType="pmc">PMC6912205</ArticleId><ArticleId IdType="pubmed">31717363</ArticleId></ArticleIdList></Reference><Reference><Citation>Park VY, Lee E, Lee HS, Kim HJ, Yoon J, Son J, et al. . Combining radiomics with ultrasound-based risk stratification systems for thyroid nodules: An approach for improving performance. Eur Radiol (2021) 31(4):2405&#x2013;13. doi:&#xa0;10.1007/s00330-020-07365-9</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-020-07365-9</ArticleId><ArticleId IdType="pubmed">33034748</ArticleId></ArticleIdList></Reference><Reference><Citation>Yoon JH, Han K, Lee E, Lee J, Kim EK, Moon HJ, et al. . Radiomics in predicting mutation status for thyroid cancer: A preliminary study using radiomics features for predicting Brafv600e mutations in papillary thyroid carcinoma. PloS One (2020) 15(2):e0228968. doi:&#xa0;10.1371/journal.pone.0228968</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0228968</ArticleId><ArticleId IdType="pmc">PMC7018006</ArticleId><ArticleId IdType="pubmed">32053670</ArticleId></ArticleIdList></Reference><Reference><Citation>Kwon MR, Shin JH, Park H, Cho H, Hahn SY, Park KW. Radiomics study of thyroid ultrasound for predicting braf mutation in papillary thyroid carcinoma: Preliminary results. AJNR Am J Neuroradiol (2020) 41(4):700&#x2013;5. doi:&#xa0;10.3174/ajnr.A6505</Citation><ArticleIdList><ArticleId IdType="doi">10.3174/ajnr.A6505</ArticleId><ArticleId IdType="pmc">PMC7144636</ArticleId><ArticleId IdType="pubmed">32273326</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang YG, Xu FJ, Agyekum EA, Xiang H, Wang YD, Zhang J, et al. . Radiomic model for determining the value of elasticity and grayscale ultrasound diagnoses for predicting Braf(V600e) mutations in papillary thyroid carcinoma. Front Endocrinol (Lausanne) (2022) 13:872153. doi:&#xa0;10.3389/fendo.2022.872153</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fendo.2022.872153</ArticleId><ArticleId IdType="pmc">PMC9074386</ArticleId><ArticleId IdType="pubmed">35527993</ArticleId></ArticleIdList></Reference><Reference><Citation>Handelman GS, Kok HK, Chandra RV, Razavi AH, Lee MJ, Asadi H. Edoctor: Machine learning and the future of medicine. J Internal Med (2018) 284(6):603&#x2013;19. doi:&#xa0;10.1111/joim.12822</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/joim.12822</ArticleId><ArticleId IdType="pubmed">30102808</ArticleId></ArticleIdList></Reference><Reference><Citation>Hosny A, Parmar C, Quackenbush J, Schwartz LH, Aerts H. Artificial intelligence in radiology. Nat Rev Cancer (2018) 18(8):500&#x2013;10. doi:&#xa0;10.1038/s41568-018-0016-5</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41568-018-0016-5</ArticleId><ArticleId IdType="pmc">PMC6268174</ArticleId><ArticleId IdType="pubmed">29777175</ArticleId></ArticleIdList></Reference><Reference><Citation>Jiang Y, Yang M, Wang S, Li X, Sun Y. Emerging role of deep learning-based artificial intelligence in tumor pathology. Cancer Commun (Lond) (2020) 40(4):154&#x2013;66. doi:&#xa0;10.1002/cac2.12012</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/cac2.12012</ArticleId><ArticleId IdType="pmc">PMC7170661</ArticleId><ArticleId IdType="pubmed">32277744</ArticleId></ArticleIdList></Reference><Reference><Citation>Bini F, Pica A, Azzimonti L, Giusti A, Ruinelli L, Marinozzi F, et al. . Artificial intelligence in thyroid field-a comprehensive review. Cancers (Basel) (2021) 13(19):4740. doi:&#xa0;10.3390/cancers13194740</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/cancers13194740</ArticleId><ArticleId IdType="pmc">PMC8507551</ArticleId><ArticleId IdType="pubmed">34638226</ArticleId></ArticleIdList></Reference><Reference><Citation>Sorrenti S, Dolcetti V, Radzina M, Bellini MI, Frezza F, Munir K, et al. . Artificial intelligence for thyroid nodule characterization: Where are we standing? Cancers (Basel) (2022) 14(14):3357. doi:&#xa0;10.3390/cancers14143357</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/cancers14143357</ArticleId><ArticleId IdType="pmc">PMC9315681</ArticleId><ArticleId IdType="pubmed">35884418</ArticleId></ArticleIdList></Reference><Reference><Citation>Mayerhoefer ME, Materka A, Langs G, Haggstrom I, Szczypinski P, Gibbs P, et al. . Introduction to radiomics. J Nucl Med (2020) 61(4):488&#x2013;95. doi:&#xa0;10.2967/jnumed.118.222893</Citation><ArticleIdList><ArticleId IdType="doi">10.2967/jnumed.118.222893</ArticleId><ArticleId IdType="pmc">PMC9374044</ArticleId><ArticleId IdType="pubmed">32060219</ArticleId></ArticleIdList></Reference><Reference><Citation>Qi Y, Zhao T, Han M. The application of radiomics in predicting gene mutations in cancer. Eur Radiol (2022) 32(6):4014&#x2013;24. doi:&#xa0;10.1007/s00330-021-08520-6</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-021-08520-6</ArticleId><ArticleId IdType="pubmed">35048135</ArticleId></ArticleIdList></Reference><Reference><Citation>Tang J, Jiang S, Ma J, Xi X, Li H, Wang L, et al. . Nomogram based on radiomics analysis of ultrasound images can improve preoperative braf mutation diagnosis for papillary thyroid microcarcinoma. Front Endocrinol (Lausanne) (2022) 13:915135. doi:&#xa0;10.3389/fendo.2022.915135</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fendo.2022.915135</ArticleId><ArticleId IdType="pmc">PMC9437521</ArticleId><ArticleId IdType="pubmed">36060960</ArticleId></ArticleIdList></Reference><Reference><Citation>Shangguan R, Hu YP, Huang J, Yang SJ, Ye L, Lin RX, et al. . Association between Braf(V600e) mutation and the American college of radiology thyroid imaging, reporting and data system in solitary papillary thyroid carcinoma. Acad Radiol (2019) 26(2):154&#x2013;60. doi:&#xa0;10.1016/j.acra.2018.05.010</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.acra.2018.05.010</ArticleId><ArticleId IdType="pubmed">29941398</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36605863</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>06</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Print">2233-7822</ISSN><JournalIssue CitedMedium="Print"><Volume>52</Volume><Issue>4</Issue><PubDate><Year>2022</Year><Month>Dec</Month></PubDate></JournalIssue><Title>Imaging science in dentistry</Title><ISOAbbreviation>Imaging Sci Dent</ISOAbbreviation></Journal><ArticleTitle>Deep learning-based apical lesion segmentation from panoramic radiographs.</ArticleTitle><Pagination><StartPage>351</StartPage><EndPage>357</EndPage><MedlinePgn>351-357</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.5624/isd.20220078</ELocationID><Abstract><AbstractText Label="PURPOSE" NlmCategory="UNASSIGNED">Convolutional neural networks (CNNs) have rapidly emerged as one of the most promising artificial intelligence methods in the field of medical and dental research. CNNs can provide an effective diagnostic methodology allowing for the detection of early-staged diseases. Therefore, this study aimed to evaluate the performance of a deep CNN algorithm for apical lesion segmentation from panoramic radiographs.</AbstractText><AbstractText Label="MATERIALS AND METHODS" NlmCategory="UNASSIGNED">A total of 1000 panoramic images showing apical lesions were separated into training (n=800, 80%), validation (n=100, 10%), and test (n=100, 10%) datasets. The performance of identifying apical lesions was evaluated by calculating the precision, recall, and F1-score.</AbstractText><AbstractText Label="RESULTS" NlmCategory="UNASSIGNED">In the test group of 180 apical lesions, 147 lesions were segmented from panoramic radiographs with an intersection over union (IoU) threshold of 0.3. The F1-score values, as a measure of performance, were 0.828, 0.815, and 0.742, respectively, with IoU thresholds of 0.3, 0.4, and 0.5.</AbstractText><AbstractText Label="CONCLUSION" NlmCategory="UNASSIGNED">This study showed the potential utility of a deep learning-guided approach for the segmentation of apical lesions. The deep CNN algorithm using U-Net demonstrated considerably high performance in detecting apical lesions.</AbstractText><CopyrightInformation>Copyright &#xa9; 2022 by Korean Academy of Oral and Maxillofacial Radiology.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Song</LastName><ForeName>Il-Seok</ForeName><Initials>IS</Initials><Identifier Source="ORCID">0000-0002-4407-5982</Identifier><AffiliationInfo><Affiliation>Department of Oral and Maxillofacial Radiology and Dental Research Institute, School of Dentistry, Seoul National University, Seoul, Korea.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Shin</LastName><ForeName>Hak-Kyun</ForeName><Initials>HK</Initials><Identifier Source="ORCID">0000-0002-7973-0262</Identifier><AffiliationInfo><Affiliation>AI Research Center, DDH Incorporation, Seoul, Korea.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kang</LastName><ForeName>Ju-Hee</ForeName><Initials>JH</Initials><Identifier Source="ORCID">0000-0003-3344-4807</Identifier><AffiliationInfo><Affiliation>Department of Oral and Maxillofacial Radiology and Dental Research Institute, School of Dentistry, Seoul National University, Seoul, Korea.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kim</LastName><ForeName>Jo-Eun</ForeName><Initials>JE</Initials><Identifier Source="ORCID">0000-0003-0218-5304</Identifier><AffiliationInfo><Affiliation>Department of Oral and Maxillofacial Radiology and Dental Research Institute, School of Dentistry, Seoul National University, Seoul, Korea.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Huh</LastName><ForeName>Kyung-Hoe</ForeName><Initials>KH</Initials><Identifier Source="ORCID">0000-0002-8771-0392</Identifier><AffiliationInfo><Affiliation>Department of Oral and Maxillofacial Radiology and Dental Research Institute, School of Dentistry, Seoul National University, Seoul, Korea.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yi</LastName><ForeName>Won-Jin</ForeName><Initials>WJ</Initials><Identifier Source="ORCID">0000-0002-5977-6634</Identifier><AffiliationInfo><Affiliation>Department of Oral and Maxillofacial Radiology and Dental Research Institute, School of Dentistry, Seoul National University, Seoul, Korea.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lee</LastName><ForeName>Sam-Sun</ForeName><Initials>SS</Initials><Identifier Source="ORCID">0000-0001-7223-9262</Identifier><AffiliationInfo><Affiliation>Department of Oral and Maxillofacial Radiology and Dental Research Institute, School of Dentistry, Seoul National University, Seoul, Korea.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Heo</LastName><ForeName>Min-Suk</ForeName><Initials>MS</Initials><Identifier Source="ORCID">0000-0003-3406-0645</Identifier><AffiliationInfo><Affiliation>Department of Oral and Maxillofacial Radiology and Dental Research Institute, School of Dentistry, Seoul National University, Seoul, Korea.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>07</Month><Day>23</Day></ArticleDate></Article><MedlineJournalInfo><Country>Korea (South)</Country><MedlineTA>Imaging Sci Dent</MedlineTA><NlmUniqueID>101559249</NlmUniqueID><ISSNLinking>2233-7822</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Artificial Intelligence</Keyword><Keyword MajorTopicYN="N">Deep Learning</Keyword><Keyword MajorTopicYN="N">Periapical Periodontitis</Keyword><Keyword MajorTopicYN="N">Radiography, Panoramic</Keyword></KeywordList><CoiStatement>Conflicts of Interest: The second author, Hak-Kyun Shin, invented the AI software model (called Deep Stack) and is still working for the company (DDH Inc., Seoul, South Korea) that developed it. The other authors have no conflict of interest to declare. The funders had no role in the design of the study; in the collection, analysis, or interpretation of data; in the writing of the manuscript, or in the decision to publish the results.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>4</Month><Day>26</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>5</Month><Day>30</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>6</Month><Day>20</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>2</Hour><Minute>42</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>ppublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36605863</ArticleId><ArticleId IdType="pmc">PMC9807797</ArticleId><ArticleId IdType="doi">10.5624/isd.20220078</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Leonardi Dutra K, Haas L, Porporatti AL, Flores-Mir C, Nascimento Santos J, Mezzomo LA, et al. Diagnostic accuracy of cone-beam computed tomography and conventional radiography on apical periodontitis: a systematic review and meta-analysis. J Endod. 2016;42:356&#x2013;364.</Citation><ArticleIdList><ArticleId IdType="pubmed">26902914</ArticleId></ArticleIdList></Reference><Reference><Citation>Choi JW. Assessment of panoramic radiography as a national oral examination tool: review of the literature. Imaging Sci Dent. 2011;41:1&#x2013;6.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC3174459</ArticleId><ArticleId IdType="pubmed">21977466</ArticleId></ArticleIdList></Reference><Reference><Citation>Nardi C, Calistri L, Grazzini G, Desideri I, Lorini C, Occhipinti M, et al. Is panoramic radiography an accurate imaging technique for the detection of endodontically treated asymptomatic apical periodontitis? J Endod. 2018;44:1500&#x2013;1508.</Citation><ArticleIdList><ArticleId IdType="pubmed">30154006</ArticleId></ArticleIdList></Reference><Reference><Citation>Nardi C, Calistri L, Pradella S, Desideri I, Lorini C, Colagrande S. Accuracy of orthopantomography for apical periodontitis without endodontic treatment. J Endod. 2017;43:1640&#x2013;1646.</Citation><ArticleIdList><ArticleId IdType="pubmed">28807372</ArticleId></ArticleIdList></Reference><Reference><Citation>Parker JM, Mol A, Rivera EM, Tawil PZ. Cone-beam computed tomography uses in clinical endodontics: observer variability in detecting periapical lesions. J Endod. 2017;43:184&#x2013;187.</Citation><ArticleIdList><ArticleId IdType="pubmed">28024758</ArticleId></ArticleIdList></Reference><Reference><Citation>Nagi R, Aravinda K, Rakesh N, Gupta R, Pal A, Mann AK. Clinical applications and performance of intelligent systems in dental and maxillofacial radiology: a review. Imaging Sci Dent. 2020;50:81&#x2013;92.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7314602</ArticleId><ArticleId IdType="pubmed">32601582</ArticleId></ArticleIdList></Reference><Reference><Citation>Hwang JJ, Jung YH, Cho BH, Heo MS. An overview of deep learning in the field of dentistry. Imaging Sci Dent. 2019;49:1&#x2013;7.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6444007</ArticleId><ArticleId IdType="pubmed">30941282</ArticleId></ArticleIdList></Reference><Reference><Citation>Schmidhuber J. Deep learning in neural networks: an overview. Neural Netw. 2015;61:85&#x2013;117.</Citation><ArticleIdList><ArticleId IdType="pubmed">25462637</ArticleId></ArticleIdList></Reference><Reference><Citation>Ekert T, Krois J, Meinhold L, Elhennawy K, Emara R, Golla T, et al. Deep learning for the radiographic detection of apical lesions. J Endod. 2019;45:917&#x2013;922.</Citation><ArticleIdList><ArticleId IdType="pubmed">31160078</ArticleId></ArticleIdList></Reference><Reference><Citation>Y&#xfc;ksel AE, G&#xfc;ltekin S, Simsar E, &#xd6;zdemir &#x15e;D, G&#xfc;ndo&#x11f;ar M, Tokg&#xf6;z SB, et al. Dental enumeration and multiple treatment detection on panoramic X-rays using deep learning. Sci Rep. 2021;11:12342.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8196057</ArticleId><ArticleId IdType="pubmed">34117279</ArticleId></ArticleIdList></Reference><Reference><Citation>Kwon O, Yong TH, Kang SR, Kim JE, Huh KH, Heo MS, et al. Automatic diagnosis for cysts and tumors of both jaws on panoramic radiographs using a deep convolution neural network. Dentomaxillofac Radiol. 2020;49:20200185.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7719862</ArticleId><ArticleId IdType="pubmed">32574113</ArticleId></ArticleIdList></Reference><Reference><Citation>Wei P, Ball JE, Anderson DT. Fusion of an ensemble of augmented image detectors for robust object detection. Sensors (Basel) 2018;18:894.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5876712</ArticleId><ArticleId IdType="pubmed">29562609</ArticleId></ArticleIdList></Reference><Reference><Citation>Le&#x2019;Clerc Arrastia J, Heilenk&#xf6;tter N, Otero Baguer D, Hauberg-Lotte L, Boskamp T, Hetzer S, et al. Deeply supervised UNet for semantic segmentation to assist dermatopathological assessment of basal cell carcinoma. J Imaging. 2021;7:71.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8321345</ArticleId><ArticleId IdType="pubmed">34460521</ArticleId></ArticleIdList></Reference><Reference><Citation>Lee JH, Kim DH, Jeong SN, Choi SH. Detection and diagnosis of dental caries using a deep learning-based convolutional neural network algorithm. J Dent. 2018;77:106&#x2013;111.</Citation><ArticleIdList><ArticleId IdType="pubmed">30056118</ArticleId></ArticleIdList></Reference><Reference><Citation>Lee JH, Kim DH, Jeong SN, Choi SH. Diagnosis and prediction of periodontally compromised teeth using a deep learning-based convolutional neural network algorithm. J Periodontal Implant Sci. 2018;48:114&#x2013;123.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5944222</ArticleId><ArticleId IdType="pubmed">29770240</ArticleId></ArticleIdList></Reference><Reference><Citation>Chlap P, Min H, Vandenberg N, Dowling J, Holloway L, Haworth A. A review of medical image data augmentation techniques for deep learning applications. J Med Imaging Radiat Oncol. 2021;65:545&#x2013;563.</Citation><ArticleIdList><ArticleId IdType="pubmed">34145766</ArticleId></ArticleIdList></Reference><Reference><Citation>Ganesan P, Rajaraman S, Long R, Ghoraani B, Antani S. Assessment of data augmentation strategies toward performance improvement of abnormality classification in chest radiographs. Annu Int Conf IEEE Eng Med Biol Soc. 2019;2019:841&#x2013;844.</Citation><ArticleIdList><ArticleId IdType="pubmed">31946026</ArticleId></ArticleIdList></Reference><Reference><Citation>Liu G, Zhou W, Tian L, Liu W, Liu Y, Xu H. An efficient and accurate iris recognition algorithm based on a novel condensed 2-ch deep convolutional neural network. Sensors (Basel) 2021;21:3721.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8197830</ArticleId><ArticleId IdType="pubmed">34071850</ArticleId></ArticleIdList></Reference><Reference><Citation>Sabottke CF, Spieler BM. The effect of image resolution on deep learning in radiography. Radiol Artif Intell. 2020;2:e190015.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8017385</ArticleId><ArticleId IdType="pubmed">33937810</ArticleId></ArticleIdList></Reference><Reference><Citation>Lakhani P, Sundaram B. Deep learning at chest radiography: automated classification of pulmonary tuberculosis by using convolutional neural networks. Radiol. 2017;284:574&#x2013;582.</Citation><ArticleIdList><ArticleId IdType="pubmed">28436741</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36605859</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>06</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Print">2233-7822</ISSN><JournalIssue CitedMedium="Print"><Volume>52</Volume><Issue>4</Issue><PubDate><Year>2022</Year><Month>Dec</Month></PubDate></JournalIssue><Title>Imaging science in dentistry</Title><ISOAbbreviation>Imaging Sci Dent</ISOAbbreviation></Journal><ArticleTitle>Comparison of Multi-Label U-Net and Mask R-CNN for panoramic radiograph segmentation to detect periodontitis.</ArticleTitle><Pagination><StartPage>383</StartPage><EndPage>391</EndPage><MedlinePgn>383-391</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.5624/isd.20220105</ELocationID><Abstract><AbstractText Label="PURPOSE" NlmCategory="UNASSIGNED">Periodontitis, the most prevalent chronic inflammatory condition affecting teeth-supporting tissues, is diagnosed and classified through clinical and radiographic examinations. The staging of periodontitis using panoramic radiographs provides information for designing computer-assisted diagnostic systems. Performing image segmentation in periodontitis is required for image processing in diagnostic applications. This study evaluated image segmentation for periodontitis staging based on deep learning approaches.</AbstractText><AbstractText Label="MATERIALS AND METHODS" NlmCategory="UNASSIGNED">Multi-Label U-Net and Mask R-CNN models were compared for image segmentation to detect periodontitis using 100 digital panoramic radiographs. Normal conditions and 4 stages of periodontitis were annotated on these panoramic radiographs. A total of 1100 original and augmented images were then randomly divided into a training (75%) dataset to produce segmentation models and a testing (25%) dataset to determine the evaluation metrics of the segmentation models.</AbstractText><AbstractText Label="RESULTS" NlmCategory="UNASSIGNED">The performance of the segmentation models against the radiographic diagnosis of periodontitis conducted by a dentist was described by evaluation metrics (i.e., dice coefficient and intersection-over-union [IoU] score). Multi-Label U-Net achieved a dice coefficient of 0.96 and an IoU score of 0.97. Meanwhile, Mask R-CNN attained a dice coefficient of 0.87 and an IoU score of 0.74. U-Net showed the characteristic of semantic segmentation, and Mask R-CNN performed instance segmentation with accuracy, precision, recall, and F1-score values of 95%, 85.6%, 88.2%, and 86.6%, respectively.</AbstractText><AbstractText Label="CONCLUSION" NlmCategory="UNASSIGNED">Multi-Label U-Net produced superior image segmentation to that of Mask R-CNN. The authors recommend integrating it with other techniques to develop hybrid models for automatic periodontitis detection.</AbstractText><CopyrightInformation>Copyright &#xa9; 2022 by Korean Academy of Oral and Maxillofacial Radiology.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Widyaningrum</LastName><ForeName>Rini</ForeName><Initials>R</Initials><Identifier Source="ORCID">0000-0002-5014-9554</Identifier><AffiliationInfo><Affiliation>Department of Dentomaxillofacial Radiology, Faculty of Dentistry, Universitas Gadjah Mada, Yogyakarta, Indonesia.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Candradewi</LastName><ForeName>Ika</ForeName><Initials>I</Initials><Identifier Source="ORCID">0000-0002-3306-2035</Identifier><AffiliationInfo><Affiliation>Department of Computer Science and Electronics, Faculty of Mathematics and Natural Sciences, Universitas Gadjah Mada, Yogyakarta, Indonesia.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Aji</LastName><ForeName>Nur Rahman Ahmad Seno</ForeName><Initials>NRAS</Initials><Identifier Source="ORCID">0000-0002-1779-8763</Identifier><AffiliationInfo><Affiliation>Department of Periodontics, Faculty of Dentistry, Universitas Gadjah Mada, Yogyakarta, Indonesia.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Aulianisa</LastName><ForeName>Rona</ForeName><Initials>R</Initials><Identifier Source="ORCID">0000-0002-0170-2683</Identifier><AffiliationInfo><Affiliation>Faculty of Dentistry, Universitas Gadjah Mada, Yogyakarta, Indonesia.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>10</Month><Day>12</Day></ArticleDate></Article><MedlineJournalInfo><Country>Korea (South)</Country><MedlineTA>Imaging Sci Dent</MedlineTA><NlmUniqueID>101559249</NlmUniqueID><ISSNLinking>2233-7822</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Deep Learning</Keyword><Keyword MajorTopicYN="N">Periodontitis</Keyword><Keyword MajorTopicYN="N">Radiography, Panoramic</Keyword><Keyword MajorTopicYN="N">Tooth</Keyword></KeywordList><CoiStatement>Conflicts of Interest: None</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>6</Month><Day>12</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>9</Month><Day>3</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>9</Month><Day>9</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>2</Hour><Minute>42</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>ppublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36605859</ArticleId><ArticleId IdType="pmc">PMC9807794</ArticleId><ArticleId IdType="doi">10.5624/isd.20220105</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Papapanou PN, Sanz M, Buduneli N, Dietrich T, Feres M, Fine DH, et al. Periodontitis: consensus report of workgroup 2 of the 2017 World Workshop on the Classification of Periodontal and Peri-Implant Diseases and Conditions. J Periodontol. 2018;89 Suppl 1:S173&#x2013;S182.</Citation><ArticleIdList><ArticleId IdType="pubmed">29926951</ArticleId></ArticleIdList></Reference><Reference><Citation>Teeuw WJ, Coelho L, Silva A, van der Palen CJ, Lessmann FG, van der Velden U, et al. Validation of a dental image analyzer tool to measure alveolar bone loss in periodontitis patients. J Periodontal Res. 2009;44:94&#x2013;102.</Citation><ArticleIdList><ArticleId IdType="pubmed">18973543</ArticleId></ArticleIdList></Reference><Reference><Citation>Preshaw PM. Detection and diagnosis of periodontal conditions amenable to prevention. BMC Oral Health. 2015;15 Suppl 1:S5.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4580822</ArticleId><ArticleId IdType="pubmed">26390822</ArticleId></ArticleIdList></Reference><Reference><Citation>Hanindriyo L, Widita E, Widyaningrum R, Priyono B, Agustina D. Influence of residential characteristics on the association between the oral health status and BMI of older adults in Indonesia. Gerodontology. 2018;35:268&#x2013;275.</Citation><ArticleIdList><ArticleId IdType="pubmed">29808561</ArticleId></ArticleIdList></Reference><Reference><Citation>Zia A, Hakim S, Khan AU, Bey A, Ateeq H, Parveen S, et al. Bone markers and bone mineral density associates with periodontitis in females with poly-cystic ovarian syndrome. J Bone Miner Metab. 2022;40:487&#x2013;497.</Citation><ArticleIdList><ArticleId IdType="pubmed">35072780</ArticleId></ArticleIdList></Reference><Reference><Citation>Widita E, Hanindriyo L, Widyaningrum R, Priyono B, Agustina D. The association between periodontal conditions and serum lipids among elderly participants in Gadjah Mada medical centre, Yogyakarta. J Dent Indones. 2017;24:63&#x2013;69.</Citation></Reference><Reference><Citation>Hosny A, Parmar C, Quackenbush J, Schwartz LH, Aerts HJ. Artificial intelligence in radiology. Nat Rev Cancer. 2018;18:500&#x2013;510.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6268174</ArticleId><ArticleId IdType="pubmed">29777175</ArticleId></ArticleIdList></Reference><Reference><Citation>Gadosey PK, Li Y, Adjei Agyekum E, Zhang T, Liu Z, Yamak PT, et al. SD-UNet: stripping down U-Net for segmentation of biomedical images on platforms with low computational budgets. Diagnostics (Basel) 2020;10:110.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7167802</ArticleId><ArticleId IdType="pubmed">32085469</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhou Z, Siddiquee MM, Tajbakhsh N, Liang J. UNet++: redesigning skip connections to exploit multiscale features in image segmentation. IEEE Trans Med Imaging. 2020;39:1856&#x2013;1867.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7357299</ArticleId><ArticleId IdType="pubmed">31841402</ArticleId></ArticleIdList></Reference><Reference><Citation>Thanathornwong B, Suebnukarn S. Automatic detection of periodontal compromised teeth in digital panoramic radiographs using faster regional convolutional neural networks. Imaging Sci Dent. 2020;50:169&#x2013;174.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7314603</ArticleId><ArticleId IdType="pubmed">32601592</ArticleId></ArticleIdList></Reference><Reference><Citation>Alotaibi G, Awawdeh M, Farook FF, Aljohani M, Aldhafiri RM, Aldhoayan M. Artificial intelligence (AI) diagnostic tools: utilizing a convolutional neural network (CNN) to assess periodontal bone level radiographically-a retrospective study. BMC Oral Health. 2022;22:399.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC9469589</ArticleId><ArticleId IdType="pubmed">36100856</ArticleId></ArticleIdList></Reference><Reference><Citation>Chang HJ, Lee SJ, Yong TH, Shin NY, Jang BG, Kim JE, et al. Deep learning hybrid method to automatically diagnose periodontal bone loss and stage periodontitis. Sci Rep. 2020;10:7531.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7200807</ArticleId><ArticleId IdType="pubmed">32372049</ArticleId></ArticleIdList></Reference><Reference><Citation>Jiang L, Chen D, Cao Z, Wu F, Zhu H, Zhu F. A two-stage deep learning architecture for radiographic staging of periodontal bone loss. BMC Oral Health. 2022;22:106.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8973652</ArticleId><ArticleId IdType="pubmed">35365122</ArticleId></ArticleIdList></Reference><Reference><Citation>Dev S, Manandhar S, Lee YH, Winkler S. Multi-label cloud segmentation using a deep network; 2019 USNC-URSI Radio Science Meeting (Joint with AP-S Symposium); 2019 Jul 7-12; Atlanta. IEEE; 2019. pp. 113&#x2013;114.</Citation></Reference><Reference><Citation>He K, Gkioxari G, Doll&#xe1;r P, Girshick R. Mask R-CNN. IEEE Trans Pattern Anal Mach Intell. 2020;42:386&#x2013;397.</Citation><ArticleIdList><ArticleId IdType="pubmed">29994331</ArticleId></ArticleIdList></Reference><Reference><Citation>Falk T, Mai D, Bensch R, &#xc7;i&#xe7;ek &#xd6;, Abdulkadir A, Marrakchi Y, et al. U-Net: deep learning for cell counting, detection, and morphometry. Nat Methods. 2019;16:67&#x2013;70.</Citation><ArticleIdList><ArticleId IdType="pubmed">30559429</ArticleId></ArticleIdList></Reference><Reference><Citation>Loh R, Yong WX, Yapeter J, Subburaj K, Chandramohanadas R. A deep learning approach to the screening of malaria infection: automated and rapid cell counting, object detection and instance segmentation using Mask R-CNN. Comput Med Imaging Graph. 2021;88:101845.</Citation><ArticleIdList><ArticleId IdType="pubmed">33582593</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhou LQ, Wang JY, Yu SY, Wu GG, Wei Q, Deng YB, et al. Artificial intelligence in medical imaging of the liver. World J Gastroenterol. 2019;25:672&#x2013;682.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6378542</ArticleId><ArticleId IdType="pubmed">30783371</ArticleId></ArticleIdList></Reference><Reference><Citation>Schwendicke F, Golla T, Dreher M, Krois J. Convolutional neural networks for dental image diagnostics: a scoping review. J Dent. 2019;91:103226.</Citation><ArticleIdList><ArticleId IdType="pubmed">31704386</ArticleId></ArticleIdList></Reference><Reference><Citation>Abdi AH, Kasaei S, Mehdizadeh M. Automatic segmentation of mandible in panoramic X-ray. J Med Imaging (Bellingham) 2015;2:044003.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4652330</ArticleId><ArticleId IdType="pubmed">26587551</ArticleId></ArticleIdList></Reference><Reference><Citation>Shorten C, Khoshgoftaar TM. A survey on image data augmentation for deep learning. J Big Data. 2019;6:60</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8287113</ArticleId><ArticleId IdType="pubmed">34306963</ArticleId></ArticleIdList></Reference><Reference><Citation>Kim J, Lee HS, Song IS, Jung KH. DeNTNet: Deep Neural Transfer Network for the detection of periodontal bone loss using panoramic dental radiographs. Sci Rep. 2019;9:17615.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6879527</ArticleId><ArticleId IdType="pubmed">31772195</ArticleId></ArticleIdList></Reference><Reference><Citation>da Silva Rocha &#xc9;, Endo PT. A comparative study of deep learning models for dental segmentation in panoramic radiograph. Appl Sci (Basel) 2022;12:3103</Citation></Reference><Reference><Citation>Kanuri N, Abdelkarim AZ, Rathore SA. Trainable WEKA (Waikato Environment for Knowledge Analysis) segmentation tool: machine-learning-enabled segmentation on features of panoramic radiographs. Cureus. 2022;14:e21777.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8890604</ArticleId><ArticleId IdType="pubmed">35251847</ArticleId></ArticleIdList></Reference><Reference><Citation>Cantu AG, Gehrung S, Krois J, Chaurasia A, Rossi JG, Gaudin R, et al. Detecting caries lesions of different radiographic extension on bitewings using deep learning. J Dent. 2020;100:103425.</Citation><ArticleIdList><ArticleId IdType="pubmed">32634466</ArticleId></ArticleIdList></Reference><Reference><Citation>Lian L, Zhu T, Zhu F, Zhu H. Deep learning for caries detection and classification. Diagnostics (Basel) 2021;11:1672.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8469830</ArticleId><ArticleId IdType="pubmed">34574013</ArticleId></ArticleIdList></Reference><Reference><Citation>Lee JH, Han SS, Kim YH, Lee C, Kim I. Application of a fully deep convolutional neural network to the automation of tooth segmentation on panoramic radiographs. Oral Surg Oral Med Oral Pathol Oral Radiol. 2019;129:635&#x2013;642.</Citation><ArticleIdList><ArticleId IdType="pubmed">31992524</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36605558</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>06</Day></DateRevised><Article PubModel="Electronic-eCollection"><Journal><ISSN IssnType="Print">1662-4548</ISSN><JournalIssue CitedMedium="Print"><Volume>16</Volume><PubDate><Year>2022</Year></PubDate></JournalIssue><Title>Frontiers in neuroscience</Title><ISOAbbreviation>Front Neurosci</ISOAbbreviation></Journal><ArticleTitle>Multiparametric magnetic resonance imaging-derived deep learning network to determine ferroptosis-related gene signatures in gliomas.</ArticleTitle><Pagination><StartPage>1082867</StartPage><MedlinePgn>1082867</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">1082867</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3389/fnins.2022.1082867</ELocationID><Abstract><AbstractText Label="INTRODUCTION" NlmCategory="UNASSIGNED">Ferroptosis-related gene (FRG) signature is important for assessing novel therapeutic approaches and prognosis in glioma. We trained a deep learning network for determining FRG signatures using multiparametric magnetic resonance imaging (MRI).</AbstractText><AbstractText Label="METHODS" NlmCategory="UNASSIGNED">FRGs of patients with glioma were acquired from public databases. FRG-related risk score stratifying prognosis was developed from The Cancer Genome Atlas (TCGA) and validated using the Chinese Glioma Genome Atlas. Multiparametric MRI-derived glioma images and the corresponding genomic information were obtained for 122 cases from TCGA and The Cancer Imaging Archive. The deep learning network was trained using 3D-Resnet, and threefold cross-validation was performed to evaluate the predictive performance.</AbstractText><AbstractText Label="RESULTS" NlmCategory="UNASSIGNED">The FRG-related risk score was associated with poor clinicopathological features and had a high predictive value for glioma prognosis. Based on the FRG-related risk score, patients with glioma were successfully classified into two subgroups (28 and 94 in the high- and low-risk groups, respectively). The deep learning networks TC (enhancing tumor and non-enhancing portion of the tumor core) mask achieved an average cross-validation accuracy of 0.842 and an average AUC of 0.781, while the deep learning networks WT (whole tumor and peritumoral edema) mask achieved an average cross-validation accuracy of 0.825 and an average AUC of 0.781.</AbstractText><AbstractText Label="DISCUSSION" NlmCategory="UNASSIGNED">Our findings indicate that FRG signature is a prognostic indicator of glioma. In addition, we developed a deep learning network that has high classification accuracy in automatically determining FRG signatures, which may be an important step toward the clinical translation of novel therapeutic approaches and prognosis of glioma.</AbstractText><CopyrightInformation>Copyright &#xa9; 2022 Zuo, Liu, Zeng, Fan, Li, Chen, Zhou, Jiang, Yang, Feng and Lu.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Zuo</LastName><ForeName>Zhichao</ForeName><Initials>Z</Initials><AffiliationInfo><Affiliation>Department of Radiology, Xiangtan Central Hospital, Xiangtan, Hunan, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Liu</LastName><ForeName>Wen</ForeName><Initials>W</Initials><AffiliationInfo><Affiliation>Department of Radiology, The Third Xiangya Hospital, Central South University, Changsha, Hunan, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zeng</LastName><ForeName>Ying</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Department of Radiology, Xiangtan Central Hospital, Xiangtan, Hunan, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Fan</LastName><ForeName>Xiaohong</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>The School of Mathematics and Computational Science, Xiangtan University, Xiangtan, Hunan, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Li</LastName><ForeName>Li</ForeName><Initials>L</Initials><AffiliationInfo><Affiliation>Department of Radiology, Hunan Children's Hospital, University of South China, Changsha, Hunan, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Chen</LastName><ForeName>Jing</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Department of Radiology, The Affiliated Hospital of Southwest Medical University, Luzhou, Sichuan, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhou</LastName><ForeName>Xiao</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>Department of Radiology, Xiangtan Central Hospital, Xiangtan, Hunan, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Jiang</LastName><ForeName>Yihong</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Department of Radiology, Xiangtan Central Hospital, Xiangtan, Hunan, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yang</LastName><ForeName>Xiuqi</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>Department of Radiology, Xiangtan Central Hospital, Xiangtan, Hunan, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Feng</LastName><ForeName>Yujie</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>The School of Mathematics and Computational Science, Xiangtan University, Xiangtan, Hunan, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lu</LastName><ForeName>Yixin</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Medical Imaging Department, Guangxi Medical University Cancer Hospital, Nanning, Guangxi, China.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>20</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Front Neurosci</MedlineTA><NlmUniqueID>101478481</NlmUniqueID><ISSNLinking>1662-453X</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">MRI</Keyword><Keyword MajorTopicYN="N">deep learning network</Keyword><Keyword MajorTopicYN="N">ferroptosis</Keyword><Keyword MajorTopicYN="N">glioma</Keyword><Keyword MajorTopicYN="N">prognosis</Keyword></KeywordList><CoiStatement>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>10</Month><Day>28</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>2</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>2</Hour><Minute>37</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36605558</ArticleId><ArticleId IdType="pmc">PMC9808079</ArticleId><ArticleId IdType="doi">10.3389/fnins.2022.1082867</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Bakas S., Akbari H., Sotiras A., Bilello M., Rozycki M., Kirby J. S., et al. (2017). Advancing the cancer genome atlas glioma MRI collections with expert segmentation labels and radiomic features. Sci. Data. 4:170117. 10.1038/sdata.2017.117</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/sdata.2017.117</ArticleId><ArticleId IdType="pmc">PMC5685212</ArticleId><ArticleId IdType="pubmed">28872634</ArticleId></ArticleIdList></Reference><Reference><Citation>Bangalore Yogananda C. G., Shah B. R., Vejdani-Jahromi M., Nalawade S. S., Murugesan G. K., Yu F. F., et al. (2020). A novel fully automated MRI-based deep-learning method for classification of IDH mutation status in brain gliomas. Neurol. Oncol. 22 402&#x2013;411. 10.1093/neuonc/noz199</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/neuonc/noz199</ArticleId><ArticleId IdType="pmc">PMC7442388</ArticleId><ArticleId IdType="pubmed">31637430</ArticleId></ArticleIdList></Reference><Reference><Citation>Ceccarelli M., Barthel F. P., Malta T. M., Sabedot T. S., Salama S. R., Murray B. A., et al. (2016). Molecular profiling reveals biologically discrete subsets and pathways of progression in diffuse glioma. Cell 164 550&#x2013;563. 10.1016/j.cell.2015.12.028</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cell.2015.12.028</ArticleId><ArticleId IdType="pmc">PMC4754110</ArticleId><ArticleId IdType="pubmed">26824661</ArticleId></ArticleIdList></Reference><Reference><Citation>Chang K., Bai H. X., Zhou H., Su C., Bi W. L., Agbodza E., et al. (2018). Residual convolutional neural network for the determination of idh status in low- and high-grade gliomas from MR imaging. Clin. Cancer Res. 24 1073&#x2013;1081. 10.1158/1078-0432.ccr-17-2236</Citation><ArticleIdList><ArticleId IdType="doi">10.1158/1078-0432.ccr-17-2236</ArticleId><ArticleId IdType="pmc">PMC6051535</ArticleId><ArticleId IdType="pubmed">29167275</ArticleId></ArticleIdList></Reference><Reference><Citation>Chang P., Grinband J., Weinberg B. D., Bardis M., Khy M., Cadena G., et al. (2018). Deep-learning convolutional neural networks accurately classify genetic mutations in gliomas. AJNR Am. J. Neuro. Radiol. 39 1201&#x2013;1207. 10.3174/ajnr.A5667</Citation><ArticleIdList><ArticleId IdType="doi">10.3174/ajnr.A5667</ArticleId><ArticleId IdType="pmc">PMC6880932</ArticleId><ArticleId IdType="pubmed">29748206</ArticleId></ArticleIdList></Reference><Reference><Citation>Choy G., Khalilzadeh O., Michalski M., Do S., Samir A. E., Pianykh O. S., et al. (2018). Current applications and future impact of machine learning in radiology. Radiology 288 318&#x2013;328. 10.1148/radiol.2018171820</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2018171820</ArticleId><ArticleId IdType="pmc">PMC6542626</ArticleId><ArticleId IdType="pubmed">29944078</ArticleId></ArticleIdList></Reference><Reference><Citation>Deluche E., Bessette B., Durand S., Caire F., Rigau V., Robert S., et al. (2019). CHI3L1, NTRK2, 1p/19q and IDH status predicts prognosis in glioma. Cancers 11:544. 10.3390/cancers11040544</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/cancers11040544</ArticleId><ArticleId IdType="pmc">PMC6521129</ArticleId><ArticleId IdType="pubmed">30991699</ArticleId></ArticleIdList></Reference><Reference><Citation>Dixon S. J., Lemberg K. M., Lamprecht M. R., Skouta R., Zaitsev E. M., Gleason C. E., et al. (2012). Ferroptosis: An iron-dependent form of nonapoptotic cell death. Cell 149 1060&#x2013;1072. 10.1016/j.cell.2012.03.042</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cell.2012.03.042</ArticleId><ArticleId IdType="pmc">PMC3367386</ArticleId><ArticleId IdType="pubmed">22632970</ArticleId></ArticleIdList></Reference><Reference><Citation>Gao X., Zhao J., Jia L., Zhang Q. (2022). Remarkable immune and clinical value of novel ferroptosis-related genes in glioma. Sci. Rep. 12:12854. 10.1038/s41598-022-17308-7</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-022-17308-7</ArticleId><ArticleId IdType="pmc">PMC9329323</ArticleId><ArticleId IdType="pubmed">35896732</ArticleId></ArticleIdList></Reference><Reference><Citation>Hangauer M. J., Viswanathan V. S., Ryan M. J., Bole D., Eaton J. K., Matov A., et al. (2017). Drug-tolerant persister cancer cells are vulnerable to GPX4 inhibition. Nature 551 247&#x2013;250. 10.1038/nature24297</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nature24297</ArticleId><ArticleId IdType="pmc">PMC5933935</ArticleId><ArticleId IdType="pubmed">29088702</ArticleId></ArticleIdList></Reference><Reference><Citation>Heinzen D., Div&#xe9; I., Lorenz N. I., Luger A. L., Steinbach J. P., Ronellenfitsch M. W. (2019). Second generation mTOR inhibitors as a double-edged sword in malignant glioma treatment. Int. J. Mol. Sci. 20:4474. 10.3390/ijms20184474</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/ijms20184474</ArticleId><ArticleId IdType="pmc">PMC6770420</ArticleId><ArticleId IdType="pubmed">31510109</ArticleId></ArticleIdList></Reference><Reference><Citation>Hu C., Leche C. A., II, Kiyatkin A., Yu Z., Stayrook S. E., Ferguson K. M., et al. (2022). Glioblastoma mutations alter EGFR dimer structure to prevent ligand bias. Nature 602 518&#x2013;522. 10.1038/s41586-021-04393-3</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41586-021-04393-3</ArticleId><ArticleId IdType="pmc">PMC8857055</ArticleId><ArticleId IdType="pubmed">35140400</ArticleId></ArticleIdList></Reference><Reference><Citation>Hu Y., Tu Z., Lei K., Huang K., Zhu X. (2021). Ferroptosis-related gene signature correlates with the tumor immune features and predicts the prognosis of glioma patients. Biosci. Rep. 41:BSR20211640. 10.1042/bsr20211640</Citation><ArticleIdList><ArticleId IdType="doi">10.1042/bsr20211640</ArticleId><ArticleId IdType="pmc">PMC8655507</ArticleId><ArticleId IdType="pubmed">34726238</ArticleId></ArticleIdList></Reference><Reference><Citation>Kamnitsas K., Ledig C., Newcombe V. F. J., Simpson J. P., Kane A. D., Menon D. K., et al. (2017). Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation. Med. Image Anal. 36 61&#x2013;78. 10.1016/j.media.2016.10.004</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2016.10.004</ArticleId><ArticleId IdType="pubmed">27865153</ArticleId></ArticleIdList></Reference><Reference><Citation>Korfiatis P., Erickson B. (2019). Deep learning can see the unseeable: Predicting molecular markers from MRI of brain gliomas. Clin. Radiol. 74 367&#x2013;373. 10.1016/j.crad.2019.01.028</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.crad.2019.01.028</ArticleId><ArticleId IdType="pubmed">30850092</ArticleId></ArticleIdList></Reference><Reference><Citation>Lee J., Wang N., Turk S., Mohammed S., Lobo R., Kim J., et al. (2020). Discriminating pseudoprogression and true progression in diffuse infiltrating glioma using multi-parametric MRI data through deep learning. Sci. Rep. 10:20331. 10.1038/s41598-020-77389-0</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-020-77389-0</ArticleId><ArticleId IdType="pmc">PMC7683728</ArticleId><ArticleId IdType="pubmed">33230285</ArticleId></ArticleIdList></Reference><Reference><Citation>Li Y., Wei D., Liu X., Fan X., Wang K., Li S., et al. (2022). Molecular subtyping of diffuse gliomas using magnetic resonance imaging: Comparison and correlation between radiomics and deep learning. Eur. Radiol. 32 747&#x2013;758. 10.1007/s00330-021-08237-6</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-021-08237-6</ArticleId><ArticleId IdType="pubmed">34417848</ArticleId></ArticleIdList></Reference><Reference><Citation>Liu D., Chen J., Hu X., Yang K., Liu Y., Hu G., et al. (2021). Imaging-genomics in glioblastoma: Combining molecular and imaging signatures. Front. Oncol. 11:699265. 10.3389/fonc.2021.699265</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fonc.2021.699265</ArticleId><ArticleId IdType="pmc">PMC8290166</ArticleId><ArticleId IdType="pubmed">34295824</ArticleId></ArticleIdList></Reference><Reference><Citation>Ma Q., Long W., Xing C., Chu J., Luo M., Wang H. Y., et al. (2018). Cancer stem cells and immunosuppressive microenvironment in glioma. Front. Immunol. 9:2924. 10.3389/fimmu.2018.02924</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fimmu.2018.02924</ArticleId><ArticleId IdType="pmc">PMC6308128</ArticleId><ArticleId IdType="pubmed">30619286</ArticleId></ArticleIdList></Reference><Reference><Citation>Pati S., Singh A., Rathore S., Gastounioti A., Bergman M., Ngo P., et al. (2020). The cancer imaging phenomics toolkit (CaPTk): Technical overview. Brainlesion 11993 380&#x2013;394. 10.1007/978-3-030-46643-5_38</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-030-46643-5_38</ArticleId><ArticleId IdType="pmc">PMC7402244</ArticleId><ArticleId IdType="pubmed">32754723</ArticleId></ArticleIdList></Reference><Reference><Citation>Rohlfing T., Zahr N. M., Sullivan E. V., Pfefferbaum A. (2010). The SRI24 multichannel atlas of normal adult human brain structure. Hum. Brain Mapp. 31 798&#x2013;819. 10.1002/hbm.20906</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/hbm.20906</ArticleId><ArticleId IdType="pmc">PMC2915788</ArticleId><ArticleId IdType="pubmed">20017133</ArticleId></ArticleIdList></Reference><Reference><Citation>Wan R. J., Peng W., Xia Q. X., Zhou H. H., Mao X. Y. (2021). Ferroptosis-related gene signature predicts prognosis and immunotherapy in glioma. CNS Neurosci. Ther. 27 973&#x2013;986. 10.1111/cns.13654</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/cns.13654</ArticleId><ArticleId IdType="pmc">PMC8265949</ArticleId><ArticleId IdType="pubmed">33969928</ArticleId></ArticleIdList></Reference><Reference><Citation>Wu W., Li J., Ye J., Wang Q., Zhang W., Xu S. (2021). Differentiation of glioma mimicking encephalitis and encephalitis using multiparametric MR-based deep learning. Front. Oncol. 11:639062. 10.3389/fonc.2021.639062</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fonc.2021.639062</ArticleId><ArticleId IdType="pmc">PMC8005708</ArticleId><ArticleId IdType="pubmed">33791225</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhou N., Bao J. (2020). FerrDb: A manually curated resource for regulators and markers of ferroptosis and ferroptosis-disease associations. Database 2020:baaa021. 10.1093/database/baaa021</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/database/baaa021</ArticleId><ArticleId IdType="pmc">PMC7100629</ArticleId><ArticleId IdType="pubmed">32219413</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhou Y., Fang C., Xu H., Yuan L., Liu Y., Wang X., et al. (2022). Ferroptosis in glioma treatment: Current situation, prospects and drug applications. Front. Oncol. 12:989896. 10.3389/fonc.2022.989896</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fonc.2022.989896</ArticleId><ArticleId IdType="pmc">PMC9557197</ArticleId><ArticleId IdType="pubmed">36249003</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhuo S., Chen Z., Yang Y., Zhang J., Tang J., Yang K. (2020). Clinical and biological significances of a ferroptosis-related gene signature in glioma. Front. Oncol. 10:590861. 10.3389/fonc.2020.590861</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fonc.2020.590861</ArticleId><ArticleId IdType="pmc">PMC7718027</ArticleId><ArticleId IdType="pubmed">33330074</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36605116</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>07</Day></DateRevised><Article PubModel="Electronic-eCollection"><Journal><ISSN IssnType="Print">2229-5089</ISSN><JournalIssue CitedMedium="Print"><Volume>13</Volume><PubDate><Year>2022</Year></PubDate></JournalIssue><Title>Journal of pathology informatics</Title><ISOAbbreviation>J Pathol Inform</ISOAbbreviation></Journal><ArticleTitle>Independent assessment of a deep learning system for lymph node metastasis detection on the Augmented Reality Microscope.</ArticleTitle><Pagination><StartPage>100142</StartPage><MedlinePgn>100142</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">100142</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1016/j.jpi.2022.100142</ELocationID><Abstract><AbstractText>Several machine learning algorithms have demonstrated high predictive capability in the identification of cancer within digitized pathology slides. The Augmented Reality Microscope (ARM) has allowed these algorithms to be seamlessly integrated within the pathology workflow by overlaying their inferences onto its microscopic field of view in real time. We present an independent assessment of the LYmph Node Assistant (LYNA) models, state-of-the-art algorithms for the identification of breast cancer metastases in lymph node biopsies, optimized for usage on the ARM. We assessed the models on 40 whole slide images at the commonly used objective magnifications of 10&#xd7;, 20&#xd7;, and 40&#xd7;. We analyzed their performance across clinically relevant subclasses of tissue, including breast cancer, lymphocytes, histiocytes, blood, and fat. Each model obtained overall AUC values of approximately 0.98, accuracy values of approximately 0.94, and sensitivity values above 0.88 at classifying small regions of a field of view as benign or cancerous. Across tissue subclasses, the models performed most accurately on fat and blood, and least accurately on histiocytes, germinal centers, and sinus. The models also struggled with the identification of isolated tumor cells, especially at lower magnifications. After testing, we reviewed the discrepancies between model predictions and ground truth to understand the causes of error. We introduce a distinction between <i>proper</i> and <i>improper</i> ground truth for analysis in cases of uncertain annotations. Taken together, these methods comprise a novel approach for exploratory model analysis over complex anatomic pathology data in which precise ground truth is difficult to establish.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Jin</LastName><ForeName>David</ForeName><Initials>D</Initials><AffiliationInfo><Affiliation>The MITRE Corporation, 7525 Colshire Dr, McLean, VA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Rosenthal</LastName><ForeName>Joseph H</ForeName><Initials>JH</Initials><AffiliationInfo><Affiliation>The Henry M. Jackson Foundation for the Advancement of Military Medicine, 6720A Rockledge Dr, Bethesda, MD, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Thompson</LastName><ForeName>Elaine E</ForeName><Initials>EE</Initials><AffiliationInfo><Affiliation>The Henry M. Jackson Foundation for the Advancement of Military Medicine, 6720A Rockledge Dr, Bethesda, MD, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Dunnmon</LastName><ForeName>Jared</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Defense Innovation Unit, 230 RT Jones Rd, Mountain View, CA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Mohtashamian</LastName><ForeName>Arash</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Naval Hospital Camp Pendleton, 200 Mercy Cir, Oceanside, CA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Ward</LastName><ForeName>Daniel</ForeName><Initials>D</Initials><AffiliationInfo><Affiliation>Naval Medical Center San Diego, 34800 Bob Wilson Dr, San Diego, CA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Austin</LastName><ForeName>Ryan</ForeName><Initials>R</Initials><AffiliationInfo><Affiliation>Naval Hospital Camp Pendleton, 200 Mercy Cir, Oceanside, CA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Tetteh</LastName><ForeName>Hassan</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>DoD Chief Digital and AI Office, 5615 Columbia Pike, Falls Church, VA 22041, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Olson</LastName><ForeName>Niels H</ForeName><Initials>NH</Initials><AffiliationInfo><Affiliation>Defense Innovation Unit, 230 RT Jones Rd, Mountain View, CA, USA.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>09</Month><Day>27</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>J Pathol Inform</MedlineTA><NlmUniqueID>101528849</NlmUniqueID></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Breast cancer metastasis</Keyword><Keyword MajorTopicYN="N">Machine learning</Keyword><Keyword MajorTopicYN="N">Medical imaging</Keyword></KeywordList><CoiStatement>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>8</Month><Day>4</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>9</Month><Day>21</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>9</Month><Day>21</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>2</Hour><Minute>25</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36605116</ArticleId><ArticleId IdType="pmc">PMC9808066</ArticleId><ArticleId IdType="doi">10.1016/j.jpi.2022.100142</ArticleId><ArticleId IdType="pii">S2153-3539(22)00736-2</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Apple S.K. Sentinel lymph node in breast cancer: review article from a pathologist&#x2019;s point of view. J Pathol Transl Med. 2016;50(2):83&#x2013;95. doi: 10.4132/jptm.2015.11.23.</Citation><ArticleIdList><ArticleId IdType="doi">10.4132/jptm.2015.11.23</ArticleId><ArticleId IdType="pmc">PMC4804148</ArticleId><ArticleId IdType="pubmed">26757203</ArticleId></ArticleIdList></Reference><Reference><Citation>Amin M.B., Greene F.L., Edge S.B., Compton C.C., Gershenwald J.E., Brookland R.K., et al. The eighth edition AJCC cancer staging manual: continuing to build a bridge from a population-based to a more &#x201c;personalized&#x201d; approach to cancer staging. CA Cancer J Clin. 2017;67(2):93&#x2013;99. doi: 10.3322/caac.21388.</Citation><ArticleIdList><ArticleId IdType="doi">10.3322/caac.21388</ArticleId><ArticleId IdType="pubmed">28094848</ArticleId></ArticleIdList></Reference><Reference><Citation>Elmore J.G., Longton G.M., Carney P.A., Geller B.M., Onega T., Tosteson A.N.A., et al. Diagnostic concordance among pathologists interpreting breast biopsy specimens. Jama. 2015;313(11):1122&#x2013;1132. doi: 10.1001/jama.2015.1405.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jama.2015.1405</ArticleId><ArticleId IdType="pmc">PMC4516388</ArticleId><ArticleId IdType="pubmed">25781441</ArticleId></ArticleIdList></Reference><Reference><Citation>Gavrielides M.A., Gallas B.D., Lenz P., Badano A., Hewitt S.M. Observer variability in the interpretation of HER2/neu immunohistochemical expression with unaided and computer-aided digital microscopy. Arch Pathol Lab Med. 2011;135(2):233&#x2013;243. doi: 10.5858/135.2.233.</Citation><ArticleIdList><ArticleId IdType="doi">10.5858/135.2.233</ArticleId><ArticleId IdType="pmc">PMC7604903</ArticleId><ArticleId IdType="pubmed">21284444</ArticleId></ArticleIdList></Reference><Reference><Citation>Renshaw A.A., Cartagena N., Granter S.R., Gould E.W. Agreement and error rates using blinded review to evaluate surgical pathology of biopsy material. Am J Clin Pathol. 2003;119(6):797&#x2013;800. doi: 10.1309/DCXAXFVCCHVHYU41.</Citation><ArticleIdList><ArticleId IdType="doi">10.1309/DCXAXFVCCHVHYU41</ArticleId><ArticleId IdType="pubmed">12817425</ArticleId></ArticleIdList></Reference><Reference><Citation>Wilson M.L., Fleming K.A., Kuti M.A., Looi L.M., Lago N., Ru K. Access to pathology and laboratory medicine services: a crucial gap. The Lancet. 2018;391(10133):1927&#x2013;1938. doi: 10.1016/S0140-6736(18)30458-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0140-6736(18)30458-6</ArticleId><ArticleId IdType="pubmed">29550029</ArticleId></ArticleIdList></Reference><Reference><Citation>Wu K.J. &#x2018;Nobody Sees Us&#x2019;: Testing-Lab Workers Strain Under Demand. New York Times. 2020;CLXX(58,901):D1. https://www.nytimes.com/2020/12/03/health/coronavirus-testing-labs-workers.html [cited 16 January 2022]. Available from.</Citation></Reference><Reference><Citation>Compass Group Roundtable &#x2018;Tight and terrible&#x2019;: Lab leaders on budgets and staffing. CAP Today. 2021;34(12):1&#x2013;27. https://www.captodayonline.com/tight-and-terrible-lab-leaders-on-budgets-and-staffing/?print=pdf [cited 16 January 2022]. Available from.</Citation></Reference><Reference><Citation>Penrod V.S. Report to the congressional armed services committees section 719 of the national defense authorization act for fiscal year 2020. Public Law. 2021:116&#x2013;192.</Citation></Reference><Reference><Citation>Whitley J.E., Gould B.R., Huff N.M., Wu L. IDA Paper P-5047 -- Medical total force management. Institute for Defense Analyses. 2014;1:25&#x2013;B-8.</Citation></Reference><Reference><Citation>Litjens G., S&#xe1;nchez C.I., Timofeeva N., Hermsen M., Nagtegaal I., Kovacs I., et al. Deep learning as a tool for increased accuracy and efficiency of histopathological diagnosis. Sci Rep. 2016;6(1):1&#x2013;11. doi: 10.1038/srep26286.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/srep26286</ArticleId><ArticleId IdType="pmc">PMC4876324</ArticleId><ArticleId IdType="pubmed">27212078</ArticleId></ArticleIdList></Reference><Reference><Citation>Nagpal K., Foote D., Tan F., Liu Y., Chen P.H.C., Steiner D.F., et al. Development and validation of a deep learning algorithm for Gleason grading of prostate cancer from biopsy specimens. JAMA Oncol. 2020;6(9):1372&#x2013;1380. doi: 10.1001/jamaoncol.2020.2485.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jamaoncol.2020.2485</ArticleId><ArticleId IdType="pmc">PMC7378872</ArticleId><ArticleId IdType="pubmed">32701148</ArticleId></ArticleIdList></Reference><Reference><Citation>Perincheri S., Levi A.W., Celli R., Gershkovich P., Rimm D., Morrow J.S., et al. An independent assessment of an artificial intelligence system for prostate cancer detection shows strong diagnostic accuracy. Mod Pathol. 2021;34(8):1588&#x2013;1595. doi: 10.1038/s41379-021-00794-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41379-021-00794-x</ArticleId><ArticleId IdType="pmc">PMC8295034</ArticleId><ArticleId IdType="pubmed">33782551</ArticleId></ArticleIdList></Reference><Reference><Citation>Bulten W., Kartasalo K., Chen P.H.C., Str&#xf6;m P., Pinckaers H., Nagpal K., et al. the PANDA challenge consortium Artificial intelligence for diagnosis and Gleason grading of prostate cancer: the PANDA challenge. Nat Med. 2022;28(1):154&#x2013;163. doi: 10.1038/s41591-021-01620-2.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41591-021-01620-2</ArticleId><ArticleId IdType="pmc">PMC8799467</ArticleId><ArticleId IdType="pubmed">35027755</ArticleId></ArticleIdList></Reference><Reference><Citation>Bejnordi B.E., Veta M., Van Diest P.J., Ginneken B., Karssemeijer N., Litjens G., et al. Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer. Jama. 2017;318(22):2199&#x2013;2210. doi: 10.1001/jama.2017.14585.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jama.2017.14585</ArticleId><ArticleId IdType="pmc">PMC5820737</ArticleId><ArticleId IdType="pubmed">29234806</ArticleId></ArticleIdList></Reference><Reference><Citation>Steiner D.F., MacDonald R., Liu Y., Truszkowski P., Hipp J.D., Gammage C., et al. Impact of deep learning assistance on the histopathologic review of lymph nodes for metastatic breast cancer. Am J Surg Pathol. 2018;42(12):1636&#x2013;1646. doi: 10.1097/PAS.0000000000001151.</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/PAS.0000000000001151</ArticleId><ArticleId IdType="pmc">PMC6257102</ArticleId><ArticleId IdType="pubmed">30312179</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen P.H.C., Gadepalli K., MacDonald R., Liu Y., Kadowaki S., Nagpal K., et al. An augmented reality microscope with real-time artificial intelligence integration for cancer diagnosis. Nat Med. 2019;25(9):1453&#x2013;1457. doi: 10.1038/s41591-019-0539-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41591-019-0539-7</ArticleId><ArticleId IdType="pubmed">31406351</ArticleId></ArticleIdList></Reference><Reference><Citation>Liu Y., Kohlberger T., Norouzi M., Dahl G.E., Smith J.L., Mohtashamian A., et al. Artificial intelligence&#x2013;based breast cancer nodal metastasis detection: insights into the black box for pathologists. Arch Pathol Lab Med. 2019;143(7):859&#x2013;868. doi: 10.5858/arpa.2018-0147-OA.</Citation><ArticleIdList><ArticleId IdType="doi">10.5858/arpa.2018-0147-OA</ArticleId><ArticleId IdType="pubmed">30295070</ArticleId></ArticleIdList></Reference><Reference><Citation>Oakden-Rayner L., Dunnmon J., Carneiro G., R&#xe9; C. Hidden stratification causes clinically meaningful failures in machine learning for medical imaging. Proc ACM Conf Health Inference Learn. 2020:151&#x2013;159. doi: 10.1145/3368555.3384468.</Citation><ArticleIdList><ArticleId IdType="doi">10.1145/3368555.3384468</ArticleId><ArticleId IdType="pmc">PMC7665161</ArticleId><ArticleId IdType="pubmed">33196064</ArticleId></ArticleIdList></Reference><Reference><Citation>Bandi P., Geessink O., Manson Q., Dijk M.v., Balkenhol M., Hermsen M., et al. From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge. IEEE Trans Med Imaging. 2018;38(2):550&#x2013;560. doi: 10.1109/TMI.2018.2867350.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2018.2867350</ArticleId><ArticleId IdType="pubmed">30716025</ArticleId></ArticleIdList></Reference><Reference><Citation>Jha V., Singh M., Mandal A.K. Photography and its relevance in pathology. Rec Adv Path. 2016;2(1):9&#x2013;14.</Citation></Reference><Reference><Citation>Layfield L.J., Witt B.L., Metzger K.G., Anderson G.M. Extraneous tissue: a potential source for diagnostic error in surgical pathology. Am J Clin Pathol. 2011;136(5):767&#x2013;772. doi: 10.1309/ajcp4ffsbphau8iu.</Citation><ArticleIdList><ArticleId IdType="doi">10.1309/ajcp4ffsbphau8iu</ArticleId><ArticleId IdType="pubmed">22031316</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36605110</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>06</Day></DateRevised><Article PubModel="Electronic-eCollection"><Journal><ISSN IssnType="Print">2229-5089</ISSN><JournalIssue CitedMedium="Print"><Volume>13</Volume><PubDate><Year>2022</Year></PubDate></JournalIssue><Title>Journal of pathology informatics</Title><ISOAbbreviation>J Pathol Inform</ISOAbbreviation></Journal><ArticleTitle>Differentiation of pancreatic ductal adenocarcinoma and chronic pancreatitis using graph neural networks on histopathology and collagen fiber features.</ArticleTitle><Pagination><StartPage>100158</StartPage><MedlinePgn>100158</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">100158</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1016/j.jpi.2022.100158</ELocationID><Abstract><AbstractText>Pancreatic ductal adenocarcinoma (PDAC) is one of the most lethal human cancers. However, the symptoms and radiographic appearance of chronic pancreatitis (CP) mimics that of PDAC, and sometimes the 2 entities can also be difficult to differentiate microscopically. The need for accurate differentiation of PDAC and CP has become a major topic in pancreatic pathology. These 2 diseases can present similar histomorphological features, such as excessive deposition of fibrotic stroma in the tissue microenvironment and inflammatory cell infiltration. In this paper, we present a quantitative analysis pipeline empowered by graph neural networks (GNN) capable of automatic detection and differentiation of PDAC and CP in human histological specimens. Modeling histological images as graphs and deploying graph convolutions can enable the capture of histomorphological features at different scales, ranging from nuclear size to the organization of ducts. The analysis pipeline combines image features computed from co-registered hematoxylin and eosin (H&amp;E) images and Second-Harmonic Generation (SHG) microscopy images, with the SHG images enabling the extraction of collagen fiber morphological features. Evaluating the analysis pipeline on a human tissue micro-array dataset consisting of 786 cores and a tissue region dataset consisting of 268 images, it attained 86.4% accuracy with an average area under the curve (AUC) of 0.954 and 88.9% accuracy with an average AUC of 0.957, respectively. Moreover, incorporating topological features of collagen fibers computed from SHG images into the model further increases the classification accuracy on the tissue region dataset to 91.3% with an average AUC of 0.962, suggesting that collagen characteristics are diagnostic features in PDAC and CP detection and differentiation.</AbstractText><CopyrightInformation>&#xa9; 2022 The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Li</LastName><ForeName>Bin</ForeName><Initials>B</Initials><AffiliationInfo><Affiliation>Department of Biomedical Engineering, University of Wisconsin-Madison, Madison 53706, WI, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Morgridge Institute for Research, Madison 53705, WI, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Nelson</LastName><ForeName>Michael S</ForeName><Initials>MS</Initials><AffiliationInfo><Affiliation>Department of Biomedical Engineering, University of Wisconsin-Madison, Madison 53706, WI, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Savari</LastName><ForeName>Omid</ForeName><Initials>O</Initials><AffiliationInfo><Affiliation>Department of Pathology, University of Pittsburgh Medical Center, Pittsburgh 15213, PA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Loeffler</LastName><ForeName>Agnes G</ForeName><Initials>AG</Initials><AffiliationInfo><Affiliation>Department of Pathology, MetroHealth Medical Center, Cleveland 44109, OH, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Eliceiri</LastName><ForeName>Kevin W</ForeName><Initials>KW</Initials><AffiliationInfo><Affiliation>Department of Biomedical Engineering, University of Wisconsin-Madison, Madison 53706, WI, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Morgridge Institute for Research, Madison 53705, WI, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Medical Physics, University of Wisconsin-Madison, Madison 53706, WI, USA.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>11</Month><Day>19</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>J Pathol Inform</MedlineTA><NlmUniqueID>101528849</NlmUniqueID></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Chronic pancreatitis</Keyword><Keyword MajorTopicYN="N">Collagen fibers</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Graph neural network</Keyword><Keyword MajorTopicYN="N">Histopathology</Keyword><Keyword MajorTopicYN="N">Pancreatic cancer</Keyword><Keyword MajorTopicYN="N">Second-harmonic generation imaging</Keyword></KeywordList><CoiStatement>The authors declare no conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>10</Month><Day>5</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>11</Month><Day>16</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>11</Month><Day>16</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>2</Hour><Minute>25</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36605110</ArticleId><ArticleId IdType="pmc">PMC9808020</ArticleId><ArticleId IdType="doi">10.1016/j.jpi.2022.100158</ArticleId><ArticleId IdType="pii">S2153-3539(22)00752-0</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Hidalgo M. Pancreatic cancer. N Engl J Med. 2010;362(17):1605&#x2013;1617.</Citation><ArticleIdList><ArticleId IdType="pubmed">20427809</ArticleId></ArticleIdList></Reference><Reference><Citation>Rawla P., Sunkara T., Gaduputi V. Epidemiology of pancreatic cancer: global trends, etiology and risk factors. World J Oncol. 2019;10(1):10.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6396775</ArticleId><ArticleId IdType="pubmed">30834048</ArticleId></ArticleIdList></Reference><Reference><Citation>Bellizzi A.M., Frankel W.L. Pancreatic pathology: a practical review. Lab Med. 2009;40(7):417&#x2013;426.</Citation></Reference><Reference><Citation>Mostafa M.E., Erbarut-Seven I., Pehlivanoglu B., Adsay V. Pathologic classification of &#x201c;pancreatic cancers&#x201d;: current concepts and challenges. Chin Clin Oncol. 2017;6(6):59.</Citation><ArticleIdList><ArticleId IdType="pubmed">29307199</ArticleId></ArticleIdList></Reference><Reference><Citation>Mihaljevic A., Esposito I., Friess H., Kleeff J. Molecular biology, models, and histopathology of chronic pancreatitis and pancreatic cancer. Eur Surg. 2009;41(6):250&#x2013;267.</Citation></Reference><Reference><Citation>Kl&#xf6;ppel G. Chronic pancreatitis, pseudotumors and other tumor-like lesions. Mod Pathol. 2007;20(1):S113&#x2013;S131.</Citation><ArticleIdList><ArticleId IdType="pubmed">17486047</ArticleId></ArticleIdList></Reference><Reference><Citation>Reddy R. Mass-forming chronic pancreatitis: diagnostic performance of PET/CT. World J Nucl Med. 2022 Sep;21(03):239&#x2013;243.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC9436516</ArticleId><ArticleId IdType="pubmed">36060080</ArticleId></ArticleIdList></Reference><Reference><Citation>Dal Molin M., Zhang M., De Wilde R.F., et al. Very long-term survival following resection for pancreatic cancer is not explained by commonly mutated genes: results of whole-exome sequencing analysis. Clin Cancer Res. 2015;21(8):1944&#x2013;1950.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4401626</ArticleId><ArticleId IdType="pubmed">25623214</ArticleId></ArticleIdList></Reference><Reference><Citation>Strobel O., Neoptolemos J., J&#xa8;ager D, Bu&#xa8;chler MW. Optimizing the outcomes of pancreatic cancer surgery. Nat Rev Clin Oncol. 2019;16(1):11&#x2013;26.</Citation><ArticleIdList><ArticleId IdType="pubmed">30341417</ArticleId></ArticleIdList></Reference><Reference><Citation>Yamamoto T., Yagi S., Kinoshita H., et al. Long-term survival after resection of pancreatic cancer: a single-center retrospective analysis. World J Gastroenterol: WJG. 2015;21(1):262.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4284344</ArticleId><ArticleId IdType="pubmed">25574100</ArticleId></ArticleIdList></Reference><Reference><Citation>Esposito I., Hruban R.H., Verbeke C., et al. Guidelines on the histopathology of chronic pancreatitis. Recommendations from the working group for the international consensus guidelines for chronic pancreatitis in collaboration with the International Association of Pancreatology, the American Pancreatic Association, the Japan Pancreas Society, and the European Pancreatic Club. Pancreatology. 2020;20(4):586&#x2013;593.</Citation><ArticleIdList><ArticleId IdType="pubmed">32414657</ArticleId></ArticleIdList></Reference><Reference><Citation>Dimastromatteo J., Brentnall T., Kelly K.A. Imaging in pancreatic disease. Nat Rev Gastroenterol Hepatol. 2017;14(2):97&#x2013;109.</Citation><ArticleIdList><ArticleId IdType="pubmed">27826137</ArticleId></ArticleIdList></Reference><Reference><Citation>Sarantis P., Koustas E., Papadimitropoulou A., Papavassiliou A.G., Karamouzis M.V. Pancreatic ductal adenocarcinoma: treatment hurdles, tumor microenvironment and immunotherapy. World J Gastrointest Oncol. 2020;12(2):173.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7031151</ArticleId><ArticleId IdType="pubmed">32104548</ArticleId></ArticleIdList></Reference><Reference><Citation>Truong L.H., Pauklin S. Pancreatic cancer microenvironment and cellular composition: current understandings and therapeutic approaches. Cancers. 2021;13(19):5028.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8507722</ArticleId><ArticleId IdType="pubmed">34638513</ArticleId></ArticleIdList></Reference><Reference><Citation>Hosein A.N., Brekken R.A., Maitra A. Pancreatic cancer stroma: an update on therapeutic targeting strategies. Nat Rev Gastroenterol Hepatol. 2020;17(8):487&#x2013;505.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8284850</ArticleId><ArticleId IdType="pubmed">32393771</ArticleId></ArticleIdList></Reference><Reference><Citation>Conklin M.W., Eickhoff J.C., Riching K.M., et al. Aligned collagen is a prognostic signature for survival in human breast carcinoma. Am J Pathol. 2011;178(3):1221&#x2013;1232.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC3070581</ArticleId><ArticleId IdType="pubmed">21356373</ArticleId></ArticleIdList></Reference><Reference><Citation>Nadiarnykh O., LaComb R.B., Brewer M.A., Campagnola P.J. Alterations of the extracellular matrix in ovarian cancer studied by Second Harmonic Generation imaging microscopy. BMC Cancer. 2010;10(1):1&#x2013;14.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC2841668</ArticleId><ArticleId IdType="pubmed">20222963</ArticleId></ArticleIdList></Reference><Reference><Citation>Drifka C.R., Loeffler A.G., Mathewson K., et al. Highly aligned stromal collagen is a negative prognostic factor following pancreatic ductal adenocarcinoma resection. Oncotarget. 2016;7(46):76197.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5342807</ArticleId><ArticleId IdType="pubmed">27776346</ArticleId></ArticleIdList></Reference><Reference><Citation>Hanley C.J., Noble F., Ward M., et al. A subset of myofibroblastic cancer-associated fibroblasts regulate collagen fiber elongation, which is prognostic in multiple cancers. Oncotarget. 2016;7(5):6159.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4868747</ArticleId><ArticleId IdType="pubmed">26716418</ArticleId></ArticleIdList></Reference><Reference><Citation>Best S.L., Liu Y., Keikhosravi A., et al. Collagen organization of renal cell carcinoma differs between low and high grade tumors. BMC Cancer. 2019;19(1):1&#x2013;8.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6533752</ArticleId><ArticleId IdType="pubmed">31122202</ArticleId></ArticleIdList></Reference><Reference><Citation>Zunder S.M., Gelderblom H., Tollenaar R.A., Mesker W.E. The significance of stromal collagen organization in cancer tissue: An in-depth discussion of literature. Crit Rev Oncol Hematol. 2020;151</Citation><ArticleIdList><ArticleId IdType="pubmed">32408009</ArticleId></ArticleIdList></Reference><Reference><Citation>Drifka C.R., Tod J., Loeffler A.G., et al. Periductal stromal collagen topology of pancreatic ductal adenocarcinoma differs from that of normal and chronic pancreatitis. Mod Pathol. 2015;28(11):1470&#x2013;1480.</Citation><ArticleIdList><ArticleId IdType="pubmed">26336888</ArticleId></ArticleIdList></Reference><Reference><Citation>Grizzi F., Fiorino S., Qehajaj D., et al. Computer-aided assessment of the extra-cellular matrix during pancreatic carcinogenesis: a pilot study. J Transl Med. 2019;17(1):1&#x2013;9.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6393991</ArticleId><ArticleId IdType="pubmed">30819202</ArticleId></ArticleIdList></Reference><Reference><Citation>Han M., Giese G., Bille J.F. Second harmonic generation imaging of collagen fibrils in cornea and sclera. Opt Express. 2005;13(15):5791&#x2013;5797.</Citation><ArticleIdList><ArticleId IdType="pubmed">19498583</ArticleId></ArticleIdList></Reference><Reference><Citation>Strupler M., Pena A.M., Hernest M., et al. Second harmonic imaging and scoring of collagen in fibrotic tissues. Opt Express. 2007;15(7):4054&#x2013;4065.</Citation><ArticleIdList><ArticleId IdType="pubmed">19532649</ArticleId></ArticleIdList></Reference><Reference><Citation>Keikhosravi A., Bredfeldt J.S., Sagar A.K., Eliceiri K.W. Second-harmonic generation imaging of cancer. Methods Cell Biol. 2014;123:531&#x2013;546.</Citation><ArticleIdList><ArticleId IdType="pubmed">24974046</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen X., Nadiarynkh O., Plotnikov S., Campagnola P.J. Second harmonic generation microscopy for quantitative analysis of collagen fibrillar structure. Nat Protoc. 2012;7(4):654&#x2013;669.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4337962</ArticleId><ArticleId IdType="pubmed">22402635</ArticleId></ArticleIdList></Reference><Reference><Citation>Bredfeldt J.S., Liu Y., Pehlke C.A., et al. Computational segmentation of collagen fibers from secondharmonic generation images of breast cancer. J Biomed Opt. 2014;19(1)</Citation><ArticleIdList><ArticleId IdType="pmc">PMC3886580</ArticleId><ArticleId IdType="pubmed">24407500</ArticleId></ArticleIdList></Reference><Reference><Citation>Valkenburg K.C., De Groot A.E., Pienta K.J. Targeting the tumour stroma to improve cancer therapy. Nat Rev Clin Oncol. 2018;15(6):366&#x2013;381.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5960434</ArticleId><ArticleId IdType="pubmed">29651130</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang J., Liu J. Tumor stroma as targets for cancer therapy. Pharmacol Ther. 2013;137(2):200&#x2013;215.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC3556178</ArticleId><ArticleId IdType="pubmed">23064233</ArticleId></ArticleIdList></Reference><Reference><Citation>Pantanowitz L., Valenstein P.N., Evans A.J., et al. Review of the current state of whole slide imaging in pathology. J Pathol Inform. 2011:2.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC3162745</ArticleId><ArticleId IdType="pubmed">21886892</ArticleId></ArticleIdList></Reference><Reference><Citation>Ghaznavi F., Evans A., Madabhushi A., Feldman M. Digital imaging in pathology: whole-slide imaging and beyond. Annu Rev Pathol Mech Dis. 2013;8:331&#x2013;359.</Citation><ArticleIdList><ArticleId IdType="pubmed">23157334</ArticleId></ArticleIdList></Reference><Reference><Citation>Farahani N., Parwani A.V., Pantanowitz L., et al. Whole slide imaging in pathology: advantages, limitations, and emerging perspectives. Pathol Lab Med Int. 2015;7(23&#x2013;33):4321.</Citation></Reference><Reference><Citation>Huss R., Coupland S.E. Software-assisted decision support in digital histopathology. J Pathol. 2020;250(5):685&#x2013;692.</Citation><ArticleIdList><ArticleId IdType="pubmed">31994192</ArticleId></ArticleIdList></Reference><Reference><Citation>Srinidhi C.L., Ciga O., Martel A.L. Deep neural network models for computational histopathology: a survey. Med Image Anal. 2021;67</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7725956</ArticleId><ArticleId IdType="pubmed">33049577</ArticleId></ArticleIdList></Reference><Reference><Citation>Ahmedt-Aristizabal D., Armin M.A., Denman S., Fookes C., Petersson L. A survey on graph-based deep learning for computational histopathology. Comput Med Imaging Graph. 2021;95</Citation><ArticleIdList><ArticleId IdType="pubmed">34959100</ArticleId></ArticleIdList></Reference><Reference><Citation>Dimitriou N., Arandjelovic O., Caie P.D. Deep learning for whole slide image analysis: an overview. Front Med. 2019:264.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6882930</ArticleId><ArticleId IdType="pubmed">31824952</ArticleId></ArticleIdList></Reference><Reference><Citation>Fu Y., Jung A.W., Torne R.V., et al. Pan-cancer computational histopathology reveals mutations, tumor composition and prognosis. Nat Cancer. 2020;1(8):800&#x2013;810.</Citation><ArticleIdList><ArticleId IdType="pubmed">35122049</ArticleId></ArticleIdList></Reference><Reference><Citation>Van der Laak J., Litjens G., Ciompi F. Deep learning in histopathology: the path to the clinic. Nat Med. 2021;27(5):775&#x2013;784.</Citation><ArticleIdList><ArticleId IdType="pubmed">33990804</ArticleId></ArticleIdList></Reference><Reference><Citation>Scarselli F., Gori M., Tsoi A.C., Hagenbuchner M., Monfardini G. The graph neural network model. IEEE Trans Neural Netw. 2008;20(1):61&#x2013;80.</Citation><ArticleIdList><ArticleId IdType="pubmed">19068426</ArticleId></ArticleIdList></Reference><Reference><Citation>Kipf T.N., Welling M.  arXiv preprint. 2016. Semi-supervised classification with graph convolutional networks.arXiv:160902907</Citation></Reference><Reference><Citation>Wu F., Souza A., Zhang T., Fifty C., Yu T., Weinberger K. International Conference on Machine Learning. PMLR; 2019. Simplifying graph convolutional networks; pp. 6861&#x2013;6871.</Citation></Reference><Reference><Citation>Prewitt J.M., Wu S.C. The Second Annual Symposium on Computer Application in Medical Care, 1978. Proceedings. IEEE; Nov 5, 1978. An application of pattern recognition to epithelial tissues; pp. 15&#x2013;25.</Citation></Reference><Reference><Citation>Ayg&#xfc;n&#xe9;s B., Aksoy S., Cinb&#xed;s R.G., K&#xf6;semehmetoglu K., Onder S., Uner A. Medical Imaging 2020: Digital Pathology. Vol. 11320. International Society for Optics and Photonics; 2020. Graph convolutional networks for region of interest classification in breast histopathology; p. 113200K.</Citation></Reference><Reference><Citation>Adnan M., Kalra S., Tizhoosh H.R. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. 2020. Representation learning of histopathology images using graph neural networks; pp. 988&#x2013;989.</Citation></Reference><Reference><Citation>Pati P., Jaume G., Foncubierta-Rodr&#x131;guez A., et al. Hierarchical graph representations in digital pathology. Med Image Anal. 2022;75</Citation><ArticleIdList><ArticleId IdType="pubmed">34781160</ArticleId></ArticleIdList></Reference><Reference><Citation>Ilse M., Tomczak J., Welling M. International Conference on Machine Learning. PMLR; 2018. Attention-based deep multiple instance learning; pp. 2127&#x2013;2136.</Citation></Reference><Reference><Citation>Li B., Li Y., Eliceiri K.W. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021. Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning; pp. 14318&#x2013;14328.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8765709</ArticleId><ArticleId IdType="pubmed">35047230</ArticleId></ArticleIdList></Reference><Reference><Citation>Keikhosravi A., Li B., Liu Y., Conklin M.W., Loeffler A.G., Eliceiri K.W. Non-disruptive collagen characterization in clinical histopathology using cross-modality image synthesis. Commun Biol. 2020;3(1):1&#x2013;12.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7395097</ArticleId><ArticleId IdType="pubmed">32737412</ArticleId></ArticleIdList></Reference><Reference><Citation>Bankhead P., Loughrey M.B., Fernandez J.A., et al. QuPath: open source software for digital pathology image analysis. Sci Rep. 2017;7(1):1&#x2013;7.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5715110</ArticleId><ArticleId IdType="pubmed">29203879</ArticleId></ArticleIdList></Reference><Reference><Citation>Edelstein A.D., Tsuchida M.A., Amodaj N., Pinkard H., Vale R.D., Stuurman N. Advanced methods of microscope control using &#x3bc;Manager software. J Biol Methods. 2014;1(2)</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4297649</ArticleId><ArticleId IdType="pubmed">25606571</ArticleId></ArticleIdList></Reference><Reference><Citation>Pinkard H., Stuurman N., Ivanov I.E., et al. Pycro-Manager: open-source software for customized and reproducible microscope control. Nat Methods. 2021;18(3):226&#x2013;228.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8532176</ArticleId><ArticleId IdType="pubmed">33674797</ArticleId></ArticleIdList></Reference><Reference><Citation>Keikhosravi A., Li B., Liu Y., Eliceiri K.W. Intensity-based registration of bright-field and second-harmonic generation images of histopathology tissue sections. Biomed Opt Express. 2020;11(1):160&#x2013;173.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6968755</ArticleId><ArticleId IdType="pubmed">32010507</ArticleId></ArticleIdList></Reference><Reference><Citation>Pielawski N., Wetzer E., Ofverstedt J., et al. CoMIR: contrastive multimodal image representation for registration. Adv Neural Inform Process Syst. 2020;33:18433&#x2013;18444.</Citation></Reference><Reference><Citation>Schindelin J., Arganda-Carreras I., Frise E., et al. Fiji: an open-source platform for biological-image analysis. Nat Methods. 2012;9(7):676&#x2013;682.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC3855844</ArticleId><ArticleId IdType="pubmed">22743772</ArticleId></ArticleIdList></Reference><Reference><Citation>Macenko M., Niethammer M., Marron J.S., et al. 2009 IEEE International Symposium on Biomedical Imaging: from Nano to Macro. IEEE; 2009. A method for normalizing histology slides for quantitative analysis; pp. 1107&#x2013;1110.</Citation></Reference><Reference><Citation>He K., Zhang X., Ren S., Sun J. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. Deep residual learning for image recognition; pp. 770&#x2013;778.</Citation></Reference><Reference><Citation>Liu Y., Keikhosravi A., Pehlke C.A., et al. Fibrillar collagen quantification with curvelet transform based computational methods. Front Bioeng Biotechnol. 2020;8:198.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7186312</ArticleId><ArticleId IdType="pubmed">32373594</ArticleId></ArticleIdList></Reference><Reference><Citation>He K., Zhang X., Ren S., Sun J. Proceedings of the IEEE International Conference on Computer Vision. 2015. Delving deep into rectifiers: surpassing human-level performance on imagenet classification; pp. 1026&#x2013;1034.</Citation></Reference><Reference><Citation>Kingma D.P., Ba J.  arXiv preprint. 2014. Adam: a method for stochastic optimization.arXiv:14126980</Citation></Reference><Reference><Citation>Loshchilov I., Hutter F.  arXiv preprint. 2016. Sgdr: stochastic gradient descent with warm restarts.arXiv:160803983</Citation></Reference><Reference><Citation>Sculley D. Proceedings of the 19th International Conference on World Wide Web. 2010. Web-scale k-means clustering; pp. 1177&#x2013;1178.</Citation></Reference><Reference><Citation>Huang Z., Li Y. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. Interpretable and accurate fine-grained recognition via region grouping; pp. 8662&#x2013;8672.</Citation></Reference><Reference><Citation>Liao P.S., Chen T.S., Chung P.C., et al. A fast algorithm for multilevel thresholding. J Inf Sci Eng. 2001;17(5):713&#x2013;727.</Citation></Reference><Reference><Citation>Neubert P., Protzel P. 2014 22nd International Conference on Pattern Recognition. IEEE; 2014. Compact watershed and preemptive slic: on improving trade-offs of superpixel segmentation algorithms; pp. 996&#x2013;1001.</Citation></Reference><Reference><Citation>Lu M.Y., Williamson D.F., Chen T.Y., Chen R.J., Barbieri M., Mahmood F. Data-efficient and weakly supervised computational pathology on whole-slide images. Nat Biomed Eng. 2021;5(6):555&#x2013;570.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8711640</ArticleId><ArticleId IdType="pubmed">33649564</ArticleId></ArticleIdList></Reference><Reference><Citation>Van der Maaten L., Hinton G. Visualizing data using t-SNE. J Mach Learn Res. 2008;9(11)</Citation></Reference><Reference><Citation>Lafert&#xe9; J.M., P&#xe9;rez P., Heitz F. Discrete Markov image modeling and inference on the quadtree. IEEE Trans Image Process. 2000;9(3):390&#x2013;404.</Citation><ArticleIdList><ArticleId IdType="pubmed">18255411</ArticleId></ArticleIdList></Reference><Reference><Citation>Sharma H., Alekseychuk A., Leskovsky P., et al. Determining similarity in histological images using graph-theoretic description and matching methods for content-based image retrieval in medical diagnostics. Diagn Pathol. 2012;7(1):1&#x2013;20.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC3554463</ArticleId><ArticleId IdType="pubmed">23035717</ArticleId></ArticleIdList></Reference><Reference><Citation>Zheng Y., Jiang Z., Shi J., et al. Encoding histopathology whole slide images with location-aware graphs for diagnostically relevant regions retrieval. Med Image Anal. 2022;76</Citation><ArticleIdList><ArticleId IdType="pubmed">34856455</ArticleId></ArticleIdList></Reference><Reference><Citation>Hein M., Andriushchenko M., Bitterwolf J. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019. Why relu networks yield high-confidence predictions far away from the training data and how to mitigate the problem; pp. 41&#x2013;50.</Citation></Reference><Reference><Citation>Leibig C., Allken V., Ayhan M.S., Berens P., Wahl S. Leveraging uncertainty information from deep neural networks for disease detection. Sci Rep. 2017;7(1):1&#x2013;14.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5736701</ArticleId><ArticleId IdType="pubmed">29259224</ArticleId></ArticleIdList></Reference><Reference><Citation>Ren J., Liu P.J., Fertig E., et al. Likelihood ratios for out-of-distribution detection. Adv Neural Inform Process Syst. 2019;32</Citation></Reference><Reference><Citation>Settles B. Active learning. Synth Lect Artif Intel Mach Learn. 2012 Jun 30;6(1):1&#x2013;14.</Citation></Reference><Reference><Citation>Hiasa Y., Otake Y., Takao M., et al. International Workshop on Simulation and Synthesis in Medical Imaging. Springer; Cham: Sep 16, 2018. Cross-modality image synthesis from unpaired data using CycleGAN; pp. 31&#x2013;41.</Citation></Reference><Reference><Citation>Wang H., Rivenson Y., Jin Y., et al. Deep learning enables cross-modality super-resolution in fluorescence microscopy. Nat Methods. 2019 Jan;16(1):103&#x2013;110.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7276094</ArticleId><ArticleId IdType="pubmed">30559434</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36604467</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>09</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">2045-2322</ISSN><JournalIssue CitedMedium="Internet"><Volume>13</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>05</Day></PubDate></JournalIssue><Title>Scientific reports</Title><ISOAbbreviation>Sci Rep</ISOAbbreviation></Journal><ArticleTitle>Head CT deep learning model is highly accurate for early infarct estimation.</ArticleTitle><Pagination><StartPage>189</StartPage><MedlinePgn>189</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">189</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1038/s41598-023-27496-5</ELocationID><Abstract><AbstractText>Non-contrast head CT (NCCT) is extremely insensitive for early (&lt;&#x2009;3-6&#xa0;h) acute infarct identification. We developed a deep learning model that detects and delineates suspected early acute infarcts on NCCT, using diffusion MRI as ground truth (3566 NCCT/MRI training patient pairs). The model substantially outperformed 3 expert neuroradiologists on a test set of 150 CT scans of patients who were potential candidates for thrombectomy (60 stroke-negative, 90 stroke-positive middle cerebral artery territory only infarcts), with sensitivity 96% (specificity 72%) for the model versus 61-66% (specificity 90-92%) for the experts; model infarct volume estimates also strongly correlated with those of diffusion MRI (r<sup>2</sup>&#x2009;&gt;&#x2009;0.98). When this 150 CT test set was expanded to include a total of 364 CT scans with a more heterogeneous distribution of infarct locations (94 stroke-negative, 270 stroke-positive mixed territory infarcts), model sensitivity was 97%, specificity 99%, for detection of infarcts larger than the 70&#xa0;mL volume threshold used for patient selection in several major randomized controlled trials of thrombectomy treatment.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y" EqualContrib="Y"><LastName>Gauriau</LastName><ForeName>Romane</ForeName><Initials>R</Initials><AffiliationInfo><Affiliation>Data Science Office, Mass General Brigham, 100 Cambridge St, Suite 1303, Boston, MA, 02114, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y" EqualContrib="Y"><LastName>Bizzo</LastName><ForeName>Bernardo C</ForeName><Initials>BC</Initials><AffiliationInfo><Affiliation>Data Science Office, Mass General Brigham, 100 Cambridge St, Suite 1303, Boston, MA, 02114, USA. bbizzo@mgh.harvard.edu.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA. bbizzo@mgh.harvard.edu.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Diagnosticos da America SA (Dasa), Barueri, SP, Brazil. bbizzo@mgh.harvard.edu.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Comeau</LastName><ForeName>Donnella S</ForeName><Initials>DS</Initials><AffiliationInfo><Affiliation>Data Science Office, Mass General Brigham, 100 Cambridge St, Suite 1303, Boston, MA, 02114, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Hillis</LastName><ForeName>James M</ForeName><Initials>JM</Initials><AffiliationInfo><Affiliation>Data Science Office, Mass General Brigham, 100 Cambridge St, Suite 1303, Boston, MA, 02114, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Neurology, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Bridge</LastName><ForeName>Christopher P</ForeName><Initials>CP</Initials><AffiliationInfo><Affiliation>Data Science Office, Mass General Brigham, 100 Cambridge St, Suite 1303, Boston, MA, 02114, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Chin</LastName><ForeName>John K</ForeName><Initials>JK</Initials><AffiliationInfo><Affiliation>Data Science Office, Mass General Brigham, 100 Cambridge St, Suite 1303, Boston, MA, 02114, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Pawar</LastName><ForeName>Jayashri</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Data Science Office, Mass General Brigham, 100 Cambridge St, Suite 1303, Boston, MA, 02114, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Pourvaziri</LastName><ForeName>Ali</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Data Science Office, Mass General Brigham, 100 Cambridge St, Suite 1303, Boston, MA, 02114, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Sesic</LastName><ForeName>Ivana</ForeName><Initials>I</Initials><AffiliationInfo><Affiliation>Data Science Office, Mass General Brigham, 100 Cambridge St, Suite 1303, Boston, MA, 02114, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Sharaf</LastName><ForeName>Elshaimaa</ForeName><Initials>E</Initials><AffiliationInfo><Affiliation>Data Science Office, Mass General Brigham, 100 Cambridge St, Suite 1303, Boston, MA, 02114, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Cao</LastName><ForeName>Jinjin</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Data Science Office, Mass General Brigham, 100 Cambridge St, Suite 1303, Boston, MA, 02114, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Noro</LastName><ForeName>Flavia T C</ForeName><Initials>FTC</Initials><AffiliationInfo><Affiliation>Data Science Office, Mass General Brigham, 100 Cambridge St, Suite 1303, Boston, MA, 02114, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wiggins</LastName><ForeName>Walter F</ForeName><Initials>WF</Initials><AffiliationInfo><Affiliation>Data Science Office, Mass General Brigham, 100 Cambridge St, Suite 1303, Boston, MA, 02114, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, Brigham and Women's Hospital, Harvard Medical School, Boston, MA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Caton</LastName><ForeName>M Travis</ForeName><Initials>MT</Initials><AffiliationInfo><Affiliation>Data Science Office, Mass General Brigham, 100 Cambridge St, Suite 1303, Boston, MA, 02114, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, Brigham and Women's Hospital, Harvard Medical School, Boston, MA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kitamura</LastName><ForeName>Felipe</ForeName><Initials>F</Initials><AffiliationInfo><Affiliation>Diagnosticos da America SA (Dasa), Barueri, SP, Brazil.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Dreyer</LastName><ForeName>Keith J</ForeName><Initials>KJ</Initials><AffiliationInfo><Affiliation>Data Science Office, Mass General Brigham, 100 Cambridge St, Suite 1303, Boston, MA, 02114, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kalafut</LastName><ForeName>John F</ForeName><Initials>JF</Initials><AffiliationInfo><Affiliation>GE Healthcare, Chicago, IL, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Andriole</LastName><ForeName>Katherine P</ForeName><Initials>KP</Initials><AffiliationInfo><Affiliation>Data Science Office, Mass General Brigham, 100 Cambridge St, Suite 1303, Boston, MA, 02114, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, Brigham and Women's Hospital, Harvard Medical School, Boston, MA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Pomerantz</LastName><ForeName>Stuart R</ForeName><Initials>SR</Initials><AffiliationInfo><Affiliation>Data Science Office, Mass General Brigham, 100 Cambridge St, Suite 1303, Boston, MA, 02114, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Gonzalez</LastName><ForeName>Ramon G</ForeName><Initials>RG</Initials><AffiliationInfo><Affiliation>Data Science Office, Mass General Brigham, 100 Cambridge St, Suite 1303, Boston, MA, 02114, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lev</LastName><ForeName>Michael H</ForeName><Initials>MH</Initials><AffiliationInfo><Affiliation>Data Science Office, Mass General Brigham, 100 Cambridge St, Suite 1303, Boston, MA, 02114, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>05</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>Sci Rep</MedlineTA><NlmUniqueID>101563288</NlmUniqueID><ISSNLinking>2045-2322</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D014057" MajorTopicYN="N">Tomography, X-Ray Computed</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D020521" MajorTopicYN="Y">Stroke</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D008279" MajorTopicYN="N">Magnetic Resonance Imaging</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D020244" MajorTopicYN="N">Infarction, Middle Cerebral Artery</DescriptorName></MeshHeading></MeshHeadingList><CoiStatement>The authors declare no competing interests.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>7</Month><Day>27</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2023</Year><Month>1</Month><Day>3</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>5</Day><Hour>23</Hour><Minute>27</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36604467</ArticleId><ArticleId IdType="pmc">PMC9814956</ArticleId><ArticleId IdType="doi">10.1038/s41598-023-27496-5</ArticleId><ArticleId IdType="pii">10.1038/s41598-023-27496-5</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Lindsay MP, et al. World stroke organization (WSO): Global stroke fact sheet 2019. Int. J. Stroke. 2019;14:806&#x2013;817. doi: 10.1177/1747493019881353.</Citation><ArticleIdList><ArticleId IdType="doi">10.1177/1747493019881353</ArticleId><ArticleId IdType="pubmed">31658892</ArticleId></ArticleIdList></Reference><Reference><Citation>Nogueira RG, et al., DAWN Trial Investigators. Thrombectomy 6 to 24 Hours after Stroke with a Mismatch between Deficit and Infarct. N. Engl. J. Med. 378, 11&#x2013;21 (2018).</Citation><ArticleIdList><ArticleId IdType="pubmed">29129157</ArticleId></ArticleIdList></Reference><Reference><Citation>Leslie-Mazwi TM, et al. Endovascular stroke treatment outcomes after patient selection based on magnetic resonance imaging and clinical criteria. JAMA Neurol. 2016;73:43&#x2013;49. doi: 10.1001/jamaneurol.2015.3000.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jamaneurol.2015.3000</ArticleId><ArticleId IdType="pubmed">26524074</ArticleId></ArticleIdList></Reference><Reference><Citation>Campbell BCV, et al., HERMES collaborators. Penumbral imaging and functional outcome in patients with anterior circulation ischaemic stroke treated with endovascular thrombectomy versus medical therapy: a meta-analysis of individual patient-level data. Lancet Neurol. 18, 46&#x2013;55 (2019).</Citation><ArticleIdList><ArticleId IdType="pubmed">30413385</ArticleId></ArticleIdList></Reference><Reference><Citation>Nogueira RG, et al., Trevo Registry and DAWN Trial Investigators. stroke imaging selection modality and endovascular therapy outcomes in the early and extended time windows. Stroke. 52, 491&#x2013;497 (2021).</Citation><ArticleIdList><ArticleId IdType="pubmed">33430634</ArticleId></ArticleIdList></Reference><Reference><Citation>Kim BJ, et al. endovascular treatment after stroke due to large vessel occlusion for patients presenting very late from time last known well. JAMA Neurol. 2021;78:21&#x2013;29. doi: 10.1001/jamaneurol.2020.2804.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jamaneurol.2020.2804</ArticleId><ArticleId IdType="pmc">PMC7418043</ArticleId><ArticleId IdType="pubmed">32777014</ArticleId></ArticleIdList></Reference><Reference><Citation>Berkhemer OA, et al. A randomized trial of intraarterial treatment for acute ischemic stroke. N. Engl. J. Med. 2015;372:11&#x2013;20. doi: 10.1056/NEJMoa1411587.</Citation><ArticleIdList><ArticleId IdType="doi">10.1056/NEJMoa1411587</ArticleId><ArticleId IdType="pubmed">25517348</ArticleId></ArticleIdList></Reference><Reference><Citation>Lev MH, et al. Acute stroke: improved nonenhanced CT detection&#x2013;benefits of soft-copy interpretation by using variable window width and center level settings. Radiology. 1999;213:150&#x2013;155. doi: 10.1148/radiology.213.1.r99oc10150.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiology.213.1.r99oc10150</ArticleId><ArticleId IdType="pubmed">10540655</ArticleId></ArticleIdList></Reference><Reference><Citation>Camargo EC, et al. Acute brain infarct: detection and delineation with CT angiographic source images versus nonenhanced CT scans. Radiology. 2007;244:541&#x2013;548. doi: 10.1148/radiol.2442061028.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2442061028</ArticleId><ArticleId IdType="pubmed">17581888</ArticleId></ArticleIdList></Reference><Reference><Citation>Mullins, et al. CT and Conventional and Diffusion-Weighted MR Imaging in Acute Stroke: Study in 691 Patients at Presentation to the Emergency Department. Radiology224, 353&#x2013;60 (2002).</Citation><ArticleIdList><ArticleId IdType="pubmed">12147827</ArticleId></ArticleIdList></Reference><Reference><Citation>Fiebach JB, et al. CT and diffusion-weighted MR imaging in randomized order: Diffusion-weighted imaging results in higher accuracy and lower interrater variability in the diagnosis of hyperacute ischemic stroke. Stroke. 2002;33:2206&#x2013;2210. doi: 10.1161/01.STR.0000026864.20339.CB.</Citation><ArticleIdList><ArticleId IdType="doi">10.1161/01.STR.0000026864.20339.CB</ArticleId><ArticleId IdType="pubmed">12215588</ArticleId></ArticleIdList></Reference><Reference><Citation>Heiss WD, et al. Probability of cortical infarction predicted by flumazenil binding and diffusion-weighted imaging signal intensity: a comparative positron emission tomography/magnetic resonance imaging study in early ischemic stroke. Stroke. 2004;35:1892&#x2013;1898. doi: 10.1161/01.STR.0000134746.93535.9b.</Citation><ArticleIdList><ArticleId IdType="doi">10.1161/01.STR.0000134746.93535.9b</ArticleId><ArticleId IdType="pubmed">15218157</ArticleId></ArticleIdList></Reference><Reference><Citation>Sims JR, et al. ABC/2 for rapid clinical estimate of infarct, perfusion, and mismatch volumes. Neurology. 2009;24:2104&#x2013;2110. doi: 10.1212/WNL.0b013e3181aa5329.</Citation><ArticleIdList><ArticleId IdType="doi">10.1212/WNL.0b013e3181aa5329</ArticleId><ArticleId IdType="pmc">PMC2697964</ArticleId><ArticleId IdType="pubmed">19528517</ArticleId></ArticleIdList></Reference><Reference><Citation>Mikhail P, Le MGD, Mair G. Computational image analysis of nonenhanced computed tomography for acute ischaemic stroke: A systematic review. J. Stroke Cerebrovasc. Dis. 2020;29:104715. doi: 10.1016/j.jstrokecerebrovasdis.2020.104715.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jstrokecerebrovasdis.2020.104715</ArticleId><ArticleId IdType="pubmed">32144071</ArticleId></ArticleIdList></Reference><Reference><Citation>Qiu W, et al. Machine learning for detecting early infarction in acute stroke with non-contrast-enhanced CT. Radiology. 2020;294:638&#x2013;644. doi: 10.1148/radiol.2020191193.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2020191193</ArticleId><ArticleId IdType="pubmed">31990267</ArticleId></ArticleIdList></Reference><Reference><Citation>Pan J, et al. Detecting the early infarct core on non-contrast CT images with a deep learning residual network. J. Stroke Cerebrovasc. Dis. 2021;30:105752. doi: 10.1016/j.jstrokecerebrovasdis.2021.105752.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jstrokecerebrovasdis.2021.105752</ArticleId><ArticleId IdType="pubmed">33784518</ArticleId></ArticleIdList></Reference><Reference><Citation>Bouslama M, et al. Noncontrast computed tomography e-stroke infarct volume is similar to RAPID computed tomography perfusion in estimating postreperfusion infarct volumes. Stroke. 2021;52:634&#x2013;641. doi: 10.1161/STROKEAHA.120.031651.</Citation><ArticleIdList><ArticleId IdType="doi">10.1161/STROKEAHA.120.031651</ArticleId><ArticleId IdType="pubmed">33430633</ArticleId></ArticleIdList></Reference><Reference><Citation>Nagel S, et al. e-ASPECTS derived acute ischemic volumes on non-contrast-enhanced computed tomography images. Int. J. Stroke. 2020;15:995&#x2013;1001. doi: 10.1177/1747493019879661.</Citation><ArticleIdList><ArticleId IdType="doi">10.1177/1747493019879661</ArticleId><ArticleId IdType="pmc">PMC7739116</ArticleId><ArticleId IdType="pubmed">31570065</ArticleId></ArticleIdList></Reference><Reference><Citation>Schaefer PW, et al. Limited reliability of computed tomographic perfusion acute infarct volume measurements compared with diffusion-weighted imaging in anterior circulation stroke. Stroke. 2015;46:419&#x2013;424. doi: 10.1161/STROKEAHA.114.007117.</Citation><ArticleIdList><ArticleId IdType="doi">10.1161/STROKEAHA.114.007117</ArticleId><ArticleId IdType="pmc">PMC4308477</ArticleId><ArticleId IdType="pubmed">25550366</ArticleId></ArticleIdList></Reference><Reference><Citation>Haenssle HA, et al. Man against machine: Diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58 dermatologists. Ann. Oncol. 2018;29:1836&#x2013;1842. doi: 10.1093/annonc/mdy166.</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/annonc/mdy166</ArticleId><ArticleId IdType="pubmed">29846502</ArticleId></ArticleIdList></Reference><Reference><Citation>McKinney SM, et al. International evaluation of an AI system for breast cancer screening. Nature. 2020;577:89&#x2013;94. doi: 10.1038/s41586-019-1799-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41586-019-1799-6</ArticleId><ArticleId IdType="pubmed">31894144</ArticleId></ArticleIdList></Reference><Reference><Citation>Ronneberger O, Fischer P, and Brox T. U-Net: Convolutional Networks for Biomedical Image Segmentation. Preprint at https://arxiv.org/abs/1505.04597v1 (2015).</Citation></Reference><Reference><Citation>Gauriau R, et al. Using DICOM metadata for radiological image series categorization: A feasibility study on large clinical brain MRI datasets. J Digit. Imaging. 2020;33:747&#x2013;762. doi: 10.1007/s10278-019-00308-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10278-019-00308-x</ArticleId><ArticleId IdType="pmc">PMC7256138</ArticleId><ArticleId IdType="pubmed">31950302</ArticleId></ArticleIdList></Reference><Reference><Citation>Pedemonte S, et al. Detection and delineation of acute cerebral infarct on DWI using weakly supervised machine learning. Med. Image Comput. Comput. Assist. Interv. (MICCAI) 2018;1107:81&#x2013;88.</Citation></Reference><Reference><Citation>Glorot X, Bengio Y. Understanding the difficulty of training deep feedforward neural networks. Proc. Thirteenth Int. Conf. Artif. Intell. Stat. 2010;9:249&#x2013;256.</Citation></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36603439</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>05</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1879-0534</ISSN><JournalIssue CitedMedium="Internet"><Volume>153</Volume><PubDate><Year>2022</Year><Month>Dec</Month><Day>31</Day></PubDate></JournalIssue><Title>Computers in biology and medicine</Title><ISOAbbreviation>Comput Biol Med</ISOAbbreviation></Journal><ArticleTitle>SwinGAN: A dual-domain Swin Transformer-based generative adversarial network for MRI reconstruction.</ArticleTitle><Pagination><StartPage>106513</StartPage><MedlinePgn>106513</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1016/j.compbiomed.2022.106513</ELocationID><ELocationID EIdType="pii" ValidYN="Y">S0010-4825(22)01221-5</ELocationID><Abstract><AbstractText>Magnetic resonance imaging (MRI) is one of the most important modalities for clinical diagnosis. However, the main disadvantages of MRI are the long scanning time and the moving artifact caused by patient movement during prolonged imaging. It can also lead to patient anxiety and discomfort, so accelerated imaging is indispensable for MRI. Convolutional neural network (CNN) based methods have become the fact standard for medical image reconstruction, and generative adversarial network (GAN) have also been widely used. Nevertheless, due to the limited ability of CNN to capture long-distance information, it may lead to defects in the structure of the reconstructed images such as blurry contour. In this paper, we propose a novel Swin Transformer-based dual-domain generative adversarial network (SwinGAN) for accelerated MRI reconstruction. The SwinGAN consists of two generators: a frequency-domain generator and an image-domain generator. Both the generators utilize Swin Transformer as backbone for effectively capturing the long-distance dependencies. A contextual image relative position encoder (ciRPE) is designed to enhance the ability to capture local information. We extensively evaluate the method on the IXI brain dataset, MICCAI 2013 dataset and MRNet knee dataset. Compared with KIGAN, the peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) are improved by 6.1% and 1.49% to 37.64&#xa0;dB and 0.98 on IXI dataset respectively, which demonstrates that our model can sufficiently utilize the local and global information of image. The model shows promising performance and robustness under different undersampling masks, different acceleration rates and different datasets. But it needs high hardware requirements with the increasing of the network parameters. The code is available at: https://github.com/learnerzx/SwinGAN.</AbstractText><CopyrightInformation>Copyright &#xa9; 2023 Elsevier Ltd. All rights reserved.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Zhao</LastName><ForeName>Xiang</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>School of Information Science and Engineering, Henan University of Technology, Zhengzhou, 450001, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yang</LastName><ForeName>Tiejun</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>School of Artificial Intelligence and Big Data, Henan University of Technology, Zhengzhou, 450001, China; Key Laboratory of Grain Information Processing and Control (HAUT), Ministry of Education, Zhengzhou, China; Henan Key Laboratory of Grain Photoelectric Detection and Control (HAUT), Zhengzhou, Henan, China. Electronic address: tjyanghlyu@126.com.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Li</LastName><ForeName>Bingjie</ForeName><Initials>B</Initials><AffiliationInfo><Affiliation>School of Information Science and Engineering, Henan University of Technology, Zhengzhou, 450001, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Xin</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>School of Information Science and Engineering, Henan University of Technology, Zhengzhou, 450001, China.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>31</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>Comput Biol Med</MedlineTA><NlmUniqueID>1250250</NlmUniqueID><ISSNLinking>0010-4825</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">GAN</Keyword><Keyword MajorTopicYN="N">MRI reconstruction</Keyword><Keyword MajorTopicYN="N">Transformer</Keyword></KeywordList><CoiStatement>Declaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>8</Month><Day>1</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>9</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>31</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>5</Day><Hour>18</Hour><Minute>16</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36603439</ArticleId><ArticleId IdType="doi">10.1016/j.compbiomed.2022.106513</ArticleId><ArticleId IdType="pii">S0010-4825(22)01221-5</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36603416</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>05</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1873-4499</ISSN><JournalIssue CitedMedium="Internet"><Volume>95</Volume><PubDate><Year>2022</Year><Month>Dec</Month><Day>28</Day></PubDate></JournalIssue><Title>Clinical imaging</Title><ISOAbbreviation>Clin Imaging</ISOAbbreviation></Journal><ArticleTitle>Detection of microcalcifications in photon-counting dedicated breast-CT using a deep convolutional neural network: Proof of principle.</ArticleTitle><Pagination><StartPage>28</StartPage><EndPage>36</EndPage><MedlinePgn>28-36</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1016/j.clinimag.2022.12.006</ELocationID><ELocationID EIdType="pii" ValidYN="Y">S0899-7071(22)00323-0</ELocationID><Abstract><AbstractText Label="OBJECTIVE" NlmCategory="OBJECTIVE">In this study, we investigate the feasibility of a deep Convolutional Neural Network (dCNN), trained with mammographic images, to detect and classify microcalcifications (MC) in breast-CT (BCT) images.</AbstractText><AbstractText Label="METHODS" NlmCategory="METHODS">This retrospective single-center study was approved by the local ethics committee. 3518 icons generated from 319 mammograms were classified into three classes: "no MC" (1121), "probably benign MC" (1332), and "suspicious MC" (1065). A dCNN was trained (70% of data), validated (20%), and tested on a "real-world" dataset (10%). The diagnostic performance of the dCNN was tested on a subset of 60 icons, generated from 30 mammograms and 30 breast-CT images, and compared to human reading. ROC analysis was used to calculate diagnostic performance. Moreover, colored probability maps for representative BCT images were calculated using a sliding-window approach.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">The dCNN reached an accuracy of 98.8% on the "real-world" dataset. The accuracy on the subset of 60 icons was 100% for mammographic images, 60% for "no MC", 80% for "probably benign MC" and 100% for "suspicious MC". Intra-class correlation between the dCNN and the readers was almost perfect (0.85). Kappa values between the two readers (0.93) and the dCNN were almost perfect (reader 1: 0.85 and reader 2: 0.82). The sliding-window approach successfully detected suspicious MC with high image quality. The diagnostic performance of the dCNN to classify benign and suspicious MC was excellent with an AUC of 93.8% (95% CI 87, 4%-100%).</AbstractText><AbstractText Label="CONCLUSION" NlmCategory="CONCLUSIONS">Deep convolutional networks can be used to detect and classify benign and suspicious MC in breast-CT images.</AbstractText><CopyrightInformation>Copyright &#xa9; 2022 The Author(s). Published by Elsevier Inc. All rights reserved.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Landsmann</LastName><ForeName>Anna</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Institute of Diagnostic and Interventional Radiology, University Hospital Zurich, Switzerland. Electronic address: anna.landsmann@usz.ch.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Ruppert</LastName><ForeName>Carlotta</ForeName><Initials>C</Initials><AffiliationInfo><Affiliation>Institute of Diagnostic and Interventional Radiology, University Hospital Zurich, Switzerland. Electronic address: Carlotta.ruppert@uzh.ch.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Borkowski</LastName><ForeName>Karol</ForeName><Initials>K</Initials><AffiliationInfo><Affiliation>Institute of Diagnostic and Interventional Radiology, University Hospital Zurich, Switzerland. Electronic address: karol.borkowski@usz.ch.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Hejduk</LastName><ForeName>Patryk</ForeName><Initials>P</Initials><AffiliationInfo><Affiliation>Institute of Diagnostic and Interventional Radiology, University Hospital Zurich, Switzerland. Electronic address: patryk.hejduk@usz.ch.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Ciritsis</LastName><ForeName>Alexander</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Institute of Diagnostic and Interventional Radiology, University Hospital Zurich, Switzerland. Electronic address: alexander.ciritsis@usz.ch.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wieler</LastName><ForeName>Jann</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Institute of Diagnostic and Interventional Radiology, University Hospital Zurich, Switzerland. Electronic address: jann.wieler@usz.ch.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Rossi</LastName><ForeName>Cristina</ForeName><Initials>C</Initials><AffiliationInfo><Affiliation>Institute of Diagnostic and Interventional Radiology, University Hospital Zurich, Switzerland. Electronic address: cristina.rossi@usz.ch.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Boss</LastName><ForeName>Andreas</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Institute of Diagnostic and Interventional Radiology, University Hospital Zurich, Switzerland. Electronic address: andreas.boss@usz.ch.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>28</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>Clin Imaging</MedlineTA><NlmUniqueID>8911831</NlmUniqueID><ISSNLinking>0899-7071</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Artificial intelligence</Keyword><Keyword MajorTopicYN="N">Breast Cancer</Keyword><Keyword MajorTopicYN="N">Breast-CT</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Microcalcifications</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>11</Month><Day>12</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>12</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>5</Day><Hour>18</Hour><Minute>15</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36603416</ArticleId><ArticleId IdType="doi">10.1016/j.clinimag.2022.12.006</ArticleId><ArticleId IdType="pii">S0899-7071(22)00323-0</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36603372</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>05</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1879-0771</ISSN><JournalIssue CitedMedium="Internet"><Volume>104</Volume><PubDate><Year>2022</Year><Month>Dec</Month><Day>30</Day></PubDate></JournalIssue><Title>Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society</Title><ISOAbbreviation>Comput Med Imaging Graph</ISOAbbreviation></Journal><ArticleTitle>Synchronous Medical Image Augmentation framework for deep learning-based image segmentation.</ArticleTitle><Pagination><StartPage>102161</StartPage><MedlinePgn>102161</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1016/j.compmedimag.2022.102161</ELocationID><ELocationID EIdType="pii" ValidYN="Y">S0895-6111(22)00131-8</ELocationID><Abstract><AbstractText>Various deep learning (DL) models are widely applied in medical image analysis, and their performance depends on the scale and diversity of available training data. However, medical images often suffer from difficulty in data acquisition, imbalance in sample categories, and high cost of labeling. In addition, most image augmentation approaches mainly focus on image synthesis only for classification tasks, and rarely consider the synthetic image-label pairs for image segmentation tasks. In this paper, we focus on the medical image augmentation for DL-based image segmentation and the synchronization between augmented image samples and their labels. We design a Synchronous Medical Image Augmentation (SMIA) framework, which includes two modules based on stochastic transformation and synthesis, and provides diverse and annotated training sets for DL models. In the transform-based SMIA module, for each medical image sample and its tissue segments, a subset of SMIA factors with a random number of factors and stochastic parameter values are selected to simultaneously generate augmented samples and the paired tissue segments. In the synthesis-based SMIA module, we randomly replace the original tissues with the augmented tissues using an equivalent replacement method to synthesize new medical images, which can well maintain the original medical implications. DL-based image segmentation experiments on bone marrow smear and dermoscopic images demonstrate that the proposed SMIA framework can generate category-balanced and diverse training data, and have a positive impact on the performance of the models.</AbstractText><CopyrightInformation>Copyright &#xa9; 2022 Elsevier Ltd. All rights reserved.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Chen</LastName><ForeName>Jianguo</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>School of Software Engineering, Sun Yat-sen University, Zhuhai, 519082, China; Donnelly Centre for Cellular and Biomolecular Research, Department of Molecular Genetics and Department of Computer Science at University of Toronto, Toronto, ON M5S 3E2, Canada.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yang</LastName><ForeName>Nan</ForeName><Initials>N</Initials><AffiliationInfo><Affiliation>Department of Infectious Disease, the First Affiliated Hospital of Xi'an Jiaotong University, Xi'an, Shaanxi 710061, China. Electronic address: Shmily.1989.2008@stu.xjtu.edu.cn.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Pan</LastName><ForeName>Yuhui</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Donnelly Centre for Cellular and Biomolecular Research, Department of Molecular Genetics and Department of Computer Science at University of Toronto, Toronto, ON M5S 3E2, Canada.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Liu</LastName><ForeName>Hailing</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>Department of Hematology, the Second Affiliated Hospital of Xi'an Jiaotong University, Xi'an, Shaanxi 710004, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Zhaolei</ForeName><Initials>Z</Initials><AffiliationInfo><Affiliation>Donnelly Centre for Cellular and Biomolecular Research, Department of Molecular Genetics and Department of Computer Science at University of Toronto, Toronto, ON M5S 3E2, Canada.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>30</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>Comput Med Imaging Graph</MedlineTA><NlmUniqueID>8806104</NlmUniqueID><ISSNLinking>0895-6111</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Image-label pair</Keyword><Keyword MajorTopicYN="N">Medical image augmentation</Keyword><Keyword MajorTopicYN="N">Synchronous augmentation</Keyword></KeywordList><CoiStatement>Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2020</Year><Month>1</Month><Day>8</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>8</Month><Day>7</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>8</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>5</Day><Hour>18</Hour><Minute>11</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36603372</ArticleId><ArticleId IdType="doi">10.1016/j.compmedimag.2022.102161</ArticleId><ArticleId IdType="pii">S0895-6111(22)00131-8</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36603098</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>09</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>12</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1095-9203</ISSN><JournalIssue CitedMedium="Internet"><Volume>379</Volume><Issue>6627</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>06</Day></PubDate></JournalIssue><Title>Science (New York, N.Y.)</Title><ISOAbbreviation>Science</ISOAbbreviation></Journal><ArticleTitle>Cilia function as calcium-mediated mechanosensors that instruct left-right asymmetry.</ArticleTitle><Pagination><StartPage>71</StartPage><EndPage>78</EndPage><MedlinePgn>71-78</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1126/science.abq7317</ELocationID><Abstract><AbstractText>The breaking of bilateral symmetry in most vertebrates is critically dependent upon the motile cilia of the embryonic left-right organizer (LRO), which generate a directional fluid flow; however, it remains unclear how this flow is sensed. Here, we demonstrated that immotile LRO cilia are mechanosensors for shear force using a methodological pipeline that combines optical tweezers, light sheet microscopy, and deep learning to permit in vivo analyses in zebrafish. Mechanical manipulation of immotile LRO cilia activated intraciliary calcium transients that required the cation channel Polycystin-2. Furthermore, mechanical force applied to LRO cilia was sufficient to rescue and reverse cardiac situs in zebrafish that lack motile cilia. Thus, LRO cilia are mechanosensitive cellular levers that convert biomechanical forces into calcium signals to instruct left-right asymmetry.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Djenoune</LastName><ForeName>Lydia</ForeName><Initials>L</Initials><Identifier Source="ORCID">0000-0002-9105-3575</Identifier><AffiliationInfo><Affiliation>Cardiovascular Research Center, Cardiology Division, Department of Medicine, Massachusetts General Hospital and Harvard Medical School, Boston, MA 02129, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Mahamdeh</LastName><ForeName>Mohammed</ForeName><Initials>M</Initials><Identifier Source="ORCID">0000-0003-3985-6992</Identifier><AffiliationInfo><Affiliation>Cardiovascular Research Center, Cardiology Division, Department of Medicine, Massachusetts General Hospital and Harvard Medical School, Boston, MA 02129, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Truong</LastName><ForeName>Thai V</ForeName><Initials>TV</Initials><Identifier Source="ORCID">0000-0001-6475-8935</Identifier><AffiliationInfo><Affiliation>Translational Imaging Center, University of Southern California, Los Angeles, CA 90089, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Nguyen</LastName><ForeName>Christopher T</ForeName><Initials>CT</Initials><Identifier Source="ORCID">0000-0003-1475-2329</Identifier><AffiliationInfo><Affiliation>Cardiovascular Research Center, Cardiology Division, Department of Medicine, Massachusetts General Hospital and Harvard Medical School, Boston, MA 02129, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Cardiovascular Innovation Research Center, Heart, Vascular, and Thoracic Institute, Cleveland Clinic, Cleveland, OH 44195, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Division of Health Science Technology, Massachusetts Institute of Technology, Cambridge, MA 02139, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Fraser</LastName><ForeName>Scott E</ForeName><Initials>SE</Initials><Identifier Source="ORCID">0000-0002-5377-0223</Identifier><AffiliationInfo><Affiliation>Translational Imaging Center, University of Southern California, Los Angeles, CA 90089, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Brueckner</LastName><ForeName>Martina</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Departments of Pediatrics and Genetics, Yale University School of Medicine, New Haven, CT 06520, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Howard</LastName><ForeName>Jonathon</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Department of Molecular Biochemistry and Biophysics, Yale University School of Medicine, New Haven, CT 06520, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yuan</LastName><ForeName>Shiaulou</ForeName><Initials>S</Initials><Identifier Source="ORCID">0000-0002-5061-6132</Identifier><AffiliationInfo><Affiliation>Cardiovascular Research Center, Cardiology Division, Department of Medicine, Massachusetts General Hospital and Harvard Medical School, Boston, MA 02129, USA.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>05</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>Science</MedlineTA><NlmUniqueID>0404511</NlmUniqueID><ISSNLinking>0036-8075</ISSNLinking></MedlineJournalInfo><ChemicalList><Chemical><RegistryNumber>SY7Q814VUP</RegistryNumber><NameOfSubstance UI="D002118">Calcium</NameOfSubstance></Chemical><Chemical><RegistryNumber>0</RegistryNumber><NameOfSubstance UI="D029961">Zebrafish Proteins</NameOfSubstance></Chemical></ChemicalList><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D000818" MajorTopicYN="N">Animals</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D015027" MajorTopicYN="Y">Zebrafish</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D002118" MajorTopicYN="Y">Calcium</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D002923" MajorTopicYN="N">Cilia</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D019521" MajorTopicYN="N">Body Patterning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D029961" MajorTopicYN="N">Zebrafish Proteins</DescriptorName></MeshHeading></MeshHeadingList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>5</Day><Hour>14</Hour><Minute>5</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>ppublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36603098</ArticleId><ArticleId IdType="doi">10.1126/science.abq7317</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36603007</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>09</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic-eCollection"><Journal><ISSN IssnType="Electronic">1932-6203</ISSN><JournalIssue CitedMedium="Internet"><Volume>18</Volume><Issue>1</Issue><PubDate><Year>2023</Year></PubDate></JournalIssue><Title>PloS one</Title><ISOAbbreviation>PLoS One</ISOAbbreviation></Journal><ArticleTitle>Motion artifact reduction for magnetic resonance imaging with deep learning and k-space analysis.</ArticleTitle><Pagination><StartPage>e0278668</StartPage><MedlinePgn>e0278668</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">e0278668</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1371/journal.pone.0278668</ELocationID><Abstract><AbstractText>Motion artifacts deteriorate the quality of magnetic resonance (MR) images. This study proposes a new method to detect phase-encoding (PE) lines corrupted by motion and remove motion artifacts in MR images. 67 cases containing 8710 slices of axial T2-weighted images from the IXI public dataset were split into three datasets, i.e., training (50 cases/6500 slices), validation (5/650), and test (12/1560) sets. First, motion-corrupted k-spaces and images were simulated using a pseudo-random sampling order and random motion tracks. A convolutional neural network (CNN) model was trained to filter the motion-corrupted images. Then, the k-space of the filtered image was compared with the motion-corrupted k-space line-by-line, to detect the PE lines affected by motion. Finally, the unaffected PE lines were used to reconstruct the final image using compressed sensing (CS). For the simulated images with 35%, 40%, 45%, and 50% unaffected PE lines, the mean peak signal-to-noise ratio (PSNRs) of resulting images (mean&#xb1;standard deviation) were 36.129&#xb1;3.678, 38.646&#xb1;3.526, 40.426&#xb1;3.223, and 41.510&#xb1;3.167, respectively, and the mean structural similarity (SSIMs) were 0.950&#xb1;0.046, 0.964&#xb1;0.035, 0.975&#xb1;0.025, and 0.979&#xb1;0.023, respectively. For images with more than 35% PE lines unaffected by motion, images reconstructed with proposed algorithm exhibited better quality than those images reconstructed with CS using 35% under-sampled data (PSNR 37.678&#xb1;3.261, SSIM 0.964&#xb1;0.028). It was proved that deep learning and k-space analysis can detect the k-space PE lines affected by motion and CS can be used to reconstruct images from unaffected data, effectively alleviating the motion artifacts.</AbstractText><CopyrightInformation>Copyright: &#xa9; 2023 Cui et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Cui</LastName><ForeName>Long</ForeName><Initials>L</Initials><AffiliationInfo><Affiliation>Department of Physics, Shanghai Key Laboratory of Magnetic Resonance, East China Normal University, Shanghai, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Song</LastName><ForeName>Yang</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Department of Physics, Shanghai Key Laboratory of Magnetic Resonance, East China Normal University, Shanghai, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wang</LastName><ForeName>Yida</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Department of Physics, Shanghai Key Laboratory of Magnetic Resonance, East China Normal University, Shanghai, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wang</LastName><ForeName>Rui</ForeName><Initials>R</Initials><AffiliationInfo><Affiliation>Department of Physics, Shanghai Key Laboratory of Magnetic Resonance, East China Normal University, Shanghai, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wu</LastName><ForeName>Dongmei</ForeName><Initials>D</Initials><AffiliationInfo><Affiliation>Department of Physics, Shanghai Key Laboratory of Magnetic Resonance, East China Normal University, Shanghai, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Xie</LastName><ForeName>Haibin</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>Department of Physics, Shanghai Key Laboratory of Magnetic Resonance, East China Normal University, Shanghai, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Li</LastName><ForeName>Jianqi</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Department of Physics, Shanghai Key Laboratory of Magnetic Resonance, East China Normal University, Shanghai, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yang</LastName><ForeName>Guang</ForeName><Initials>G</Initials><Identifier Source="ORCID">0000-0001-8942-427X</Identifier><AffiliationInfo><Affiliation>Department of Physics, Shanghai Key Laboratory of Magnetic Resonance, East China Normal University, Shanghai, China.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>05</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>PLoS One</MedlineTA><NlmUniqueID>101285081</NlmUniqueID><ISSNLinking>1932-6203</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D016477" MajorTopicYN="N">Artifacts</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D008279" MajorTopicYN="N">Magnetic Resonance Imaging</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D016571" MajorTopicYN="N">Neural Networks, Computer</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D009038" MajorTopicYN="N">Motion</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D007091" MajorTopicYN="N">Image Processing, Computer-Assisted</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading></MeshHeadingList><CoiStatement>The authors have declared that no competing interests exist.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2021</Year><Month>9</Month><Day>4</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>11</Month><Day>22</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>5</Day><Hour>13</Hour><Minute>44</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36603007</ArticleId><ArticleId IdType="doi">10.1371/journal.pone.0278668</ArticleId><ArticleId IdType="pii">PONE-D-21-28686</ArticleId><ArticleId IdType="pmc">PMC9815594</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Zaitsev M., Maclaren J. &amp; Herbst M. Motion artifacts in MRI: a complex problem with many partial solutions. J Magn Reson Imaging. 42(4), 887&#x2013;901 (2015). doi: 10.1002/jmri.24850</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmri.24850</ArticleId><ArticleId IdType="pmc">PMC4517972</ArticleId><ArticleId IdType="pubmed">25630632</ArticleId></ArticleIdList></Reference><Reference><Citation>Lustig M., Donoho D. &amp; Pauly J.M. Sparse MRI: The Application of Compressed Sensing for Rapid MR Imaging. Magn Reson Med. 58, 1182&#x2013;1195 (2007). doi: 10.1002/mrm.21391</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.21391</ArticleId><ArticleId IdType="pubmed">17969013</ArticleId></ArticleIdList></Reference><Reference><Citation>Goldstein T. &amp; Osher S. The split Bregman method for L1-regularized problems. Siam Journal on Imaging Sciences. 2(2), 323&#x2013;343 (2009).</Citation></Reference><Reference><Citation>Dong L., Lu Z., Liu X.Y. &amp; Li Z.W. Performance optimization of three down-sampling imaging strategies and their comparison with the conventional Fourier telescope. Acta Physica Sinica. 68(7) (2019).</Citation></Reference><Reference><Citation>Rumelhart D.E., Hinton G.E. &amp; Williams R.J. Learning internal representations by error propagation, in Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol. 1. Cambridge, MA: MIT Press, p 318&#x2013;362 (1986).</Citation></Reference><Reference><Citation>McCann M.T., Jin K.H. &amp; Unser M. Convolutional Neural Networks for Inverse Problems in Imaging: A review. IEEE Signal Processing Magazine. 34(6), 85&#x2013;95 (2017).</Citation><ArticleIdList><ArticleId IdType="pubmed">28641250</ArticleId></ArticleIdList></Reference><Reference><Citation>Jin K.H., McCann M.T., Froustey E. &amp; Unser M. Deep Convolutional Neural Network for Inverse Problems in Imaging. IEEE Transactions on Image Processing. 26(9), 4509&#x2013;4522 (2017). doi: 10.1109/TIP.2017.2713099</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TIP.2017.2713099</ArticleId><ArticleId IdType="pubmed">28641250</ArticleId></ArticleIdList></Reference><Reference><Citation>Malave M.O., et al.. Reconstruction of undersampled 3D non-Cartesian image-based navigators for coronary MRA using an unrolled deep learning model. Magn Reson Med. (2020). doi: 10.1002/mrm.28177</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.28177</ArticleId><ArticleId IdType="pmc">PMC8331070</ArticleId><ArticleId IdType="pubmed">32011021</ArticleId></ArticleIdList></Reference><Reference><Citation>Hyun C.M., Kim H.P., Lee S.M., Lee S. &amp; Seo J.K. Deep learning for undersampled MRI reconstruction. Physics in Medicine and Biology. 63(13) (2018). doi: 10.1088/1361-6560/aac71a</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/1361-6560/aac71a</ArticleId><ArticleId IdType="pubmed">29787383</ArticleId></ArticleIdList></Reference><Reference><Citation>Dietz B., et al.. Single patient convolutional neural networks for real-time MR reconstruction: a proof of concept application in lung tumor segmentation for adaptive radiotherapy. Physics in Medicine and Biology. 64(19) (2019). doi: 10.1088/1361-6560/ab408e</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/1361-6560/ab408e</ArticleId><ArticleId IdType="pubmed">31476750</ArticleId></ArticleIdList></Reference><Reference><Citation>Kustner T., et al.. Automated reference-free detection of motion artifacts in magnetic resonance images. Magnetic Resonance Materials in Physics Biology and Medicine. 31(2), 243&#x2013;256 (2018). doi: 10.1007/s10334-017-0650-z</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10334-017-0650-z</ArticleId><ArticleId IdType="pubmed">28932991</ArticleId></ArticleIdList></Reference><Reference><Citation>Sreekumari A., et al.. A Deep Learning-Based Approach to Reduce Rescan and Recall Rates in Clinical MRI Examinations. American Journal of Neuroradiology. 40(2), 217&#x2013;223 (2019). doi: 10.3174/ajnr.A5926</Citation><ArticleIdList><ArticleId IdType="doi">10.3174/ajnr.A5926</ArticleId><ArticleId IdType="pmc">PMC7028612</ArticleId><ArticleId IdType="pubmed">30606726</ArticleId></ArticleIdList></Reference><Reference><Citation>Meding, K., Loktyushin, A. &amp; Hirsch, M. Automatic detection of motion artifacts in MR images using CNNS. 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 811&#x2013;815.</Citation></Reference><Reference><Citation>Haskell M.W., et al.. Network Accelerated Motion Estimation and Reduction (NAMER): Convolutional neural network guided retrospective motion correction using a separable motion model. Magn Reson Med. 82(4), 1452&#x2013;1461 (2019). doi: 10.1002/mrm.27771</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.27771</ArticleId><ArticleId IdType="pmc">PMC6626557</ArticleId><ArticleId IdType="pubmed">31045278</ArticleId></ArticleIdList></Reference><Reference><Citation>Johnson P.M. &amp; Drangova M. Conditional generative adversarial network for 3D rigid-body motion correction in MRI. Magnetic Resonance in Medicine. 82(3), 901&#x2013;910 (2019). doi: 10.1002/mrm.27772</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.27772</ArticleId><ArticleId IdType="pubmed">31006909</ArticleId></ArticleIdList></Reference><Reference><Citation>Duffy B.A., et al.. Retrospective correction of motion artifact affected structural MRI images using deep learning of simulated motion. Medical Imaging with Deep Learning (MIDL) 2018. Conference.</Citation></Reference><Reference><Citation>Armanious, K., Gatidis, S., Nikolaou, K., Yang, B. &amp; Kustner, T. Retrospective Correction of Rigid and Non-Rigid Mr Motion Artifacts Using Gans. 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019). 1550&#x2013;1554.</Citation></Reference><Reference><Citation>Oh Gyutaek, Lee Jeong Eun, Ye Jong Chul. Unsupervised MR Motion Artifact Deep Learning using Outlier-Rejecting Bootstrap Aggregation. arXiv:2011.06337v1 [cs.CV].
2020. Available From: https://arxiv.org/abs/2011.06337</Citation></Reference><Reference><Citation>Ronneberger Olaf, Fischer Philipp, Brox Thomas. U-Net: Convolutional Networks for Biomedical Image Segmentation. ArXiv: 1505.04597v1.</Citation></Reference><Reference><Citation>Otsu N. A threshold selection method from gray-level histograms. IEEE Transaction on Systems, Man, and Cybernetics. doi: 10.1109/TSMC.4310076, 62&#x2013;66 (1979).</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TSMC.4310076</ArticleId></ArticleIdList></Reference><Reference><Citation>Pawar K., Chen Z.L., Shah N.J. &amp; Egan G.F. Suppressing motion artefacts in MRI using an Inception-ResNet network with motion simulation augmentation. Nmr in Biomedicine. doi: 10.1002/nbm.4225, 4225 (2019).</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/nbm.4225</ArticleId><ArticleId IdType="pubmed">31865624</ArticleId></ArticleIdList></Reference><Reference><Citation>Tamada D., Kromrey M.L., Ichikawa S., Onishi H. &amp; Motosugi U. Motion Artifact Reduction Using a Convolutional Neural Network for Dynamic Contrast Enhanced MR Imaging of the Liver. Magnetic Resonance in Medical Sciences. 19(1), 64&#x2013;76 (2020). doi: 10.2463/mrms.mp.2018-0156</Citation><ArticleIdList><ArticleId IdType="doi">10.2463/mrms.mp.2018-0156</ArticleId><ArticleId IdType="pmc">PMC7067907</ArticleId><ArticleId IdType="pubmed">31061259</ArticleId></ArticleIdList></Reference><Reference><Citation>Kustner T., et al.. Retrospective correction of motion-affected MR images using deep learning frameworks. Magnetic Resonance in Medicine. 82(4), 1527&#x2013;1540 (2019). doi: 10.1002/mrm.27783</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.27783</ArticleId><ArticleId IdType="pubmed">31081955</ArticleId></ArticleIdList></Reference><Reference><Citation>Mansfield P. Multi-planar image-formation using NMR spin echoes. Journal of Physics C&#x2014;Solid State Physics. 10(3), L55&#x2013;L58 (1977).</Citation></Reference><Reference><Citation>Larkman D.J., Nunes R.G. Parallel magnetic resonance imaging. Physics in Medicine and Biology. 52(7), R15&#x2013;R55 (2007). doi: 10.1088/0031-9155/52/7/R01</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/0031-9155/52/7/R01</ArticleId><ArticleId IdType="pubmed">17374908</ArticleId></ArticleIdList></Reference><Reference><Citation>Guo J.Y., Kholmovski E.G., Zhang L. &amp; Parker D.L. Evaluation of motion effects on parallel MR imaging with precalibration. Magn Reson Imaging. 25(8), 1130&#x2013;1137 (2007). doi: 10.1016/j.mri.2007.01.117</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.mri.2007.01.117</ArticleId><ArticleId IdType="pubmed">17905245</ArticleId></ArticleIdList></Reference><Reference><Citation>Peeters J.M. &amp; Fuderer M. SENSE with improved tolerance to inaccuracies in coil sensitivity maps. Magn Reson Med. 69(6), 1665&#x2013;1669 (2013). doi: 10.1002/mrm.24400</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.24400</ArticleId><ArticleId IdType="pubmed">22847672</ArticleId></ArticleIdList></Reference><Reference><Citation>Ehman R.L. &amp; Felmlee J.P. Adaptive technique for high-definition MR imaging of moving structures. Radiology. 173(1), 255&#x2013;263 (1989). doi: 10.1148/radiology.173.1.2781017</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiology.173.1.2781017</ArticleId><ArticleId IdType="pubmed">2781017</ArticleId></ArticleIdList></Reference><Reference><Citation>Atkinson D. &amp; Hill D.L.G. Reconstruction after rotational motion. Magn Reson Med. 49, 183&#x2013;187 (2003). doi: 10.1002/mrm.10333</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.10333</ArticleId><ArticleId IdType="pubmed">12509836</ArticleId></ArticleIdList></Reference><Reference><Citation>Pipe J.G. Motion correction with PROPELLER MRI: application to head motion and free-breathing cardiac imaging. Magn Reson Med. 42(5), 963&#x2013;969 (1999). doi: 10.1002/(sici)1522-2594(199911)42:5&amp;lt;963::aid-mrm17&amp;gt;3.0.co;2-l</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/(sici)1522-2594(199911)42:5&amp;lt;963::aid-mrm17&amp;gt;3.0.co;2-l</ArticleId><ArticleId IdType="pubmed">10542356</ArticleId></ArticleIdList></Reference><Reference><Citation>Forbes K.P.N., Pipe J.G., Bird C.R. &amp; Heiserman J.E. PROPELLER MRI: clinical testing of a novel technique for quantification and compensation of head motion. Journal of Magnetic Resonance Imaging. 14(3), 215&#x2013;222 (2001). doi: 10.1002/jmri.1176</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmri.1176</ArticleId><ArticleId IdType="pubmed">11536397</ArticleId></ArticleIdList></Reference><Reference><Citation>Ooi M.B., Aksoy M., Maclaren J., Watkins R.D. &amp; Bammer R. Prospective motion correction using inductively coupled wireless RF coils. Magn Reson Med. 70(3), 639&#x2013;647 (2013). doi: 10.1002/mrm.24845</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.24845</ArticleId><ArticleId IdType="pmc">PMC4006309</ArticleId><ArticleId IdType="pubmed">23813444</ArticleId></ArticleIdList></Reference><Reference><Citation>Ooi M.B., Krueger S., Thomas W.J., Swaminathan S.V. &amp; Brown T.R. Prospective real-time correction for arbitrary head motion using active markers. Magn Reson Med. 62(4), 943&#x2013;954 (2009). doi: 10.1002/mrm.22082</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.22082</ArticleId><ArticleId IdType="pmc">PMC3033410</ArticleId><ArticleId IdType="pubmed">19488989</ArticleId></ArticleIdList></Reference><Reference><Citation>Schulz J., et al.. An embedded optical tracking system for motion-corrected magnetic resonance imaging at 7T. Magn Reson Mater Phy. 25(6), 443&#x2013;453 (2012). doi: 10.1007/s10334-012-0320-0</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10334-012-0320-0</ArticleId><ArticleId IdType="pubmed">22695771</ArticleId></ArticleIdList></Reference><Reference><Citation>Feinberg D.A., et al.. Hybrid ultrasound MRI for improved cardiac imaging and real time respiration control. Magn Reson Med. 63(2), 290&#x2013;296 (2010). doi: 10.1002/mrm.22250</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.22250</ArticleId><ArticleId IdType="pmc">PMC2813925</ArticleId><ArticleId IdType="pubmed">20025068</ArticleId></ArticleIdList></Reference><Reference><Citation>Atkinson D., et al.. Automatic compensation of motion artifacts in MRI. Magn Reson Med. 41(1), 163&#x2013;170 (1999). doi: 10.1002/(sici)1522-2594(199901)41:1&amp;lt;163::aid-mrm23&amp;gt;3.0.co;2-9</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/(sici)1522-2594(199901)41:1&amp;lt;163::aid-mrm23&amp;gt;3.0.co;2-9</ArticleId><ArticleId IdType="pubmed">10025625</ArticleId></ArticleIdList></Reference><Reference><Citation>McGee K.P., et al.. Autocorrection of three-dimensional time-of-flight MR angiography of the Circle of Willis. AJR Am J Roentgenol. 176(2), 513&#x2013;518 (2001). doi: 10.2214/ajr.176.2.1760513</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/ajr.176.2.1760513</ArticleId><ArticleId IdType="pubmed">11159106</ArticleId></ArticleIdList></Reference><Reference><Citation>Loktyushin A., Nickisch H., Pohmann R. &amp; Scholkopf B. Blind retrospective motion correction of MR images. Magn Reson Med. 70(6), 1608&#x2013;1618 (2013). doi: 10.1002/mrm.24615</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.24615</ArticleId><ArticleId IdType="pubmed">23401078</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36602952</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>09</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic-eCollection"><Journal><ISSN IssnType="Electronic">1553-7358</ISSN><JournalIssue CitedMedium="Internet"><Volume>19</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month></PubDate></JournalIssue><Title>PLoS computational biology</Title><ISOAbbreviation>PLoS Comput Biol</ISOAbbreviation></Journal><ArticleTitle>Ten quick tips for computational analysis of medical images.</ArticleTitle><Pagination><StartPage>e1010778</StartPage><MedlinePgn>e1010778</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">e1010778</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1371/journal.pcbi.1010778</ELocationID><Abstract><AbstractText>Medical imaging is a great asset for modern medicine, since it allows physicians to spatially interrogate a disease site, resulting in precise intervention for diagnosis and treatment, and to observe particular aspect of patients' conditions that otherwise would not be noticeable. Computational analysis of medical images, moreover, can allow the discovery of disease patterns and correlations among cohorts of patients with the same disease, thus suggesting common causes or providing useful information for better therapies and cures. Machine learning and deep learning applied to medical images, in particular, have produced new, unprecedented results that can pave the way to advanced frontiers of medical discoveries. While computational analysis of medical images has become easier, however, the possibility to make mistakes or generate inflated or misleading results has become easier, too, hindering reproducibility and deployment. In this article, we provide ten quick tips to perform computational analysis of medical images avoiding common mistakes and pitfalls that we noticed in multiple studies in the past. We believe our ten guidelines, if taken into practice, can help the computational-medical imaging community to perform better scientific research that eventually can have a positive impact on the lives of patients worldwide.</AbstractText><CopyrightInformation>Copyright: &#xa9; 2023 Chicco, Shiradkar. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Chicco</LastName><ForeName>Davide</ForeName><Initials>D</Initials><Identifier Source="ORCID">0000-0001-9655-7142</Identifier><AffiliationInfo><Affiliation>Institute of Health Policy Management and Evaluation, University of Toronto, Toronto, Ontario, Canada.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Shiradkar</LastName><ForeName>Rakesh</ForeName><Initials>R</Initials><AffiliationInfo><Affiliation>Department of Biomedical Engineering, Emory University, Atlanta, Georgia, United States of America.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>05</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>PLoS Comput Biol</MedlineTA><NlmUniqueID>101238922</NlmUniqueID><ISSNLinking>1553-734X</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D015203" MajorTopicYN="N">Reproducibility of Results</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000069550" MajorTopicYN="Y">Machine Learning</DescriptorName></MeshHeading></MeshHeadingList><CoiStatement>The authors have declared that no competing interests exist.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>5</Day><Hour>13</Hour><Minute>33</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>10</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36602952</ArticleId><ArticleId IdType="pmc">PMC9815662</ArticleId><ArticleId IdType="doi">10.1371/journal.pcbi.1010778</ArticleId><ArticleId IdType="pii">PCOMPBIOL-D-22-01299</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Al-Galal SAY, Alshaikhli IFT, Abdulrazzaq M. MRI brain tumor medical images analysis using deep learning techniques: a systematic review. Health and Technology. 2021;11(2):267&#x2013;282.</Citation></Reference><Reference><Citation>Maksoud EAA, Barakat S, Elmogy M. Medical images analysis based on multilabel classification. Machine Learning in Bio-Signal Analysis and Diagnostic Imaging. Elsevier; 2019. p. 209&#x2013;245.</Citation></Reference><Reference><Citation>Farouk R, Aziz MA, Habib S. Medical images analysis based on fractal dimension and wavelet transform. Journal of Computer Science Approaches. 2016;2(1).</Citation></Reference><Reference><Citation>Domingos P. A few useful things to know about machine learning. Communications of the ACM. 2012;55(10):78&#x2013;87.</Citation></Reference><Reference><Citation>Chicco D. Ten quick tips for machine learning in computational biology. BioData Mining. 2017;10(1):1&#x2013;17.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5721660</ArticleId><ArticleId IdType="pubmed">29234465</ArticleId></ArticleIdList></Reference><Reference><Citation>Jones DT. Setting the standards for machine learning in biology. Nature Reviews Molecular Cell Biology. 2019;20(11):659&#x2013;660. doi: 10.1038/s41580-019-0176-5</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41580-019-0176-5</ArticleId><ArticleId IdType="pubmed">31548714</ArticleId></ArticleIdList></Reference><Reference><Citation>Walsh I, Fishman D, Garcia-Gasulla D, Titma T, Pollastri G, Harrow J, et al.. DOME: Recommendations for supervised machine learning validation in biology. Nature Methods. 2021;18(10):1122&#x2013;1127. doi: 10.1038/s41592-021-01205-4</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41592-021-01205-4</ArticleId><ArticleId IdType="pubmed">34316068</ArticleId></ArticleIdList></Reference><Reference><Citation>Whalen S, Schreiber J, Noble WS, Pollard KS. Navigating the pitfalls of applying machine learning in genomics. Nature Reviews Genetics. 2021;23:169&#x2013;181. doi: 10.1038/s41576-021-00434-9</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41576-021-00434-9</ArticleId><ArticleId IdType="pubmed">34837041</ArticleId></ArticleIdList></Reference><Reference><Citation>Lee BD, Gitter A, Greene CS, Raschka S, Maguire F, Titus AJ, et al.. Ten quick tips for deep learning in biology. PLoS Computational Biology. 2022;18(3):e1009803. doi: 10.1371/journal.pcbi.1009803</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pcbi.1009803</ArticleId><ArticleId IdType="pmc">PMC8946751</ArticleId><ArticleId IdType="pubmed">35324884</ArticleId></ArticleIdList></Reference><Reference><Citation>Cho SM, Austin PC, Ross HJ, Abdel-Qadir H, Chicco D, Tomlinson G, et al.. Machine learning compared with conventional statistical models for predicting myocardial infarction readmission and mortality: a systematic review. Canadian Journal of Cardiology. 2021;37(8):1207&#x2013;1214. doi: 10.1016/j.cjca.2021.02.020</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cjca.2021.02.020</ArticleId><ArticleId IdType="pubmed">33677098</ArticleId></ArticleIdList></Reference><Reference><Citation>Cabitza F, Campagner A. The need to separate the wheat from the chaff in medical informatics: introducing a comprehensive checklist for the (self)-assessment of medical AI studies. International Journal of Medical Informatics. 2021;153:104510. doi: 10.1016/j.ijmedinf.2021.104510</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ijmedinf.2021.104510</ArticleId><ArticleId IdType="pubmed">34108105</ArticleId></ArticleIdList></Reference><Reference><Citation>Chicco D, Jurman G. The ABC recommendations for validation of supervised machine learning results in biomedical sciences. Frontiers in Big Data. 2022;5(979465):1&#x2013;5. doi: 10.3389/fdata.2022.979465</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fdata.2022.979465</ArticleId><ArticleId IdType="pmc">PMC9552836</ArticleId><ArticleId IdType="pubmed">36238654</ArticleId></ArticleIdList></Reference><Reference><Citation>Makin TR, de Xivry JJO. Science forum: ten common statistical mistakes to watch out for when writing or reviewing a manuscript. eLife. 2019;8:e48175.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6785265</ArticleId><ArticleId IdType="pubmed">31596231</ArticleId></ArticleIdList></Reference><Reference><Citation>Benjamin DJ, Berger JO, Johannesson M, Nosek BA, Wagenmakers EJ, Berk R, et al.. Redefine statistical significance. Nature Human Behaviour. 2018;2(1):6&#x2013;10. doi: 10.1038/s41562-017-0189-z</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41562-017-0189-z</ArticleId><ArticleId IdType="pubmed">30980045</ArticleId></ArticleIdList></Reference><Reference><Citation>Mubeen S, Tom Kodamullil A, Hofmann-Apitius M, Domingo-Fern&#xe1;ndez D. On the influence of several factors on pathway enrichment analysis. Briefings in Bioinformatics. 2022;23(3):bbac143. doi: 10.1093/bib/bbac143</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/bib/bbac143</ArticleId><ArticleId IdType="pmc">PMC9116215</ArticleId><ArticleId IdType="pubmed">35453140</ArticleId></ArticleIdList></Reference><Reference><Citation>Wieder C, Frainay C, Poupin N, et al.. Pathway analysis in metabolomics: recommendations for the use of over-representation analysis. PLoS Computational Biology. 2021;17(9):e1009105. doi: 10.1371/journal.pcbi.1009105</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pcbi.1009105</ArticleId><ArticleId IdType="pmc">PMC8448349</ArticleId><ArticleId IdType="pubmed">34492007</ArticleId></ArticleIdList></Reference><Reference><Citation>Chicco D, Agapito G. Nine quick tips for pathway enrichment analysis. PLoS Computational Biology. 2022;18(8):1010348. doi: 10.1371/journal.pcbi.1010348</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pcbi.1010348</ArticleId><ArticleId IdType="pmc">PMC9371296</ArticleId><ArticleId IdType="pubmed">35951505</ArticleId></ArticleIdList></Reference><Reference><Citation>Jin W, Li X, Fatehi M, Hamarneh G. Guidelines and evaluation for clinical explainable AI on medical image analysis. arXiv:220210553 [Preprint]. 2022.</Citation><ArticleIdList><ArticleId IdType="pubmed">36516555</ArticleId></ArticleIdList></Reference><Reference><Citation>Varoquaux G, Cheplygina V. Machine learning for medical imaging: methodological failures and recommendations for the future. npj Digital Medicine. 2022;5(1):1&#x2013;8.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC9005663</ArticleId><ArticleId IdType="pubmed">35413988</ArticleId></ArticleIdList></Reference><Reference><Citation>Block KT. Subtle pitfalls in the search for faster medical imaging. Proceedings of the National Academy of Sciences. 2022;119(17):e2203040119. doi: 10.1073/pnas.2203040119</Citation><ArticleIdList><ArticleId IdType="doi">10.1073/pnas.2203040119</ArticleId><ArticleId IdType="pmc">PMC9170040</ArticleId><ArticleId IdType="pubmed">35452309</ArticleId></ArticleIdList></Reference><Reference><Citation>Guillermo M, Pengo T, Sanders MA. Imaging methods are vastly underreported in biomedical research. eLife. 2020;9:e55133. doi: 10.7554/eLife.55133</Citation><ArticleIdList><ArticleId IdType="doi">10.7554/eLife.55133</ArticleId><ArticleId IdType="pmc">PMC7434332</ArticleId><ArticleId IdType="pubmed">32780019</ArticleId></ArticleIdList></Reference><Reference><Citation>Van Vliet M. Seven quick tips for analysis scripts in neuroimaging. PLoS Computational Biology. 2020;16(3):e1007358. doi: 10.1371/journal.pcbi.1007358</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pcbi.1007358</ArticleId><ArticleId IdType="pmc">PMC7098566</ArticleId><ArticleId IdType="pubmed">32214316</ArticleId></ArticleIdList></Reference><Reference><Citation>Chicco D, Jurman G. Arterial disease computational prediction and health record feature ranking among patients diagnosed with inflammatory bowel disease. IEEE Access. 2021;9:78648&#x2013;78657.</Citation></Reference><Reference><Citation>Le Gall G, Kirchgesner J, Bejaoui M, Landman C, Nion-Larmurier I, Bourrier A, et al.. Clinical activity is an independent risk factor of ischemic heart and cerebrovascular arterial disease in patients with inflammatory bowel disease. PLoS ONE. 2018;13(8):e0201991. doi: 10.1371/journal.pone.0201991</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0201991</ArticleId><ArticleId IdType="pmc">PMC6118365</ArticleId><ArticleId IdType="pubmed">30169521</ArticleId></ArticleIdList></Reference><Reference><Citation>Chicco D, Oneto L. An enhanced Random Forests approach to predict heart failure from small imbalanced gene expression data. IEEE/ACM Transactions on Computational Biology and Bioinformatics. 2020;18(6):2759&#x2013;2765.</Citation><ArticleIdList><ArticleId IdType="pubmed">33259306</ArticleId></ArticleIdList></Reference><Reference><Citation>Maciejak A, Kiliszek M, Michalak M, Tulacz D, Opolski G, Matlak K, et al.. Gene expression profiling reveals potential prognostic biomarkers associated with the progression of heart failure. Genome Medicine. 2015;7(1):1&#x2013;15.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4432772</ArticleId><ArticleId IdType="pubmed">25984239</ArticleId></ArticleIdList></Reference><Reference><Citation>Clark K, Vendt B, Smith K, Freymann J, Kirby J, Koppel P, et al.. The Cancer Imaging Archive (TCIA): maintaining and operating a public information repository. Journal of Digital Imaging. 2013;26(6):1045&#x2013;1057. doi: 10.1007/s10278-013-9622-7</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10278-013-9622-7</ArticleId><ArticleId IdType="pmc">PMC3824915</ArticleId><ArticleId IdType="pubmed">23884657</ArticleId></ArticleIdList></Reference><Reference><Citation>OASIS. Open Access Series of Imaging Studies; 2022. Available from: http://www.oasis-brains.org/ [cited 2022 Aug 2].</Citation></Reference><Reference><Citation>Marcus DS, Wang TH, Parker J, Csernansky JG, Morris JC, Buckner RL. Open Access Series of Imaging Studies (OASIS): cross-sectional MRI data in young, middle aged, nondemented, and demented older adults. Journal of Cognitive Neuroscience. 2007;19(9):1498&#x2013;1507. doi: 10.1162/jocn.2007.19.9.1498</Citation><ArticleIdList><ArticleId IdType="doi">10.1162/jocn.2007.19.9.1498</ArticleId><ArticleId IdType="pubmed">17714011</ArticleId></ArticleIdList></Reference><Reference><Citation>Luecken MD, Theis FJ. Current best practices in single-cell RNA-seq analysis: a tutorial. Molecular Systems Biology. 2019;15(6):e8746. doi: 10.15252/msb.20188746</Citation><ArticleIdList><ArticleId IdType="doi">10.15252/msb.20188746</ArticleId><ArticleId IdType="pmc">PMC6582955</ArticleId><ArticleId IdType="pubmed">31217225</ArticleId></ArticleIdList></Reference><Reference><Citation>Kikinis R, Warfield S, Westin CF. High performance computing (HPC) in medical image analysis (MIA) at the surgical planning laboratory (SPL). Proceedings of Supercomputing ASIA 2023 &#x2013;the 3rd High Performance Computing Asia Conference &amp; Exhibition. Citeseer; 1998. p. 1&#x2013;15.</Citation></Reference><Reference><Citation>Gulo CA, Sementille AC, Tavares JMR. Techniques of medical image processing and analysis accelerated by high-performance computing: a systematic literature review. Journal of Real-Time Image Processing. 2019;16(6):1891&#x2013;1908.</Citation></Reference><Reference><Citation>Gu J, Wang Z, Kuen J, Ma L, Shahroudy A, Shuai B, et al.. Recent advances in convolutional neural networks. Pattern Recognition. 2018;77:354&#x2013;377.</Citation></Reference><Reference><Citation>MacFarland TW, Yates JM. Mann-Whitney U test. Introduction to nonparametric statistics for the biological sciences using R. Springer; 2016. p. 103&#x2013;132.</Citation></Reference><Reference><Citation>Alnasir JJ. Fifteen quick tips for success with HPC, ie, responsibly BASHing that Linux cluster. PLoS Computational Biology. 2021;17(8):e1009207. doi: 10.1371/journal.pcbi.1009207</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pcbi.1009207</ArticleId><ArticleId IdType="pmc">PMC8341507</ArticleId><ArticleId IdType="pubmed">34351904</ArticleId></ArticleIdList></Reference><Reference><Citation>Bizzego A, Bussola N, Chierici M, Maggio V, Francescatto M, Cima L, et al.. Evaluating reproducibility of AI algorithms in digital pathology with DAPPER. PLoS Computational Biology. 2019;15(3):e1006269. doi: 10.1371/journal.pcbi.1006269</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pcbi.1006269</ArticleId><ArticleId IdType="pmc">PMC6467397</ArticleId><ArticleId IdType="pubmed">30917113</ArticleId></ArticleIdList></Reference><Reference><Citation>Balki I, Amirabadi A, Levman J, Martel AL, Emersic Z, Meden B, et al.. Sample-size determination methodologies for machine learning in medical imaging research: a systematic review. Canadian Association of Radiologists Journal. 2019;70(4):344&#x2013;353. doi: 10.1016/j.carj.2019.06.002</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.carj.2019.06.002</ArticleId><ArticleId IdType="pubmed">31522841</ArticleId></ArticleIdList></Reference><Reference><Citation>Beam AL, Manrai AK, Ghassemi M. Challenges to the reproducibility of machine learning models in health care. JAMA. 2020;323(4):305&#x2013;306. doi: 10.1001/jama.2019.20866</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jama.2019.20866</ArticleId><ArticleId IdType="pmc">PMC7335677</ArticleId><ArticleId IdType="pubmed">31904799</ArticleId></ArticleIdList></Reference><Reference><Citation>Schoonjans F, Zalata A, Depuydt C, Comhaire F. MedCalc: a new computer program for medical statistics. Computer Methods and Programs in Biomedicine. 1995;48(3):257&#x2013;262. doi: 10.1016/0169-2607(95)01703-8</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/0169-2607(95)01703-8</ArticleId><ArticleId IdType="pubmed">8925653</ArticleId></ArticleIdList></Reference><Reference><Citation>PASS. Sample Size &amp; Power; 2022. Available from: https://www.ncss.com/software/pass/ [cited 2022 Aug 24].</Citation></Reference><Reference><Citation>Baldassaro M. sampler R package; 2021. Available from: https://cran.r-project.org/web/packages/sampler/https://cran.r-project.org/web/packages/sampler/ [cited 2022 Aug 24].</Citation></Reference><Reference><Citation>Champely S, Ekstrom C, Dalgaard P, Gill J, Weibelzahl S, Anandkumar A, et al.. pwr R package;
2018. Available from: https://cran.r-project.org/web/packages/pwr/ [cited 2022 Aug 24].</Citation></Reference><Reference><Citation>Field A. Discovering statistics using IBM SPSS statistics. SAGE; 2013.</Citation></Reference><Reference><Citation>He H, Garcia EA. Learning from imbalanced data. IEEE Transactions on Knowledge and Data Engineering. 2009;21(9):1263&#x2013;1284.</Citation></Reference><Reference><Citation>Anand A, Pugalenthi G, Fogel GB, Suganthan P. An approach for classification of highly imbalanced data using weighting and undersampling. Amino Acids. 2010;39(5):1385&#x2013;1391. doi: 10.1007/s00726-010-0595-2</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00726-010-0595-2</ArticleId><ArticleId IdType="pubmed">20411285</ArticleId></ArticleIdList></Reference><Reference><Citation>Gosain A, Sardana S. Handling class imbalance problem using oversampling techniques: a review. In: Proceedings of ICACCI 2017 &#x2013;the 2017 International Conference on Advances in Computing, Communications and Informatics. IEEE; 2017. p. 79&#x2013;85.</Citation></Reference><Reference><Citation>Hussain Z, Gimenez F, Yi D, Rubin D. Differential data augmentation techniques for medical imaging classification tasks. AMIA Annual Symposium Proceedings. vol. 2017. American Medical Informatics Association; 2017. p. 979.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5977656</ArticleId><ArticleId IdType="pubmed">29854165</ArticleId></ArticleIdList></Reference><Reference><Citation>Goel N, Yadav A, Singh BM. Medical image processing: a review. Proceedings of CIPECH 2016 &#x2013;the 2nd International Innovative Applications of Computational Intelligence on Power, Energy and Controls with their Impact on Humanity. 2016. p. 57&#x2013;62.</Citation></Reference><Reference><Citation>Lee D, Choi S, Kim HJ. Performance evaluation of image denoising developed using convolutional denoising autoencoders in chest radiography. Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment. 2018;884:97&#x2013;104.</Citation></Reference><Reference><Citation>Mredhula L, Dorairangasamy M. An extensive review of significant researches on medical image denoising techniques. International Journal of Computer Applications. 2013;64(14).</Citation></Reference><Reference><Citation>Sun Y, Liu X, Cong P, Li L, Zhao Z. Digital radiography image denoising using a generative adversarial network. Journal of X-ray Science and Technology. 2018;26(4):523&#x2013;534. doi: 10.3233/XST-17356</Citation><ArticleIdList><ArticleId IdType="doi">10.3233/XST-17356</ArticleId><ArticleId IdType="pmc">PMC6130336</ArticleId><ArticleId IdType="pubmed">29889095</ArticleId></ArticleIdList></Reference><Reference><Citation>Mohammadi S, Leventouri T. A study of wavelet-based denoising and a new shrinkage function for low-dose CT scans. Biomedical Physics &amp; Engineering Express. 2019;5(3):035018.</Citation></Reference><Reference><Citation>Diwakar M, Kumar M. A review on CT image noise and its denoising. Biomedical Signal Processing and Control. 2018;42:73&#x2013;88.</Citation></Reference><Reference><Citation>Gajera B, Kapil SR, Ziaei D, Mangalagiri J, Siegel E, Chapman D. CT-scan denoising using a charbonnier loss generative adversarial network. IEEE Access. 2021;9:84093&#x2013;84109.</Citation></Reference><Reference><Citation>Heunis S, Lamerichs R, Zinger S, Caballero-Gaudes C, Jansen JF, Aldenkamp B, et al.. Quality and denoising in real-time functional magnetic resonance imaging neurofeedback: a methods review. Human Brain Mapping. 2020;41(12):3439&#x2013;3467. doi: 10.1002/hbm.25010</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/hbm.25010</ArticleId><ArticleId IdType="pmc">PMC7375116</ArticleId><ArticleId IdType="pubmed">32333624</ArticleId></ArticleIdList></Reference><Reference><Citation>Bhujle HV, Vadavadagi BH. NLM based magnetic resonance image denoising&#x2013;A review. Biomedical Signal Processing and Control. 2019;47:252&#x2013;261.</Citation></Reference><Reference><Citation>Mohan J, Krishnaveni V, Guo Y. A survey on the magnetic resonance image denoising methods. Biomedical Signal Processing and Control. 2014;9:56&#x2013;69.</Citation></Reference><Reference><Citation>Ragesh N, Anil A, Rajesh R. Digital image denoising in medical ultrasound images: a survey. Proceedings of AIML-11 &#x2013;the ICGST International Conference on Artificial Intelligence and Machine Learning. vol. 12; 2011. p. 14.</Citation></Reference><Reference><Citation>Sagheer SVM, George SN. A review on medical image denoising algorithms. Biomedical Signal Processing and Control. 2020;61:102036.</Citation></Reference><Reference><Citation>Gong K, Guan J, Liu CC, Qi J. PET image denoising using a deep neural network through fine tuning. IEEE Transactions on Radiation and Plasma Medical Sciences. 2018;3(2):153&#x2013;161. doi: 10.1109/TRPMS.2018.2877644</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TRPMS.2018.2877644</ArticleId><ArticleId IdType="pmc">PMC7402614</ArticleId><ArticleId IdType="pubmed">32754674</ArticleId></ArticleIdList></Reference><Reference><Citation>Li XT, Huang RY. Standardization of imaging methods for machine learning in neuro-oncology. Neuro-Oncology. Advances. 2020;2(Supplement 4):iv49&#x2013;iv55. doi: 10.1093/noajnl/vdaa054</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/noajnl/vdaa054</ArticleId><ArticleId IdType="pmc">PMC7829470</ArticleId><ArticleId IdType="pubmed">33521640</ArticleId></ArticleIdList></Reference><Reference><Citation>Papadimitroulas P, Brocki L, Chung NC, Marchadour W, Vermet F, Gaubert L, et al.. Artificial intelligence: deep learning in oncological radiomics and challenges of interpretability and data harmonization. Physica Medica. 2021;83:108&#x2013;121. doi: 10.1016/j.ejmp.2021.03.009</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejmp.2021.03.009</ArticleId><ArticleId IdType="pubmed">33765601</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhu AH, Moyer DC, Nir TM, Thompson PM, Jahanshad N. Challenges and opportunities in dMRI data harmonization. In: Proceedings of MICCAI 2019 &#x2013;the 22nd International Conference on Medical Image Computing and Computer-Assisted Intervention, Computational Diffusion MRI Workshop. Springer; 2019. p. 157&#x2013;172.</Citation></Reference><Reference><Citation>Sadri AR, Janowczyk A, Zhou R, Verma R, Beig N, Antunes J, et al.. MRQy&#x2014;An open-source tool for quality control of MR imaging data. Medical Physics. 2020;47(12):6029&#x2013;6038.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8176950</ArticleId><ArticleId IdType="pubmed">33176026</ArticleId></ArticleIdList></Reference><Reference><Citation>Vogelbacher C, Bopp MH, Schuster V, Herholz P, Jansen A, Sommer J. LAB&#x2013;QA2GO: a free, easy-to-use toolbox for the quality assessment of magnetic resonance imaging data. Frontiers in Neuroscience. 2019;13:688. doi: 10.3389/fnins.2019.00688</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fnins.2019.00688</ArticleId><ArticleId IdType="pmc">PMC6617644</ArticleId><ArticleId IdType="pubmed">31333406</ArticleId></ArticleIdList></Reference><Reference><Citation>Ny&#xfa;l LG, Udupa JK. On standardizing the MR image intensity scale. Magnetic Resonance in Medicine. 1999;42(6):1072&#x2013;1081. doi: 10.1002/(sici)1522-2594(199912)42:6&amp;lt;1072::aid-mrm11&amp;gt;3.0.co;2-m</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/(sici)1522-2594(199912)42:6&amp;lt;1072::aid-mrm11&amp;gt;3.0.co;2-m</ArticleId><ArticleId IdType="pubmed">10571928</ArticleId></ArticleIdList></Reference><Reference><Citation>Bashyam VM, Doshi J, Erus G, Srinivasan D, Abdulkadir A, Singh A, et al.. Deep generative medical image harmonization for improving cross-site generalization in deep learning predictors. Journal of Magnetic Resonance Imaging. 2021;55(3):908&#x2013;916. doi: 10.1002/jmri.27908</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmri.27908</ArticleId><ArticleId IdType="pmc">PMC8844038</ArticleId><ArticleId IdType="pubmed">34564904</ArticleId></ArticleIdList></Reference><Reference><Citation>Shiradkar R, Ghose S, Mahran A, Li L, Hubbard I, Fu P, et al.. Prostate surface distension and tumor texture descriptors from pre-treatment MRI are associated with biochemical recurrence following radical prostatectomy: preliminary findings. Frontiers in Oncology. 2022. p. 2055. doi: 10.3389/fonc.2022.841801</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fonc.2022.841801</ArticleId><ArticleId IdType="pmc">PMC9163353</ArticleId><ArticleId IdType="pubmed">35669420</ArticleId></ArticleIdList></Reference><Reference><Citation>Cadwallader L, Mac Gabhann F, Papin J, Pitzer VE. Advancing code sharing in the computational biology community. PLoS Computational Biology. 2022;18(6):e1010193. doi: 10.1371/journal.pcbi.1010193</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pcbi.1010193</ArticleId><ArticleId IdType="pmc">PMC9162310</ArticleId><ArticleId IdType="pubmed">35653366</ArticleId></ArticleIdList></Reference><Reference><Citation>TIOBE. TIOBE Index for July 2022; 2022. https://www.tiobe.com/tiobe-index/ URL visited on 2nd August 2022.</Citation></Reference><Reference><Citation>Pang B, Nijkamp E, Wu YN. Deep learning with TensorFlow: a review. Journal of Educational and Behavioral Statistics. 2020;45(2):227&#x2013;248.</Citation></Reference><Reference><Citation>Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, et al.. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing systems. 2019;32.</Citation></Reference><Reference><Citation>Ioannidis JP. Why most published research findings are false. PLOS Medicine. 2005;2(8):e124. doi: 10.1371/journal.pmed.0020124</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pmed.0020124</ArticleId><ArticleId IdType="pmc">PMC1182327</ArticleId><ArticleId IdType="pubmed">16060722</ArticleId></ArticleIdList></Reference><Reference><Citation>Li L, Pahwa S, Penzias G, Rusu M, Gollamudi J, Viswanath S, et al.. Co-registration of ex vivo surgical histopathology and in vivo T2 weighted MRI of the prostate via multi-scale spectral embedding representation. Scientific Reports. 2017;7(1):1&#x2013;12.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5562695</ArticleId><ArticleId IdType="pubmed">28821786</ArticleId></ArticleIdList></Reference><Reference><Citation>Wu H, Li X, Cheng KT. Exploring feature representation learning for semi-supervised medical image segmentation. ArXiv. 2021;arXiv:2111.10989:1&#x2013;10.</Citation></Reference><Reference><Citation>Van Griethuysen JJ, Fedorov A, Parmar C, Hosny A, Aucoin N, Narayan V, et al.. Computational radiomics system to decode the radiographic phenotype. Cancer Research. 2017;77(21):e104&#x2013;e107. doi: 10.1158/0008-5472.CAN-17-0339</Citation><ArticleIdList><ArticleId IdType="doi">10.1158/0008-5472.CAN-17-0339</ArticleId><ArticleId IdType="pmc">PMC5672828</ArticleId><ArticleId IdType="pubmed">29092951</ArticleId></ArticleIdList></Reference><Reference><Citation>Rong Q, Thangaraj C, Easwaramoorthy D, He S. Multifractal based image processing for estimating the complexity of COVID-19 dynamics. The European Physical Journal Special Topics. 2021;230(21):3947&#x2013;3954. doi: 10.1140/epjs/s11734-021-00336-1</Citation><ArticleIdList><ArticleId IdType="doi">10.1140/epjs/s11734-021-00336-1</ArticleId><ArticleId IdType="pmc">PMC8601099</ArticleId><ArticleId IdType="pubmed">34815830</ArticleId></ArticleIdList></Reference><Reference><Citation>Alilou M, Prasanna P, Bera K, Gupta A, Rajiah P, Yang M, et al.. A novel nodule edge sharpness radiomic biomarker improves performance of lung-RADS for distinguishing adenocarcinomas from granulomas on non-contrast CT scans. Cancers. 2021;13(11):2781. doi: 10.3390/cancers13112781</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/cancers13112781</ArticleId><ArticleId IdType="pmc">PMC8199879</ArticleId><ArticleId IdType="pubmed">34205005</ArticleId></ArticleIdList></Reference><Reference><Citation>Lai Z, Deng H. Medical image classification based on deep features extracted by deep model and statistic feature fusion with multilayer perceptronn. Computational Intelligence and Neuroscience. 2018;2018:1&#x2013;13.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6157177</ArticleId><ArticleId IdType="pubmed">30298088</ArticleId></ArticleIdList></Reference><Reference><Citation>Olabarriaga SD, Smeulders AW. Interaction in the segmentation of medical images: a survey. Medical Image Analysis. 2001;5(2):127&#x2013;142. doi: 10.1016/s1361-8415(00)00041-4</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/s1361-8415(00)00041-4</ArticleId><ArticleId IdType="pubmed">11516707</ArticleId></ArticleIdList></Reference><Reference><Citation>Oliveira FP, Tavares JMR. Medical image registration: a review. Computer Methods in Biomechanics and Biomedical Engineering. 2014;17(2):73&#x2013;93. doi: 10.1080/10255842.2012.670855</Citation><ArticleIdList><ArticleId IdType="doi">10.1080/10255842.2012.670855</ArticleId><ArticleId IdType="pubmed">22435355</ArticleId></ArticleIdList></Reference><Reference><Citation>Mwangi B, Tian TS, Soares JC. A review of feature reduction techniques in neuroimaging. Neuroinformatics. 2014;12(2):229&#x2013;244. doi: 10.1007/s12021-013-9204-3</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s12021-013-9204-3</ArticleId><ArticleId IdType="pmc">PMC4040248</ArticleId><ArticleId IdType="pubmed">24013948</ArticleId></ArticleIdList></Reference><Reference><Citation>Debie E, Shafi K. Implications of the curse of dimensionality for supervised learning classifier systems: theoretical and empirical analyses. Pattern Analysis and Applications. 2019;22(2):519&#x2013;536.</Citation></Reference><Reference><Citation>Radovic M, Ghalwash M, Filipovic N, Obradovic Z. Minimum redundancy maximum relevance feature selection approach for temporal gene expression data. BMC Bioinformatics. 2017;18(1):1&#x2013;14.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5209828</ArticleId><ArticleId IdType="pubmed">28049413</ArticleId></ArticleIdList></Reference><Reference><Citation>Ginsburg SB, Lee G, Ali S, Madabhushi A. Feature importance in nonlinear embeddings (FINE): applications in digital pathology. IEEE Transactions on Medical Imaging. 2015;35(1):76&#x2013;88. doi: 10.1109/TMI.2015.2456188</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2015.2456188</ArticleId><ArticleId IdType="pubmed">26186772</ArticleId></ArticleIdList></Reference><Reference><Citation>Nguyen LH, Holmes S. Ten quick tips for effective dimensionality reduction. PLoS Computational Biology. 2019;15(6):e1006907. doi: 10.1371/journal.pcbi.1006907</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pcbi.1006907</ArticleId><ArticleId IdType="pmc">PMC6586259</ArticleId><ArticleId IdType="pubmed">31220072</ArticleId></ArticleIdList></Reference><Reference><Citation>Chicco D, Masseroli M. Software suite for gene and protein annotation prediction and similarity search. IEEE/ACM Transactions on Computational Biology and Bioinformatics. 2014;12(4):837&#x2013;843.</Citation><ArticleIdList><ArticleId IdType="pubmed">26357324</ArticleId></ArticleIdList></Reference><Reference><Citation>Reddy GT, Reddy MPK, Lakshmanna K, Kaluri R, Rajput DS, Srivastava G, et al.. Analysis of dimensionality reduction techniques on big data. IEEE Access. 2020;8:54776&#x2013;54788.</Citation></Reference><Reference><Citation>McInnes L, Healy J, Melville J. UMAP: Uniform manifold approximation and projection for dimension reduction. arXiv. 2018;arXiv:1802.03426:1&#x2013;63.</Citation></Reference><Reference><Citation>Belkina AC, Ciccolella CO, Anno R, Halpert R, Spidlen J, Snyder-Cappione JE. Automated optimized parameters for T-distributed stochastic neighbor embedding improve visualization and analysis of large datasets. Nature Communications. 2019;10(1):1&#x2013;12.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6882880</ArticleId><ArticleId IdType="pubmed">31780669</ArticleId></ArticleIdList></Reference><Reference><Citation>Poldrack RA, Barch DM, Mitchell JP, Wager TD, Wagner AD, Devlin JT, et al.. Toward open sharing of task-based fMRI data: the OpenfMRI project. Frontiers in Neuroinformatics. 2013;7:12. doi: 10.3389/fninf.2013.00012</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fninf.2013.00012</ArticleId><ArticleId IdType="pmc">PMC3703526</ArticleId><ArticleId IdType="pubmed">23847528</ArticleId></ArticleIdList></Reference><Reference><Citation>Poldrack RA, Gorgolewski KJ. OpenfMRI: Open sharing of task fMRI data. Neuroimage. 2017;144:259&#x2013;261. doi: 10.1016/j.neuroimage.2015.05.073</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.neuroimage.2015.05.073</ArticleId><ArticleId IdType="pmc">PMC4669234</ArticleId><ArticleId IdType="pubmed">26048618</ArticleId></ArticleIdList></Reference><Reference><Citation>Re3data. Registry of research data repositories; 2022. Available from: https://www.re3data.org/ [cited 2022 Jun 24].</Citation></Reference><Reference><Citation>Google. Google Dataset Search; 2022. Available from: https://datasetsearch.research.google.com/ [cited 2022 Jul 29].</Citation></Reference><Reference><Citation>Kaggle. Kaggle datasets&#x2013;Explore, analyze, and share quality data; 2022. Available from: https://www.kaggle.com/datasets [cited 2022 Jun 24].</Citation></Reference><Reference><Citation>University of California Irvine. Machine Learning Repository; 1987. Available from: https://archive.ics.uci.edu/ml [cited 2022 Jun 24].</Citation></Reference><Reference><Citation>Matthews BW. Comparison of the predicted and observed secondary structure of T4 phage lysozyme. Biochimica et Biophysica Acta (BBA)-Protein. Structure. 1975;405(2):442&#x2013;451.</Citation><ArticleIdList><ArticleId IdType="pubmed">1180967</ArticleId></ArticleIdList></Reference><Reference><Citation>Jurman G, Riccadonna S, Furlanello C. A comparison of MCC and CEN error measures in multi-class prediction. PLOS ONE. 2012;7(8):e41882. doi: 10.1371/journal.pone.0041882</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0041882</ArticleId><ArticleId IdType="pmc">PMC3414515</ArticleId><ArticleId IdType="pubmed">22905111</ArticleId></ArticleIdList></Reference><Reference><Citation>Chicco D, Jurman G. The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation. BMC Genomics. 2020;21(1):6. doi: 10.1186/s12864-019-6413-7</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s12864-019-6413-7</ArticleId><ArticleId IdType="pmc">PMC6941312</ArticleId><ArticleId IdType="pubmed">31898477</ArticleId></ArticleIdList></Reference><Reference><Citation>Chicco D, T&#xf6;tsch N, Jurman G. The Matthews correlation coefficient (MCC) is more reliable than balanced accuracy, bookmaker informedness, and markedness in two-class confusion matrix evaluation. BioData Mining. 2021;14(1):1&#x2013;22.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7863449</ArticleId><ArticleId IdType="pubmed">33541410</ArticleId></ArticleIdList></Reference><Reference><Citation>Chicco D, Starovoitov V, Jurman G. The benefits of the Matthews correlation coefficient (MCC) over the diagnostic odds ratio (DOR) in binary classification assessment. IEEE Access. 2021;9:47112&#x2013;47124.</Citation></Reference><Reference><Citation>Chicco D, Warrens MJ, Jurman G. The Matthews correlation coefficient (MCC) is more informative than Cohen&#x2019;s Kappa and Brier score in binary classification assessment. IEEE Access. 2021;9:78368&#x2013;78381.</Citation></Reference><Reference><Citation>Wald NJ, Bestwick JP. Is the area under an ROC curve a valid measure of the performance of a screening or diagnostic test? Journal of Medical Screening. 2014;21(1):51&#x2013;56. doi: 10.1177/0969141313517497</Citation><ArticleIdList><ArticleId IdType="doi">10.1177/0969141313517497</ArticleId><ArticleId IdType="pubmed">24407586</ArticleId></ArticleIdList></Reference><Reference><Citation>Muschelli J. ROC and AUC with a binary predictor: a potentially misleading metric. Journal of Classification. 2020;37(3):696&#x2013;708. doi: 10.1007/s00357-019-09345-1</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00357-019-09345-1</ArticleId><ArticleId IdType="pmc">PMC7695228</ArticleId><ArticleId IdType="pubmed">33250548</ArticleId></ArticleIdList></Reference><Reference><Citation>Movahedi F, Padman R, Antaki JF. Limitations of receiver operating characteristic curve on imbalanced data: assist device mortality risk scores. Journal of Thoracic and Cardiovascular Surgery. 2021. doi: 10.1016/j.jtcvs.2021.07.041</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jtcvs.2021.07.041</ArticleId><ArticleId IdType="pmc">PMC8800945</ArticleId><ArticleId IdType="pubmed">34446286</ArticleId></ArticleIdList></Reference><Reference><Citation>Halligan S, Altman DG, Mallett S. Disadvantages of using the area under the receiver operating characteristic curve to assess imaging tests: a discussion and proposal for an alternative approach. European Radiology. 2015;25(4):932&#x2013;939. doi: 10.1007/s00330-014-3487-0</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-014-3487-0</ArticleId><ArticleId IdType="pmc">PMC4356897</ArticleId><ArticleId IdType="pubmed">25599932</ArticleId></ArticleIdList></Reference><Reference><Citation>Lobo JM, Jim&#xe9;nez-Valverde A, Real R. AUC: a misleading measure of the performance of predictive distribution models. Global Ecology and Biogeography. 2008;17(2):145&#x2013;151.</Citation></Reference><Reference><Citation>Chicco D, Warrens MJ, Jurman G. The coefficient of determination R-squared is more informative than SMAPE, MAE, MAPE, MSE and RMSE in regression analysis evaluation. PeerJ Computer Science. 2021;7:e623. doi: 10.7717/peerj-cs.623</Citation><ArticleIdList><ArticleId IdType="doi">10.7717/peerj-cs.623</ArticleId><ArticleId IdType="pmc">PMC8279135</ArticleId><ArticleId IdType="pubmed">34307865</ArticleId></ArticleIdList></Reference><Reference><Citation>Davies DL, Bouldin DW. A cluster separation measure. IEEE Transactions on Pattern Analysis and Machine Intelligence. 1979;PAMI-1(2):224&#x2013;227.</Citation><ArticleIdList><ArticleId IdType="pubmed">21868852</ArticleId></ArticleIdList></Reference><Reference><Citation>Dunn JC. Well-separated clusters and optimal fuzzy partitions. Journal of Cybernetics. 1974;4(1):95&#x2013;104.</Citation></Reference><Reference><Citation>Kaufman L, Rousseeuw PJ. Finding groups in data: an introduction to cluster analysis. John Wiley &amp; Sons; 2009.</Citation></Reference><Reference><Citation>Jafari M, Ansari-Pour N. Why, when and how to adjust your P values? Cell Journal. 2019;20(4):604. doi: 10.22074/cellj.2019.5992</Citation><ArticleIdList><ArticleId IdType="doi">10.22074/cellj.2019.5992</ArticleId><ArticleId IdType="pmc">PMC6099145</ArticleId><ArticleId IdType="pubmed">30124010</ArticleId></ArticleIdList></Reference><Reference><Citation>Taha AA, Hanbury A. Metrics for evaluating 3D medical image segmentation: analysis, selection, and tool. BMC Medical Imaging. 2015;15(1):1&#x2013;28.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4533825</ArticleId><ArticleId IdType="pubmed">26263899</ArticleId></ArticleIdList></Reference><Reference><Citation>Doran D, Schulz S, Besold TR. What does explainable AI really mean? A new conceptualization of perspectives. arXiv. 2017;arXiv:1710.00794:1&#x2013;8.</Citation></Reference><Reference><Citation>van der Velden BH, Kuijf HJ, Gilhuijs KG, Viergever MA. Explainable artificial intelligence (XAI) in deep learning-based medical image analysis. Medical Image Analysis. 2022:102470. doi: 10.1016/j.media.2022.102470</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2022.102470</ArticleId><ArticleId IdType="pubmed">35576821</ArticleId></ArticleIdList></Reference><Reference><Citation>Bourdon P, Ahmed OB, Urruty T, Djemal K, Fernandez-Maloigne C. Explainable AI for medical imaging: knowledge matters. Multi-Faceted Deep Learning. Springer; 2021. p. 267&#x2013;292.</Citation></Reference><Reference><Citation>Folke T, Yang SCH, Anderson S, Shafto P. Explainable AI for medical imaging explaining pneumothorax diagnoses with Bayesian teaching. Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III. vol. 11746. SPIE; 2021. p. 644&#x2013;664.</Citation></Reference><Reference><Citation>Jin W, Li X, Hamarneh G. Evaluating explainable AI on a multi-modal medical imaging task: can existing algorithms fulfill clinical requirements? Association for the Advancement of Artificial Intelligence Conference (AAAI); 2022. p. 1&#x2013;9.</Citation></Reference><Reference><Citation>Cabitza F, Campagner A, Malgieri G, Natali C, Schneeberger D, Stoeger K, et al.. Quod erat demonstrandum?&#x2014;Towards a typology of the concept of explanation for the design of explainable AI. Expert Systems with Applications. 2023;213:118888.</Citation></Reference><Reference><Citation>Cabitza F, Campagner A, Sconfienza LM. As if sand were stone. New concepts and metrics to probe the ground on which to build trustable AI. BMC Medical Informatics and Decision Making. 2020;20(1):1&#x2013;21.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7488864</ArticleId><ArticleId IdType="pubmed">32917183</ArticleId></ArticleIdList></Reference><Reference><Citation>FigShare. Store, share, discover research; 2011. Available from: https://www.figshare.com [cited 2022 Jul 25].</Citation></Reference><Reference><Citation>Zenodo. Zenodo: research, shared; 2013. Available from: https://www.zenodo.org [cited 2022 Jul 25].</Citation></Reference><Reference><Citation>Wilkinson MD, Dumontier M, Aalbersberg IJ, Appleton G, Axton M, Baak A, et al.. The FAIR guiding principles for scientific data management and stewardship. Scientific Data. 2016;3(1):1&#x2013;9. doi: 10.1038/sdata.2016.18</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/sdata.2016.18</ArticleId><ArticleId IdType="pmc">PMC4792175</ArticleId><ArticleId IdType="pubmed">26978244</ArticleId></ArticleIdList></Reference><Reference><Citation>Scimago Journal Ranking. Health informatics open access journals; 2022. Available from: https://www.scimagojr.com/journalrank.php?openaccess=true&amp;type=j&amp;category=2718 [cited 2022 Jun 26].</Citation></Reference><Reference><Citation>Poggio T, Fontana M. L&#x2019;occhio e il cervello (in Italian). Theoria; 1991. p. 1&#x2013;124.</Citation></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36602576</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>05</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1432-2161</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>05</Day></PubDate></JournalIssue><Title>Skeletal radiology</Title><ISOAbbreviation>Skeletal Radiol</ISOAbbreviation></Journal><ArticleTitle>Rapid lumbar MRI protocol using 3D imaging and deep learning reconstruction.</ArticleTitle><ELocationID EIdType="doi" ValidYN="Y">10.1007/s00256-022-04268-2</ELocationID><Abstract><AbstractText Label="BACKGROUND AND PURPOSE" NlmCategory="OBJECTIVE">Three-dimensional (3D) imaging of the spine, augmented with AI-enabled image enhancement and denoising, has the potential to reduce imaging times without compromising image quality or diagnostic performance. This work evaluates the time savings afforded by a novel, rapid lumbar spine MRI protocol as well as image quality and diagnostic differences stemming from the use of an AI-enhanced 3D T2 sequence combined with a single Dixon acquisition.</AbstractText><AbstractText Label="MATERIALS AND METHODS" NlmCategory="METHODS">Thirty-five subjects underwent MRI using standard 2D lumbar imaging in addition to a "rapid protocol" consisting of 3D imaging, enhanced and denoised using a prototype DL reconstruction algorithm as well as a two-point Dixon sequence. Images were graded by subspecialized radiologists and imaging times were collected. Comparison was made between 2D sagittal T1 and Dixon fat images for neural foraminal stenosis, intraosseous lesions, and fracture detection.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">This study demonstrated a 54% reduction in total acquisition time of a 3D AI-enhanced imaging lumbar spine MRI rapid protocol combined with a sagittal 2D Dixon sequence, compared to a 2D standard-of-care protocol. The rapid protocol also demonstrated strong agreement with the standard-of-care protocol with respect to osseous lesions (&#x3ba;&#x2009;=&#x2009;0.88), fracture detection (&#x3ba;&#x2009;=&#x2009;0.96), and neural foraminal stenosis (ICC&#x2009;&gt;&#x2009;0.9 at all levels).</AbstractText><AbstractText Label="CONCLUSION" NlmCategory="CONCLUSIONS">3D imaging of the lumbar spine with AI-enhanced DL reconstruction and Dixon imaging demonstrated a significant reduction in imaging time with similar performance for common diagnostic metrics. Although previously limited by long postprocessing times, this technique has the potential to enhance patient throughput in busy radiology practices while providing similar or improved image quality.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s), under exclusive licence to International Skeletal Society (ISS).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Chazen</LastName><ForeName>J Levi</ForeName><Initials>JL</Initials><Identifier Source="ORCID">0000-0002-6146-2191</Identifier><AffiliationInfo><Affiliation>Department of Radiology &amp; Imaging, Hospital for Special Surgery, 535 E 70th St, 3rd Floor, New York, NY, 10021, USA. chazenjl@hss.edu.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Tan</LastName><ForeName>Ek Tsoon</ForeName><Initials>ET</Initials><AffiliationInfo><Affiliation>Department of Radiology &amp; Imaging, Hospital for Special Surgery, 535 E 70th St, 3rd Floor, New York, NY, 10021, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Fiore</LastName><ForeName>Jake</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Department of Radiology &amp; Imaging, Hospital for Special Surgery, 535 E 70th St, 3rd Floor, New York, NY, 10021, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Nguyen</LastName><ForeName>Joseph T</ForeName><Initials>JT</Initials><AffiliationInfo><Affiliation>Biostatistics Core, Hospital for Special Surgery, New York, NY, 10021, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Sun</LastName><ForeName>Simon</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Department of Radiology, Memorial Sloan Kettering Cancer Center, New York, NY, 10065, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Sneag</LastName><ForeName>Darryl B</ForeName><Initials>DB</Initials><AffiliationInfo><Affiliation>Department of Radiology &amp; Imaging, Hospital for Special Surgery, 535 E 70th St, 3rd Floor, New York, NY, 10021, USA.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>05</Day></ArticleDate></Article><MedlineJournalInfo><Country>Germany</Country><MedlineTA>Skeletal Radiol</MedlineTA><NlmUniqueID>7701953</NlmUniqueID><ISSNLinking>0364-2348</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">3D imaging</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Lumbar spine</Keyword><Keyword MajorTopicYN="N">MRI</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>10</Month><Day>20</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>12</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>12</Month><Day>11</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>5</Day><Hour>11</Hour><Minute>13</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36602576</ArticleId><ArticleId IdType="doi">10.1007/s00256-022-04268-2</ArticleId><ArticleId IdType="pii">10.1007/s00256-022-04268-2</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Patel ND, et al. ACR appropriateness criteria low back pain. J Am Coll Radiol. 2016;13:1069&#x2013;78.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jacr.2016.06.008</ArticleId></ArticleIdList></Reference><Reference><Citation>Sebro R. Leveraging the electronic health record to evaluate the validity of the current RVU system for radiologists. Clin Imaging. 2021;78:286&#x2013;92.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.clinimag.2021.02.007</ArticleId></ArticleIdList></Reference><Reference><Citation>Sartoretti E, et al. Reduction of procedure times in routine clinical practice with compressed SENSE magnetic resonance imaging technique. PLoS ONE. 2019;14:e0214887.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0214887</ArticleId></ArticleIdList></Reference><Reference><Citation>Chea P, Mandell JC. Current applications and future directions of deep learning in musculoskeletal radiology. Skeletal Radiol. 2020;49:183&#x2013;97.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00256-019-03284-z</ArticleId></ArticleIdList></Reference><Reference><Citation>Mart&#xed;n Noguerol T, Paulano-Godino F, Mart&#xed;n-Valdivia MT, Menias CO, Luna A. Strengths, weaknesses, opportunities, and threats analysis of artificial intelligence and machine learning applications in radiology. J American College of Radiol. 2019;16:1239&#x2013;47.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jacr.2019.05.047</ArticleId></ArticleIdList></Reference><Reference><Citation>Jardon M, et al. Deep-learning-reconstructed high-resolution 3D cervical spine MRI for foraminal stenosis evaluation. Skeletal Radiol. 2022. https://doi.org/10.1007/s00256-022-04211-5 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00256-022-04211-5</ArticleId></ArticleIdList></Reference><Reference><Citation>Lundervold AS, Lundervold A. An overview of deep learning in medical imaging focusing on MRI. Z Med Phys. 2019;29:102&#x2013;27.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.zemedi.2018.11.002</ArticleId></ArticleIdList></Reference><Reference><Citation>Bash S, et al. Deep learning image processing enables 40% faster spinal MR scans which match or exceed quality of standard of care. Clin Neuroradiol. 2021. https://doi.org/10.1007/s00062-021-01121-2 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00062-021-01121-2</ArticleId></ArticleIdList></Reference><Reference><Citation>Fritz J, Kijowski R, Recht MP. Artificial intelligence in musculoskeletal imaging: a perspective on value propositions, clinical use, and obstacles. Skeletal Radiol. 2022;51:239&#x2013;43.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00256-021-03802-y</ArticleId></ArticleIdList></Reference><Reference><Citation>Hossein J, Fariborz F, Mehrnaz R, Babak R. Evaluation of diagnostic value and T2-weighted three-dimensional isotropic turbo spin-echo (3D-SPACE) image quality in comparison with T2-weighted two-dimensional turbo spin-echo (2D-TSE) sequences in lumbar spine MR imaging. Europ J of Radiol Open. 2019;6:36&#x2013;41.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejro.2018.12.003</ArticleId></ArticleIdList></Reference><Reference><Citation>Sayah A, Jay AK, Toaff JS, Makariou EV, Berkowitz F. Effectiveness of a rapid lumbar spine MRI protocol using 3D T2-weighted SPACE imaging versus a standard protocol for evaluation of degenerative changes of the lumbar spine. American J Roentgenol. 2016;207:614&#x2013;20.</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/AJR.15.15764</ArticleId></ArticleIdList></Reference><Reference><Citation>Sun S, et al. Evaluation of deep learning reconstructed high-resolution 3D lumbar spine MRI. Eur Radiol. 2022. https://doi.org/10.1007/s00330-022-08708-4 .</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-022-08708-4</ArticleId></ArticleIdList></Reference><Reference><Citation>Lebel RM. Performance characterization of a novel deep learning-based MR image reconstruction pipeline. Arxiv. 2020. https://doi.org/10.48550/arxiv.2008.06559</Citation></Reference><Reference><Citation>Argentieri EC, et al. Diagnostic accuracy of zero-echo time MRI for the evaluation of cervical neural foraminal stenosis. Spine (Phila Pa 1976). 2018;43:928&#x2013;33.</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/BRS.0000000000002462</ArticleId></ArticleIdList></Reference><Reference><Citation>McHugh ML. Interrater reliability: the kappa statistic. Biochem Med (Zagreb). 2012;22:276&#x2013;82.</Citation><ArticleIdList><ArticleId IdType="doi">10.11613/BM.2012.031</ArticleId></ArticleIdList></Reference><Reference><Citation>Blizzard DJ, et al. 3D-FSE Isotropic MRI of the lumbar spine. J Spinal Disor Tech. 2015;28:152&#x2013;7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/BSD.0b013e31827a32ee</ArticleId></ArticleIdList></Reference><Reference><Citation>Lee S, et al. MRI of the lumbar spine: comparison of 3D isotropic turbo spin-echo SPACE sequence versus conventional 2D sequences at 3.0 T. Acta Radiol. 2015;56:174&#x2013;81.</Citation><ArticleIdList><ArticleId IdType="doi">10.1177/0284185114524196</ArticleId></ArticleIdList></Reference><Reference><Citation>Ma J. Dixon techniques for water and fat imaging. J Magn Reson Imaging. 2008;28:543&#x2013;58.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmri.21492</ArticleId></ArticleIdList></Reference><Reference><Citation>Sahr M, Tan ET, Sneag DB. 3D MRI of the spine. Semin Musculoskelet Radiol. 2021;25:433&#x2013;40.</Citation><ArticleIdList><ArticleId IdType="doi">10.1055/s-0041-1731060</ArticleId></ArticleIdList></Reference><Reference><Citation>Glaser C, et al. Understanding 3D TSE sequences: advantages, disadvantages, and application in MSK imaging. Semin Musculoskelet Radiol. 2015;19:321&#x2013;7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1055/s-0035-1563732</ArticleId></ArticleIdList></Reference><Reference><Citation>Kawakyu-O&#x2019;Connor D, Bordia R, Nicola R. Magnetic resonance imaging of spinal emergencies. Magnet Reso Imaging Clin. 2016;24:325&#x2013;44.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.mric.2015.11.004</ArticleId></ArticleIdList></Reference><Reference><Citation>Bash S, et al. Deep learning enables 60% accelerated volumetric brain MRI while preserving quantitative performance: a prospective, multicenter, multireadertrial. AJNR Am J Neuroradiol. 2021;42:2130&#x2013;7.</Citation><ArticleIdList><ArticleId IdType="doi">10.3174/ajnr.A7358</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36601803</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>06</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">2059-7983</ISSN><JournalIssue CitedMedium="Internet"><Volume>79</Volume><Issue>Pt 1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>01</Day></PubDate></JournalIssue><Title>Acta crystallographica. Section D, Structural biology</Title><ISOAbbreviation>Acta Crystallogr D Struct Biol</ISOAbbreviation></Journal><ArticleTitle>Protein model refinement for cryo-EM maps using AlphaFold2 and the DAQ score.</ArticleTitle><Pagination><StartPage>10</StartPage><EndPage>21</EndPage><MedlinePgn>10-21</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1107/S2059798322011676</ELocationID><Abstract><AbstractText>As more protein structure models have been determined from cryogenic electron microscopy (cryo-EM) density maps, establishing how to evaluate the model accuracy and how to correct models in cases where they contain errors is becoming crucial to ensure the quality of the structural models deposited in the public database, the PDB. Here, a new protocol is presented for evaluating a protein model built from a cryo-EM map and applying local structure refinement in the case where the model has potential errors. Firstly, model evaluation is performed using a deep-learning-based model-local map assessment score, DAQ, that has recently been developed. The subsequent local refinement is performed by a modified AlphaFold2 procedure, in which a trimmed template model and a trimmed multiple sequence alignment are provided as input to control which structure regions to refine while leaving other more confident regions of the model intact. A benchmark study showed that this protocol, DAQ-refine, consistently improves low-quality regions of the initial models. Among 18 refined models generated for an initial structure, DAQ shows a high correlation with model quality and can identify the best accurate model for most of the tested cases. The improvements obtained by DAQ-refine were on average larger than other existing methods.</AbstractText><CopyrightInformation>open access.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Terashi</LastName><ForeName>Genki</ForeName><Initials>G</Initials><AffiliationInfo><Affiliation>Department of Biological Sciences, Purdue University, West Lafayette, IN 47907, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wang</LastName><ForeName>Xiao</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>Department of Computer Science, Purdue University, West Lafayette, IN 47907, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kihara</LastName><ForeName>Daisuke</ForeName><Initials>D</Initials><AffiliationInfo><Affiliation>Department of Biological Sciences, Purdue University, West Lafayette, IN 47907, USA.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>01</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>Acta Crystallogr D Struct Biol</MedlineTA><NlmUniqueID>101676043</NlmUniqueID><ISSNLinking>2059-7983</ISSNLinking></MedlineJournalInfo><ChemicalList><Chemical><RegistryNumber>4584-63-8</RegistryNumber><NameOfSubstance UI="C027262">DAQ</NameOfSubstance></Chemical><Chemical><RegistryNumber>0</RegistryNumber><NameOfSubstance UI="D011506">Proteins</NameOfSubstance></Chemical></ChemicalList><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D020285" MajorTopicYN="N">Cryoelectron Microscopy</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D008958" MajorTopicYN="N">Models, Molecular</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D011506" MajorTopicYN="Y">Proteins</DescriptorName><QualifierName UI="Q000737" MajorTopicYN="N">chemistry</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D011487" MajorTopicYN="N">Protein Conformation</DescriptorName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">AlphaFold2</Keyword><Keyword MajorTopicYN="N">DAQ score</Keyword><Keyword MajorTopicYN="N">computational methods</Keyword><Keyword MajorTopicYN="N">cryo-electron microscopy</Keyword><Keyword MajorTopicYN="N">protein model refinement</Keyword><Keyword MajorTopicYN="N">protein model validatation</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>8</Month><Day>24</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>6</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>5</Day><Hour>4</Hour><Minute>13</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>ppublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36601803</ArticleId><ArticleId IdType="pmc">PMC9815095</ArticleId><ArticleId IdType="doi">10.1107/S2059798322011676</ArticleId><ArticleId IdType="pii">S2059798322011676</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Aderinwale, T., Bharadwaj, V., Christoffer, C., Terashi, G., Zhang, Z., Jahandideh, R., Kagaya, Y. &amp; Kihara, D. (2022). Commun. Biol. 5, 316.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8983703</ArticleId><ArticleId IdType="pubmed">35383281</ArticleId></ArticleIdList></Reference><Reference><Citation>Afonine, P. V., Headd, J. J., Terwilliger, T. C. &amp; Adams, P. D. (2013). Comput. Crystallogr. Newsl. 4, 43&#x2013;44.</Citation></Reference><Reference><Citation>Alamo, D. del, Sala, D., Mchaourab, H. S. &amp; Meiler, J. (2022). eLife, 11, e75751.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC9023059</ArticleId><ArticleId IdType="pubmed">35238773</ArticleId></ArticleIdList></Reference><Reference><Citation>Barad, B. A., Echols, N., Wang, R. Y., Cheng, Y., DiMaio, F., Adams, P. D. &amp; Fraser, J. S. (2015). Nat. Methods, 12, 943&#x2013;946.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4589481</ArticleId><ArticleId IdType="pubmed">26280328</ArticleId></ArticleIdList></Reference><Reference><Citation>Berman, H. M., Westbrook, J., Feng, Z., Gilliland, G., Bhat, T. N., Weissig, H., Shindyalov, I. N. &amp; Bourne, P. E. (2000). Nucleic Acids Res. 28, 235&#x2013;242.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC102472</ArticleId><ArticleId IdType="pubmed">10592235</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen, V. B., Arendall, W. B., Headd, J. J., Keedy, D. A., Immormino, R. M., Kapral, G. J., Murray, L. W., Richardson, J. S. &amp; Richardson, D. C. (2010). Acta Cryst. D66, 12&#x2013;21.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC2803126</ArticleId><ArticleId IdType="pubmed">20057044</ArticleId></ArticleIdList></Reference><Reference><Citation>Cheng, Y. (2018). Curr. Opin. Struct. Biol. 52, 58&#x2013;63.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6296881</ArticleId><ArticleId IdType="pubmed">30219656</ArticleId></ArticleIdList></Reference><Reference><Citation>Conway, P., Tyka, M. D., DiMaio, F., Konerding, D. E. &amp; Baker, D. (2014). Protein Sci. 23, 47&#x2013;55.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC3892298</ArticleId><ArticleId IdType="pubmed">24265211</ArticleId></ArticleIdList></Reference><Reference><Citation>Gao, Y., Eskici, G., Ramachandran, S., Poitevin, F., Seven, A. B., Panova, O., Skiniotis, G. &amp; Cerione, R. A. (2020). Mol. Cell, 80, 237&#x2013;245.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7597677</ArticleId><ArticleId IdType="pubmed">33007200</ArticleId></ArticleIdList></Reference><Reference><Citation>Gao, Y., Eskici, G., Ramachandran, S., Poitevin, F., Seven, A. B., Panova, O., Skiniotis, G. &amp; Cerione, R. A. (2021). Mol. Cell, 81, 2496.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8208777</ArticleId><ArticleId IdType="pubmed">34087182</ArticleId></ArticleIdList></Reference><Reference><Citation>Heo, L. &amp; Feig, M. (2022). Proteins, 90, 1873&#x2013;1885.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC9561049</ArticleId><ArticleId IdType="pubmed">35510704</ArticleId></ArticleIdList></Reference><Reference><Citation>Jones, D. T. &amp; Thornton, J. M. (2022). Nat. Methods, 19, 15&#x2013;20.</Citation><ArticleIdList><ArticleId IdType="pubmed">35017725</ArticleId></ArticleIdList></Reference><Reference><Citation>Joseph, A. P., Lagerstedt, I., Patwardhan, A., Topf, M. &amp; Winn, M. (2017). J. Struct. Biol. 199, 12&#x2013;26.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5479444</ArticleId><ArticleId IdType="pubmed">28552721</ArticleId></ArticleIdList></Reference><Reference><Citation>Joseph, A. P., Olek, M., Malhotra, S., Zhang, P., Cowtan, K., Burnley, T. &amp; Winn, M. D. (2022). Acta Cryst. D78, 152&#x2013;161.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8805302</ArticleId><ArticleId IdType="pubmed">35102881</ArticleId></ArticleIdList></Reference><Reference><Citation>Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., &#x17d;&#xed;dek, A., Potapenko, A., Bridgland, A., Meyer, C., Kohl, S. A. A., Ballard, A. J., Cowie, A., Romera-Paredes, B., Nikolov, S., Jain, R., Adler, J., Back, T., Petersen, S., Reiman, D., Clancy, E., Zielinski, M., Steinegger, M., Pacholska, M., Berghammer, T., Bodenstein, S., Silver, D., Vinyals, O., Senior, A. W., Kavukcuoglu, K., Kohli, P. &amp; Hassabis, D. (2021a). Nature, 596, 583&#x2013;589.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8371605</ArticleId><ArticleId IdType="pubmed">34265844</ArticleId></ArticleIdList></Reference><Reference><Citation>Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., &#x17d;&#xed;dek, A., Potapenko, A., Bridgland, A., Meyer, C., Kohl, S. A. A., Ballard, A. J., Cowie, A., Romera-Paredes, B., Nikolov, S., Jain, R., Adler, J., Back, T., Petersen, S., Reiman, D., Clancy, E., Zielinski, M., Steinegger, M., Pacholska, M., Berghammer, T., Silver, D., Vinyals, O., Senior, A. W., Kavukcuoglu, K., Kohli, P. &amp; Hassabis, D. (2021b). Proteins, 89, 1711&#x2013;1721.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC9299164</ArticleId><ArticleId IdType="pubmed">34599769</ArticleId></ArticleIdList></Reference><Reference><Citation>Kampjut, D. &amp; Sazanov, L. A. (2020). Science, 370, eabc4209.</Citation><ArticleIdList><ArticleId IdType="pubmed">32972993</ArticleId></ArticleIdList></Reference><Reference><Citation>Kopp, J., Bordoli, L., Battey, J. N., Kiefer, F. &amp; Schwede, T. (2007). Proteins, 69, 38&#x2013;56.</Citation><ArticleIdList><ArticleId IdType="pubmed">17894352</ArticleId></ArticleIdList></Reference><Reference><Citation>Kryshtafovych, A., Schwede, T., Topf, M., Fidelis, K. &amp; Moult, J. (2021). Proteins, 89, 1607&#x2013;1617.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8726744</ArticleId><ArticleId IdType="pubmed">34533838</ArticleId></ArticleIdList></Reference><Reference><Citation>K&#xfc;hlbrandt, W. (2014). Science, 343, 1443&#x2013;1444.</Citation><ArticleIdList><ArticleId IdType="pubmed">24675944</ArticleId></ArticleIdList></Reference><Reference><Citation>Langer, L. M., Gat, Y., Bonneau, F. &amp; Conti, E. (2020). eLife, 9, e57127.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7334022</ArticleId><ArticleId IdType="pubmed">32469312</ArticleId></ArticleIdList></Reference><Reference><Citation>Lawson, C. L., Kryshtafovych, A., Adams, P. D., Afonine, P. V., Baker, M. L., Barad, B. A., Bond, P., Burnley, T., Cao, R., Cheng, J., Chojnowski, G., Cowtan, K., Dill, K. A., DiMaio, F., Farrell, D. P., Fraser, J. S., Herzik, M. A. Jr, Hoh, S. W., Hou, J., Hung, L.-W., Igaev, M., Joseph, A. P., Kihara, D., Kumar, D., Mittal, S., Monastyrskyy, B., Olek, M., Palmer, C. M., Patwardhan, A., Perez, A., Pfab, J., Pintilie, G. D., Richardson, J. S., Rosenthal, P. B., Sarkar, D., Sch&#xe4;fer, L. U., Schmid, M. F., Schr&#xf6;der, G. F., Shekhar, M., Si, D., Singharoy, A., Terashi, G., Terwilliger, T. C., Vaiana, A., Wang, L., Wang, Z., Wankowicz, S. A., Williams, C. J., Winn, M., Wu, T., Yu, X., Zhang, K., Berman, H. M. &amp; Chiu, W. (2021). Nat. Methods, 18, 156&#x2013;164.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7864804</ArticleId><ArticleId IdType="pubmed">33542514</ArticleId></ArticleIdList></Reference><Reference><Citation>McGreevy, R., Teo, I., Singharoy, A. &amp; Schulten, K. (2016). Methods, 100, 50&#x2013;60.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4848153</ArticleId><ArticleId IdType="pubmed">26804562</ArticleId></ArticleIdList></Reference><Reference><Citation>Mirdita, M., Sch&#xfc;tze, K., Moriwaki, Y., Heo, L., Ovchinnikov, S. &amp; Steinegger, M. (2022). Nat. Methods, 19, 679&#x2013;682.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC9184281</ArticleId><ArticleId IdType="pubmed">35637307</ArticleId></ArticleIdList></Reference><Reference><Citation>Mirdita, M., Steinegger, M. &amp; S&#xf6;ding, J. (2019). Bioinformatics, 35, 2856&#x2013;2858.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6691333</ArticleId><ArticleId IdType="pubmed">30615063</ArticleId></ArticleIdList></Reference><Reference><Citation>Nakane, T., Kotecha, A., Sente, A., McMullan, G., Masiulis, S., Brown, P., Grigoras, I. T., Malinauskaite, L., Malinauskas, T., Miehling, J., Ucha&#x144;ski, T., Yu, L., Karia, D., Pechnikova, E. V., de Jong, E., Keizer, J., Bischoff, M., McCormack, J., Tiemeijer, P., Hardwick, S. W., Chirgadze, D. Y., Murshudov, G., Aricescu, A. R. &amp; Scheres, S. H. W. (2020). Nature, 587, 152&#x2013;156.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7611073</ArticleId><ArticleId IdType="pubmed">33087931</ArticleId></ArticleIdList></Reference><Reference><Citation>Niv&#xf3;n, L. G., Moretti, R. &amp; Baker, D. (2013). PLoS One, 8, e59004.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC3614904</ArticleId><ArticleId IdType="pubmed">23565140</ArticleId></ArticleIdList></Reference><Reference><Citation>Pintilie, G., Zhang, K., Su, Z., Li, S., Schmid, M. F. &amp; Chiu, W. (2020). Nat. Methods, 17, 328&#x2013;334.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7446556</ArticleId><ArticleId IdType="pubmed">32042190</ArticleId></ArticleIdList></Reference><Reference><Citation>Prisant, M. G., Williams, C. J., Chen, V. B., Richardson, J. S. &amp; Richardson, D. C. (2020). Protein Sci. 29, 315&#x2013;329.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6933861</ArticleId><ArticleId IdType="pubmed">31724275</ArticleId></ArticleIdList></Reference><Reference><Citation>Shin, M., Watson, E. R., Song, A. S., Mindrebo, J. T., Novick, S. J., Griffin, P. R., Wiseman, R. L. &amp; Lander, G. C. (2021). Nat. Commun. 12, 3239.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8163871</ArticleId><ArticleId IdType="pubmed">34050165</ArticleId></ArticleIdList></Reference><Reference><Citation>Singharoy, A., Teo, I., McGreevy, R., Stone, J. E., Zhao, J. &amp; Schulten, K. (2016). eLife, 5, e16105.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4990421</ArticleId><ArticleId IdType="pubmed">27383269</ArticleId></ArticleIdList></Reference><Reference><Citation>Steinegger, M. &amp; S&#xf6;ding, J. (2017). Nat. Biotechnol. 35, 1026&#x2013;1028.</Citation><ArticleIdList><ArticleId IdType="pubmed">29035372</ArticleId></ArticleIdList></Reference><Reference><Citation>Terashi, G., Wang, X., Maddhuri Venkata Subramaniya, S. R., Tesmer, J. J. G. &amp; Kihara, D. (2022). Nat. Methods, 19, 1116&#x2013;1125.</Citation><ArticleIdList><ArticleId IdType="pubmed">35953671</ArticleId></ArticleIdList></Reference><Reference><Citation>Terwilliger, T. C., Poon, B. K., Afonine, P. V., Schlicksup, C. J., Croll, T. I., Mill&#xe1;n, C., Richardson, J. S., Read, R. J. &amp; Adams, P. D. (2022). Nat. Methods, 19, 1376&#x2013;1382.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC9636017</ArticleId><ArticleId IdType="pubmed">36266465</ArticleId></ArticleIdList></Reference><Reference><Citation>Yip, K. M., Fischer, N., Paknia, E., Chari, A. &amp; Stark, H. (2020). Nature, 587, 157&#x2013;161.</Citation><ArticleIdList><ArticleId IdType="pubmed">33087927</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhou, X., Li, Y., Zhang, C., Zheng, W., Zhang, G. &amp; Zhang, Y. (2022). Nat. Comput. Sci. 2, 265&#x2013;275.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC9281201</ArticleId><ArticleId IdType="pubmed">35844960</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhu, J., Vinothkumar, K. R. &amp; Hirst, J. (2016). Nature, 536, 354&#x2013;358.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5027920</ArticleId><ArticleId IdType="pubmed">27509854</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhu, L., Li, L., Qi, Y., Yu, Z. &amp; Xu, Y. (2019). Cell Res. 29, 1027&#x2013;1034.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6951342</ArticleId><ArticleId IdType="pubmed">31729466</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36601655</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>06</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>12</Day></DateRevised><Article PubModel="Print"><Journal><ISSN IssnType="Electronic">1533-0338</ISSN><JournalIssue CitedMedium="Internet"><Volume>22</Volume><PubDate><Year>2023</Year><Season>Jan-Dec</Season></PubDate></JournalIssue><Title>Technology in cancer research &amp; treatment</Title><ISOAbbreviation>Technol Cancer Res Treat</ISOAbbreviation></Journal><ArticleTitle>Segmentation of Clinical Target Volume From CT Images for Cervical Cancer Using Deep Learning.</ArticleTitle><Pagination><StartPage>15330338221139164</StartPage><MedlinePgn>15330338221139164</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">15330338221139164</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1177/15330338221139164</ELocationID><Abstract><AbstractText><b>Introduction:</b> Segmentation of clinical target volume (CTV) from CT images is critical for cervical cancer brachytherapy, but this task is time-consuming, laborious, and not reproducible. In this work, we aim to propose an end-to-end model to segment CTV for cervical cancer brachytherapy accurately. <b>Methods:</b> In this paper, an improved M-Net model (Mnet_IM) is proposed to segment CTV of cervical cancer from CT images. An input and an output branch are both proposed to attach to the bottom layer to deal with CTV locating challenges due to its lower contrast than surrounding organs and tissues. A progressive fusion approach is then proposed to recover the prediction results layer by layer to enhance the smoothness of segmentation results. A loss function is defined on each of the multiscale outputs to form a deep supervision mechanism. Numbers of feature map channels that are directly connected to inputs are finally homogenized for each image resolution to reduce feature redundancy and computational burden. <b>Result:</b> Experimental results of the proposed model and some representative models on 5438 image slices from 53 cervical cancer patients demonstrate advantages of the proposed model in terms of segmentation accuracy, such as average surface distance, 95% Hausdorff distance, surface overlap, surface dice, and volumetric dice. <b>Conclusion:</b> A better agreement between the predicted CTV from the proposed model Mnet_IM and manually labeled ground truth is obtained compared to some representative state-of-the-art models.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Huang</LastName><ForeName>Mingxu</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Shenyang, Liaoning, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Feng</LastName><ForeName>Chaolu</ForeName><Initials>C</Initials><AffiliationInfo><Affiliation>Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Shenyang, Liaoning, China.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>School of 504890Computer Science and Engineering, Northeastern University, Shenyang, Liaoning, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Sun</LastName><ForeName>Deyu</ForeName><Initials>D</Initials><AffiliationInfo><Affiliation>Department of Radiation Oncology Gastrointestinal and Urinary and Musculoskeletal Cancer, 74665Cancer Hospital of China Medical University, Shenyang, Liaoning, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Cui</LastName><ForeName>Ming</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Radiation Oncology Gastrointestinal and Urinary and Musculoskeletal Cancer, 74665Cancer Hospital of China Medical University, Shenyang, Liaoning, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhao</LastName><ForeName>Dazhe</ForeName><Initials>D</Initials><Identifier Source="ORCID">0000-0001-8410-6002</Identifier><AffiliationInfo><Affiliation>Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Shenyang, Liaoning, China.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>School of 504890Computer Science and Engineering, Northeastern University, Shenyang, Liaoning, China.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>Technol Cancer Res Treat</MedlineTA><NlmUniqueID>101140941</NlmUniqueID><ISSNLinking>1533-0338</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D005260" MajorTopicYN="N">Female</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D002583" MajorTopicYN="Y">Uterine Cervical Neoplasms</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName><QualifierName UI="Q000532" MajorTopicYN="N">radiotherapy</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D014057" MajorTopicYN="N">Tomography, X-Ray Computed</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D007091" MajorTopicYN="N">Image Processing, Computer-Assisted</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D001918" MajorTopicYN="Y">Brachytherapy</DescriptorName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">CTV segmentation</Keyword><Keyword MajorTopicYN="N">M-Net</Keyword><Keyword MajorTopicYN="N">U-Net</Keyword><Keyword MajorTopicYN="N">cervical cancer</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword></KeywordList><CoiStatement>The author(s) declared no potential conflicts of interest with respect to the
research, authorship, and/or publication of this article.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>5</Day><Hour>2</Hour><Minute>37</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>ppublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36601655</ArticleId><ArticleId IdType="pmc">PMC9829994</ArticleId><ArticleId IdType="doi">10.1177/15330338221139164</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Sung H, Ferlay J, Siegel RL, et al. Global cancer statistics 2020: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries. CA Cancer J Clin. 2021;71(3):209&#x2013;249.</Citation><ArticleIdList><ArticleId IdType="pubmed">33538338</ArticleId></ArticleIdList></Reference><Reference><Citation>Mohammadi R, Shokatian I, Salehi M, et al. Deep learning-based auto-segmentation of organs at risk in high-dose rate brachytherapy of cervical cancer. Radiother Oncol. 2021;159:231&#x2013;240.</Citation><ArticleIdList><ArticleId IdType="pubmed">33831446</ArticleId></ArticleIdList></Reference><Reference><Citation>Jin D, Guo D, Ho T Y, et al. Deeptarget: Gross tumor and clinical target volume segmentation in esophageal cancer radiotherapy. Med Image Anal. 2021;68:101909.</Citation><ArticleIdList><ArticleId IdType="pubmed">33341494</ArticleId></ArticleIdList></Reference><Reference><Citation>Rigaud B, Klopp A, Vedam S, et al. Deformable image registration for dose mapping between external beam radiotherapy and brachytherapy images of cervical cancer. Phys Med Biol. 2019;64(11):115023.</Citation><ArticleIdList><ArticleId IdType="pubmed">30913542</ArticleId></ArticleIdList></Reference><Reference><Citation>Langerak T, Heijkoop S, Quint S, et al.. Towards automatic plan selection for radiotherapy of cervical cancer by fast automatic segmentation of cone beam CT scans. Paper presented at: International Conference on Medical Image Computing and Computer-Assisted Intervention. September 14-18, 2014; Boston, MA.</Citation><ArticleIdList><ArticleId IdType="pubmed">25333159</ArticleId></ArticleIdList></Reference><Reference><Citation>Staring M, Van Der Heide UA, Klein S, et al. Registration of cervical MRI using ultifeatured mutual information. IEEE Trans Med Imaging. 2009;28(9):1412&#x2013;1421.</Citation><ArticleIdList><ArticleId IdType="pubmed">19278929</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen K, Chen W, Ni X, et al. Systematic evaluation of atlas-based autosegmentation (ABAS) software for adaptive radiation therapy in cervical cancer. Chin J Radiol Med Prot. 2015;35(2):111&#x2013;113.</Citation></Reference><Reference><Citation>Ghose S, Holloway L, Lim K, et al. A review of segmentation and deformable registration methods applied to adaptive cervical cancer radiation therapy treatment planning. Artif Intell Med. 2015;64(2):75&#x2013;87.</Citation><ArticleIdList><ArticleId IdType="pubmed">26025124</ArticleId></ArticleIdList></Reference><Reference><Citation>Bnouni N, Rekik I, Rhim M S, et al.. Dynamic multi-scale CNN forest learning for automatic cervical cancer segmentation. Paper presented at: International Workshop on Machine Learning in Medical Imaging. September 16, 2018; Granada, Spain.</Citation></Reference><Reference><Citation>Beekman C, van Beek S, Stam J, et al. Improving predictive CTV segmentation on CT and CBCT for cervical cancer by diffeomorphic registration of a prior. Med Phys. 2021;49(3):1701&#x2013;1711.</Citation><ArticleIdList><ArticleId IdType="pubmed">34964986</ArticleId></ArticleIdList></Reference><Reference><Citation>Yi H, Shi J, Yan B, et al.. Global multi-level attention network for the segmentation of clinical target volume in the planning CT for cervical cancer. Paper presented at: 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI). April 13-16, 2021; Nice, France.</Citation></Reference><Reference><Citation>Liu Z, Liu X, Guan H, et al. Development and validation of a deep learning algorithm for auto-delineation of clinical target volume and organs at risk in cervical cancer radiotherapy. Radiother Oncol. 2020;153:172&#x2013;179.</Citation><ArticleIdList><ArticleId IdType="pubmed">33039424</ArticleId></ArticleIdList></Reference><Reference><Citation>Lin YC, Lin CH, Lu HY, et al. Deep learning for fully automated tumor segmentation and extraction of magnetic resonance radiomics features in cervical cancer[J]. Eur Radiol. 2020;30(3):1297&#x2013;1305.</Citation><ArticleIdList><ArticleId IdType="pubmed">31712961</ArticleId></ArticleIdList></Reference><Reference><Citation>Ronneberger O, Fischer P, Brox T. U-net: convolutional networks for biomedical image segmentation. Paper presented at: International Conference on Medical Image Computing and Computer-Assisted Intervention. October 5-9, 2015; Munich, Germany.</Citation></Reference><Reference><Citation>Chen L C, Papandreou G, Kokkinos I, et al.. Semantic image segmentation with deep convolutional nets and fully connected crfs. arXiv preprint arXiv:1412.7062, 2014.</Citation><ArticleIdList><ArticleId IdType="pubmed">28463186</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhou Z, Rahman Siddiquee MM, Tajbakhsh N, et al. Unet++: a nested u-net architecture for medical image segmentation. Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support. Springer; 2018:3&#x2013;11.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7329239</ArticleId><ArticleId IdType="pubmed">32613207</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang J, Liu X. Medical image recognition and segmentation of pathological slices of gastric cancer based on deeplab v3+ neural network. Comput Methods Programs Biomed. 2021;207:106210.</Citation><ArticleIdList><ArticleId IdType="pubmed">34130088</ArticleId></ArticleIdList></Reference><Reference><Citation>Tang W, Zou D, Yang S, et al.. DSL: automatic liver segmentation with faster R-CNN and DeepLab. Paper presented at: International Conference on Artificial Neural Networks. October 4-7, 2018; Rhodes, Greece.</Citation></Reference><Reference><Citation>Chen LC, Zhu Y, Papandreou G, et al.. Encoder-decoder with atrous separable convolution for semantic image segmentation. Paper presented at: Proceedings of the European conference on computer vision (ECCV). September 8-14, 2018; Munich, Germany.</Citation></Reference><Reference><Citation>Fu H, Cheng J, Xu Y, et al. Joint optic disc and cup segmentation based on multi-label deep network and polar transformation. IEEE Trans Med Imaging. 2018;37(7):1597&#x2013;1605.</Citation><ArticleIdList><ArticleId IdType="pubmed">29969410</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen LC, Papandreou G, Kokkinos I, et al. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Trans Pattern Anal Mach Intell. 2017;40(4):834&#x2013;848.</Citation><ArticleIdList><ArticleId IdType="pubmed">28463186</ArticleId></ArticleIdList></Reference><Reference><Citation>He K, Zhang X, Ren S, et al.. Deep residual learning for image recognition. Paper presented at: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. June 2016; Las Vegas Nevada.</Citation></Reference><Reference><Citation>Huang G, Liu Z, Van Der Maaten L, et al.. Densely connected convolutional networks. Paper presented at: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. July 21-26, 2017; Honolulu, Hawaii.</Citation></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36600120</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>07</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Print">1869-4101</ISSN><JournalIssue CitedMedium="Print"><Volume>14</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>05</Day></PubDate></JournalIssue><Title>Insights into imaging</Title><ISOAbbreviation>Insights Imaging</ISOAbbreviation></Journal><ArticleTitle>Deep learning enables automated MRI-based estimation of uterine volume also in patients with uterine fibroids undergoing high-intensity focused ultrasound therapy.</ArticleTitle><Pagination><StartPage>1</StartPage><MedlinePgn>1</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">1</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1186/s13244-022-01342-0</ELocationID><Abstract><AbstractText Label="BACKGROUND" NlmCategory="BACKGROUND">High-intensity focused ultrasound (HIFU) is used for the treatment of symptomatic leiomyomas. We aim to automate uterine volumetry for tracking changes after therapy with a 3D deep learning approach.</AbstractText><AbstractText Label="METHODS" NlmCategory="METHODS">A 3D nnU-Net model in the default setting and in a modified version including convolutional block attention modules (CBAMs) was developed on 3D T2-weighted MRI scans. Uterine segmentation was performed in 44 patients with routine pelvic MRI (standard group) and 56 patients with uterine fibroids undergoing ultrasound-guided HIFU therapy (HIFU group). Here, preHIFU scans (n&#x2009;=&#x2009;56), postHIFU imaging maximum one day after HIFU (n&#x2009;=&#x2009;54), and the last available follow-up examination (n&#x2009;=&#x2009;53, days after HIFU: 420&#x2009;&#xb1;&#x2009;377) were included. The training was performed on 80% of the data with fivefold cross-validation. The remaining data were used as a hold-out test&#xa0;set. Ground truth was generated by a board-certified radiologist and a radiology resident. For the assessment of inter-reader agreement, all preHIFU examinations were segmented independently by both.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">High segmentation performance was already observed for the default 3D nnU-Net (mean Dice score&#x2009;=&#x2009;0.95&#x2009;&#xb1;&#x2009;0.05) on the validation sets. Since the CBAM nnU-Net showed no significant benefit, the less complex default model was applied to the hold-out test set, which resulted in accurate uterus segmentation (Dice scores: standard group 0.92&#x2009;&#xb1;&#x2009;0.07; HIFU group 0.96&#x2009;&#xb1;&#x2009;0.02), which was comparable to the agreement between the two readers.</AbstractText><AbstractText Label="CONCLUSIONS" NlmCategory="CONCLUSIONS">This study presents a method for automatic uterus segmentation which allows a fast and consistent assessment of uterine volume. Therefore, this method could be used in the clinical setting for objective assessment of therapeutic response to HIFU therapy.</AbstractText><CopyrightInformation>&#xa9; 2022. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y" EqualContrib="Y"><LastName>Theis</LastName><ForeName>Maike</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Diagnostic and Interventional Radiology, University Hospital Bonn, Venusberg-Campus 1, 53127, Bonn, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y" EqualContrib="Y"><LastName>Tonguc</LastName><ForeName>Tolga</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>Department of Diagnostic and Interventional Radiology, University Hospital Bonn, Venusberg-Campus 1, 53127, Bonn, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Savchenko</LastName><ForeName>Oleksandr</ForeName><Initials>O</Initials><AffiliationInfo><Affiliation>Department of Diagnostic and Interventional Radiology, University Hospital Bonn, Venusberg-Campus 1, 53127, Bonn, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Nowak</LastName><ForeName>Sebastian</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Department of Diagnostic and Interventional Radiology, University Hospital Bonn, Venusberg-Campus 1, 53127, Bonn, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Block</LastName><ForeName>Wolfgang</ForeName><Initials>W</Initials><AffiliationInfo><Affiliation>Department of Diagnostic and Interventional Radiology, University Hospital Bonn, Venusberg-Campus 1, 53127, Bonn, Germany.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiotherapy and Radiation Oncology, University Hospital Bonn, Venusberg-Campus 1, 53127, Bonn, Germany.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Neuroradiology, University Hospital Bonn, Venusberg-Campus 1, 53127, Bonn, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Recker</LastName><ForeName>Florian</ForeName><Initials>F</Initials><AffiliationInfo><Affiliation>Department of Gynaecology and Gynaecological Oncology, University Hospital Bonn, Bonn, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Essler</LastName><ForeName>Markus</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Nuclear Medicine, University Hospital Bonn, Bonn, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Mustea</LastName><ForeName>Alexander</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Department of Gynaecology and Gynaecological Oncology, University Hospital Bonn, Bonn, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Attenberger</LastName><ForeName>Ulrike</ForeName><Initials>U</Initials><AffiliationInfo><Affiliation>Department of Diagnostic and Interventional Radiology, University Hospital Bonn, Venusberg-Campus 1, 53127, Bonn, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y" EqualContrib="Y"><LastName>Marinova</LastName><ForeName>Milka</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Diagnostic and Interventional Radiology, University Hospital Bonn, Venusberg-Campus 1, 53127, Bonn, Germany.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Nuclear Medicine, University Hospital Bonn, Bonn, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y" EqualContrib="Y"><LastName>Sprinkart</LastName><ForeName>Alois M</ForeName><Initials>AM</Initials><Identifier Source="ORCID">0000-0002-1435-9562</Identifier><AffiliationInfo><Affiliation>Department of Diagnostic and Interventional Radiology, University Hospital Bonn, Venusberg-Campus 1, 53127, Bonn, Germany. sprinkart@uni-bonn.de.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>05</Day></ArticleDate></Article><MedlineJournalInfo><Country>Germany</Country><MedlineTA>Insights Imaging</MedlineTA><NlmUniqueID>101532453</NlmUniqueID><ISSNLinking>1869-4101</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Leiomyoma</Keyword><Keyword MajorTopicYN="N">Magnetic resonance imaging</Keyword><Keyword MajorTopicYN="N">Uterus</Keyword></KeywordList><CoiStatement>The authors declare that they have no competing interests.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>9</Month><Day>2</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>2</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>4</Day><Hour>23</Hour><Minute>25</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>5</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>5</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36600120</ArticleId><ArticleId IdType="pmc">PMC9813298</ArticleId><ArticleId IdType="doi">10.1186/s13244-022-01342-0</ArticleId><ArticleId IdType="pii">10.1186/s13244-022-01342-0</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Boosz AS, Reimer P, Matzko M, R&#xf6;mer T, M&#xfc;ller A. The conservative and interventional treatment of fibroids. Dtsch Arztebl Int. 2014;111:877&#x2013;183.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4298239</ArticleId><ArticleId IdType="pubmed">25597366</ArticleId></ArticleIdList></Reference><Reference><Citation>Stewart EA, Cookson CL, Gandolfo RA, Schulze-Rath R. Epidemiology of uterine fibroids: a systematic review. BJOG. 2017;124:1501&#x2013;1512. doi: 10.1111/1471-0528.14640.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/1471-0528.14640</ArticleId><ArticleId IdType="pubmed">28296146</ArticleId></ArticleIdList></Reference><Reference><Citation>Al-Hendy A, Myers ER, Stewart E. Uterine fibroids: burden and unmet medical need. Semin Reprod Med. 2017;35:473&#x2013;480. doi: 10.1055/s-0037-1607264.</Citation><ArticleIdList><ArticleId IdType="doi">10.1055/s-0037-1607264</ArticleId><ArticleId IdType="pmc">PMC6193285</ArticleId><ArticleId IdType="pubmed">29100234</ArticleId></ArticleIdList></Reference><Reference><Citation>Donnez J, Dolmans MM. Uterine fibroid management: from the present to the future. Hum Reprod Update. 2016;22:665&#x2013;686. doi: 10.1093/humupd/dmw023.</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/humupd/dmw023</ArticleId><ArticleId IdType="pmc">PMC5853598</ArticleId><ArticleId IdType="pubmed">27466209</ArticleId></ArticleIdList></Reference><Reference><Citation>Mas A, Tarazona M, Das&#xed; Carrasco J, Estaca G, Crist&#xf3;bal I, Monle&#xf3;n J. Updated approaches for management of uterine fibroids. Int J Womens Health. 2017;9:607&#x2013;617. doi: 10.2147/IJWH.S138982.</Citation><ArticleIdList><ArticleId IdType="doi">10.2147/IJWH.S138982</ArticleId><ArticleId IdType="pmc">PMC5592915</ArticleId><ArticleId IdType="pubmed">28919823</ArticleId></ArticleIdList></Reference><Reference><Citation>Gurusamy KS, Vaughan J, Fraser IS, Best LMJ, Richards T. Medical therapies for uterine fibroids &#x2013; a systematic review and network meta-analysis of randomised controlled trials. PLoS One. 2016;11:e0149631. doi: 10.1371/journal.pone.0149631.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0149631</ArticleId><ArticleId IdType="pmc">PMC4769153</ArticleId><ArticleId IdType="pubmed">26919185</ArticleId></ArticleIdList></Reference><Reference><Citation>Marinova M, Ghaei S, Recker F, et al. Efficacy of ultrasound-guided high-intensity focused ultrasound (USgHIFU) for uterine fibroids: an observational single-center study. Int J Hyperthermia. 2021;38:30&#x2013;38. doi: 10.1080/02656736.2021.1939444.</Citation><ArticleIdList><ArticleId IdType="doi">10.1080/02656736.2021.1939444</ArticleId><ArticleId IdType="pubmed">34420447</ArticleId></ArticleIdList></Reference><Reference><Citation>Recker F, Thudium M, Strunk H, et al. Multidisciplinary management to optimize outcome of ultrasound-guided high-intensity focused ultrasound (HIFU) in patients with uterine fibroids. Sci Rep. 2021;11:22768. doi: 10.1038/s41598-021-02217-y.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-021-02217-y</ArticleId><ArticleId IdType="pmc">PMC8611035</ArticleId><ArticleId IdType="pubmed">34815488</ArticleId></ArticleIdList></Reference><Reference><Citation>Tonguc T, Strunk H, Gonzalez-Carmona MA, et al. US-guided high-intensity focused ultrasound (HIFU) of abdominal tumors: outcome, early ablation-related laboratory changes and inflammatory reaction. a single-center experience from Germany. Int J Hyperthermia. 2021;38:65&#x2013;74. doi: 10.1080/02656736.2021.1900926.</Citation><ArticleIdList><ArticleId IdType="doi">10.1080/02656736.2021.1900926</ArticleId><ArticleId IdType="pubmed">34420445</ArticleId></ArticleIdList></Reference><Reference><Citation>Kim HS, Baik JH, Pham LD, Jacobs MA. MR-guided high-intensity focused ultrasound treatment for symptomatic uterine leiomyomata: long-term outcomes. Acad Radiol. 2011;18:970&#x2013;976. doi: 10.1016/j.acra.2011.03.008.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.acra.2011.03.008</ArticleId><ArticleId IdType="pmc">PMC3401073</ArticleId><ArticleId IdType="pubmed">21718955</ArticleId></ArticleIdList></Reference><Reference><Citation>Hindley J, Gedroyc WM, Regan L, et al. MRI guidance of focused ultrasound therapy of uterine fibroids: early results. AJR Am J Roentgenol. 2004;183:1713&#x2013;1719. doi: 10.2214/ajr.183.6.01831713.</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/ajr.183.6.01831713</ArticleId><ArticleId IdType="pubmed">15547216</ArticleId></ArticleIdList></Reference><Reference><Citation>Wu F, Wang Z-B, Chen W-Z, et al. Extracorporeal high intensity focused ultrasound ablation in the treatment of 1038 patients with solid carcinomas in China: an overview. Ultrason Sonochem. 2004;11:149&#x2013;154. doi: 10.1016/j.ultsonch.2004.01.011.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ultsonch.2004.01.011</ArticleId><ArticleId IdType="pubmed">15081972</ArticleId></ArticleIdList></Reference><Reference><Citation>Hahn M, Fugunt R, Schoenfisch B, et al. High intensity focused ultrasound (HIFU) for the treatment of symptomatic breast fibroadenoma. Int J Hyperthermia. 2018;35:463&#x2013;470. doi: 10.1080/02656736.2018.1508757.</Citation><ArticleIdList><ArticleId IdType="doi">10.1080/02656736.2018.1508757</ArticleId><ArticleId IdType="pubmed">30204024</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang R, Chen J-Y, Zhang L, et al. The safety and ablation efficacy of ultrasound-guided high-intensity focused ultrasound ablation for desmoid tumors. Int J Hyperthermia. 2021;38:89&#x2013;95. doi: 10.1080/02656736.2021.1894359.</Citation><ArticleIdList><ArticleId IdType="doi">10.1080/02656736.2021.1894359</ArticleId><ArticleId IdType="pubmed">34420439</ArticleId></ArticleIdList></Reference><Reference><Citation>Marinova M, Huxold HC, Henseler J, et al. Clinical effectiveness and potential survival benefit of US-guided high-intensity focused ultrasound therapy in patients with advanced-stage pancreatic cancer. Ultraschall Med. 2019;40:625&#x2013;637. doi: 10.1055/a-0591-3386.</Citation><ArticleIdList><ArticleId IdType="doi">10.1055/a-0591-3386</ArticleId><ArticleId IdType="pubmed">29665583</ArticleId></ArticleIdList></Reference><Reference><Citation>Marinova M, Wilhelm-Buchstab T, Strunk H. Advanced pancreatic cancer: high-intensity focused ultrasound (HIFU) and other local ablative therapies. Rofo. 2019;191:216&#x2013;227. doi: 10.1055/a-0820-5564.</Citation><ArticleIdList><ArticleId IdType="doi">10.1055/a-0820-5564</ArticleId><ArticleId IdType="pubmed">30703824</ArticleId></ArticleIdList></Reference><Reference><Citation>Coppola F, Faggioni L, Gabelloni M, et al. Human, all too human? An all-around appraisal of the &#x201c;artificial intelligence revolution&#x201d; in medical imaging. Front Psychol. 2021;12:710982. doi: 10.3389/fpsyg.2021.710982.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fpsyg.2021.710982</ArticleId><ArticleId IdType="pmc">PMC8505993</ArticleId><ArticleId IdType="pubmed">34650476</ArticleId></ArticleIdList></Reference><Reference><Citation>Hosny A, Parmar C, Quackenbush J, Schwartz LH, Aerts HJWL. Artificial intelligence in radiology. Nat Rev Cancer. 2018;18:500&#x2013;510. doi: 10.1038/s41568-018-0016-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41568-018-0016-5</ArticleId><ArticleId IdType="pmc">PMC6268174</ArticleId><ArticleId IdType="pubmed">29777175</ArticleId></ArticleIdList></Reference><Reference><Citation>Yu K-H, Beam AL, Kohane IS. Artificial intelligence in healthcare. Nat Biomed Eng. 2018;2:719&#x2013;731. doi: 10.1038/s41551-018-0305-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41551-018-0305-z</ArticleId><ArticleId IdType="pubmed">31015651</ArticleId></ArticleIdList></Reference><Reference><Citation>Nowak S, Mesropyan N, Faron A, et al. Detection of liver cirrhosis in standard T2-weighted MRI using deep transfer learning. Eur Radiol. 2021;31:8807&#x2013;8815. doi: 10.1007/s00330-021-07858-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-021-07858-1</ArticleId><ArticleId IdType="pmc">PMC8523404</ArticleId><ArticleId IdType="pubmed">33974149</ArticleId></ArticleIdList></Reference><Reference><Citation>Luetkens JA, Nowak S, Mesropyan N, et al. Deep learning supports the differentiation of alcoholic and other-than-alcoholic cirrhosis based on MRI. Sci Rep. 2022;12:8297. doi: 10.1038/s41598-022-12410-2.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-022-12410-2</ArticleId><ArticleId IdType="pmc">PMC9117223</ArticleId><ArticleId IdType="pubmed">35585118</ArticleId></ArticleIdList></Reference><Reference><Citation>Ronneberger O, Fischer P, Brox T. U-Net: convolutional networks for biomedical image segmentation. Proc MICCAI. 2015;2015:234&#x2013;241.</Citation></Reference><Reference><Citation>Isensee F, Jaeger PF, Kohl SAA, Petersen J, Maier-Hein KH. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nat Methods. 2021;18:203&#x2013;211. doi: 10.1038/s41592-020-01008-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41592-020-01008-z</ArticleId><ArticleId IdType="pubmed">33288961</ArticleId></ArticleIdList></Reference><Reference><Citation>Nowak S, Theis M, Wichtmann BD, et al. End-to-end automated body composition analyses with integrated quality control for opportunistic assessment of sarcopenia in CT. Eur Radiol. 2021 doi: 10.1007/s00330-021-08313-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-021-08313-x</ArticleId><ArticleId IdType="pmc">PMC9038788</ArticleId><ArticleId IdType="pubmed">34595539</ArticleId></ArticleIdList></Reference><Reference><Citation>Woo S, Park J, Lee J-Y, Kweon IS. CBAM: convolutional block attention module. Proc ECCV. 2018;2018:3&#x2013;19.</Citation></Reference><Reference><Citation>Yang J, Xie M, Hu C, et al. Deep learning for detecting cerebral aneurysms with CT angiography. Radiology. 2021;298:155&#x2013;163. doi: 10.1148/radiol.2020192154.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2020192154</ArticleId><ArticleId IdType="pubmed">33141003</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen M, Zhao C, Tian X, et al. Placental super micro-vessels segmentation based on resnext with convolutional block attention and U-Net. Proc IEEE EMBC. 2021;2021:4015&#x2013;4018.</Citation><ArticleIdList><ArticleId IdType="pubmed">34892111</ArticleId></ArticleIdList></Reference><Reference><Citation>Trebing K, Sta&#x1f9;czyk T, Mehrkanoon S. SmaAt-UNet: Precipitation nowcasting using a small attention-UNet architecture. Pattern Recognit Lett. 2021;145:178&#x2013;186. doi: 10.1016/j.patrec.2021.01.036.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.patrec.2021.01.036</ArticleId></ArticleIdList></Reference><Reference><Citation>Shahedi M, Spong CY, Dormer JD, et al. Deep learning-based segmentation of the placenta and uterus on MR images. J Med Imaging. 2021;8:054001. doi: 10.1117/1.JMI.8.5.054001.</Citation><ArticleIdList><ArticleId IdType="doi">10.1117/1.JMI.8.5.054001</ArticleId><ArticleId IdType="pmc">PMC8463933</ArticleId><ArticleId IdType="pubmed">34589556</ArticleId></ArticleIdList></Reference><Reference><Citation>Behboodi B, Rivaz H, Lalondrelle S, Harris E. Automatic 3D ultrasound segmentation of uterus using deep learning. Proc IEEE IUS. 2021;2021:1&#x2013;4.</Citation></Reference><Reference><Citation>Niu Y, Zhang Y, Ying L, et al. Uterine magnetic resonance image segmentation based on deep learning. J Phys Conf Ser. 2021;1861:012067. doi: 10.1088/1742-6596/1861/1/012067.</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/1742-6596/1861/1/012067</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang C, Shu H, Yang G, et al. HIFUNet: multi-class segmentation of uterine regions from MR images using global convolutional networks for HIFU surgery planning. IEEE Trans Med Imaging. 2020;39:3309&#x2013;3320. doi: 10.1109/TMI.2020.2991266.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2020.2991266</ArticleId><ArticleId IdType="pubmed">32356741</ArticleId></ArticleIdList></Reference><Reference><Citation>Fedorov A, Beichel R, Kalpathy-Cramer J, et al. 3D slicer as an image computing platform for the quantitative imaging network. Magn Reson Imaging. 2012;30:1323&#x2013;1341. doi: 10.1016/j.mri.2012.05.001.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.mri.2012.05.001</ArticleId><ArticleId IdType="pmc">PMC3466397</ArticleId><ArticleId IdType="pubmed">22770690</ArticleId></ArticleIdList></Reference><Reference><Citation>Waskom ML. seaborn: statistical data visualization. J Open Source Softw. 2021;6:3021. doi: 10.21105/joss.03021.</Citation><ArticleIdList><ArticleId IdType="doi">10.21105/joss.03021</ArticleId></ArticleIdList></Reference><Reference><Citation>jaketmp, &amp; Lee Tirrell. (2021). jaketmp/pyCompare: (v1.5.2). Zenodo. 10.5281/zenodo.4926654</Citation></Reference><Reference><Citation>He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. Proc IEEE CVPR. 2016;2016:770&#x2013;778.</Citation></Reference><Reference><Citation>Rundo L, Militello C, Vitabile S, et al. Combining split-and-merge and multi-seed region growing algorithms for uterine fibroid segmentation in MRgFUS treatments. Med Biol Eng Comput. 2016;54:1071&#x2013;1084. doi: 10.1007/s11517-015-1404-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11517-015-1404-6</ArticleId><ArticleId IdType="pubmed">26530047</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36599960</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>06</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>07</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">2045-2322</ISSN><JournalIssue CitedMedium="Internet"><Volume>13</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>04</Day></PubDate></JournalIssue><Title>Scientific reports</Title><ISOAbbreviation>Sci Rep</ISOAbbreviation></Journal><ArticleTitle>Tubule-U-Net: a novel dataset and deep learning-based tubule segmentation framework in whole slide images of breast cancer.</ArticleTitle><Pagination><StartPage>128</StartPage><MedlinePgn>128</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">128</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1038/s41598-022-27331-3</ELocationID><Abstract><AbstractText>The tubule index is a vital prognostic measure in breast cancer tumor grading and is visually evaluated by pathologists. In this paper, a computer-aided patch-based deep learning tubule segmentation framework, named Tubule-U-Net, is developed and proposed to segment tubules in Whole Slide Images (WSI) of breast cancer. Moreover, this paper presents a new tubule segmentation dataset consisting of 30820 polygonal annotated tubules in 8225 patches. The Tubule-U-Net framework first uses a patch enhancement technique such as reflection or mirror padding and then employs an asymmetric encoder-decoder semantic segmentation model. The encoder is developed in the model by various deep learning architectures such as EfficientNetB3, ResNet34, and DenseNet161, whereas the decoder is similar to U-Net. Thus, three different models are obtained, which are EfficientNetB3-U-Net, ResNet34-U-Net, and DenseNet161-U-Net. The proposed framework with three different models, U-Net, U-Net++, and Trans-U-Net segmentation methods are trained on the created dataset and tested on five different WSIs. The experimental results demonstrate that the proposed framework with the EfficientNetB3 model trained on patches obtained using the reflection padding and tested on patches with overlapping provides the best segmentation results on the test data and achieves 95.33%, 93.74%, and 90.02%, dice, recall, and specificity scores, respectively.</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Tekin</LastName><ForeName>Eren</ForeName><Initials>E</Initials><AffiliationInfo><Affiliation>Artificial Intelligence Research Team, Virasoft Corporation, New York, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yaz&#x131;c&#x131;</LastName><ForeName>&#xc7;isem</ForeName><Initials>&#xc7;</Initials><AffiliationInfo><Affiliation>Research and Development Team, Virasoft Corporation, New York, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kusetogullari</LastName><ForeName>Huseyin</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>Department of Computer Science, Blekinge Institute of Technology, 371 41, Karlskrona, Sweden. huseyinkusetogullari@gmail.com.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Computer Science, Heriot-Watt University, Dubai, United Arab Emirates. huseyinkusetogullari@gmail.com.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Tokat</LastName><ForeName>Fatma</ForeName><Initials>F</Initials><AffiliationInfo><Affiliation>Pathology Department, Acibadem University Teaching Hospital, Istanbul, Turkey.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yavariabdi</LastName><ForeName>Amir</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Department of Mechatronics Engineering, KTO Karatay University, Konya, Turkey.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Iheme</LastName><ForeName>Leonardo Obinna</ForeName><Initials>LO</Initials><AffiliationInfo><Affiliation>Artificial Intelligence Research Team, Virasoft Corporation, New York, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>&#xc7;ay&#x131;r</LastName><ForeName>Sercan</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Artificial Intelligence Research Team, Virasoft Corporation, New York, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Bozaba</LastName><ForeName>Engin</ForeName><Initials>E</Initials><AffiliationInfo><Affiliation>Artificial Intelligence Research Team, Virasoft Corporation, New York, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Solmaz</LastName><ForeName>Gizem</ForeName><Initials>G</Initials><AffiliationInfo><Affiliation>Research and Development Team, Virasoft Corporation, New York, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Darbaz</LastName><ForeName>Berkan</ForeName><Initials>B</Initials><AffiliationInfo><Affiliation>Artificial Intelligence Research Team, Virasoft Corporation, New York, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>&#xd6;zsoy</LastName><ForeName>G&#xfc;l&#x15f;ah</ForeName><Initials>G</Initials><AffiliationInfo><Affiliation>Research and Development Team, Virasoft Corporation, New York, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Ayalt&#x131;</LastName><ForeName>Samet</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Artificial Intelligence Research Team, Virasoft Corporation, New York, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Research and Development Team, Virasoft Corporation, New York, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kayhan</LastName><ForeName>Cavit Kerem</ForeName><Initials>CK</Initials><AffiliationInfo><Affiliation>Department of Biotechnology, Nisantasi University, Istanbul, Turkey.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>&#x130;nce</LastName><ForeName>&#xdc;mit</ForeName><Initials>&#xdc;</Initials><AffiliationInfo><Affiliation>Pathology Department, Acibadem University Teaching Hospital, Istanbul, Turkey.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Uzel</LastName><ForeName>Burak</ForeName><Initials>B</Initials><AffiliationInfo><Affiliation>Internal Medicine Department, &#xc7;aml&#x131;k Hospital, Istanbul, Turkey.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>04</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>Sci Rep</MedlineTA><NlmUniqueID>101563288</NlmUniqueID><ISSNLinking>2045-2322</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D005260" MajorTopicYN="N">Female</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D001943" MajorTopicYN="Y">Breast Neoplasms</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D007091" MajorTopicYN="N">Image Processing, Computer-Assisted</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D012660" MajorTopicYN="N">Semantics</DescriptorName></MeshHeading></MeshHeadingList><CoiStatement>The authors declare no competing interests.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>9</Month><Day>22</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>30</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>4</Day><Hour>23</Hour><Minute>19</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>5</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36599960</ArticleId><ArticleId IdType="pmc">PMC9812986</ArticleId><ArticleId IdType="doi">10.1038/s41598-022-27331-3</ArticleId><ArticleId IdType="pii">10.1038/s41598-022-27331-3</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>&#x141;ukasiewicz, S. et al. Breast cancer&#x2014;Epidemiology, risk factors, classification, prognostic markers, and current treatment strategies&#x2014;An updated review. Cancers13 (2021).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8428369</ArticleId><ArticleId IdType="pubmed">34503097</ArticleId></ArticleIdList></Reference><Reference><Citation>Cay&#x131;r S, et al. Mitnet: A novel dataset and a two-stage deep learning approach for mitosis recognition in whole slide images of breast cancer tissue. Neural Comput. Appl. 2022;34:17837&#x2013;17851. doi: 10.1007/s00521-022-07441-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00521-022-07441-9</ArticleId></ArticleIdList></Reference><Reference><Citation>Rakha E, et al. Prognostic significance of nottingham histologic grade in invasive breast carcinoma. J. Clin. Oncol. 2008;26:3153&#x2013;8. doi: 10.1200/JCO.2007.15.5986.</Citation><ArticleIdList><ArticleId IdType="doi">10.1200/JCO.2007.15.5986</ArticleId><ArticleId IdType="pubmed">18490649</ArticleId></ArticleIdList></Reference><Reference><Citation>Dalton LW, Page DL, Dupont WD. Histologic grading of breast carcinoma. Cancer. 1994;73:2765&#x2013;2770. doi: 10.1002/1097-0142(19940601)73:11&lt;2765::AID-CNCR2820731119&gt;3.0.CO;2-K.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/1097-0142(19940601)73:11&lt;2765::AID-CNCR2820731119&gt;3.0.CO;2-K</ArticleId><ArticleId IdType="pubmed">8194018</ArticleId></ArticleIdList></Reference><Reference><Citation>Dalle, J.-R., Leow, W. K., Racoceanu, D., Tutac, A. E. &amp; Putti, T. C. Automatic breast cancer grading of histopathological images. In 2008 30th Annual Int. Conf. of the IEEE Eng. in Medicine and Biology Society, 3052&#x2013;3055 (2008).</Citation><ArticleIdList><ArticleId IdType="pubmed">19163350</ArticleId></ArticleIdList></Reference><Reference><Citation>Lee, S., Fu, C., Salama, P., Dunn, K. &amp; Delp, E. Tubule segmentation of fluorescence microscopy images based on convolutional neural networks with inhomogeneity correction. In Int. Symp. on Electr. Imaging, 1&#x2013;8 (2018).</Citation></Reference><Reference><Citation>Kumar N, et al. A dataset and a technique for generalized nuclear segmentation for computational pathology. IEEE Trans. Med. Imaging. 2017;36:1550&#x2013;1560. doi: 10.1109/TMI.2017.2677499.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2017.2677499</ArticleId><ArticleId IdType="pubmed">28287963</ArticleId></ArticleIdList></Reference><Reference><Citation>Saha M, Chakraborty C, Racoceanu D. Efficient deep learning model for mitosis detection using breast histopathology images. Comput. Med. Imaging Graph. 2018;64:29&#x2013;40. doi: 10.1016/j.compmedimag.2017.12.001.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compmedimag.2017.12.001</ArticleId><ArticleId IdType="pubmed">29409716</ArticleId></ArticleIdList></Reference><Reference><Citation>Qin, X. et al. Basnet: Boundary-aware salient object detection. In IEEE Conference on Computer Vision and Pattern Recognition, 7471&#x2013;7481 (2019).</Citation></Reference><Reference><Citation>Ronneberger, O., Fischer, P. &amp; Brox, T. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention, 234&#x2013;241 (2015).</Citation></Reference><Reference><Citation>Mamonov AV, Figueiredo IN, Figueiredo PN, Tsai YH. Automated polyp detection in colon capsule endoscopy. IEEE Trans. Med. Imaging. 2014;33:1488&#x2013;1502. doi: 10.1109/TMI.2014.2314959.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2014.2314959</ArticleId><ArticleId IdType="pubmed">24710829</ArticleId></ArticleIdList></Reference><Reference><Citation>Borgli H, Thambawita V, Smedsrud P, et al. A comprehensive multi-class image and video dataset for gastrointestinal endoscopy. Sci. Data. 2020;7:1&#x2013;14. doi: 10.1038/s41597-020-00622-y.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41597-020-00622-y</ArticleId><ArticleId IdType="pmc">PMC7455694</ArticleId><ArticleId IdType="pubmed">32859981</ArticleId></ArticleIdList></Reference><Reference><Citation>Bellens S, Probst GM, Janssens M, Vandewalle P, Dewulf W. Evaluating conventional and deep learning segmentation for fast X-ray CT porosity measurements of polymer laser sintered am parts. Polym. Test. 2022;110:107540. doi: 10.1016/j.polymertesting.2022.107540.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.polymertesting.2022.107540</ArticleId></ArticleIdList></Reference><Reference><Citation>Fawzi A, et al. Brain image segmentation in recent years: A narrative review. Brain Sci. 2021;11:1&#x2013;31. doi: 10.3390/brainsci11081055.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/brainsci11081055</ArticleId><ArticleId IdType="pmc">PMC8392552</ArticleId><ArticleId IdType="pubmed">34439674</ArticleId></ArticleIdList></Reference><Reference><Citation>Naik, S. et al. Automated gland and nuclei segmentation for grading of prostate and breast cancer histopathology. In IEEE Int. Symp. on Biomedical Imaging: From Nano to Macro, 284&#x2013;287 (2008).</Citation></Reference><Reference><Citation>Tutac, A. E. et al. Knowledge-guided semantic indexing of breast cancer histopathology images. In International Conference on BioMedical Engineering and Informatics, 107&#x2013;112 (2008).</Citation></Reference><Reference><Citation>Basavanhally, A. et al. Incorporating domain knowledge for tubule detection in breast histopathology using o&#x2019;callaghan neighborhoods. Proc. SPIE Med. Imag. Comput.-Aid. Diagn.7963 (2011).</Citation></Reference><Reference><Citation>Paramanandam, M., Thamburaj, R., Mammen, J. &amp; Nagar, A. Automatic detection of tubules in breast histopathological images. In Advances in Intelligent Systems and Computing, 311&#x2013;321 (2013).</Citation></Reference><Reference><Citation>Kien, N., Barnes, M., Srinivas, C. &amp; Chefd&#x2019;hotel, C. Automatic glandular and tubule region segmentation in histological grading of breast cancer. In SPIE Medical Imaging: Digital Pathology, 92&#x2013;98 (2015).</Citation></Reference><Reference><Citation>Tan, X. J., Mustafa, N., Mashor, M. Y. &amp; Ab Rahman, K. S. A novel quantitative measurement method for irregular tubules in breast carcinoma. Eng. Sci. Technol. Int. J. (2021).</Citation></Reference><Reference><Citation>Romo-Bucheli, D., Janowczyk, A., Romero, E., Gilmore, H. &amp; Madabhushi, A. Automated tubule nuclei quantification and correlation with oncotype dx risk categories in er+ breast cancer whole slide images. In SPIE Medical Imaging (2016).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5013328</ArticleId><ArticleId IdType="pubmed">27599752</ArticleId></ArticleIdList></Reference><Reference><Citation>He, K., Zhang, X., Ren, S. &amp; Sun, J. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, 770&#x2013;778 (2016).</Citation></Reference><Reference><Citation>Tan, M. &amp; Le, Q. EfficientNet: Rethinking model scaling for convolutional neural networks. In Proceedings of International Conference on Machine Learning, 6105&#x2013;6114 (2019).</Citation></Reference><Reference><Citation>Huang, G., Liu, Z., Van Der Maaten, L. &amp; Weinberger, K. Q. Densely connected convolutional networks. In IEEE Conference on Computer Vision and Pattern Recognition, 2261&#x2013;2269 (2017).</Citation></Reference><Reference><Citation>Bloom, H. J. &amp; Richardson, W. W. Histological grading and prognosis in breast cancer; A study of 1409 cases of which 359 have been followed for 15 years. Br J Cancer11 (1957).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC2073885</ArticleId><ArticleId IdType="pubmed">13499785</ArticleId></ArticleIdList></Reference><Reference><Citation>Siddique N, Paheding S, Elkin CP, Devabhaktuni V. U-net and its variants for medical image segmentation: A review of theory and applications. IEEE Access. 2021;9:82031&#x2013;82057. doi: 10.1109/ACCESS.2021.3086020.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2021.3086020</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhou Z, Siddiquee MM, Tajbakhsh R, Liang J. UNet++: A nested U-Net architecture for medical image segmentation. Deep Learn. Med. Image Anal. Multimodal Learn. Clin. Decis. Support. 2018;11045:37&#x2013;63.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7329239</ArticleId><ArticleId IdType="pubmed">32613207</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen, J. et al. Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprintarXiv:2102.04306 1&#x2013;13 (2021).</Citation></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36599879</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>06</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>08</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">2045-2322</ISSN><JournalIssue CitedMedium="Internet"><Volume>13</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>04</Day></PubDate></JournalIssue><Title>Scientific reports</Title><ISOAbbreviation>Sci Rep</ISOAbbreviation></Journal><ArticleTitle>End-to-end differentiable blind tip reconstruction for noisy atomic force microscopy images.</ArticleTitle><Pagination><StartPage>129</StartPage><MedlinePgn>129</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">129</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1038/s41598-022-27057-2</ELocationID><Abstract><AbstractText>Observing the structural dynamics of biomolecules is vital to deepening our understanding of biomolecular functions. High-speed (HS) atomic force microscopy (AFM) is a powerful method to measure biomolecular behavior at near physiological conditions. In the AFM, measured image profiles on a molecular surface are distorted by the tip shape through the interactions between the tip and molecule. Once the tip shape is known, AFM images can be approximately deconvolved to reconstruct the surface geometry of the sample molecule. Thus, knowing the correct tip shape is an important issue in the AFM image analysis. The blind tip reconstruction (BTR) method developed by Villarrubia (J Res Natl Inst Stand Technol 102:425, 1997) is an algorithm that estimates tip shape only from AFM images using mathematical morphology operators. While the BTR works perfectly for noise-free AFM images, the algorithm is susceptible to noise. To overcome this issue, we here propose an alternative BTR method, called end-to-end differentiable BTR, based on a modern machine learning approach. In the method, we introduce a loss function including a regularization term to prevent overfitting to noise, and the tip shape is optimized with automatic differentiation and backpropagations developed in deep learning frameworks. Using noisy pseudo-AFM images of myosin V motor domain as test cases, we show that our end-to-end differentiable BTR is robust against noise in AFM images. The method can also detect a double-tip shape and deconvolve doubled molecular images. Finally, application to real HS-AFM data of myosin V walking on an actin filament shows that the method can reconstruct the accurate surface geometry of actomyosin consistent with the structural model. Our method serves as a general post-processing for reconstructing hidden molecular surfaces from any AFM images. Codes are available at https://github.com/matsunagalab/differentiable_BTR .</AbstractText><CopyrightInformation>&#xa9; 2023. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Matsunaga</LastName><ForeName>Yasuhiro</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Graduate School of Science and Engineering, Saitama University, Saitama, 338-8570, Japan. ymatsunaga@mail.saitama-u.ac.jp.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Fuchigami</LastName><ForeName>Sotaro</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Department of Biophysics, Graduate School of Science, Kyoto University, Kyoto, 606-8502, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Ogane</LastName><ForeName>Tomonori</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>Graduate School of Science and Engineering, Saitama University, Saitama, 338-8570, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Takada</LastName><ForeName>Shoji</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Department of Biophysics, Graduate School of Science, Kyoto University, Kyoto, 606-8502, Japan.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>04</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>Sci Rep</MedlineTA><NlmUniqueID>101563288</NlmUniqueID><ISSNLinking>2045-2322</ISSNLinking></MedlineJournalInfo><ChemicalList><Chemical><RegistryNumber>EC 3.6.1.-</RegistryNumber><NameOfSubstance UI="D024701">Myosin Type V</NameOfSubstance></Chemical><Chemical><RegistryNumber>9013-26-7</RegistryNumber><NameOfSubstance UI="D000205">Actomyosin</NameOfSubstance></Chemical></ChemicalList><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D018625" MajorTopicYN="N">Microscopy, Atomic Force</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D024701" MajorTopicYN="Y">Myosin Type V</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D007091" MajorTopicYN="N">Image Processing, Computer-Assisted</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D000465" MajorTopicYN="N">Algorithms</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000205" MajorTopicYN="N">Actomyosin</DescriptorName></MeshHeading></MeshHeadingList><CoiStatement>The authors declare no competing interests.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>11</Month><Day>9</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>23</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>4</Day><Hour>23</Hour><Minute>15</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>5</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36599879</ArticleId><ArticleId IdType="doi">10.1038/s41598-022-27057-2</ArticleId><ArticleId IdType="pii">10.1038/s41598-022-27057-2</ArticleId><ArticleId IdType="pmc">PMC9813222</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Binnig G, Quate CF, Gerber Ch. Atomic force microscope. Phys. Rev. Lett. 1986;56:930&#x2013;933. doi: 10.1103/PhysRevLett.56.930.</Citation><ArticleIdList><ArticleId IdType="doi">10.1103/PhysRevLett.56.930</ArticleId><ArticleId IdType="pubmed">10033323</ArticleId></ArticleIdList></Reference><Reference><Citation>Ando T, Uchihashi T, Fukuma T. High-speed atomic force microscopy for nano-visualization of dynamic biomolecular processes. Prog. Surf. Sci. 2008;83:337&#x2013;437. doi: 10.1016/j.progsurf.2008.09.001.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.progsurf.2008.09.001</ArticleId></ArticleIdList></Reference><Reference><Citation>Ando T. High-Speed Atomic Force Microscopy in Biology. Springer; 2022.</Citation></Reference><Reference><Citation>Kodera N, Yamamoto D, Ishikawa R, Ando T. Video imaging of walking myosin V by high-speed atomic force microscopy. Nature. 2010;468:72&#x2013;76. doi: 10.1038/nature09450.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nature09450</ArticleId><ArticleId IdType="pubmed">20935627</ArticleId></ArticleIdList></Reference><Reference><Citation>Uchihashi T, Iino R, Ando T, Noji H. High-speed atomic force microscopy reveals rotary catalysis of rotorless F1-ATPase. Science. 2011;333:755&#x2013;758. doi: 10.1126/science.1205510.</Citation><ArticleIdList><ArticleId IdType="doi">10.1126/science.1205510</ArticleId><ArticleId IdType="pubmed">21817054</ArticleId></ArticleIdList></Reference><Reference><Citation>Kodera N, et al. Structural and dynamics analysis of intrinsically disordered proteins by high-speed atomic force microscopy. Nat. Nanotechnol. 2021;16:181&#x2013;189. doi: 10.1038/s41565-020-00798-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41565-020-00798-9</ArticleId><ArticleId IdType="pubmed">33230318</ArticleId></ArticleIdList></Reference><Reference><Citation>Shibata M, et al. Real-space and real-time dynamics of CRISPR-Cas9 visualized by high-speed atomic force microscopy. Nat. Commun. 2017;8:1430. doi: 10.1038/s41467-017-01466-8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41467-017-01466-8</ArticleId><ArticleId IdType="pmc">PMC5681550</ArticleId><ArticleId IdType="pubmed">29127285</ArticleId></ArticleIdList></Reference><Reference><Citation>Ando T. High-speed atomic force microscopy and its future prospects. Biophys. Rev. 2018;10:285&#x2013;292. doi: 10.1007/s12551-017-0356-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s12551-017-0356-5</ArticleId><ArticleId IdType="pmc">PMC5899716</ArticleId><ArticleId IdType="pubmed">29256119</ArticleId></ArticleIdList></Reference><Reference><Citation>Villarrubia JS. Algorithms for scanned probe microscope image simulation, surface reconstruction, and tip estimation. J. Res. Natl. Inst. Stand. Technol. 1997;102:425. doi: 10.6028/jres.102.030.</Citation><ArticleIdList><ArticleId IdType="doi">10.6028/jres.102.030</ArticleId><ArticleId IdType="pmc">PMC4882144</ArticleId><ArticleId IdType="pubmed">27805154</ArticleId></ArticleIdList></Reference><Reference><Citation>Keller D. Reconstruction of STM and AFM images distorted by finite-size tips. Surf. Sci. 1991;253:353&#x2013;364. doi: 10.1016/0039-6028(91)90606-S.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/0039-6028(91)90606-S</ArticleId></ArticleIdList></Reference><Reference><Citation>Keller DJ, Franke FS. Envelope reconstruction of probe microscope images. Surf. Sci. 1993;294:409&#x2013;419. doi: 10.1016/0039-6028(93)90126-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/0039-6028(93)90126-5</ArticleId></ArticleIdList></Reference><Reference><Citation>Ando T, et al. High-speed atomic force microscopy for capturing dynamic behavior of protein molecules at work. J. Surf. Sci. Nanotechnol. 2005;3:384&#x2013;392. doi: 10.1380/ejssnt.2005.384.</Citation><ArticleIdList><ArticleId IdType="doi">10.1380/ejssnt.2005.384</ArticleId></ArticleIdList></Reference><Reference><Citation>Scheuring S, Busselez J, L&#xe9;vy D. Structure of the dimeric PufX-containing core complex of rhodobacter blasticus by in situ atomic force microscopy. J. Biol. Chem. 2005;280:1426&#x2013;1431. doi: 10.1074/jbc.M411334200.</Citation><ArticleIdList><ArticleId IdType="doi">10.1074/jbc.M411334200</ArticleId><ArticleId IdType="pubmed">15522874</ArticleId></ArticleIdList></Reference><Reference><Citation>Scheuring S, Boudier T, Sturgis JN. From high-resolution AFM topographs to atomic models of supramolecular assemblies. J. Struct. Biol. 2007;159:268&#x2013;276. doi: 10.1016/j.jsb.2007.01.021.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jsb.2007.01.021</ArticleId><ArticleId IdType="pubmed">17399998</ArticleId></ArticleIdList></Reference><Reference><Citation>Asakawa H, et al. Submolecular-scale imaging of &#x3b1;-helices and C-terminal domains of tubulins by frequency modulation atomic force microscopy in liquid. Biophys. J. 2011;101:1270&#x2013;1276. doi: 10.1016/j.bpj.2011.07.020.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bpj.2011.07.020</ArticleId><ArticleId IdType="pmc">PMC3164191</ArticleId><ArticleId IdType="pubmed">21889465</ArticleId></ArticleIdList></Reference><Reference><Citation>Trinh M-H, et al. Computational reconstruction of multidomain proteins using atomic force microscopy data. Structure. 2012;20:113&#x2013;120. doi: 10.1016/j.str.2011.10.023.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.str.2011.10.023</ArticleId><ArticleId IdType="pmc">PMC3264848</ArticleId><ArticleId IdType="pubmed">22244760</ArticleId></ArticleIdList></Reference><Reference><Citation>Chaves RC, et al. Conformational dynamics of individual antibodies using computational docking and AFM: Conformational dynamics of IGG using docking and AFM. J. Mol. Recognit. 2013;26:596&#x2013;604. doi: 10.1002/jmr.2310.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmr.2310</ArticleId><ArticleId IdType="pubmed">24089367</ArticleId></ArticleIdList></Reference><Reference><Citation>Dasgupta B, Miyashita O, Tama F. Reconstruction of low-resolution molecular structures from simulated atomic force microscopy images. Biochim. Biophys. Acta Gen. Subj. 2020;1864:129420. doi: 10.1016/j.bbagen.2019.129420.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bbagen.2019.129420</ArticleId><ArticleId IdType="pubmed">31472175</ArticleId></ArticleIdList></Reference><Reference><Citation>Amyot R, Flechsig H. BioAFMviewer: An interactive interface for simulated AFM scanning of biomolecular structures and dynamics. PLoS Comput. Biol. 2020;16:e1008444. doi: 10.1371/journal.pcbi.1008444.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pcbi.1008444</ArticleId><ArticleId IdType="pmc">PMC7710046</ArticleId><ArticleId IdType="pubmed">33206646</ArticleId></ArticleIdList></Reference><Reference><Citation>Niina T, Matsunaga Y, Takada S. Rigid-body fitting to atomic force microscopy images for inferring probe shape and biomolecular structure. PLoS Comput. Biol. 2021;17:e1009215. doi: 10.1371/journal.pcbi.1009215.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pcbi.1009215</ArticleId><ArticleId IdType="pmc">PMC8323932</ArticleId><ArticleId IdType="pubmed">34283829</ArticleId></ArticleIdList></Reference><Reference><Citation>Amyot R, Marchesi A, Franz CM, Casuso I, Flechsig H. Simulation atomic force microscopy for atomic reconstruction of biomolecular structures from resolution-limited experimental images. PLoS Comput. Biol. 2022;18:e1009970. doi: 10.1371/journal.pcbi.1009970.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pcbi.1009970</ArticleId><ArticleId IdType="pmc">PMC8959186</ArticleId><ArticleId IdType="pubmed">35294442</ArticleId></ArticleIdList></Reference><Reference><Citation>Flater EE, Zacharakis-Jutz GE, Dumba BG, White IA, Clifford CA. Towards easy and reliable AFM tip shape determination using blind tip reconstruction. Ultramicroscopy. 2014;146:130&#x2013;143. doi: 10.1016/j.ultramic.2013.06.022.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ultramic.2013.06.022</ArticleId><ArticleId IdType="pubmed">24934394</ArticleId></ArticleIdList></Reference><Reference><Citation>Ruggeri FS, &#x160;neideris T, Vendruscolo M, Knowles TPJ. Atomic force microscopy for single molecule characterisation of protein aggregation. Arch. Biochem. Biophys. 2019;664:134&#x2013;148. doi: 10.1016/j.abb.2019.02.001.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.abb.2019.02.001</ArticleId><ArticleId IdType="pmc">PMC6420408</ArticleId><ArticleId IdType="pubmed">30742801</ArticleId></ArticleIdList></Reference><Reference><Citation>Takenaka H, et al. AFM tip characterizer fabricated by Si/SiO2 multilayers. J. Surf. Sci. Nanotechnol. 2011;9:293&#x2013;296. doi: 10.1380/ejssnt.2011.293.</Citation><ArticleIdList><ArticleId IdType="doi">10.1380/ejssnt.2011.293</ArticleId></ArticleIdList></Reference><Reference><Citation>Zeng Z, et al. A simple method for AFM tip characterization by polystyrene spheres. Ultramicroscopy. 2008;108:975&#x2013;980. doi: 10.1016/j.ultramic.2008.04.001.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ultramic.2008.04.001</ArticleId><ArticleId IdType="pubmed">18514419</ArticleId></ArticleIdList></Reference><Reference><Citation>Dongmo LS, et al. Experimental test of blind tip reconstruction for scanning probe microscopy. Ultramicroscopy. 2000;85:141&#x2013;153. doi: 10.1016/S0304-3991(00)00051-6.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0304-3991(00)00051-6</ArticleId></ArticleIdList></Reference><Reference><Citation>Tian F, Qian X, Villarrubia JS. Blind estimation of general tip shape in AFM imaging. Ultramicroscopy. 2008;109:44&#x2013;53. doi: 10.1016/j.ultramic.2008.08.002.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ultramic.2008.08.002</ArticleId><ArticleId IdType="pubmed">18835101</ArticleId></ArticleIdList></Reference><Reference><Citation>J&#xf3;&#x17a;wiak G, Henrykowski A, Masalska A, Gotszalk T. Regularization mechanism in blind tip reconstruction procedure. Ultramicroscopy. 2012;118:1&#x2013;10. doi: 10.1016/j.ultramic.2012.04.013.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ultramic.2012.04.013</ArticleId><ArticleId IdType="pubmed">22728398</ArticleId></ArticleIdList></Reference><Reference><Citation>Bakucz P, Kr&#xfc;ger-Sehm R, Koenders L. Investigation of blind tip estimation. Rev. Sci. Instrum. 2008;79:073703. doi: 10.1063/1.2901616.</Citation><ArticleIdList><ArticleId IdType="doi">10.1063/1.2901616</ArticleId><ArticleId IdType="pubmed">18681704</ArticleId></ArticleIdList></Reference><Reference><Citation>Goodfellow I, Bengio Y, Courville A. Deep Learning. MIT Press; 2016.</Citation></Reference><Reference><Citation>Schoenholz SS, Jax MD. A framework for differentiable physics. J. Stat. Mech. 2021;22:1&#x2013;10.</Citation></Reference><Reference><Citation>Greener JG, Jones DT. Differentiable molecular simulation can learn all the parameters in a coarse-grained force field for proteins. PLoS ONE. 2021;16:e0256990. doi: 10.1371/journal.pone.0256990.</Citation><ArticleIdList><ArticleId IdType="doi">10.1371/journal.pone.0256990</ArticleId><ArticleId IdType="pmc">PMC8412298</ArticleId><ArticleId IdType="pubmed">34473813</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhou T, Cherukara M, Phatak C. Differential programming enabled functional imaging with Lorentz transmission electron microscopy. NPJ Comput. Mater. 2021;7:141. doi: 10.1038/s41524-021-00600-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41524-021-00600-x</ArticleId></ArticleIdList></Reference><Reference><Citation>Villarrubia JS. Morphological Estimation of Tip Geometry for Scanned Probe Microscopy. Springer; 2022.</Citation></Reference><Reference><Citation>Franchi G, Fehri A, Yao A. Deep morphological networks. Pattern Recogn. 2020;102:107246. doi: 10.1016/j.patcog.2020.107246.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.patcog.2020.107246</ArticleId></ArticleIdList></Reference><Reference><Citation>Nakashizuka M, Ashihara Y, Iiguni Y. Morphological Regularization for Adaptation of Image Opening. Springer; 2022.</Citation></Reference><Reference><Citation>Breiman L, Friedman JH, Olshen RA, Stone CJ. Classification And Regression Trees. Routledge; 2017.</Citation></Reference><Reference><Citation>Innes M, et al. Fashionable modelling with Flux. Pattern Recogn. 2018 doi: 10.48550/ARXIV.1811.01457.</Citation><ArticleIdList><ArticleId IdType="doi">10.48550/ARXIV.1811.01457</ArticleId></ArticleIdList></Reference><Reference><Citation>Schr&#xf6;dinger, LLC. The PyMOL Molecular Graphics System. (2015).</Citation></Reference><Reference><Citation>Coureux P-D, et al. A structural state of the myosin V motor without bound nucleotide. Nature. 2003;425:419&#x2013;423. doi: 10.1038/nature01927.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nature01927</ArticleId><ArticleId IdType="pubmed">14508494</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen Y. Elucidation and identification of double-tip effects in atomic force microscopy studies of biological structures. JSEMAT. 2012;02:238&#x2013;247. doi: 10.4236/jsemat.2012.223037.</Citation><ArticleIdList><ArticleId IdType="doi">10.4236/jsemat.2012.223037</ArticleId></ArticleIdList></Reference><Reference><Citation>Kodera N, Sakashita M, Ando T. Dynamic proportional-integral-differential controller for high-speed atomic force microscopy. Rev. Sci. Instrum. 2006;77:083704. doi: 10.1063/1.2336113.</Citation><ArticleIdList><ArticleId IdType="doi">10.1063/1.2336113</ArticleId></ArticleIdList></Reference><Reference><Citation>Fuchigami S, Takada S. Inferring conformational state of myosin motor in an atomic force microscopy image via flexible fitting molecular simulations. Front. Mol. Biosci. 2022;9:882989. doi: 10.3389/fmolb.2022.882989.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fmolb.2022.882989</ArticleId><ArticleId IdType="pmc">PMC9100425</ArticleId><ArticleId IdType="pubmed">35573735</ArticleId></ArticleIdList></Reference><Reference><Citation>Akiba, T., Sano, S., Yanase, T., Ohta, T. &amp; Koyama, M. Optuna: A Next-generation Hyperparameter Optimization Framework. in Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining 2623&#x2013;2631 (ACM, 2019). 10.1145/3292500.3330701.</Citation></Reference><Reference><Citation>Sumikama T, Foster AS, Fukuma T. Computed atomic force microscopy images of chromosomes by calculating forces with oscillating probes. J. Phys. Chem. C. 2020;124(2213):2218.</Citation></Reference><Reference><Citation>Heath GR, et al. Localization atomic force microscopy. Nature. 2021;594:385&#x2013;390. doi: 10.1038/s41586-021-03551-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41586-021-03551-x</ArticleId><ArticleId IdType="pmc">PMC8697813</ArticleId><ArticleId IdType="pubmed">34135520</ArticleId></ArticleIdList></Reference><Reference><Citation>Loshchilov, I. &amp; Hutter, F. Decoupled Weight Decay Regularization. (2017) 10.48550/ARXIV.1711.05101.</Citation></Reference><Reference><Citation>Frames Catherine White et al.JuliaDiff/ChainRulesCore.jl: v1.15.2. (2022) 10.5281/ZENODO.6835210.</Citation></Reference><Reference><Citation>Innes, M. Don&#x2019;t Unroll Adjoint: Differentiating SSA-Form Programs. http://arxiv.org/abs/1810.07951 (2019).</Citation></Reference><Reference><Citation>Webb B, Sali A. Comparative protein structure modeling using MODELLER. Curr. Protoc. Bioinform. 2016;54:10. doi: 10.1002/cpbi.3.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/cpbi.3</ArticleId><ArticleId IdType="pmc">PMC5031415</ArticleId><ArticleId IdType="pubmed">27322406</ArticleId></ArticleIdList></Reference><Reference><Citation>Kenzaki H, et al. CafeMol: A coarse-grained biomolecular simulator for simulating proteins at work. J. Chem. Theory Comput. 2011;7:1979&#x2013;1989. doi: 10.1021/ct2001045.</Citation><ArticleIdList><ArticleId IdType="doi">10.1021/ct2001045</ArticleId><ArticleId IdType="pubmed">26596457</ArticleId></ArticleIdList></Reference><Reference><Citation>Li W, Wang W, Takada S. Energy landscape views for interplays among folding, binding, and allostery of calmodulin domains. Proc. Natl. Acad. Sci. USA. 2014;111:10550&#x2013;10555. doi: 10.1073/pnas.1402768111.</Citation><ArticleIdList><ArticleId IdType="doi">10.1073/pnas.1402768111</ArticleId><ArticleId IdType="pmc">PMC4115553</ArticleId><ArticleId IdType="pubmed">25002491</ArticleId></ArticleIdList></Reference><Reference><Citation>Niina T, Fuchigami S, Takada S. Flexible fitting of biomolecular structures to atomic force microscopy images via biased molecular simulations. J. Chem. Theory Comput. 2020;16:1349&#x2013;1358. doi: 10.1021/acs.jctc.9b00991.</Citation><ArticleIdList><ArticleId IdType="doi">10.1021/acs.jctc.9b00991</ArticleId><ArticleId IdType="pubmed">31909999</ArticleId></ArticleIdList></Reference><Reference><Citation>Cossio P, Hummer G. Bayesian analysis of individual electron microscopy images: Towards structures of dynamic and heterogeneous biomolecular assemblies. J. Struct. Biol. 2013;184:427&#x2013;437. doi: 10.1016/j.jsb.2013.10.006.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jsb.2013.10.006</ArticleId><ArticleId IdType="pmc">PMC3855270</ArticleId><ArticleId IdType="pubmed">24161733</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36598691</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>06</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>06</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1875-8312</ISSN><JournalIssue CitedMedium="Internet"><Volume>39</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month></PubDate></JournalIssue><Title>The international journal of cardiovascular imaging</Title><ISOAbbreviation>Int J Cardiovasc Imaging</ISOAbbreviation></Journal><ArticleTitle>Systematic assessment of coronary calcium detectability and quantification on four generations of CT reconstruction techniques: a patient and phantom study.</ArticleTitle><Pagination><StartPage>221</StartPage><EndPage>231</EndPage><MedlinePgn>221-231</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1007/s10554-022-02703-y</ELocationID><Abstract><AbstractText>In computed tomography, coronary artery calcium (CAC) scores are influenced by image reconstruction. The effect of a newly introduced deep learning-based reconstruction (DLR) on CAC scoring in relation to other algorithms is unknown. The aim of this study was to evaluate the effect of four generations of image reconstruction techniques (filtered back projection (FBP), hybrid iterative reconstruction (HIR), model-based iterative reconstruction (MBIR), and DLR) on CAC detectability, quantification, and risk classification. First, CAC detectability was assessed with a dedicated static phantom containing 100 small calcifications varying in size and density. Second, CAC quantification was assessed with a dynamic coronary phantom with velocities equivalent to heart rates of 60-75&#xa0;bpm. Both phantoms were scanned and reconstructed with four techniques. Last, scans of fifty patients were included and the Agatston calcium score was calculated for all four reconstruction techniques. FBP was used as a reference. In the phantom studies, all reconstruction techniques resulted in less detected small calcifications, up to 22%. No clinically relevant quantification changes occurred with different reconstruction techniques (less than 10%). In the patient study, the cardiovascular risk classification resulted, for all reconstruction techniques, in excellent agreement with the reference (&#x3ba;&#x2009;=&#x2009;0.96-0.97). However, MBIR resulted in significantly higher Agatston scores (61 (5.5-435.0) vs. 81.5 (9.25-435.0); p&#x2009;&lt;&#x2009;0.001) and 6% reclassification rate. In conclusion, HIR and DLR reconstructed scans resulted in similar Agatston scores with excellent agreement and low-risk reclassification rate compared with routine reconstructed scans (FBP). However, caution should be taken with low Agatston scores, as based on phantom study, detectability of small calcifications varies with the used reconstruction algorithm, especially with MBIR and DLR.</AbstractText><CopyrightInformation>&#xa9; 2022. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y" EqualContrib="Y"><LastName>Dobrolinska</LastName><ForeName>M M</ForeName><Initials>MM</Initials><AffiliationInfo><Affiliation>Department of Nuclear Medicine and Molecular Imaging, Medical Imaging Center, University Medical Center Groningen, University of Groningen, Hanzeplein 1, PO 9700 RB, Groningen, The Netherlands. Magdalena.dobrolinska@gmail.com.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y" EqualContrib="Y"><LastName>van Praagh</LastName><ForeName>G D</ForeName><Initials>GD</Initials><AffiliationInfo><Affiliation>Department of Nuclear Medicine and Molecular Imaging, Medical Imaging Center, University Medical Center Groningen, University of Groningen, Hanzeplein 1, PO 9700 RB, Groningen, The Netherlands.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, Stanford University School of Medicine, Stanford, CA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y" EqualContrib="Y"><LastName>Oostveen</LastName><ForeName>L J</ForeName><Initials>LJ</Initials><AffiliationInfo><Affiliation>Department of Medical Imaging, Radboud University Medical Center, Nijmegen, The Netherlands.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Poelhekken</LastName><ForeName>K</ForeName><Initials>K</Initials><AffiliationInfo><Affiliation>Department of Radiology, Medical Imaging Center, University Medical Center Groningen, University of Groningen, Groningen, The Netherlands.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Greuter</LastName><ForeName>M J W</ForeName><Initials>MJW</Initials><AffiliationInfo><Affiliation>Department of Radiology, Medical Imaging Center, University Medical Center Groningen, University of Groningen, Groningen, The Netherlands.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Robotics and Mechatronics, University of Twente, Enschede, The Netherlands.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Fleischmann</LastName><ForeName>D</ForeName><Initials>D</Initials><AffiliationInfo><Affiliation>Department of Radiology, Stanford University School of Medicine, Stanford, CA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Willemink</LastName><ForeName>M J</ForeName><Initials>MJ</Initials><AffiliationInfo><Affiliation>Department of Radiology, Stanford University School of Medicine, Stanford, CA, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>de Lange</LastName><ForeName>F</ForeName><Initials>F</Initials><AffiliationInfo><Affiliation>Department of Medical Imaging, Radboud University Medical Center, Nijmegen, The Netherlands.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Slart</LastName><ForeName>R H J A</ForeName><Initials>RHJA</Initials><AffiliationInfo><Affiliation>Department of Nuclear Medicine and Molecular Imaging, Medical Imaging Center, University Medical Center Groningen, University of Groningen, Hanzeplein 1, PO 9700 RB, Groningen, The Netherlands.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Biomedical Photonic Imaging, Faculty of Science and Technology, University of Twente, Enschede, The Netherlands.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Leiner</LastName><ForeName>T</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>Department of Radiology, University Medical Center Utrecht, Utrecht, The Netherlands.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>van der Werf</LastName><ForeName>N R</ForeName><Initials>NR</Initials><AffiliationInfo><Affiliation>Department of Radiology, University Medical Center Utrecht, Utrecht, The Netherlands.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology &amp; Nuclear Medicine, Erasmus University Medical Center, Rotterdam, The Netherlands.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>08</Month><Day>13</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>Int J Cardiovasc Imaging</MedlineTA><NlmUniqueID>100969716</NlmUniqueID><ISSNLinking>1569-5794</ISSNLinking></MedlineJournalInfo><ChemicalList><Chemical><RegistryNumber>SY7Q814VUP</RegistryNumber><NameOfSubstance UI="D002118">Calcium</NameOfSubstance></Chemical></ChemicalList><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D003324" MajorTopicYN="Y">Coronary Artery Disease</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D002118" MajorTopicYN="N">Calcium</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D011237" MajorTopicYN="N">Predictive Value of Tests</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D014057" MajorTopicYN="N">Tomography, X-Ray Computed</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D002114" MajorTopicYN="Y">Calcinosis</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D019047" MajorTopicYN="N">Phantoms, Imaging</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000465" MajorTopicYN="N">Algorithms</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D011829" MajorTopicYN="N">Radiation Dosage</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D011857" MajorTopicYN="N">Radiographic Image Interpretation, Computer-Assisted</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Calcification</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Image reconstruction</Keyword><Keyword MajorTopicYN="N">Imaging</Keyword><Keyword MajorTopicYN="N">Phantoms</Keyword><Keyword MajorTopicYN="N">Radiation dosage</Keyword><Keyword MajorTopicYN="N">Tomography</Keyword><Keyword MajorTopicYN="N">X-ray computed</Keyword></KeywordList><CoiStatement>None.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>6</Month><Day>3</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>7</Month><Day>24</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>4</Day><Hour>11</Hour><Minute>22</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>5</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>ppublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36598691</ArticleId><ArticleId IdType="doi">10.1007/s10554-022-02703-y</ArticleId><ArticleId IdType="pii">10.1007/s10554-022-02703-y</ArticleId><ArticleId IdType="pmc">PMC9813085</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Knuuti J, Wijns W, Achenbach S, et al. 2019 ESC guidelines for the diagnosis and management of chronic coronary syndromes. Eur Heart J. 2020;41(3):407&#x2013;477. doi: 10.1093/eurheartj/ehz425.</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/eurheartj/ehz425</ArticleId><ArticleId IdType="pubmed">31504439</ArticleId></ArticleIdList></Reference><Reference><Citation>Agatston AS, Janowitz WR, Hildner FJ, Zusmer NR, Viamonte M, Detrano R. Quantification of coronary artery calcium using ultrafast computed tomography. J Am Coll Cardiol. 1990;15(4):827&#x2013;832. doi: 10.1016/0735-1097(90)90282-T.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/0735-1097(90)90282-T</ArticleId><ArticleId IdType="pubmed">2407762</ArticleId></ArticleIdList></Reference><Reference><Citation>Blaha MJ, Cainzos-Achirica M, Greenland P, et al. Role of coronary artery calcium score of zero and other negative risk markers for cardiovascular disease : the multi-ethnic study of atherosclerosis (MESA) Circulation. 2016 doi: 10.1161/CIRCULATIONAHA.115.018524.</Citation><ArticleIdList><ArticleId IdType="doi">10.1161/CIRCULATIONAHA.115.018524</ArticleId><ArticleId IdType="pmc">PMC4775391</ArticleId><ArticleId IdType="pubmed">26801055</ArticleId></ArticleIdList></Reference><Reference><Citation>Kurata A, Dharampal A, Dedic A, et al. Impact of iterative reconstruction on CT coronary calcium quantification. Eur Radiol. 2013;23(12):3246&#x2013;3252. doi: 10.1007/s00330-013-3022-8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-013-3022-8</ArticleId><ArticleId IdType="pubmed">24057202</ArticleId></ArticleIdList></Reference><Reference><Citation>Willemink MJ, No&#xeb;l PB. The evolution of image reconstruction for CT-from filtered back projection to artificial intelligence. Eur Radiol. 2019;29(5):2185&#x2013;2195. doi: 10.1007/s00330-018-5810-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-018-5810-7</ArticleId><ArticleId IdType="pmc">PMC6443602</ArticleId><ArticleId IdType="pubmed">30377791</ArticleId></ArticleIdList></Reference><Reference><Citation>Xu F, Mueller K. Real-time 3D computed tomographic reconstruction using commodity graphics hardware. Phys Med Biol. 2007 doi: 10.1088/0031-9155/52/12/006.</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/0031-9155/52/12/006</ArticleId><ArticleId IdType="pubmed">17664551</ArticleId></ArticleIdList></Reference><Reference><Citation>Willemink MJ, De Jong PA, Leiner T, et al. Iterative reconstruction techniques for computed tomography Part 1: technical principles. Eur Radiol. 2013 doi: 10.1007/s00330-012-2765-y.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-012-2765-y</ArticleId><ArticleId IdType="pubmed">23314600</ArticleId></ArticleIdList></Reference><Reference><Citation>Choi AD, Leifer ES, Yu JH, et al. Reduced radiation dose with model based iterative reconstruction coronary artery calcium scoring. Eur J Radiol. 2019;111:1&#x2013;5. doi: 10.1016/j.ejrad.2018.12.010.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejrad.2018.12.010</ArticleId><ArticleId IdType="pubmed">30691659</ArticleId></ArticleIdList></Reference><Reference><Citation>Tatsugami F, Higaki T, Fukumoto W, et al. Radiation dose reduction for coronary artery calcium scoring at 320-detector CT with adaptive iterative dose reduction 3D. Int J Cardiovasc Imaging. 2015 doi: 10.1007/s10554-015-0637-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10554-015-0637-7</ArticleId><ArticleId IdType="pubmed">25754302</ArticleId></ArticleIdList></Reference><Reference><Citation>Tang YC, Liu YC, Hsu MY, Tsai HY, Chen CM. Adaptive iterative dose reduction 3D integrated with automatic tube current modulation for CT coronary artery calcium quantification: comparison to traditional filtered back projection in an anthropomorphic phantom and patients. Acad Radiol. 2018 doi: 10.1016/j.acra.2017.12.018.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.acra.2017.12.018</ArticleId><ArticleId IdType="pubmed">29395796</ArticleId></ArticleIdList></Reference><Reference><Citation>Akagi M, Nakamura Y, Higaki T, et al. Deep learning reconstruction improves image quality of abdominal ultra-high-resolution CT. Eur Radiol. 2019 doi: 10.1007/s00330-019-06170-3.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-019-06170-3</ArticleId><ArticleId IdType="pubmed">30976831</ArticleId></ArticleIdList></Reference><Reference><Citation>Geyer LL, Schoepf UJ, Meinel FG, et al. State of the art: iterative CT reconstruction techniques. Radiology. 2015;276(2):339&#x2013;357. doi: 10.1148/radiol.2015132766.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2015132766</ArticleId><ArticleId IdType="pubmed">26203706</ArticleId></ArticleIdList></Reference><Reference><Citation>Higaki T, Nakamura Y, Zhou J, et al. Deep learning reconstruction at CT: phantom study of the image characteristics. Acad Radiol. 2020 doi: 10.1016/j.acra.2019.09.008.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.acra.2019.09.008</ArticleId><ArticleId IdType="pubmed">31818389</ArticleId></ArticleIdList></Reference><Reference><Citation>Hecht HS, Cronin P, Blaha MJ, et al. SCCT/STR guidelines for coronary artery calcium scoring of noncontrast noncardiac chest CT scans: a report of the society of cardiovascular computed tomography and society of thoracic radiology. J Cardiovasc Comput Tomogr. 2016 doi: 10.1016/j.jcct.2016.11.003.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jcct.2016.11.003</ArticleId><ArticleId IdType="pubmed">27916431</ArticleId></ArticleIdList></Reference><Reference><Citation>Hecht H, Blaha MJ, Berman DS, et al. Clinical indications for coronary artery calcium scoring in asymptomatic patients: expert consensus statement from the society of cardiovascular computed tomography. J Cardiovasc Comput Tomogr. 2017 doi: 10.1016/j.jcct.2017.02.010.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jcct.2017.02.010</ArticleId><ArticleId IdType="pubmed">28283309</ArticleId></ArticleIdList></Reference><Reference><Citation>McCollough CH, Ulzheimer S, Halliburton SS, Shanneik K, White RD, Kalender WA. Coronary artery calcium: a multi-institutional, multimanufacturer international standard for quantification at cardiac CT. Radiology. 2007;243(2):527&#x2013;538. doi: 10.1148/radiol.2432050808.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2432050808</ArticleId><ArticleId IdType="pubmed">17456875</ArticleId></ArticleIdList></Reference><Reference><Citation>Groen JM, Kofoed KF, Zacho M, Vliegenthart R, Willems TP, Greuter MJW. Calcium score of small coronary calcifications on multidetector computed tomography: results from a static phantom study. Eur J Radiol. 2013;82(2):e58&#x2013;63. doi: 10.1016/j.ejrad.2012.09.018.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejrad.2012.09.018</ArticleId><ArticleId IdType="pubmed">23092538</ArticleId></ArticleIdList></Reference><Reference><Citation>Husmann L, Leschka S, Desbiolles L, et al. Coronary artery motion and cardiac phases: dependency on heart rate&#x2014;implications for CT image reconstruction. Radiology. 2007;245(2):567&#x2013;576. doi: 10.1148/radiol.2451061791.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2451061791</ArticleId><ArticleId IdType="pubmed">17848683</ArticleId></ArticleIdList></Reference><Reference><Citation>van der Werf NR, Willemink MJ, Willems TP, Vliegenthart R, Greuter MJW, Leiner T. Influence of heart rate on coronary calcium scores: a multi-manufacturer phantom study. Int J Cardiovasc Imaging. 2018;34(6):959&#x2013;966. doi: 10.1007/s10554-017-1293-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10554-017-1293-x</ArticleId><ArticleId IdType="pubmed">29285727</ArticleId></ArticleIdList></Reference><Reference><Citation>van Praagh GD, van der Werf NR, Wang J, et al. Fully automated quantification method (FQM) of coronary calcium in an anthropomorphic phantom. Med Phys. 2021 doi: 10.1002/mp.14912.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mp.14912</ArticleId><ArticleId IdType="pmc">PMC8360117</ArticleId><ArticleId IdType="pubmed">33932026</ArticleId></ArticleIdList></Reference><Reference><Citation>Booij R, van der Werf NR, Budde RPJ, Bos D, van Straten M. Dose reduction for CT coronary calcium scoring with a calcium-aware image reconstruction technique: a phantom study. Eur Radiol. 2020 doi: 10.1007/s00330-020-06709-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-020-06709-9</ArticleId><ArticleId IdType="pmc">PMC7248036</ArticleId><ArticleId IdType="pubmed">32072259</ArticleId></ArticleIdList></Reference><Reference><Citation>van der Werf NR, Booij R, Schmidt B, et al. Evaluating a calcium-aware kernel for CT CAC scoring with varying surrounding materials and heart rates: a dynamic phantom study. Eur Radiol. 2021 doi: 10.1007/s00330-021-08076-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-021-08076-5</ArticleId><ArticleId IdType="pmc">PMC8589753</ArticleId><ArticleId IdType="pubmed">34050386</ArticleId></ArticleIdList></Reference><Reference><Citation>Detrano R, Guerci AD, Carr JJ, et al. Coronary calcium as a predictor of coronary events in four racial or ethnic groups. N Engl J Med. 2008 doi: 10.1056/nejmoa072100.</Citation><ArticleIdList><ArticleId IdType="doi">10.1056/nejmoa072100</ArticleId><ArticleId IdType="pubmed">18367736</ArticleId></ArticleIdList></Reference><Reference><Citation>Trattner S, Halliburton S, Thompson CM, et al. Cardiac-specific conversion factors to estimate radiation effective dose from dose-length product in computed tomography. JACC Cardiovasc Imaging. 2018;11(1):64&#x2013;74. doi: 10.1016/j.jcmg.2017.06.006.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jcmg.2017.06.006</ArticleId><ArticleId IdType="pmc">PMC5756125</ArticleId><ArticleId IdType="pubmed">28823748</ArticleId></ArticleIdList></Reference><Reference><Citation>van der Werf NR, Willemink MJ, Willems TP, Greuter MJW, Leiner T. Influence of iterative reconstruction on coronary calcium scores at multiple heart rates: a multivendor phantom study on state-of-the-art CT systems. Int J Cardiovasc Imaging. 2018;34(6):947&#x2013;957. doi: 10.1007/s10554-017-1292-y.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10554-017-1292-y</ArticleId><ArticleId IdType="pubmed">29285725</ArticleId></ArticleIdList></Reference><Reference><Citation>Groen JM, Greuter MJ, Schmidt B, Suess C, Vliegenthart R, Oudkerk M. The Influence of heart rate, slice thickness, and calcification density on calcium scores using 64-slice multidetector computed tomography: a systematic phantom study. Invest Radiol. 2007;42(12):848. doi: 10.1097/RLI.0b013e318154c549.</Citation><ArticleIdList><ArticleId IdType="doi">10.1097/RLI.0b013e318154c549</ArticleId><ArticleId IdType="pubmed">18007157</ArticleId></ArticleIdList></Reference><Reference><Citation>Han D, Klein E, Friedman J, et al. Prognostic significance of subtle coronary calcification in patients with zero coronary artery calcium score: from the CONFIRM registry. Atherosclerosis. 2020;309:33&#x2013;38. doi: 10.1016/j.atherosclerosis.2020.07.011.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.atherosclerosis.2020.07.011</ArticleId><ArticleId IdType="pubmed">32862086</ArticleId></ArticleIdList></Reference><Reference><Citation>Szilveszter B, Elzomor H, K&#xe1;rolyi M, et al. The effect of iterative model reconstruction on coronary artery calcium quantification. Int J Cardiovasc Imaging. 2016;32(1):153&#x2013;160. doi: 10.1007/s10554-015-0740-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10554-015-0740-9</ArticleId><ArticleId IdType="pubmed">26285899</ArticleId></ArticleIdList></Reference><Reference><Citation>Willemink MJ, Vliegenthart R, Takx RAP, et al. Coronary artery calcification scoring with state-of-the-art ct scanners from different vendors has substantial effect on risk classification. Radiology. 2014;273(3):695&#x2013;702. doi: 10.1148/radiol.14140066.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.14140066</ArticleId><ArticleId IdType="pubmed">25153157</ArticleId></ArticleIdList></Reference><Reference><Citation>Osei AD, Mirbolouk M, Berman D, et al. Prognostic value of coronary artery calcium score, area, and density among individuals on statin therapy vs. non-users: the coronary artery calcium consortium. Atherosclerosis. 2021;316:79&#x2013;83. doi: 10.1016/j.atherosclerosis.2020.10.009.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.atherosclerosis.2020.10.009</ArticleId><ArticleId IdType="pmc">PMC7770042</ArticleId><ArticleId IdType="pubmed">33121743</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Curated"><PMID Version="1">36598690</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>10</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1875-8312</ISSN><JournalIssue CitedMedium="Internet"><Volume>39</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month></PubDate></JournalIssue><Title>The international journal of cardiovascular imaging</Title><ISOAbbreviation>Int J Cardiovasc Imaging</ISOAbbreviation></Journal><ArticleTitle>The mechanisms of arterial signal intensity profile in non-contrast coronary MRA (NC-MRCA): a 3D printed phantom investigation and clinical translations.</ArticleTitle><Pagination><StartPage>209</StartPage><EndPage>220</EndPage><MedlinePgn>209-220</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1007/s10554-022-02700-1</ELocationID><Abstract><AbstractText>Signal intensity (SI) drop has been proposed as an indirect stenosis assessment in non-contrast coronary MRA (NC-MRCA) but it uses unproven assumptions. We aimed to clarify the mechanisms that govern the SI in vitro and develop a stenosis detection method in vivo. Flow phantom tubes with/without stenosis were scanned under two spatial resolutions (0.5/1.0&#xa0;mm<sup>3</sup>) on a 3.0&#xa0;T MRI. Thirty-two coronary arteries from 11 volunteers were prospectively scanned with an EKG- and respiratory-gated 3D NC-MRCA with a resolution of 1.0&#xa0;mm<sup>3</sup>, with coronary computed tomography angiography (CTA) as reference. The normalized SI along the centerline of the tubes or the coronary arteries was assessed against the distance from the orifice using a linear regression model. Its coefficient (SI decay slope) and goodness-of-fit (R2) were extracted to assess the effect of flow velocity and stenosis on the SI profile curve. The R2 was utilized for the stenosis detection. Phantom study: A slow flow velocity caused a steep SI decay slope. The SI drop revealed only at the inlet and outlet of stenosis due to the flow turbulence/vortex and yielded low R2, in which shape changed by the resolution. Clinical study: The R2 cutoff to detect&#x2009;&#x2265;&#x2009;50% stenosis for the left and right coronary arteries were 0.64 and 0.20 with a sensitivity/specificity of 71.5/71.5 and 66.7/100 (%), respectively. The SI drop did not reflect the actual stenosis position and not suitable for the stenosis localization. The R2 cutoff represents an alternative method to detect stenoses on NC-MRCA at vessel level.Trial registration: ClinicalTrials.gov; NCT03768999, registered on December 7, 2018.</AbstractText><CopyrightInformation>&#xa9; 2022. The Author(s), under exclusive licence to Springer Nature B.V.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Kato</LastName><ForeName>Yoko</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Division of Cardiology, Johns Hopkins University School of Medicine, Baltimore, MD, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Noda</LastName><ForeName>Chikara</ForeName><Initials>C</Initials><AffiliationInfo><Affiliation>Division of Cardiology, Johns Hopkins University School of Medicine, Baltimore, MD, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Ambale-Venkatesh</LastName><ForeName>Bharath</ForeName><Initials>B</Initials><AffiliationInfo><Affiliation>Division of Radiology, Johns Hopkins University School of Medicine, Baltimore, MD, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Ortman</LastName><ForeName>Jason M</ForeName><Initials>JM</Initials><AffiliationInfo><Affiliation>Division of Cardiology, Johns Hopkins University School of Medicine, Baltimore, MD, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Kassai</LastName><ForeName>Yoshimori</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Canon Medical Systems Corporation, 1385 Shimoishigami, Otawara-shi, Tochigi, 324-8550, Japan.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lima</LastName><ForeName>Joao A C</ForeName><Initials>JAC</Initials><AffiliationInfo><Affiliation>Division of Cardiology, Johns Hopkins University School of Medicine, Baltimore, MD, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Liu</LastName><ForeName>Chia-Ying</ForeName><Initials>CY</Initials><AffiliationInfo><Affiliation>Canon Medical Systems Corporation, 1385 Shimoishigami, Otawara-shi, Tochigi, 324-8550, Japan. cliu@mru.medical.canon.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><DataBankList CompleteYN="Y"><DataBank><DataBankName>ClinicalTrials.gov</DataBankName><AccessionNumberList><AccessionNumber>NCT03768999</AccessionNumber></AccessionNumberList></DataBank></DataBankList><PublicationTypeList><PublicationType UI="D000068397">Clinical Study</PublicationType><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>08</Month><Day>11</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>Int J Cardiovasc Imaging</MedlineTA><NlmUniqueID>100969716</NlmUniqueID><ISSNLinking>1569-5794</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D003251" MajorTopicYN="N">Constriction, Pathologic</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D018810" MajorTopicYN="Y">Magnetic Resonance Angiography</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D011237" MajorTopicYN="N">Predictive Value of Tests</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D066330" MajorTopicYN="N">Printing, Three-Dimensional</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D012680" MajorTopicYN="N">Sensitivity and Specificity</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D014057" MajorTopicYN="Y">Tomography, X-Ray Computed</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">3D-printed phantom</Keyword><Keyword MajorTopicYN="N">Coronary stenosis detection</Keyword><Keyword MajorTopicYN="N">Deep learning-based denoising</Keyword><Keyword MajorTopicYN="N">Non-contrast coronary MRA</Keyword><Keyword MajorTopicYN="N">SI profile curve</Keyword><Keyword MajorTopicYN="N">Transluminal attenuation gradient (TAG)</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>3</Month><Day>9</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>7</Month><Day>22</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>4</Day><Hour>11</Hour><Minute>22</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>5</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>ppublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36598690</ArticleId><ArticleId IdType="doi">10.1007/s10554-022-02700-1</ArticleId><ArticleId IdType="pii">10.1007/s10554-022-02700-1</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Benjamin EJ, Muntner P, Alonso A et al (2019) Heart disease and stroke statistics-2019 update: a report from the American Heart Association. Circulation 139:e56&#x2013;e66. https://doi.org/10.1161/CIR.0000000000000659</Citation><ArticleIdList><ArticleId IdType="doi">10.1161/CIR.0000000000000659</ArticleId></ArticleIdList></Reference><Reference><Citation>Foley DP, Escaned J, Strauss BH et al (1994) Quantitative coronary angiography (QCA) in interventional cardiology: clinical application of QCA measurements. Prog Cardiovasc Dis 36:363&#x2013;384. https://doi.org/10.1016/S0033-0620(05)80027-1</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0033-0620(05)80027-1</ArticleId></ArticleIdList></Reference><Reference><Citation>Miller JM, Rochitte CE, Dewey M et al (2008) Diagnostic performance of coronary angiography by 64-row CT. N Engl J Med 359:2324&#x2013;2336. https://doi.org/10.1056/NEJMoa0806576</Citation><ArticleIdList><ArticleId IdType="doi">10.1056/NEJMoa0806576</ArticleId></ArticleIdList></Reference><Reference><Citation>Gulati M, Levy PD, Mukherjee D et al (2021) 2021 AHA/ACC/ASE/CHEST/SAEM/SCCT/SCMR guideline for the evaluation and diagnosis of chest pain: a report of the American College of Cardiology/American Heart Association Joint Committee on Clinical Practice Guidelines. Circulation 144:e368&#x2013;e454. https://doi.org/10.1161/CIR.0000000000001029</Citation><ArticleIdList><ArticleId IdType="doi">10.1161/CIR.0000000000001029</ArticleId></ArticleIdList></Reference><Reference><Citation>Bech GJ, De Bruyne B, Pijls NH et al (2001) Fractional flow reserve to determine the appropriateness of angioplasty in moderate coronary stenosis: a randomized trial. Circulation 103:2928&#x2013;2934. https://doi.org/10.1161/01.CIR.103.24.2928</Citation><ArticleIdList><ArticleId IdType="doi">10.1161/01.CIR.103.24.2928</ArticleId></ArticleIdList></Reference><Reference><Citation>Pijls NHJ, van Schaardenburgh P, Manoharan G et al (2007) Percutaneous coronary intervention of functionally nonsignificant stenosis: 5-year follow-up of the DEFER Study. J Am Coll Cardiol 49:2105&#x2013;2111. https://doi.org/10.1016/j.jacc.2007.01.087</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jacc.2007.01.087</ArticleId></ArticleIdList></Reference><Reference><Citation>Tonino PAL, De Bruyne B, Pijls NHJ et al (2009) Fractional flow reserve versus angiography for guiding percutaneous coronary intervention. N Engl J Med 360:213&#x2013;224. https://doi.org/10.1056/NEJMoa0807611</Citation><ArticleIdList><ArticleId IdType="doi">10.1056/NEJMoa0807611</ArticleId></ArticleIdList></Reference><Reference><Citation>Rochitte CE, George RT, Chen MY et al (2014) Computed tomography angiography and perfusion to assess coronary artery stenosis causing perfusion defects by single photon emission computed tomography: the CORE320 study. Eur Heart J 35:1120&#x2013;1130. https://doi.org/10.1093/eurheartj/eht488</Citation><ArticleIdList><ArticleId IdType="doi">10.1093/eurheartj/eht488</ArticleId></ArticleIdList></Reference><Reference><Citation>Kishi S, Giannopoulos AA, Tang A et al (2018) Fractional flow reserve estimated at coronary CT angiography in intermediate lesions: comparison of diagnostic accuracy of different methods to determine coronary flow distribution. Radiology 287:76&#x2013;84. https://doi.org/10.1148/radiol.2017162620</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2017162620</ArticleId></ArticleIdList></Reference><Reference><Citation>Lardo AC, Rahsepar AA, Seo JH et al (2015) Estimating coronary blood flow using CT transluminal attenuation flow encoding: formulation, preclinical validation, and clinical feasibility. J Cardiovasc Comput Tomogr 9:559&#x2013;566. https://doi.org/10.1016/j.jcct.2015.03.018</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jcct.2015.03.018</ArticleId></ArticleIdList></Reference><Reference><Citation>Wong DTL, Ko BS, Cameron JD et al (2013) Transluminal attenuation gradient in coronary computed tomography angiography is a novel noninvasive approach to the identification of functionally significant coronary artery stenosis: a comparison with fractional flow reserve. J Am Coll Cardiol 61:1271&#x2013;1279. https://doi.org/10.1016/j.jacc.2012.12.029</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jacc.2012.12.029</ArticleId></ArticleIdList></Reference><Reference><Citation>Choi JH, Min JK, Labounty TM et al (2011) Intracoronary transluminal attenuation gradient in coronary CT angiography for determining coronary artery stenosis. JACC Cardiovasc Imaging 4:1149&#x2013;1157. https://doi.org/10.1016/j.jcmg.2011.09.006</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jcmg.2011.09.006</ArticleId></ArticleIdList></Reference><Reference><Citation>Kato Y, Ambale-Venkatesh B, Kassai Y et al (2020) Non-contrast coronary magnetic resonance angiography: current frontiers and future horizons. MAGMA 33:591&#x2013;612. https://doi.org/10.1007/s10334-020-00834-8</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10334-020-00834-8</ArticleId></ArticleIdList></Reference><Reference><Citation>Hajhosseiny R, Bustin A, Munoz C et al (2020) Coronary magnetic resonance angiography: technical innovations leading us to the promised land? JACC Cardiovasc Imaging 13:2653&#x2013;2672. https://doi.org/10.1016/j.jcmg.2020.01.006</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jcmg.2020.01.006</ArticleId></ArticleIdList></Reference><Reference><Citation>Nakamura M, Kido T, Kido T et al (2018) Non-contrast compressed sensing whole-heart coronary magnetic resonance angiography at 3T: a comparison with conventional imaging. Eur J Radiol 104:43&#x2013;48. https://doi.org/10.1016/j.ejrad.2018.04.025</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejrad.2018.04.025</ArticleId></ArticleIdList></Reference><Reference><Citation>Feng L, Coppo S, Piccini D et al (2018) 5D whole-heart sparse MRI. Magn Reson Med 79:826&#x2013;838. https://doi.org/10.1002/mrm.26745</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.26745</ArticleId></ArticleIdList></Reference><Reference><Citation>Haris K, Hedstr&#xf6;m E, Bidhult S et al (2017) Self-gated fetal cardiac MRI with tiny golden angle iGRASP: a feasibility study. J Magn Reson Imaging 46:207&#x2013;217. https://doi.org/10.1002/jmri.25599</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmri.25599</ArticleId></ArticleIdList></Reference><Reference><Citation>Pang J, Sharif B, Fan Z et al (2014) ECG and navigator-free four-dimensional whole-heart coronary MRA for simultaneous visualization of cardiac anatomy and function. Magn Reson Med 72:1208&#x2013;1217. https://doi.org/10.1002/mrm.25450</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.25450</ArticleId></ArticleIdList></Reference><Reference><Citation>Prieto C, Doneva M, Usman M et al (2015) Highly efficient respiratory motion compensated free-breathing coronary MRA using golden-step Cartesian acquisition. J Magn Reson Imaging 41:738&#x2013;746. https://doi.org/10.1002/jmri.24602</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmri.24602</ArticleId></ArticleIdList></Reference><Reference><Citation>Bastiaansen JAM, Piccini D, Di Sopra L et al (2020) Natively fat-suppressed 5D whole-heart MRI with a radial free-running fast-interrupted steady-state (FISS) sequence at 1.5T and 3T. Magn Reson Med 83:45&#x2013;55. https://doi.org/10.1002/mrm.27942</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.27942</ArticleId></ArticleIdList></Reference><Reference><Citation>Bastiaansen JAM, van Heeswijk RB, Stuber M, Piccini D (2019) Noncontrast free-breathing respiratory self-navigated coronary artery cardiovascular magnetic resonance angiography at 3 T using lipid insensitive binomial off-resonant excitation (LIBRE). J Cardiovasc Magn Reson 21:38. https://doi.org/10.1186/s12968-019-0543-6</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s12968-019-0543-6</ArticleId></ArticleIdList></Reference><Reference><Citation>Isogawa K, Ida T, Shiodera T, Takeguchi T (2018) Deep shrinkage convolutional neural network for adaptive noise reduction. IEEE Signal Process Lett 25:224&#x2013;228. https://doi.org/10.1109/LSP.2017.2782270</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/LSP.2017.2782270</ArticleId></ArticleIdList></Reference><Reference><Citation>Hajhosseiny R, Rashid I, Bustin A et al (2021) Clinical comparison of sub-mm high-resolution non-contrast coronary CMR angiography against coronary CT angiography in patients with low-intermediate risk of coronary artery disease: a single center trial. J Cardiovasc Magn Reson 23:1&#x2013;14. https://doi.org/10.1186/s12968-021-00758-9</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s12968-021-00758-9</ArticleId></ArticleIdList></Reference><Reference><Citation>Nederkoorn PJ, Van der Graaf Y, Eikelboom BC et al (2002) Time-of-flight MR angiography of carotid artery stenosis: Does a flow void represent severe stenosis? Am J Neuroradiol 23:1779&#x2013;1784</Citation></Reference><Reference><Citation>Heiserman JE, Zabramski JM, Drayer BP, Keller PJ (1996) Clinical significance of the flow gap in carotid magnetic resonance angiography. J Neurosurg 85:384&#x2013;387. https://doi.org/10.3171/jns.1996.85.3.0384</Citation><ArticleIdList><ArticleId IdType="doi">10.3171/jns.1996.85.3.0384</ArticleId></ArticleIdList></Reference><Reference><Citation>Yonezawa M, Nagata M, Kitagawa K et al (2014) Quantitative analysis of 1.5-T whole-heart coronary MR angiograms obtained with 32-channel cardiac coils: a comparison with conventional quantitative coronary angiography. Radiology 271:356&#x2013;364. https://doi.org/10.1148/radiol.13122491</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.13122491</ArticleId></ArticleIdList></Reference><Reference><Citation>Pennell DJ, Bogren HG, Keegan J et al (1996) Assessment of coronary artery stenosis by magnetic resonance imaging. Heart 75:127&#x2013;133. https://doi.org/10.1136/hrt.75.2.127</Citation><ArticleIdList><ArticleId IdType="doi">10.1136/hrt.75.2.127</ArticleId></ArticleIdList></Reference><Reference><Citation>Kidoh M, Shinoda K, Kitajima M et al (2019) Deep learning based noise reduction for brain MR imaging: tests on phantoms and healthy volunteers. Magn Reson Med Sci. https://doi.org/10.2463/mrms.mp.2019-0018</Citation><ArticleIdList><ArticleId IdType="doi">10.2463/mrms.mp.2019-0018</ArticleId></ArticleIdList></Reference><Reference><Citation>Kato Y, Ambale-venkatesh B, Kassai Y, et al (2019) Application of deep learning reconstruction for denoising of compressed sensing non-contrast coronary MRA images to achieve improved Diagnostic Confidence. ISMRM 2019 Abstr</Citation></Reference><Reference><Citation>Kato Y, Kapoor K, Ortman J, et al (2020) Comparison of Diagnostic Confidence score and calculation-based image quality scores on MRCA with different Compressed Sensing (CS) and Deep Learning Reconstruction (DLR) levels . SCMR 2020 Abstr</Citation></Reference><Reference><Citation>Heer T, Reiter S, Tri&#xdf;ler M et al (2017) Effect of nitroglycerin on the performance of MR coronary angiography. J Magn Reson Imaging 45:1419&#x2013;1428. https://doi.org/10.1002/jmri.25483</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/jmri.25483</ArticleId></ArticleIdList></Reference><Reference><Citation>Parker DL, Yuan C, Blatter DD (1991) MR angiography by multiple thin slab 3D acquisition. Magn Reson Med 17:434&#x2013;451. https://doi.org/10.1002/mrm.1910170215</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/mrm.1910170215</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang Q, Chen Z, Chen S et al (2020) Angiographic contrast mechanism comparison between Simultaneous Non-contrast Angiography and intraPlaque hemorrhage (SNAP) sequence and Time of Flight (TOF) sequence for intracranial artery. Magn Reson Imaging 66:199&#x2013;207. https://doi.org/10.1016/j.mri.2019.09.001</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.mri.2019.09.001</ArticleId></ArticleIdList></Reference><Reference><Citation>Dodge JT, Brown BG, Bolson EL, Dodge HT (1992) Lumen diameter of normal human coronary arteries: influence of age, sex, anatomic variation, and left ventricular hypertrophy or dilation. Circulation 86:232&#x2013;246. https://doi.org/10.1161/01.CIR.86.1.232</Citation><ArticleIdList><ArticleId IdType="doi">10.1161/01.CIR.86.1.232</ArticleId></ArticleIdList></Reference><Reference><Citation>Ofili EO, Labovitz AJ, Kern MJ (1993) Coronary flow velocity dynamics in normal and diseased arteries. Am J Cardiol. https://doi.org/10.1016/0002-9149(93)90128-Y</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/0002-9149(93)90128-Y</ArticleId></ArticleIdList></Reference><Reference><Citation>Steigner ML, Mitsouras D, Whitmore AG et al (2010) Iodinated contrast opacification gradients in normal coronary arteries imaged with prospectively ECG-gated single heart beat 320-detector row computed tomography. Circ Cardiovasc Imaging 3:179&#x2013;186. https://doi.org/10.1161/CIRCIMAGING.109.854307</Citation><ArticleIdList><ArticleId IdType="doi">10.1161/CIRCIMAGING.109.854307</ArticleId></ArticleIdList></Reference><Reference><Citation>Park EA, Lee W, Park SJ et al (2016) Influence of coronary artery diameter on intracoronary transluminal attenuation gradient during CT angiography. JACC Cardiovasc Imaging 9:1074&#x2013;1083. https://doi.org/10.1016/j.jcmg.2015.10.028</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jcmg.2015.10.028</ArticleId></ArticleIdList></Reference><Reference><Citation>Park E-A, Lee W (2016) The authors reply. JACC Cardiovasc Imaging 9:1360&#x2013;1361. https://doi.org/10.1016/j.jcmg.2016.09.008</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jcmg.2016.09.008</ArticleId></ArticleIdList></Reference><Reference><Citation>Sharif D, Sharif-Rasslan A, Shahla C et al (2015) Differences in coronary artery blood velocities in the setting of normal coronary angiography and normal stress echocardiography. Heart Int 10:e6&#x2013;e11. https://doi.org/10.5301/heartint.5000221</Citation><ArticleIdList><ArticleId IdType="doi">10.5301/heartint.5000221</ArticleId></ArticleIdList></Reference><Reference><Citation>Anderson HV, Stokes MJ, Leon M et al (2000) Coronary artery flow velocity is related to lumen area and regional left ventricular mass. Circulation 102:48&#x2013;54. https://doi.org/10.1161/01.CIR.102.1.48</Citation><ArticleIdList><ArticleId IdType="doi">10.1161/01.CIR.102.1.48</ArticleId></ArticleIdList></Reference><Reference><Citation>Mahalingam A, Gawandalkar UU, Kini G et al (2016) Numerical analysis of the effect of turbulence transition on the hemodynamic parameters in human coronary arteries. Cardiovasc Diagn Ther 6:208&#x2013;220. https://doi.org/10.21037/cdt.2016.03.08</Citation><ArticleIdList><ArticleId IdType="doi">10.21037/cdt.2016.03.08</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36598505</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>06</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>10</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">2052-2525</ISSN><JournalIssue CitedMedium="Internet"><Volume>10</Volume><Issue>Pt 1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>01</Day></PubDate></JournalIssue><Title>IUCrJ</Title><ISOAbbreviation>IUCrJ</ISOAbbreviation></Journal><ArticleTitle>Learning to automate cryo-electron microscopy data collection with Ptolemy.</ArticleTitle><Pagination><StartPage>90</StartPage><EndPage>102</EndPage><MedlinePgn>90-102</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1107/S2052252522010612</ELocationID><Abstract><AbstractText>Over the past decade, cryo-electron microscopy (cryoEM) has emerged as an important method for determining near-native, near-atomic resolution 3D structures of biological macromolecules. To meet the increasing demand for cryoEM, automated methods that improve throughput and efficiency of microscope operation are needed. Currently, the targeting algorithms provided by most data-collection software require time-consuming manual tuning of parameters for each grid, and, in some cases, operators must select targets completely manually. However, the development of fully automated targeting algorithms is non-trivial, because images often have low signal-to-noise ratios and optimal targeting strategies depend on a range of experimental parameters and macromolecule behaviors that vary between projects and collection sessions. To address this, Ptolemy provides a pipeline to automate low- and medium-magnification targeting using a suite of purpose-built computer vision and machine-learning algorithms, including mixture models, convolutional neural networks and U-Nets. Learned models in this pipeline are trained on a large set of images from real-world cryoEM data-collection sessions, labeled with locations selected by human operators. These models accurately detect and classify regions of interest in low- and medium-magnification images, and generalize to unseen sessions, as well as to images collected on different microscopes at another facility. This open-source, modular pipeline can be integrated with existing microscope control software to enable automation of cryoEM data collection and can serve as a foundation for future cryoEM automation software.</AbstractText><CopyrightInformation>open access.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Kim</LastName><ForeName>Paul T</ForeName><Initials>PT</Initials><Identifier Source="ORCID">0000-0002-8950-7179</Identifier><AffiliationInfo><Affiliation>Simons Machine Learning Center, Simons Electron Microscopy Center, New York Structural Biology Center, New York, NY USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Noble</LastName><ForeName>Alex J</ForeName><Initials>AJ</Initials><Identifier Source="ORCID">0000-0001-8634-2279</Identifier><AffiliationInfo><Affiliation>Simons Machine Learning Center, Simons Electron Microscopy Center, New York Structural Biology Center, New York, NY USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Cheng</LastName><ForeName>Anchi</ForeName><Initials>A</Initials><Identifier Source="ORCID">0000-0003-0466-4376</Identifier><AffiliationInfo><Affiliation>Simons Electron Microscopy Center, New York Structural Biology Center, New York, NY USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Bepler</LastName><ForeName>Tristan</ForeName><Initials>T</Initials><Identifier Source="ORCID">0000-0001-5595-9954</Identifier><AffiliationInfo><Affiliation>Simons Machine Learning Center, Simons Electron Microscopy Center, New York Structural Biology Center, New York, NY USA.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>01</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>IUCrJ</MedlineTA><NlmUniqueID>101623101</NlmUniqueID><ISSNLinking>2052-2525</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D020285" MajorTopicYN="N">Cryoelectron Microscopy</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D012984" MajorTopicYN="Y">Software</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000465" MajorTopicYN="Y">Algorithms</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000069550" MajorTopicYN="N">Machine Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D003625" MajorTopicYN="N">Data Collection</DescriptorName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">automated cryoEM data collection</Keyword><Keyword MajorTopicYN="N">automation</Keyword><Keyword MajorTopicYN="N">computer vision</Keyword><Keyword MajorTopicYN="N">cryoEM</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword><Keyword MajorTopicYN="N">machine learning</Keyword><Keyword MajorTopicYN="N">microscope automation software</Keyword><Keyword MajorTopicYN="N">single-particle cryoEM</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>6</Month><Day>17</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>11</Month><Day>3</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>4</Day><Hour>11</Hour><Minute>3</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>5</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>7</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36598505</ArticleId><ArticleId IdType="pmc">PMC9812219</ArticleId><ArticleId IdType="doi">10.1107/S2052252522010612</ArticleId><ArticleId IdType="pii">S2052252522010612</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Bai, X., McMullan, G. &amp; Scheres, S. H. W. (2015). Trends Biochem. Sci. 40, 49&#x2013;57.</Citation><ArticleIdList><ArticleId IdType="pubmed">25544475</ArticleId></ArticleIdList></Reference><Reference><Citation>Bepler, T., Morin, A., Rapp, M., Brasch, J., Shapiro, L., Noble, A. J. &amp; Berger, B. (2019). Nat. Methods, 16, 1153&#x2013;1160.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6858545</ArticleId><ArticleId IdType="pubmed">31591578</ArticleId></ArticleIdList></Reference><Reference><Citation>Bouvette, J., Huang, Q., Riccio, A. A., Copeland, W. C., Bartesaghi, A. &amp; Borgnia, M. J. (2022). eLife, 11, e80047.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC9398423</ArticleId><ArticleId IdType="pubmed">35997703</ArticleId></ArticleIdList></Reference><Reference><Citation>Brent, R. P. (1973). Algorithms for Minimization Without Derivatives, 1st ed. Englewood Cliffs: Prentice-Hall.</Citation></Reference><Reference><Citation>Carragher, B., Kisseberth, N., Kriegman, D., Milligan, R. A., Potter, C. S., Pulokas, J. &amp; Reilein, A. (2000). J. Struct. Biol. 132, 33&#x2013;45.</Citation><ArticleIdList><ArticleId IdType="pubmed">11121305</ArticleId></ArticleIdList></Reference><Reference><Citation>Cheng, A., Kim, P. T., Kuang, H., Mendez, J. H., Chua, E. Y. D., Maruthi, K., Wei, H., Sawh, A., Aragon, M. F., Serbynovskyi, V., Neselu, K., Eng, E. T., Potter, C. P., Carragher, B., Bepler, T. &amp; Noble, A. J. (2022). IUCrJ, 9, 77&#x2013;89.</Citation></Reference><Reference><Citation>Cheng, Y., Grigorieff, N., Penczek, P. A. &amp; Walz, T. (2015). Cell, 161, 438&#x2013;449.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4409659</ArticleId><ArticleId IdType="pubmed">25910204</ArticleId></ArticleIdList></Reference><Reference><Citation>Chua, E. Y. D., Mendez, J. H., Rapp, M., Ilca, S. L., Tan, Y. Z., Maruthi, K., Kuang, H., Zimanyi, C. M., Cheng, A., Eng, E. T., Noble, A. J., Potter, C. S. &amp; Carragher, B. (2022). Annu. Rev. Biochem. 91, 1&#x2013;32.</Citation><ArticleIdList><ArticleId IdType="pubmed">35320683</ArticleId></ArticleIdList></Reference><Reference><Citation>Egelman, E. H. (2016). Biophys. J. 110, 1008&#x2013;1012.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4788751</ArticleId><ArticleId IdType="pubmed">26958874</ArticleId></ArticleIdList></Reference><Reference><Citation>Forbes, F. (2018). Mixture Models for Image Analysis, edited by S. Fruhwirth-Schnatter, G. Celeux &amp; C. P. Robert, pp. 397&#x2013;418. New York: CRC press.</Citation></Reference><Reference><Citation>George, B., Assaiya, A., Roy, R. J., Kembhavi, A., Chauhan, R., Paul, G., Kumar, J. &amp; Philip, N. S. (2021). Commun. Biol. 4, 200.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7884729</ArticleId><ArticleId IdType="pubmed">33589717</ArticleId></ArticleIdList></Reference><Reference><Citation>Gupta, L. &amp; Sortrakul, T. (1998). Pattern Recognit. 31, 315&#x2013;325.</Citation></Reference><Reference><Citation>Ioffe, S. &amp; Szegedy, C. (2015). arXiv:150203167.</Citation></Reference><Reference><Citation>Kingma, D. P. &amp; Ba, J. (2017). arXiv:14126980.</Citation></Reference><Reference><Citation>LeCun, Y., Bengio, Y. &amp; Hinton, G. (2015). Nature, 521, 436&#x2013;444.</Citation><ArticleIdList><ArticleId IdType="pubmed">26017442</ArticleId></ArticleIdList></Reference><Reference><Citation>Li, Y., Cash, J. N., Tesmer, J. J. G. &amp; Cianfrocco, M. A. (2020). Structure, 28, 858&#x2013;869.e3.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7347462</ArticleId><ArticleId IdType="pubmed">32294468</ArticleId></ArticleIdList></Reference><Reference><Citation>Li, Y., Fan, Q., Cohn, J., Demers, V., Lee, J. Y., Yip, L., Cianfrocco, M. A. &amp; Vos, S. M. (2022). bioRxiv, 2022.06.17.496614.</Citation></Reference><Reference><Citation>Lyumkis, D. (2019). J. Biol. Chem. 294, 5181&#x2013;5197.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6442032</ArticleId><ArticleId IdType="pubmed">30804214</ArticleId></ArticleIdList></Reference><Reference><Citation>Mastronarde, D. N. (2005). J. Struct. Biol. 152, 36&#x2013;51.</Citation><ArticleIdList><ArticleId IdType="pubmed">16182563</ArticleId></ArticleIdList></Reference><Reference><Citation>Noble, A. J., Dandey, V. P., Wei, H., Brasch, J., Chase, J., Acharya, P., Tan, Y. Z., Zhang, Z., Kim, L. Y., Scapin, G., Rapp, M., Eng, E. T., Rice, W. J., Cheng, A., Negro, C. J., Shapiro, L., Kwong, P. D., Jeruzalmi, D., des Georges, A., Potter, C. S. &amp; Carragher, B. (2018). eLife, 7, e34257.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5999397</ArticleId><ArticleId IdType="pubmed">29809143</ArticleId></ArticleIdList></Reference><Reference><Citation>Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., K&#xf6;pf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J. &amp; Chintala, S. (2019). Advances in Neural Information Processing Systems 32, edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. d&#x2019;Alch&#xe9;-Buc, E. Fox &amp; R. Garnett, pp. 8026&#x2013;8037. Curran Associates Inc.</Citation></Reference><Reference><Citation>Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M. &amp; Duchesnay, &#xc9;. (2011). J. Mach. Learn. Res. 12, 2825&#x2013;2830.</Citation></Reference><Reference><Citation>Punjani, A., Rubinstein, J., Fleet, D. &amp; Brubaker, M. A. (2017). Nat. Methods, 14, 290&#x2013;296.</Citation><ArticleIdList><ArticleId IdType="pubmed">28165473</ArticleId></ArticleIdList></Reference><Reference><Citation>Redmon, J. &amp; Farhadi, A. (2018). arXiv:180402767.</Citation></Reference><Reference><Citation>Ronneberger, O., Fischer, P. &amp; Brox, T. (2015). arXiv:150504597.</Citation></Reference><Reference><Citation>Sanchez-Garcia, R., Segura, J., Maluenda, D., Sorzano, C. O. S. &amp; Carazo, J. M. (2020). J. Struct. Biol. 210, 107498.</Citation><ArticleIdList><ArticleId IdType="pubmed">32276087</ArticleId></ArticleIdList></Reference><Reference><Citation>Shorten, C. &amp; Khoshgoftaar, T. M. (2019). J. Big Data, 6, 60.</Citation></Reference><Reference><Citation>Suloway, C., Pulokas, J., Fellmann, D., Cheng, A., Guerra, F., Quispe, J., Stagg, S., Potter, C. S. &amp; Carragher, B. (2005). J. Struct. Biol. 151, 41&#x2013;60.</Citation><ArticleIdList><ArticleId IdType="pubmed">15890530</ArticleId></ArticleIdList></Reference><Reference><Citation>Tegunov, D. &amp; Cramer, P. (2019). Nat. Methods, 16, 1146&#x2013;1152.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6858868</ArticleId><ArticleId IdType="pubmed">31591575</ArticleId></ArticleIdList></Reference><Reference><Citation>Wagner, T., Merino, F., Stabrin, M., Moriya, T., Antoni, C., Apelbaum, A., Hagel, P., Sitsel, O., Raisch, T., Prumbaum, D., Quentin, D., Roderer, D., Tacke, S., Siebolds, B., Schubert, E., Shaikh, T. R., Lill, P., Gatsogiannis, C. &amp; Raunser, S. (2019). Commun. Biol. 2, 218.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6584505</ArticleId><ArticleId IdType="pubmed">31240256</ArticleId></ArticleIdList></Reference><Reference><Citation>Weissenberger, G., Henderikx, R. J. M. &amp; Peters, P. J. (2021). Nat. Methods, 18, 463&#x2013;471.</Citation><ArticleIdList><ArticleId IdType="pubmed">33963356</ArticleId></ArticleIdList></Reference><Reference><Citation>Wu, M. &amp; Lander, G. C. (2020). Biophys. J. 119, 1281&#x2013;1289.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7567993</ArticleId><ArticleId IdType="pubmed">32919493</ArticleId></ArticleIdList></Reference><Reference><Citation>Yokoyama, Y., Terada, T., Shimizu, K., Nishikawa, K., Kozai, D., Shimada, A., Mizoguchi, A., Fujiyoshi, Y. &amp; Tani, K. (2020). Biophys. Rev. 12, 349&#x2013;354.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7242580</ArticleId><ArticleId IdType="pubmed">32162215</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhong, E. D., Bepler, T., Berger, B. &amp; Davis, J. H. (2021). Nat. Methods, 18, 176&#x2013;185.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8183613</ArticleId><ArticleId IdType="pubmed">33542510</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36597401</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>05</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>05</Day></DateRevised><Article PubModel="Print"><Journal><ISSN IssnType="Print">1671-7104</ISSN><JournalIssue CitedMedium="Print"><Volume>46</Volume><Issue>6</Issue><PubDate><Year>2022</Year><Month>Nov</Month><Day>30</Day></PubDate></JournalIssue><Title>Zhongguo yi liao qi xie za zhi = Chinese journal of medical instrumentation</Title><ISOAbbreviation>Zhongguo Yi Liao Qi Xie Za Zhi</ISOAbbreviation></Journal><ArticleTitle>[Automatic Delineation of Clinical Target Volume and Organ at Risk by Deep Learning for Prostate Cancer Adaptive Radiotherapy].</ArticleTitle><Pagination><StartPage>691</StartPage><EndPage>695</EndPage><MedlinePgn>691-695</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.3969/j.issn.1671-7104.2022.06.021</ELocationID><Abstract><AbstractText>Adaptive radiotherapy can modify the treatment plan online based on the clinical target volume (CTV) and organ at risk (OAR) contours on the cone-beam CT (CBCT) before treatment, improving the accuracy of radiotherapy. However, manual delineation of CTV and OAR on CBCT is time-consuming. In this study, a deep neural network-based method based on U-Net was purposed. CBCT images and corresponding mask were used for model training and validation, showing superior performance in terms of the segmentation accuracy. The proposed method could be used in the clinic to support rapid CTV and OAR contouring for prostate adaptive radiotherapy.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Song</LastName><ForeName>Xinyu</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>Department of Radiotherapy, West China Hospital, Sichuan University, Chengdu, 610041.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Xiangyu</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>Department of Radiotherapy, West China Hospital, Sichuan University, Chengdu, 610041.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Li</LastName><ForeName>Jing</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Department of Radiotherapy, West China Hospital, Sichuan University, Chengdu, 610041.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Liang</LastName><ForeName>Lan</ForeName><Initials>L</Initials><AffiliationInfo><Affiliation>Department of Radiotherapy, West China Hospital, Sichuan University, Chengdu, 610041.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yang</LastName><ForeName>Yang</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>School of Physical Science and Technology, Wuhan University, Wuhan, 430072.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Li</LastName><ForeName>Guangjun</ForeName><Initials>G</Initials><AffiliationInfo><Affiliation>Department of Radiotherapy, West China Hospital, Sichuan University, Chengdu, 610041.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Bai</LastName><ForeName>Sen</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Department of Radiotherapy, West China Hospital, Sichuan University, Chengdu, 610041.</Affiliation></AffiliationInfo></Author></AuthorList><Language>chi</Language><PublicationTypeList><PublicationType UI="D004740">English Abstract</PublicationType><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList></Article><MedlineJournalInfo><Country>China</Country><MedlineTA>Zhongguo Yi Liao Qi Xie Za Zhi</MedlineTA><NlmUniqueID>9426153</NlmUniqueID><ISSNLinking>1671-7104</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D008297" MajorTopicYN="N">Male</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D011880" MajorTopicYN="N">Radiotherapy Planning, Computer-Assisted</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D054893" MajorTopicYN="N">Cone-Beam Computed Tomography</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D016571" MajorTopicYN="N">Neural Networks, Computer</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D011471" MajorTopicYN="Y">Prostatic Neoplasms</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName><QualifierName UI="Q000532" MajorTopicYN="N">radiotherapy</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D007091" MajorTopicYN="N">Image Processing, Computer-Assisted</DescriptorName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">U-Net</Keyword><Keyword MajorTopicYN="N">cone-beam CT</Keyword><Keyword MajorTopicYN="N">multi-organ delineation</Keyword><Keyword MajorTopicYN="N">prostate cancer</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>4</Day><Hour>1</Hour><Minute>43</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>5</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>ppublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36597401</ArticleId><ArticleId IdType="doi">10.3969/j.issn.1671-7104.2022.06.021</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" VersionID="2" Owner="NLM"><PMID Version="2">36597113</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>06</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">2332-8886</ISSN><JournalIssue CitedMedium="Internet"><Volume>9</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>03</Day></PubDate></JournalIssue><Title>Bioelectronic medicine</Title><ISOAbbreviation>Bioelectron Med</ISOAbbreviation></Journal><ArticleTitle>A radiographic, deep transfer learning framework, adapted to estimate lung opacities from chest x-rays.</ArticleTitle><Pagination><StartPage>1</StartPage><MedlinePgn>1</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">1</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1186/s42234-022-00103-0</ELocationID><Abstract><AbstractText>Chest radiographs (CXRs) are the most widely available radiographic imaging modality used to detect respiratory diseases that result in lung opacities. CXR reports often use non-standardized language that result in subjective, qualitative, and non-reproducible opacity estimates. Our goal was to develop a robust deep transfer learning framework and adapt it to estimate the degree of lung opacity from CXRs. Following CXR data selection based on exclusion criteria, segmentation schemes were used for ROI (Region Of Interest) extraction, and all combinations of segmentation, data balancing, and classification methods were tested to pick the top performing models. Multifold cross validation was used to determine the best model from the initial selected top models, based on appropriate performance metrics, as well as a novel Macro-Averaged Heatmap Concordance Score (MA HCS). Performance of the best model is compared against that of expert physician annotators, and heatmaps were produced. Finally, model performance sensitivity analysis across patient populations of interest was performed. The proposed framework was adapted to the specific use case of estimation of degree of CXR lung opacity using ordinal multiclass classification. Acquired between March 24, 2020, and May 22, 2020, 38,365 prospectively annotated CXRs from 17,418 patients were used. We tested three neural network architectures (ResNet-50, VGG-16, and ChexNet), three segmentation schemes (no segmentation, lung segmentation, and lateral segmentation based on spine detection), and three data balancing strategies (undersampling, double-stage sampling, and synthetic minority oversampling) using 38,079 CXR images for training, and validation with 286 images as the out-of-the-box dataset that underwent expert radiologist adjudication. Based on the results of these experiments, the ResNet-50 model with undersampling and no ROI segmentation is recommended for lung opacity classification, based on optimal values for the MAE metric and HCS (Heatmap Concordance Score). The degree of agreement between the opacity scores predicted by this model with respect to the two sets of radiologist scores (OR or Original Reader and OOBTR or Out Of Box Reader) in terms of performance metrics is superior to the inter-radiologist opacity score agreement.</AbstractText><CopyrightInformation>&#xa9; 2022. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Vardhan</LastName><ForeName>Avantika</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Institute of Health System Science, Feinstein Institutes for Medical Research, Northwell Health, Manhasset, NY, 11030, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Institute of Bioelectronic Medicine, Feinstein Institutes for Medical Research, Northwell Health, Manhasset, NY, 11030, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Makhnevich</LastName><ForeName>Alex</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Institute of Health System Science, Feinstein Institutes for Medical Research, Northwell Health, Manhasset, NY, 11030, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Donald and Barbara Zucker School of Medicine at Hofstra/Northwell, Northwell Health, Hempstead, NY, 11549, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Omprakash</LastName><ForeName>Pravan</ForeName><Initials>P</Initials><AffiliationInfo><Affiliation>Institute of Bioelectronic Medicine, Feinstein Institutes for Medical Research, Northwell Health, Manhasset, NY, 11030, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Hirschorn</LastName><ForeName>David</ForeName><Initials>D</Initials><AffiliationInfo><Affiliation>Institute of Health System Science, Feinstein Institutes for Medical Research, Northwell Health, Manhasset, NY, 11030, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Information Services, Northwell Health, New Hyde Park, NY, 11042, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Barish</LastName><ForeName>Matthew</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Institute of Health System Science, Feinstein Institutes for Medical Research, Northwell Health, Manhasset, NY, 11030, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Information Services, Northwell Health, New Hyde Park, NY, 11042, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Cohen</LastName><ForeName>Stuart L</ForeName><Initials>SL</Initials><AffiliationInfo><Affiliation>Institute of Health System Science, Feinstein Institutes for Medical Research, Northwell Health, Manhasset, NY, 11030, USA.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Donald and Barbara Zucker School of Medicine at Hofstra/Northwell, Northwell Health, Hempstead, NY, 11549, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zanos</LastName><ForeName>Theodoros P</ForeName><Initials>TP</Initials><Identifier Source="ORCID">0000-0002-9204-9551</Identifier><AffiliationInfo><Affiliation>Institute of Health System Science, Feinstein Institutes for Medical Research, Northwell Health, Manhasset, NY, 11030, USA. tzanos@northwell.edu.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Institute of Bioelectronic Medicine, Feinstein Institutes for Medical Research, Northwell Health, Manhasset, NY, 11030, USA. tzanos@northwell.edu.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Donald and Barbara Zucker School of Medicine at Hofstra/Northwell, Northwell Health, Hempstead, NY, 11549, USA. tzanos@northwell.edu.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>03</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>Bioelectron Med</MedlineTA><NlmUniqueID>101660849</NlmUniqueID><ISSNLinking>2332-8886</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Chest X-ray (CXR)</Keyword><Keyword MajorTopicYN="N">Deep transfer learning</Keyword><Keyword MajorTopicYN="N">Heatmap concordance</Keyword><Keyword MajorTopicYN="N">Lung opacity</Keyword><Keyword MajorTopicYN="N">Ordinal classification</Keyword><Keyword MajorTopicYN="N">Pretrained model</Keyword></KeywordList><CoiStatement>The authors declare they have no competing interests.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>11</Month><Day>14</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>12</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>3</Day><Hour>23</Hour><Minute>37</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>4</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>4</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36597113</ArticleId><ArticleId IdType="pmc">PMC9809517</ArticleId><ArticleId IdType="doi">10.1186/s42234-022-00103-0</ArticleId><ArticleId IdType="pii">10.1186/s42234-022-00103-0</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Alghamdi HS, Amoudi G, Elhag S, Saeedi K, Nasser J. Deep learning approaches for detecting COVID-19 from chest X-ray images: a survey. IEEE Access. 2021;9:20235&#x2013;20254. doi: 10.1109/ACCESS.2021.3054484.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2021.3054484</ArticleId><ArticleId IdType="pmc">PMC8545235</ArticleId><ArticleId IdType="pubmed">34786304</ArticleId></ArticleIdList></Reference><Reference><Citation>Alzubaidi L, Al-Amidie M, Al-Asadi A, Humaidi AJ, Al-Shamma O, Fadhel MA, Zhang J, Santamar&#xed;a J, Duan Y. Novel Transfer Learning Approach for Medical Imaging with Limited Labeled Data. Cancers. 2021;13:1590. doi: 10.3390/cancers13071590.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/cancers13071590</ArticleId><ArticleId IdType="pmc">PMC8036379</ArticleId><ArticleId IdType="pubmed">33808207</ArticleId></ArticleIdList></Reference><Reference><Citation>Au-Yong I, Higashi Y, Giannotti E, Fogarty A, Morling JR, Grainge M, Race A, Juurlink I, Simmonds M, Briggs S, Cruikshank S, Hammond-Pears S, West J, Crooks CJ, Card T. Chest radiograph scoring alone or combined with other risk scores for predicting outcomes in COVID-19. Radiology. 2022;302:460&#x2013;469. doi: 10.1148/radiol.2021210986.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2021210986</ArticleId><ArticleId IdType="pmc">PMC8475750</ArticleId><ArticleId IdType="pubmed">34519573</ArticleId></ArticleIdList></Reference><Reference><Citation>Balbi M, Caroli A, Corsi A, Milanese G, Surace A, Di Marco F, Novelli L, Silva M, Lorini FL, Duca A, Cosentini R, Sverzellati N, Bonaffini PA, Sironi S. Chest X-ray for predicting mortality and the need for ventilatory support in COVID-19 patients presenting to the emergency department. Eur Radiol. 2021;31:1999&#x2013;2012. doi: 10.1007/s00330-020-07270-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-020-07270-1</ArticleId><ArticleId IdType="pmc">PMC7543667</ArticleId><ArticleId IdType="pubmed">33033861</ArticleId></ArticleIdList></Reference><Reference><Citation>Brady AP. Error and discrepancy in radiology: inevitable or avoidable? Insights Imaging. 2017;8:171&#x2013;182. doi: 10.1007/s13244-016-0534-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s13244-016-0534-1</ArticleId><ArticleId IdType="pmc">PMC5265198</ArticleId><ArticleId IdType="pubmed">27928712</ArticleId></ArticleIdList></Reference><Reference><Citation>Cheplygina V, de Bruijne M, Pluim JP. Not-so-supervised: a survey of semi-supervised, multi-instance, and transfer learning in medical image analysis. Med Image Anal. 2019;54:280&#x2013;296. doi: 10.1016/j.media.2019.03.009.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2019.03.009</ArticleId><ArticleId IdType="pubmed">30959445</ArticleId></ArticleIdList></Reference><Reference><Citation>Cohen JP, Dao L, Roth K, Morrison P, Bengio Y, Abbasi AF, Shen B, Mahsa HK, Ghassemi M, Li H, Duong TQ. Predicting COVID-19 pneumonia severity on chest X-ray with deep learning. Cureus. 2020;12:e9448.</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7451075</ArticleId><ArticleId IdType="pubmed">32864270</ArticleId></ArticleIdList></Reference><Reference><Citation>Dembczy&#x144;ski K, Kot&#x142;owski W, S&#x142;owi&#x144;ski R. Ordinal classification with decision rules. Berlin: Springer Berlin Heidelberg; 2008. pp. 169&#x2013;181.</Citation></Reference><Reference><Citation>Dur&#xe1;n-Rosal AM, Camacho-Ca&#xf1;am&#xf3;n J, Guti&#xe9;rrez PA, Guiote Moreno MV, Rodr&#xed;guez-C&#xe1;ceres E, Vallejo Casas JA, Herv&#xe1;s-Mart&#xed;nez C. Ordinal classification of the affectation level of 3D-images in Parkinson diseases. Sci Rep. 2021;11:7067. doi: 10.1038/s41598-021-86538-y.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-021-86538-y</ArticleId><ArticleId IdType="pmc">PMC8007580</ArticleId><ArticleId IdType="pubmed">33782476</ArticleId></ArticleIdList></Reference><Reference><Citation>Erickson BJ, Korfiatis P, Akkus Z, Kline TL. Machine learning for medical imaging. Radiographics. 2017;37:505&#x2013;515. doi: 10.1148/rg.2017160130.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/rg.2017160130</ArticleId><ArticleId IdType="pmc">PMC5375621</ArticleId><ArticleId IdType="pubmed">28212054</ArticleId></ArticleIdList></Reference><Reference><Citation>Hicks SA, Str&#xfc;mke I, Thambawita V, Hammou M, Riegler MA, Halvorsen P, Parasa S. On evaluation metrics for medical applications of artificial intelligence. Sci Rep. 2022;12:5979. doi: 10.1038/s41598-022-09954-8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-022-09954-8</ArticleId><ArticleId IdType="pmc">PMC8993826</ArticleId><ArticleId IdType="pubmed">35395867</ArticleId></ArticleIdList></Reference><Reference><Citation>Johnson JM, Khoshgoftaar TM. Survey on deep learning with class imbalance. J Big Data. 2019;6:27. doi: 10.1186/s40537-019-0192-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s40537-019-0192-5</ArticleId></ArticleIdList></Reference><Reference><Citation>Khan AI, Shah JL, Bhat MM. CoroNet: a deep neural network for detection and diagnosis of COVID-19 from chest x-ray images. Comput Methods Prog Biomed. 2020;196:105581. doi: 10.1016/j.cmpb.2020.105581.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cmpb.2020.105581</ArticleId><ArticleId IdType="pmc">PMC7274128</ArticleId><ArticleId IdType="pubmed">32534344</ArticleId></ArticleIdList></Reference><Reference><Citation>Li MD, Arun NT, Gidwani M, Chang K, Deng F, Little BP, Mendoza DP, Lang M, Lee SI, O'Shea A, Parakh A, Singh P, Kalpathy-Cramer J. Automated assessment and tracking of COVID-19 pulmonary disease severity on chest radiographs using convolutional Siamese neural networks. Radiol Artif Intell. 2020;2:e200079. doi: 10.1148/ryai.2020200079.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/ryai.2020200079</ArticleId><ArticleId IdType="pmc">PMC7392327</ArticleId><ArticleId IdType="pubmed">33928256</ArticleId></ArticleIdList></Reference><Reference><Citation>Little BP. Disease severity scoring for COVID-19: a welcome Semiquantitative role for chest radiography. Radiology. 2022;302:470&#x2013;472. doi: 10.1148/radiol.2021212212.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2021212212</ArticleId><ArticleId IdType="pmc">PMC8451247</ArticleId><ArticleId IdType="pubmed">34519581</ArticleId></ArticleIdList></Reference><Reference><Citation>Makhnevich A, Sinvani L, Cohen SL, Feldhamer KH, Zhang M, Lesser ML, McGinn TG. The clinical utility of chest radiography for identifying pneumonia: accounting for diagnostic uncertainty in radiology reports. AJR Am J Roentgenol. 2019;213:1207&#x2013;1212. doi: 10.2214/AJR.19.21521.</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/AJR.19.21521</ArticleId><ArticleId IdType="pubmed">31509449</ArticleId></ArticleIdList></Reference><Reference><Citation>Makhnevich A, Sinvani L, Feldhamer KH, Zhang M, Richardson S, McGinn TG, Cohen SL. Comparison of chest radiograph impressions for diagnosing pneumonia: accounting for categories of language certainty. J Am Coll Radiol. 2022;19(10):1130&#x2013;1137. doi: 10.1016/j.jacr.2022.05.020.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jacr.2022.05.020</ArticleId><ArticleId IdType="pubmed">35792164</ArticleId></ArticleIdList></Reference><Reference><Citation>Malhotra P, Gupta S, Koundal D, Zaguia A, Enbeyle W. Deep neural networks for medical image segmentation. J Healthcare Eng. 2022;2022:9580991. doi: 10.1155/2022/9580991.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2022/9580991</ArticleId><ArticleId IdType="pmc">PMC8930223</ArticleId><ArticleId IdType="pubmed">35310182</ArticleId></ArticleIdList></Reference><Reference><Citation>Mettler FA, Jr, Bhargavan M, Faulkner K, Gilley DB, Gray JE, Ibbott GS, Lipoti JA, Mahesh M, McCrohan JL, Stabin MG, Thomadsen BR, Yoshizumi TT. Radiologic and nuclear medicine studies in the United States and worldwide: frequency, radiation dose, and comparison with other radiation sources--1950-2007. Radiology. 2009;253:520&#x2013;531. doi: 10.1148/radiol.2532082010.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2532082010</ArticleId><ArticleId IdType="pubmed">19789227</ArticleId></ArticleIdList></Reference><Reference><Citation>Monaco CG, Zaottini F, Schiaffino S, Villa A, Della Pepa G, Carbonaro LA, Menicagli L, Cozzi A, Carriero S, Arpaia F, Di Leo G, Astengo D, Rosenberg I, Sardanelli F. Chest x-ray severity score in COVID-19 patients on emergency department admission: a two-Centre study. Eur Radiol Exp. 2020;4:68. doi: 10.1186/s41747-020-00195-w.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s41747-020-00195-w</ArticleId><ArticleId IdType="pmc">PMC7735892</ArticleId><ArticleId IdType="pubmed">33319321</ArticleId></ArticleIdList></Reference><Reference><Citation>Mushtaq J, Pennella R, Lavalle S, Colarieti A, Steidler S, Martinenghi CMA, Palumbo D, Esposito A, Rovere-Querini P, Tresoldi M, Landoni G, Ciceri F, Zangrillo A, De Cobelli F. Initial chest radiographs and artificial intelligence (AI) predict clinical outcomes in COVID-19 patients: analysis of 697 Italian patients. Eur Radiol. 2021;31:1770&#x2013;1779. doi: 10.1007/s00330-020-07269-8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-020-07269-8</ArticleId><ArticleId IdType="pmc">PMC7499014</ArticleId><ArticleId IdType="pubmed">32945968</ArticleId></ArticleIdList></Reference><Reference><Citation>Mutasa S, Sun S, Ha R. Understanding artificial intelligence based radiology studies: what is overfitting? Clin Imaging. 2020;65:96&#x2013;99. doi: 10.1016/j.clinimag.2020.04.025.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.clinimag.2020.04.025</ArticleId><ArticleId IdType="pmc">PMC8150901</ArticleId><ArticleId IdType="pubmed">32387803</ArticleId></ArticleIdList></Reference><Reference><Citation>Oh Y, Park S, Ye JC. Deep learning COVID-19 features on CXR using limited training data sets. IEEE Trans Med Imaging. 2020;39:2688&#x2013;2700. doi: 10.1109/TMI.2020.2993291.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TMI.2020.2993291</ArticleId><ArticleId IdType="pubmed">32396075</ArticleId></ArticleIdList></Reference><Reference><Citation>Rajaraman S, Siegelman J, Alderson PO, Folio LS, Folio LR, Antani SK. Iteratively pruned deep learning ensembles for COVID-19 detection in chest X-rays. IEEE Access. 2020;8:115041&#x2013;115050. doi: 10.1109/ACCESS.2020.3003810.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2020.3003810</ArticleId><ArticleId IdType="pmc">PMC7394290</ArticleId><ArticleId IdType="pubmed">32742893</ArticleId></ArticleIdList></Reference><Reference><Citation>Rajpurkar, P., Irvin, J., Zhu, K., Yang, B., Mehta, H., Duan, T., Ding, D., Bagul, A., Langlotz, C., Shpanskaya, K., Lungren, M.P., Ng, A.Y., 2017. Radiologist-level pneumonia detection on chest X-rays with deep learning. arXiv.</Citation></Reference><Reference><Citation>Reeves RA, Pomeranz C, Gomella AA, Gulati A, Metra B, Hage AN, Lange S, Parekh M, Donuru A, Lakhani P, Sundaram B. Performance of a severity score on admission chest radiography in predicting clinical outcomes in hospitalized patients with coronavirus disease (COVID-19) AJR Am J Roentgenol. 2021;217:623&#x2013;632. doi: 10.2214/AJR.20.24801.</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/AJR.20.24801</ArticleId><ArticleId IdType="pubmed">33112201</ArticleId></ArticleIdList></Reference><Reference><Citation>Roberts M, Driggs D, Thorpe M, Gilbey J, Yeung M, Ursprung S, Aviles-Rivero AI, Etmann C, McCague C, Beer L, Weir-McCall JR, Teng Z, Gkrania-Klotsas E, Ruggiero A, Korhonen A, Jefferson E, Ako E, Langs G, Gozaliasl G, Yang G, Prosch H, Preller J, Stanczuk J, Tang J, Hofmanninger J, Babar J, S&#xe1;nchez LE, Thillai M, Gonzalez PM, Teare P, Zhu X, Patel M, Cafolla C, Azadbakht H, Jacob J, Lowe J, Zhang K, Bradley K, Wassin M, Holzer M, Ji K, Ortet MD, Ai T, Walton N, Lio P, Stranks S, Shadbahr T, Lin W, Zha Y, Niu Z, Rudd JHF, Sala E, Sch&#xf6;nlieb C-B, Aix C. Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans. Nat Machine Intell. 2021;3:199&#x2013;217. doi: 10.1038/s42256-021-00307-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s42256-021-00307-0</ArticleId></ArticleIdList></Reference><Reference><Citation>Ronneberger, O., Fischer, P., Brox, T., 2015. Convolutional networks for biomedical image segmentation. arXiv.</Citation></Reference><Reference><Citation>Samek W, Binder A, Montavon G, Lapuschkin S, M&#xfc;ller KR. Evaluating the visualization of what a deep neural network has learned. IEEE Trans Neural Netw Learn Syst. 2016;28(11):2660&#x2013;2673. doi: 10.1109/TNNLS.2016.2599820.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TNNLS.2016.2599820</ArticleId><ArticleId IdType="pubmed">27576267</ArticleId></ArticleIdList></Reference><Reference><Citation>Seah JCY, Tang CHM, Buchlak QD, Holt XG, Wardman JB, Aimoldin A, Esmaili N, Ahmad H, Pham H, Lambert JF, Hachey B, Hogg SJF, Johnston BP, Bennett C, Oakden-Rayner L, Brotchie P, Jones CM. Effect of a comprehensive deep-learning model on the accuracy of chest x-ray interpretation by radiologists: a retrospective, multireader multicase study. Lancet Digit Health. 2021;3:e496&#x2013;e506. doi: 10.1016/S2589-7500(21)00106-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S2589-7500(21)00106-0</ArticleId><ArticleId IdType="pubmed">34219054</ArticleId></ArticleIdList></Reference><Reference><Citation>Selvaraju RR, Cogswell M, Das A, Vedantam R, Parikh D, Batra D. Grad-CAM: visual explanations from deep networks via gradient-based localization. Int J Comput Vis. 2020;128:336&#x2013;359. doi: 10.1007/s11263-019-01228-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11263-019-01228-7</ArticleId></ArticleIdList></Reference><Reference><Citation>Seyyed-Kalantari L, Zhang H, McDermott MBA, Chen IY, Ghassemi M. Underdiagnosis bias of artificial intelligence algorithms applied to chest radiographs in under-served patient populations. Nat Med. 2021;27:2176&#x2013;2182. doi: 10.1038/s41591-021-01595-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41591-021-01595-0</ArticleId><ArticleId IdType="pmc">PMC8674135</ArticleId><ArticleId IdType="pubmed">34893776</ArticleId></ArticleIdList></Reference><Reference><Citation>Shiraishi J, Katsuragawa S, Ikezoe J, Matsumoto T, Kobayashi T, Komatsu K, Matsui M, Fujita H, Kodera Y, Doi K. Development of a digital image database for chest radiographs with and without a lung nodule: receiver operating characteristic analysis of radiologists' detection of pulmonary nodules. AJR Am J Roentgenol. 2000;174:71&#x2013;74. doi: 10.2214/ajr.174.1.1740071.</Citation><ArticleIdList><ArticleId IdType="doi">10.2214/ajr.174.1.1740071</ArticleId><ArticleId IdType="pubmed">10628457</ArticleId></ArticleIdList></Reference><Reference><Citation>United Nations Scientific Committee on the Effects of Atomic Radiation . Sources and effects of ionizing radiation. New York: United Nations; 2008.</Citation></Reference><Reference><Citation>van Ginneken B, Stegmann MB, Loog M. Segmentation of anatomical structures in chest radiographs using supervised methods: a comparative study on a public database. Med Image Anal. 2006;10:19&#x2013;40. doi: 10.1016/j.media.2005.02.002.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2005.02.002</ArticleId><ArticleId IdType="pubmed">15919232</ArticleId></ArticleIdList></Reference><Reference><Citation>Voigt I, Mighali M, Manda D, Aurich P, Bruder O. Radiographic assessment of lung edema (RALE) score is associated with clinical outcomes in patients with refractory cardiogenic shock and refractory cardiac arrest after percutaneous implantation of extracorporeal life support. Intern Emerg Med. 2022;17:1463&#x2013;1470. doi: 10.1007/s11739-022-02937-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11739-022-02937-7</ArticleId><ArticleId IdType="pubmed">35169942</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang G, Liu X, Shen J, Wang C, Li Z, Ye L, Wu X, Chen T, Wang K, Zhang X, Zhou Z, Yang J, Sang Y, Deng R, Liang W, Yu T, Gao M, Wang J, Yang Z, Cai H, Lu G, Zhang L, Yang L, Xu W, Wang W, Olvera A, Ziyar I, Zhang C, Li O, Liao W, Liu J, Chen W, Chen W, Shi J, Zheng L, Zhang L, Yan Z, Zou X, Lin G, Cao G, Lau LL, Mo L, Liang Y, Roberts M, Sala E, Sch&#xf6;nlieb CB, Fok M, Lau JY, Xu T, He J, Zhang K, Li W, Lin T. A deep-learning pipeline for the diagnosis and discrimination of viral, non-viral and COVID-19 pneumonia from chest X-ray images. Nat Biomed Eng. 2021;5:509&#x2013;521. doi: 10.1038/s41551-021-00704-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41551-021-00704-1</ArticleId><ArticleId IdType="pmc">PMC7611049</ArticleId><ArticleId IdType="pubmed">33859385</ArticleId></ArticleIdList></Reference><Reference><Citation>Wu JT, Wong KCL, Gur Y, Ansari N, Karargyris A, Sharma A, Morris M, Saboury B, Ahmad H, Boyko O, Syed A, Jadhav A, Wang H, Pillai A, Kashyap S, Moradi M, Syeda-Mahmood T. Comparison of chest radiograph interpretations by artificial intelligence algorithm vs radiology residents. JAMA Netw Open. 2020;3:e2022779. doi: 10.1001/jamanetworkopen.2020.22779.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jamanetworkopen.2020.22779</ArticleId><ArticleId IdType="pmc">PMC7547369</ArticleId><ArticleId IdType="pubmed">33034642</ArticleId></ArticleIdList></Reference><Reference><Citation>Yamashita R, Nishio M, Do RKG, Togashi K. Convolutional neural networks: an overview and application in radiology. Insights Imaging. 2018;9:611&#x2013;629. doi: 10.1007/s13244-018-0639-9.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s13244-018-0639-9</ArticleId><ArticleId IdType="pmc">PMC6108980</ArticleId><ArticleId IdType="pubmed">29934920</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., Xiong, H., He, Q., 2019. A comprehensive survey on transfer learning. arXiv.</Citation></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36596937</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>06</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1618-727X</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>03</Day></PubDate></JournalIssue><Title>Journal of digital imaging</Title><ISOAbbreviation>J Digit Imaging</ISOAbbreviation></Journal><ArticleTitle>Identification of Asymptomatic COVID-19 Patients on Chest CT Images Using Transformer-Based or Convolutional Neural Network-Based Deep Learning Models.</ArticleTitle><Pagination><StartPage>1</StartPage><EndPage>10</EndPage><MedlinePgn>1-10</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1007/s10278-022-00754-0</ELocationID><Abstract><AbstractText>Novel coronavirus disease 2019 (COVID-19) has rapidly spread throughout the world; however, it is difficult for clinicians to make early diagnoses. This study is to evaluate the feasibility of using deep learning (DL) models to identify asymptomatic COVID-19 patients based on chest CT images. In this retrospective study, six DL models (Xception, NASNet, ResNet, EfficientNet, ViT, and Swin), based on convolutional neural networks (CNNs) or transformer architectures, were trained to identify asymptomatic patients with COVID-19 on chest CT images. Data from Yangzhou were randomly split into a training set (n&#x2009;=&#x2009;2140) and an internal-validation set (n&#x2009;=&#x2009;360). Data from Suzhou was the external-test set (n&#x2009;=&#x2009;200). Model performance was assessed by the metrics accuracy, recall, and specificity and was compared with the assessments of two radiologists. A total of 2700 chest CT images were collected in this study. In the validation dataset, the Swin model achieved the highest accuracy of 0.994, followed by the EfficientNet model (0.954). The recall and the precision of the Swin model were 0.989 and 1.000, respectively. In the test dataset, the Swin model was still the best and achieved the highest accuracy (0.980). All the DL models performed remarkably better than the two experts. Last, the time on the test set diagnosis spent by two experts-42&#xa0;min, 17&#xa0;s (junior); and 29&#xa0;min, 43&#xa0;s (senior)-was significantly higher than those of the DL models (all below 2&#xa0;min). This study evaluated the feasibility of multiple DL models in distinguishing asymptomatic patients with COVID-19 from healthy subjects on chest CT images. It found that a transformer-based model, the Swin model, performed best.</AbstractText><CopyrightInformation>&#xa9; 2022. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y" EqualContrib="Y"><LastName>Yin</LastName><ForeName>Minyue</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Gastroenterology, the First Affiliated Hospital of Soochow University, Suzhou, 215006, Jiangsu, China.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Suzhou Clinical Center of Digestive Diseases, Suzhou, 215006, Jiangsu, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y" EqualContrib="Y"><LastName>Liang</LastName><ForeName>Xiaolong</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>Department of Orthopedics, the First Affiliated Hospital of Soochow University, Suzhou, 215006, Jiangsu, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y" EqualContrib="Y"><LastName>Wang</LastName><ForeName>Zilan</ForeName><Initials>Z</Initials><AffiliationInfo><Affiliation>Department of Neurosurgery, the First Affiliated Hospital of Soochow University, Suzhou, 215006, Jiangsu, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhou</LastName><ForeName>Yijia</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Medical School, Soochow University, Suzhou, 215006, Jiangsu, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>He</LastName><ForeName>Yu</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Medical School, Soochow University, Suzhou, 215006, Jiangsu, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Xue</LastName><ForeName>Yuhan</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Medical School, Soochow University, Suzhou, 215006, Jiangsu, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Gao</LastName><ForeName>Jingwen</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Department of Gastroenterology, the First Affiliated Hospital of Soochow University, Suzhou, 215006, Jiangsu, China.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Suzhou Clinical Center of Digestive Diseases, Suzhou, 215006, Jiangsu, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lin</LastName><ForeName>Jiaxi</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Department of Gastroenterology, the First Affiliated Hospital of Soochow University, Suzhou, 215006, Jiangsu, China.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Suzhou Clinical Center of Digestive Diseases, Suzhou, 215006, Jiangsu, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yu</LastName><ForeName>Chenyan</ForeName><Initials>C</Initials><AffiliationInfo><Affiliation>Department of Gastroenterology, the First Affiliated Hospital of Soochow University, Suzhou, 215006, Jiangsu, China.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Suzhou Clinical Center of Digestive Diseases, Suzhou, 215006, Jiangsu, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Liu</LastName><ForeName>Lu</ForeName><Initials>L</Initials><AffiliationInfo><Affiliation>Department of Gastroenterology, the First Affiliated Hospital of Soochow University, Suzhou, 215006, Jiangsu, China.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Suzhou Clinical Center of Digestive Diseases, Suzhou, 215006, Jiangsu, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Liu</LastName><ForeName>Xiaolin</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>Department of Gastroenterology, the First Affiliated Hospital of Soochow University, Suzhou, 215006, Jiangsu, China.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Suzhou Clinical Center of Digestive Diseases, Suzhou, 215006, Jiangsu, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Xu</LastName><ForeName>Chao</ForeName><Initials>C</Initials><AffiliationInfo><Affiliation>Department of Radiotherapy, the First Affiliated Hospital of Soochow University, Suzhou, 215006, Jiangsu, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhu</LastName><ForeName>Jinzhou</ForeName><Initials>J</Initials><Identifier Source="ORCID">0000-0003-0544-9248</Identifier><AffiliationInfo><Affiliation>Department of Gastroenterology, the First Affiliated Hospital of Soochow University, Suzhou, 215006, Jiangsu, China. jzzhu@zju.edu.cn.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Suzhou Clinical Center of Digestive Diseases, Suzhou, 215006, Jiangsu, China. jzzhu@zju.edu.cn.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>The 23Rd Ward, Yangzhou Third People's Hospital, Yangzhou, 225000, Jiangsu, China. jzzhu@zju.edu.cn.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>03</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>J Digit Imaging</MedlineTA><NlmUniqueID>9100529</NlmUniqueID><ISSNLinking>0897-1889</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Asymptomatic coronavirus-disease-2019 patients</Keyword><Keyword MajorTopicYN="N">Chest CT images</Keyword><Keyword MajorTopicYN="N">Convolutional neural networks</Keyword><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Transfer learning</Keyword><Keyword MajorTopicYN="N">Transformer</Keyword></KeywordList><CoiStatement>The authors declare no competing interests.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>10</Month><Day>19</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>7</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2022</Year><Month>11</Month><Day>30</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>3</Day><Hour>23</Hour><Minute>23</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>4</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>4</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36596937</ArticleId><ArticleId IdType="pmc">PMC9810383</ArticleId><ArticleId IdType="doi">10.1007/s10278-022-00754-0</ArticleId><ArticleId IdType="pii">10.1007/s10278-022-00754-0</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Guan WJ, Ni ZY, Hu Y, et al. Clinical Characteristics of Coronavirus Disease 2019 in China. N Engl J Med. 2020;382(18):1708&#x2013;1720. doi: 10.1056/NEJMoa2002032.</Citation><ArticleIdList><ArticleId IdType="doi">10.1056/NEJMoa2002032</ArticleId><ArticleId IdType="pmc">PMC7092819</ArticleId><ArticleId IdType="pubmed">32109013</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen G, Lu M, Shi Z, et al. Development and validation of machine learning prediction model based on computed tomography angiography-derived hemodynamics for rupture status of intracranial aneurysms: a Chinese multicenter study. Eur Radiol. 2020;30(9):5170&#x2013;5182. doi: 10.1007/s00330-020-06886-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-020-06886-7</ArticleId><ArticleId IdType="pubmed">32350658</ArticleId></ArticleIdList></Reference><Reference><Citation>Ozdemir MA, Ozdemir GD, Guren O. Classification of COVID-19 electrocardiograms by using hexaxial feature mapping and deep learning. BMC Med Inform Decis Mak. 2021;21(1):170. doi: 10.1186/s12911-021-01521-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s12911-021-01521-x</ArticleId><ArticleId IdType="pmc">PMC8146190</ArticleId><ArticleId IdType="pubmed">34034715</ArticleId></ArticleIdList></Reference><Reference><Citation>Togacar M, Ergen B, Comert Z. COVID-19 detection using deep learning models to exploit Social Mimic Optimization and structured chest X-ray images using fuzzy color and stacking approaches. Comput Biol Med. 2020;121:103805. doi: 10.1016/j.compbiomed.2020.103805.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2020.103805</ArticleId><ArticleId IdType="pmc">PMC7202857</ArticleId><ArticleId IdType="pubmed">32568679</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen N, Zhou M, Dong X, et al. Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study. Lancet. 2020;395(10223):507&#x2013;513. doi: 10.1016/s0140-6736(20)30211-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/s0140-6736(20)30211-7</ArticleId><ArticleId IdType="pmc">PMC7135076</ArticleId><ArticleId IdType="pubmed">32007143</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang D, Hu B, Hu C, et al. Clinical Characteristics of 138 Hospitalized Patients With 2019 Novel Coronavirus-Infected Pneumonia in Wuhan, China. JAMA. 2020;323(11):1061&#x2013;1069. doi: 10.1001/jama.2020.1585.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jama.2020.1585</ArticleId><ArticleId IdType="pmc">PMC7042881</ArticleId><ArticleId IdType="pubmed">32031570</ArticleId></ArticleIdList></Reference><Reference><Citation>Ai T, Yang Z, Hou H, et al. Correlation of Chest CT and RT-PCR Testing for Coronavirus Disease 2019 (COVID-19) in China: A Report of 1014 Cases. Radiology. 2020;296(2):E32&#x2013;e40. doi: 10.1148/radiol.2020200642.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2020200642</ArticleId><ArticleId IdType="pmc">PMC7233399</ArticleId><ArticleId IdType="pubmed">32101510</ArticleId></ArticleIdList></Reference><Reference><Citation>Chung M, Bernheim A, Mei X, et al. CT Imaging Features of 2019 Novel Coronavirus (2019-nCoV) Radiology. 2020;295(1):202&#x2013;207. doi: 10.1148/radiol.2020200230.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2020200230</ArticleId><ArticleId IdType="pmc">PMC7194022</ArticleId><ArticleId IdType="pubmed">32017661</ArticleId></ArticleIdList></Reference><Reference><Citation>Huang C, Wang Y, Li X, et al. Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China. Lancet. 2020;395(10223):497&#x2013;506. doi: 10.1016/s0140-6736(20)30183-5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/s0140-6736(20)30183-5</ArticleId><ArticleId IdType="pmc">PMC7159299</ArticleId><ArticleId IdType="pubmed">31986264</ArticleId></ArticleIdList></Reference><Reference><Citation>Bernheim A, Mei X, Huang M, et al. Chest CT Findings in Coronavirus Disease-19 (COVID-19): Relationship to Duration of Infection. Radiology. 2020;295(3):200463. doi: 10.1148/radiol.2020200463.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2020200463</ArticleId><ArticleId IdType="pmc">PMC7233369</ArticleId><ArticleId IdType="pubmed">32077789</ArticleId></ArticleIdList></Reference><Reference><Citation>Rubin GD, Ryerson CJ, Haramati LB, et al. The Role of Chest Imaging in Patient Management During the COVID-19 Pandemic: A Multinational Consensus Statement From the Fleischner Society. Chest. 2020;158(1):106&#x2013;116. doi: 10.1016/j.chest.2020.04.003.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.chest.2020.04.003</ArticleId><ArticleId IdType="pmc">PMC7138384</ArticleId><ArticleId IdType="pubmed">32275978</ArticleId></ArticleIdList></Reference><Reference><Citation>Wiersinga WJ, Rhodes A, Cheng AC, Peacock SJ, Prescott HC. Pathophysiology, Transmission, Diagnosis, and Treatment of Coronavirus Disease 2019 (COVID-19): A Review. Jama. 2020;324(8):782&#x2013;793. doi: 10.1001/jama.2020.12839.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jama.2020.12839</ArticleId><ArticleId IdType="pubmed">32648899</ArticleId></ArticleIdList></Reference><Reference><Citation>Pham TD. A comprehensive study on classification of COVID-19 on computed tomography with pretrained convolutional neural networks. Sci Rep. 2020;10(1):16942. doi: 10.1038/s41598-020-74164-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-020-74164-z</ArticleId><ArticleId IdType="pmc">PMC7547710</ArticleId><ArticleId IdType="pubmed">33037291</ArticleId></ArticleIdList></Reference><Reference><Citation>Bai HX, Hsieh B, Xiong Z, et al. Performance of Radiologists in Differentiating COVID-19 from Non-COVID-19 Viral Pneumonia at Chest CT. Radiology. 2020;296(2):E46&#x2013;e54. doi: 10.1148/radiol.2020200823.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2020200823</ArticleId><ArticleId IdType="pmc">PMC7233414</ArticleId><ArticleId IdType="pubmed">32155105</ArticleId></ArticleIdList></Reference><Reference><Citation>McCloskey B, Dar O, Zumla A, Heymann DL. Emerging infectious diseases and pandemic potential: status quo and reducing risk of global spread. Lancet Infect Dis. 2014;14(10):1001&#x2013;1010. doi: 10.1016/s1473-3099(14)70846-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/s1473-3099(14)70846-1</ArticleId><ArticleId IdType="pmc">PMC7106439</ArticleId><ArticleId IdType="pubmed">25189351</ArticleId></ArticleIdList></Reference><Reference><Citation>Bai Y, Yao L, Wei T, et al. Presumed Asymptomatic Carrier Transmission of COVID-19. Jama. 2020;323(14):1406&#x2013;1407. doi: 10.1001/jama.2020.2565.</Citation><ArticleIdList><ArticleId IdType="doi">10.1001/jama.2020.2565</ArticleId><ArticleId IdType="pmc">PMC7042844</ArticleId><ArticleId IdType="pubmed">32083643</ArticleId></ArticleIdList></Reference><Reference><Citation>Kronbichler A, Kresse D, Yoon S, Lee KH, Effenberger M, Shin JI. Asymptomatic patients as a source of COVID-19 infections: A systematic review and meta-analysis. Int J Infect Dis. 2020;98:180&#x2013;186. doi: 10.1016/j.ijid.2020.06.052.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ijid.2020.06.052</ArticleId><ArticleId IdType="pmc">PMC7832751</ArticleId><ArticleId IdType="pubmed">32562846</ArticleId></ArticleIdList></Reference><Reference><Citation>Harmon SA, Sanford TH, Xu S, et al. Artificial intelligence for the detection of COVID-19 pneumonia on chest CT using multinational datasets. Nat Commun. 2020;11(1):4080. doi: 10.1038/s41467-020-17971-2.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41467-020-17971-2</ArticleId><ArticleId IdType="pmc">PMC7429815</ArticleId><ArticleId IdType="pubmed">32796848</ArticleId></ArticleIdList></Reference><Reference><Citation>Li Z, Zhong Z, Li Y, et al. From community-acquired pneumonia to COVID-19: a deep learning-based method for quantitative analysis of COVID-19 on thick-section CT scans. Eur Radiol. 2020;30(12):6828&#x2013;6837. doi: 10.1007/s00330-020-07042-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-020-07042-x</ArticleId><ArticleId IdType="pmc">PMC7368602</ArticleId><ArticleId IdType="pubmed">32683550</ArticleId></ArticleIdList></Reference><Reference><Citation>Sedik A, Iliyasu AM, Abd El-Rahiem B, et al: Deploying Machine and Deep Learning Models for Efficient Data-Augmented Detection of COVID-19 Infections. Viruses 12(7):769, 2020. 10.3390/v12070769</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7411959</ArticleId><ArticleId IdType="pubmed">32708803</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang J, Xie Y, Pang G, et al. Viral Pneumonia Screening on Chest X-Rays Using Confidence-Aware Anomaly Detection. IEEE Trans Med Imaging. 2021;40(3):879&#x2013;890. doi: 10.1109/tmi.2020.3040950.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/tmi.2020.3040950</ArticleId><ArticleId IdType="pmc">PMC8544953</ArticleId><ArticleId IdType="pubmed">33245693</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang K, Liu X, Shen J, et al. Clinically Applicable AI System for Accurate Diagnosis, Quantitative Measurements, and Prognosis of COVID-19 Pneumonia Using Computed Tomography. Cell. 2020;181(6):1423&#x2013;1433.e11. doi: 10.1016/j.cell.2020.04.045.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cell.2020.04.045</ArticleId><ArticleId IdType="pmc">PMC7196900</ArticleId><ArticleId IdType="pubmed">32416069</ArticleId></ArticleIdList></Reference><Reference><Citation>Ozturk T, Talo M, Yildirim EA, Baloglu UB, Yildirim O, Rajendra Acharya U. Automated detection of COVID-19 cases using deep neural networks with X-ray images. Comput Biol Med. 2020;121:103792. doi: 10.1016/j.compbiomed.2020.103792.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2020.103792</ArticleId><ArticleId IdType="pmc">PMC7187882</ArticleId><ArticleId IdType="pubmed">32568675</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang L, Lin ZQ, Wong A. COVID-Net: a tailored deep convolutional neural network design for detection of COVID-19 cases from chest X-ray images. Sci Rep. 2020;10(1):19549. doi: 10.1038/s41598-020-76550-z.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-020-76550-z</ArticleId><ArticleId IdType="pmc">PMC7658227</ArticleId><ArticleId IdType="pubmed">33177550</ArticleId></ArticleIdList></Reference><Reference><Citation>Turkoglu M: COVIDetectioNet: COVID-19 diagnosis system based on X-ray images using features selected from pre-learned deep features ensemble. Appl Intell (Dordr) 51:1213&#x2013;1226, 2020. 10.1007/s10489-020-01888-w</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7498308</ArticleId><ArticleId IdType="pubmed">34764550</ArticleId></ArticleIdList></Reference><Reference><Citation>Chollet F: Xception: Deep Learning with Depthwise Separable Convolutions. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR): 1800&#x2013;1807, 2017</Citation></Reference><Reference><Citation>Zoph B, Vasudevan V, Shlens J, et al: Learning Transferable Architectures for Scalable Image Recognition. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition: 8697&#x2013;8710, 2018</Citation></Reference><Reference><Citation>He K, Zhang X, Ren S, et al: Deep Residual Learning for Image Recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR):770&#x2013;778, 2016</Citation></Reference><Reference><Citation>Tan M, Le QVJA: EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.&#xa0;arXiv:19805.11946, 2019</Citation></Reference><Reference><Citation>Liu Z, Lin Y, Cao Y, et al: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. arXiv:2013.14030, 2021</Citation></Reference><Reference><Citation>Dosovitskiy A, Beyer L, Kolesnikov A, et al: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv:2010.11929, 2021</Citation></Reference><Reference><Citation>Roberts A, Chouhan RS, Shahdeo D, et al. A Recent Update on Advanced Molecular Diagnostic Techniques for COVID-19 Pandemic: An Overview. Front Immunol. 2021;12:732756. doi: 10.3389/fimmu.2021.732756.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fimmu.2021.732756</ArticleId><ArticleId IdType="pmc">PMC8712736</ArticleId><ArticleId IdType="pubmed">34970254</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang M, Fu A, Hu B, et al. Nanopore Targeted Sequencing for the Accurate and Comprehensive Detection of SARS-CoV-2 and Other Respiratory Viruses. Small (Weinheim an der Bergstrasse, Germany). 2020;16(32):e2002169. doi: 10.1002/smll.202002169.</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/smll.202002169</ArticleId><ArticleId IdType="pmc">PMC7361204</ArticleId><ArticleId IdType="pubmed">32578378</ArticleId></ArticleIdList></Reference><Reference><Citation>Matute T, Nu&#xf1;ez I, Rivera M, et al: Homebrew reagents for low cost RT-LAMP. medRxiv : the preprint server for health sciences. 2021. 10.1101/2021.05.08.21256891</Citation></Reference><Reference><Citation>Dong L, Zhou J, Niu C, et al. Highly accurate and sensitive diagnostic detection of SARS-CoV-2 by digital PCR. Talanta. 2021;224:121726. doi: 10.1016/j.talanta.2020.121726.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.talanta.2020.121726</ArticleId><ArticleId IdType="pmc">PMC7588801</ArticleId><ArticleId IdType="pubmed">33379001</ArticleId></ArticleIdList></Reference><Reference><Citation>Zu ZY, Jiang MD, Xu PP, et al. Coronavirus Disease 2019 (COVID-19): A Perspective from China. Radiology. 2020;296(2):E15&#x2013;e25. doi: 10.1148/radiol.2020200490.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2020200490</ArticleId><ArticleId IdType="pmc">PMC7233368</ArticleId><ArticleId IdType="pubmed">32083985</ArticleId></ArticleIdList></Reference><Reference><Citation>Rubin GD, Ryerson CJ, Haramati LB, et al. The Role of Chest Imaging in Patient Management during the COVID-19 Pandemic: A Multinational Consensus Statement from the Fleischner Society. Radiology. 2020;296(1):172&#x2013;180. doi: 10.1148/radiol.2020201365.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2020201365</ArticleId><ArticleId IdType="pmc">PMC7233395</ArticleId><ArticleId IdType="pubmed">32255413</ArticleId></ArticleIdList></Reference><Reference><Citation>Vashist SK: In Vitro Diagnostic Assays for COVID-19: Recent Advances and Emerging Trends. Diagnostics (Basel, Switzerland). 10(4):202, 2020. 10.3390/diagnostics10040202</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7235801</ArticleId><ArticleId IdType="pubmed">32260471</ArticleId></ArticleIdList></Reference><Reference><Citation>Dong D, Fang MJ, Tang L, et al. Deep learning radiomic nomogram can predict the number of lymph node metastasis in locally advanced gastric cancer: an international multicenter study. Ann Oncol. 2020;31(7):912&#x2013;920. doi: 10.1016/j.annonc.2020.04.003.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.annonc.2020.04.003</ArticleId><ArticleId IdType="pubmed">32304748</ArticleId></ArticleIdList></Reference><Reference><Citation>Long C, Xu H, Shen Q, et al. Diagnosis of the Coronavirus disease (COVID-19): rRT-PCR or CT? Eur J Radiol. 2020;126:108961. doi: 10.1016/j.ejrad.2020.108961.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ejrad.2020.108961</ArticleId><ArticleId IdType="pmc">PMC7102545</ArticleId><ArticleId IdType="pubmed">32229322</ArticleId></ArticleIdList></Reference><Reference><Citation>Xie X, Zhong Z, Zhao W, Zheng C, Wang F, Liu J. Chest CT for Typical Coronavirus Disease 2019 (COVID-19) Pneumonia: Relationship to Negative RT-PCR Testing. Radiology. 2020;296(2):E41&#x2013;e45. doi: 10.1148/radiol.2020200343.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2020200343</ArticleId><ArticleId IdType="pmc">PMC7233363</ArticleId><ArticleId IdType="pubmed">32049601</ArticleId></ArticleIdList></Reference><Reference><Citation>Li R, Pei S, Chen B, et al. Substantial undocumented infection facilitates the rapid dissemination of novel coronavirus (SARS-CoV-2) Science (New York, NY). 2020;368(6490):489&#x2013;493. doi: 10.1126/science.abb3221.</Citation><ArticleIdList><ArticleId IdType="doi">10.1126/science.abb3221</ArticleId><ArticleId IdType="pmc">PMC7164387</ArticleId><ArticleId IdType="pubmed">32179701</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang S, Kang B, Ma J, et al. A deep learning algorithm using CT images to screen for Corona virus disease (COVID-19) Eur Radiol. 2021;31(8):6096&#x2013;6104. doi: 10.1007/s00330-021-07715-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s00330-021-07715-1</ArticleId><ArticleId IdType="pmc">PMC7904034</ArticleId><ArticleId IdType="pubmed">33629156</ArticleId></ArticleIdList></Reference><Reference><Citation>Shi F, Wang J, Shi J, et al. Review of Artificial Intelligence Techniques in Imaging Data Acquisition, Segmentation, and Diagnosis for COVID-19. IEEE Rev Biomed Eng. 2021;14:4&#x2013;15. doi: 10.1109/rbme.2020.2987975.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/rbme.2020.2987975</ArticleId><ArticleId IdType="pubmed">32305937</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang HK, Cheng Y, Song K. Remaining Useful Life Estimation of Aircraft Engines Using a Joint Deep Learning Model Based on TCNN and Transformer. Comput Intell Neurosci. 2021;2021:5185938. doi: 10.1155/2021/5185938.</Citation><ArticleIdList><ArticleId IdType="doi">10.1155/2021/5185938</ArticleId><ArticleId IdType="pmc">PMC8635935</ArticleId><ArticleId IdType="pubmed">34868292</ArticleId></ArticleIdList></Reference><Reference><Citation>Sen S, Saha S, Chatterjee S, Mirjalili S, Sarkar R: A bi-stage feature selection approach for COVID-19 prediction using chest CT images. Appl Intell (Dordrecht, Netherlands). 51:8985&#x2013;9000, 2021. 10.1007/s10489-021-02292-8</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8053442</ArticleId><ArticleId IdType="pubmed">34764594</ArticleId></ArticleIdList></Reference><Reference><Citation>Bai HX, Wang R, Xiong Z, et al. Artificial Intelligence Augmentation of Radiologist Performance in Distinguishing COVID-19 from Pneumonia of Other Origin at Chest CT. Radiology. 2020;296(3):E156&#x2013;e165. doi: 10.1148/radiol.2020201491.</Citation><ArticleIdList><ArticleId IdType="doi">10.1148/radiol.2020201491</ArticleId><ArticleId IdType="pmc">PMC7233483</ArticleId><ArticleId IdType="pubmed">32339081</ArticleId></ArticleIdList></Reference><Reference><Citation>Celard P, Iglesias EL, Sorribes-Fdez JM, Romero R, Vieira AS, Borrajo L: A survey on deep learning applied to medical images: from simple artificial neural networks to generative models. Neural Comput Appl 1&#x2013;33, 2022. 10.1007/s00521-022-07953-4</Citation><ArticleIdList><ArticleId IdType="pmc">PMC9638354</ArticleId><ArticleId IdType="pubmed">36373133</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhao Y, Wang G, Tang C, et al: A Battle of Network Structures: An Empirical Study of CNN, Transformer, and MLP. arXiv:abs/2108.13002,&#xa0;2021</Citation></Reference><Reference><Citation>Shin HC, Roth HR, Gao M, et al. Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning. IEEE Trans Med Imaging. 2016;35(5):1285&#x2013;1298. doi: 10.1109/tmi.2016.2528162.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/tmi.2016.2528162</ArticleId><ArticleId IdType="pmc">PMC4890616</ArticleId><ArticleId IdType="pubmed">26886976</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36596660</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>03</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1468-2079</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2023</Year><Month>Jan</Month><Day>03</Day></PubDate></JournalIssue><Title>The British journal of ophthalmology</Title><ISOAbbreviation>Br J Ophthalmol</ISOAbbreviation></Journal><ArticleTitle>Deep segmentation of OCTA for evaluation and association of changes of retinal microvasculature with Alzheimer's disease and mild cognitive impairment.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">bjophthalmol-2022-321399</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1136/bjo-2022-321399</ELocationID><Abstract><AbstractText Label="BACKGROUND" NlmCategory="BACKGROUND">Optical coherence tomography angiography (OCTA) enables fast and non-invasive high-resolution imaging of retinal microvasculature and is suggested as a potential tool in the early detection of retinal microvascular changes in Alzheimer's Disease (AD). We developed a standardised OCTA analysis framework and compared their extracted parameters among controls and AD/mild cognitive impairment (MCI) in a cross-section study.</AbstractText><AbstractText Label="METHODS" NlmCategory="METHODS">We defined and extracted geometrical parameters of retinal microvasculature at different retinal layers and in the foveal avascular zone (FAZ) from segmented OCTA images obtained using well-validated state-of-the-art deep learning models. We studied these parameters in 158 subjects (62 healthy control, 55 AD and 41 MCI) using logistic regression to determine their potential in predicting the status of our subjects.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">In the AD group, there was a significant decrease in vessel area and length densities in the inner vascular complexes (IVC) compared with controls. The number of vascular bifurcations in AD is also significantly lower than that of healthy people. The MCI group demonstrated a decrease in vascular area, length densities, vascular fractal dimension and the number of bifurcations in both the superficial vascular complexes (SVC) and the IVC compared with controls. A larger vascular tortuosity in the IVC, and a larger roundness of FAZ in the SVC, can also be observed in MCI compared with controls.</AbstractText><AbstractText Label="CONCLUSION" NlmCategory="CONCLUSIONS">Our study demonstrates the applicability of OCTA for the diagnosis of AD and MCI, and provides a standard tool for future clinical service and research. Biomarkers from retinal OCTA images can provide useful information for clinical decision-making and diagnosis of AD and MCI.</AbstractText><CopyrightInformation>&#xa9; Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y" EqualContrib="Y"><LastName>Xie</LastName><ForeName>Jianyang</ForeName><Initials>J</Initials><Identifier Source="ORCID">0000-0002-4565-5807</Identifier><AffiliationInfo><Affiliation>Cixi Institute of Biomedical Engineering, Ningbo Institute of Materials Technology and Engineering, Chinese Academy of Sciences, Ningbo, Zhejiang, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y" EqualContrib="Y"><LastName>Yi</LastName><ForeName>Quanyong</ForeName><Initials>Q</Initials><Identifier Source="ORCID">0000-0002-9369-3998</Identifier><AffiliationInfo><Affiliation>Ningbo Eye Hospital, Ningbo, Zhejiang, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y" EqualContrib="Y"><LastName>Wu</LastName><ForeName>Yufei</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Department of Ophthalmology, The Affiliated People's Hospital of Ningbo University, Ningbo, Zhejiang, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zheng</LastName><ForeName>Yalin</ForeName><Initials>Y</Initials><Identifier Source="ORCID">0000-0002-7873-0922</Identifier><AffiliationInfo><Affiliation>Department of Eye and Vision Science, University of Liverpool, Liverpool, UK.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Liu</LastName><ForeName>Yonghuai</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Department of Computer Science, Edge Hill University, Ormskirk, UK.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Macerollo</LastName><ForeName>Antonella</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Institute of Systems, Molecular and Integrative Biology, University of Liverpool, Liverpool, UK.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Fu</LastName><ForeName>Huazhu</ForeName><Initials>H</Initials><Identifier Source="ORCID">0000-0002-9702-5524</Identifier><AffiliationInfo><Affiliation>Institute of High Performance Computing, Agency for Science, Technology and Research (A*STAR), Singapore.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Xu</LastName><ForeName>Yanwu</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Intelligent Healthcare Unit, Baidu Inc, Beijing, Haidian, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Jiong</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Cixi Institute of Biomedical Engineering, Ningbo Institute of Materials Technology and Engineering, Chinese Academy of Sciences, Ningbo, Zhejiang, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Behera</LastName><ForeName>Ardhendu</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Department of Computer Science, Edge Hill University, Ormskirk, UK.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Fan</LastName><ForeName>Chenlei</ForeName><Initials>C</Initials><AffiliationInfo><Affiliation>Department of Neurology, The Affiliated People's Hospital of Ningbo University, Ningbo, Zhejiang, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Frangi</LastName><ForeName>Alejandro F</ForeName><Initials>AF</Initials><AffiliationInfo><Affiliation>School of Computing, University of Leeds, Leeds, UK.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Liu</LastName><ForeName>Jiang</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, Guangdong, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lu</LastName><ForeName>Qinkang</ForeName><Initials>Q</Initials><AffiliationInfo><Affiliation>Department of Ophthalmology, Wenzhou Medical University, Wenzhou, Zhejiang, China yitian.zhao@nimte.ac.cn doctorqihong@163.com luqinkang@163.com.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Qi</LastName><ForeName>Hong</ForeName><Initials>H</Initials><Identifier Source="ORCID">0000-0003-3066-8020</Identifier><AffiliationInfo><Affiliation>Ophthalmology, Peking University Third Hospital, Haidian, Beijing, China yitian.zhao@nimte.ac.cn doctorqihong@163.com luqinkang@163.com.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhao</LastName><ForeName>Yitian</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Cixi Institute of Biomedical Engineering, Ningbo Institute of Materials Technology and Engineering, Chinese Academy of Sciences, Ningbo, Zhejiang, China yitian.zhao@nimte.ac.cn doctorqihong@163.com luqinkang@163.com.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>03</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>Br J Ophthalmol</MedlineTA><NlmUniqueID>0421041</NlmUniqueID><ISSNLinking>0007-1161</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Degeneration</Keyword><Keyword MajorTopicYN="N">Retina</Keyword></KeywordList><CoiStatement>Competing interests: None declared.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>4</Month><Day>15</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>17</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>3</Day><Hour>21</Hour><Minute>23</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>4</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>4</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36596660</ArticleId><ArticleId IdType="doi">10.1136/bjo-2022-321399</ArticleId><ArticleId IdType="pii">bjo-2022-321399</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36596624</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>05</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>05</Day></DateRevised><Article PubModel="Print"><Journal><ISSN IssnType="Print">2053-3624</ISSN><JournalIssue CitedMedium="Print"><Volume>10</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Jan</Month></PubDate></JournalIssue><Title>Open heart</Title><ISOAbbreviation>Open Heart</ISOAbbreviation></Journal><ArticleTitle>Deep learning-based prediction of future myocardial infarction using invasive coronary angiography: a feasibility study.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">e002237</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1136/openhrt-2022-002237</ELocationID><Abstract><AbstractText Label="BACKGROUND" NlmCategory="BACKGROUND">Angiographic parameters can facilitate the risk stratification of coronary lesions but remain insufficient in the prediction of future myocardial infarction (MI).</AbstractText><AbstractText Label="AIMS" NlmCategory="OBJECTIVE">We compared the ability of humans, angiographic parameters and deep learning (DL) to predict the lesion that would be responsible for a future MI in a population of patients with non-significant CAD at baseline.</AbstractText><AbstractText Label="METHODS" NlmCategory="METHODS">We retrospectively included patients who underwent invasive coronary angiography (ICA) for MI, in whom a previous angiogram had been performed within 5 years. The ability of human visual assessment, diameter stenosis, area stenosis, quantitative flow ratio (QFR) and DL to predict the future culprit lesion (FCL) was compared.</AbstractText><AbstractText Label="RESULTS" NlmCategory="RESULTS">In total, 746 cropped ICA images of FCL and non-culprit lesions (NCL) were analysed. Predictive models for each modality were developed in a training set before validation in a test set. DL exhibited the best predictive performance with an area under the curve of 0.81, compared with diameter stenosis (0.62, p=0.04), area stenosis (0.58, p=0.05) and QFR (0.67, p=0.13). DL exhibited a significant net reclassification improvement (NRI) compared with area stenosis (0.75, p=0.03) and QFR (0.95, p=0.01), and a positive nonsignificant NRI when compared with diameter stenosis. Among all models, DL demonstrated the highest accuracy (0.78) followed by QFR (0.70) and area stenosis (0.68). Predictions based on human visual assessment and diameter stenosis had the lowest accuracy (0.58).</AbstractText><AbstractText Label="CONCLUSION" NlmCategory="CONCLUSIONS">In this feasibility study, DL outperformed human visual assessment and established angiographic parameters in the prediction of FCLs. Larger studies are now required to confirm this finding.</AbstractText><CopyrightInformation>&#xa9; Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y" EqualContrib="Y"><LastName>Mahendiran</LastName><ForeName>Thabo</ForeName><Initials>T</Initials><Identifier Source="ORCID">0000-0002-0025-8162</Identifier><AffiliationInfo><Affiliation>Cardiology Department, Lausanne University Center Hospital, Lausanne, Switzerland.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Chair of Mathematical Data Science and LTS4 laboratory, EPFL, Lausanne, Switzerland.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y" EqualContrib="Y"><LastName>Thanou</LastName><ForeName>Dorina</ForeName><Initials>D</Initials><AffiliationInfo><Affiliation>Chair of Mathematical Data Science and LTS4 laboratory, EPFL, Lausanne, Switzerland.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Senouf</LastName><ForeName>Ortal</ForeName><Initials>O</Initials><AffiliationInfo><Affiliation>Chair of Mathematical Data Science and LTS4 laboratory, EPFL, Lausanne, Switzerland.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Meier</LastName><ForeName>David</ForeName><Initials>D</Initials><AffiliationInfo><Affiliation>Cardiology Department, Lausanne University Center Hospital, Lausanne, Switzerland.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Dayer</LastName><ForeName>Nicolas</ForeName><Initials>N</Initials><AffiliationInfo><Affiliation>Cardiology Department, Lausanne University Center Hospital, Lausanne, Switzerland.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Aminfar</LastName><ForeName>Fahrang</ForeName><Initials>F</Initials><AffiliationInfo><Affiliation>Cardiology Department, Lausanne University Center Hospital, Lausanne, Switzerland.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Auberson</LastName><ForeName>Denise</ForeName><Initials>D</Initials><AffiliationInfo><Affiliation>Cardiology Department, Lausanne University Center Hospital, Lausanne, Switzerland.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Raita</LastName><ForeName>Omar</ForeName><Initials>O</Initials><AffiliationInfo><Affiliation>Chair of Mathematical Data Science and LTS4 laboratory, EPFL, Lausanne, Switzerland.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Frossard</LastName><ForeName>Pascal</ForeName><Initials>P</Initials><AffiliationInfo><Affiliation>LTS4 laboratory, School of Engineering, EPFL, Lausanne, Switzerland.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Pagnoni</LastName><ForeName>Mattia</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Cardiology Department, Lausanne University Center Hospital, Lausanne, Switzerland.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Cook</LastName><ForeName>St&#xe9;phane</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Cardiology Department, University and hospital Fribourg, Fribourg, Switzerland.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>De Bruyne</LastName><ForeName>Bernard</ForeName><Initials>B</Initials><AffiliationInfo><Affiliation>Cardiovascular Center OLV Aalst, Aalst, Belgium.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Muller</LastName><ForeName>Olivier</ForeName><Initials>O</Initials><AffiliationInfo><Affiliation>Cardiology Department, Lausanne University Center Hospital, Lausanne, Switzerland.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Abb&#xe9;</LastName><ForeName>Emmanuel</ForeName><Initials>E</Initials><AffiliationInfo><Affiliation>Chair of Mathematical Data Science, Institute of Mathematics and School of Computer and Communication Sciences, EPFL, Lausanne, Switzerland stephane.fournier@chuv.ch emmanuel.abbe@epfl.ch.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Fournier</LastName><ForeName>Stephane</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Cardiology Department, Lausanne University Center Hospital, Lausanne, Switzerland stephane.fournier@chuv.ch emmanuel.abbe@epfl.ch.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>Open Heart</MedlineTA><NlmUniqueID>101631219</NlmUniqueID><ISSNLinking>2053-3624</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D023921" MajorTopicYN="Y">Coronary Stenosis</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D017023" MajorTopicYN="N">Coronary Angiography</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D003251" MajorTopicYN="N">Constriction, Pathologic</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D005240" MajorTopicYN="N">Feasibility Studies</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D012189" MajorTopicYN="N">Retrospective Studies</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D003331" MajorTopicYN="N">Coronary Vessels</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D053805" MajorTopicYN="Y">Fractional Flow Reserve, Myocardial</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D009203" MajorTopicYN="Y">Myocardial Infarction</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Coronary Angiography</Keyword><Keyword MajorTopicYN="N">Coronary Artery Disease</Keyword><Keyword MajorTopicYN="N">Myocardial Infarction</Keyword></KeywordList><CoiStatement>Competing interests: None declared.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>12</Month><Day>14</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>14</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>3</Day><Hour>21</Hour><Minute>2</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>4</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>ppublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36596624</ArticleId><ArticleId IdType="doi">10.1136/openhrt-2022-002237</ArticleId><ArticleId IdType="pii">openhrt-2022-002237</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36595679</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>05</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>05</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1091-6490</ISSN><JournalIssue CitedMedium="Internet"><Volume>120</Volume><Issue>2</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>10</Day></PubDate></JournalIssue><Title>Proceedings of the National Academy of Sciences of the United States of America</Title><ISOAbbreviation>Proc Natl Acad Sci U S A</ISOAbbreviation></Journal><ArticleTitle>Anatomically interpretable deep learning of brain age captures domain-specific cognitive impairment.</ArticleTitle><Pagination><StartPage>e2214634120</StartPage><MedlinePgn>e2214634120</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1073/pnas.2214634120</ELocationID><Abstract><AbstractText>The gap between chronological age (CA) and biological brain age, as estimated from magnetic resonance images (MRIs), reflects how individual patterns of neuroanatomic aging deviate from their typical trajectories. MRI-derived brain age (BA) estimates are often obtained using deep learning models that may perform relatively poorly on new data or that lack neuroanatomic interpretability. This study introduces a convolutional neural network (CNN) to estimate BA after training on the MRIs of 4,681 cognitively normal (CN) participants and testing on 1,170 CN participants from an independent sample. BA estimation errors are notably lower than those of previous studies. At both individual and cohort levels, the CNN provides detailed anatomic maps of brain aging patterns that reveal sex dimorphisms and neurocognitive trajectories in adults with mild cognitive impairment (MCI, <i>N</i>&#x2004;=&#x2004;351) and Alzheimer's disease (AD, <i>N</i>&#x2004;=&#x2004;359). In individuals with MCI (54% of whom were diagnosed with dementia within 10.9 y from MRI acquisition), BA is significantly better than CA in capturing dementia symptom severity, functional disability, and executive function. Profiles of sex dimorphism and lateralization in brain aging also map onto patterns of neuroanatomic change that reflect cognitive decline. Significant associations between BA and neurocognitive measures suggest that the proposed framework can map, systematically, the relationship between aging-related neuroanatomy changes in CN individuals and in participants with MCI or AD. Early identification of such neuroanatomy changes can help to screen individuals according to their AD risk.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Yin</LastName><ForeName>Chenzhong</ForeName><Initials>C</Initials><Identifier Source="ORCID">0000-0001-6411-7441</Identifier><AffiliationInfo><Affiliation>Ming Hsieh Department of Electrical and Computer Engineering, Viterbi School of Engineering, University of Southern California, Los Angeles, CA 90089.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Imms</LastName><ForeName>Phoebe</ForeName><Initials>P</Initials><Identifier Source="ORCID">0000-0002-7205-4177</Identifier><AffiliationInfo><Affiliation>Ethel Percy Andrus Gerontology Center, Leonard Davis School of Gerontology, University of Southern California, Los Angeles, CA 90089.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Cheng</LastName><ForeName>Mingxi</ForeName><Initials>M</Initials><Identifier Source="ORCID">0000-0002-8070-6665</Identifier><AffiliationInfo><Affiliation>Ming Hsieh Department of Electrical and Computer Engineering, Viterbi School of Engineering, University of Southern California, Los Angeles, CA 90089.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Amgalan</LastName><ForeName>Anar</ForeName><Initials>A</Initials><Identifier Source="ORCID">0000-0001-8210-5030</Identifier><AffiliationInfo><Affiliation>Ethel Percy Andrus Gerontology Center, Leonard Davis School of Gerontology, University of Southern California, Los Angeles, CA 90089.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Chowdhury</LastName><ForeName>Nahian F</ForeName><Initials>NF</Initials><Identifier Source="ORCID">0000-0003-3535-7280</Identifier><AffiliationInfo><Affiliation>Ethel Percy Andrus Gerontology Center, Leonard Davis School of Gerontology, University of Southern California, Los Angeles, CA 90089.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Massett</LastName><ForeName>Roy J</ForeName><Initials>RJ</Initials><AffiliationInfo><Affiliation>Ethel Percy Andrus Gerontology Center, Leonard Davis School of Gerontology, University of Southern California, Los Angeles, CA 90089.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Chaudhari</LastName><ForeName>Nikhil N</ForeName><Initials>NN</Initials><Identifier Source="ORCID">0000-0003-3048-6710</Identifier><AffiliationInfo><Affiliation>Ethel Percy Andrus Gerontology Center, Leonard Davis School of Gerontology, University of Southern California, Los Angeles, CA 90089.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Corwin D. Denney Research Center, Department of Biomedical Engineering, Viterbi School of Engineering, University of Southern California, Los Angeles, CA 90089.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Chen</LastName><ForeName>Xinghe</ForeName><Initials>X</Initials><Identifier Source="ORCID">0000-0002-8686-359X</Identifier><AffiliationInfo><Affiliation>Ming Hsieh Department of Electrical and Computer Engineering, Viterbi School of Engineering, University of Southern California, Los Angeles, CA 90089.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Thompson</LastName><ForeName>Paul M</ForeName><Initials>PM</Initials><Identifier Source="ORCID">0000-0002-4720-8867</Identifier><AffiliationInfo><Affiliation>Corwin D. Denney Research Center, Department of Biomedical Engineering, Viterbi School of Engineering, University of Southern California, Los Angeles, CA 90089.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Imaging Genetics Center, Stevens Neuroimaging and Informatics Institute, Keck School of Medicine, University of Southern California, Marina del Rey, CA 90033.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Quantitative &amp; Computational Biology, Dana &amp; David Dornsife College of Arts &amp; Sciences, University of Southern California, Los Angeles, CA 90089.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Ophthalmology, Keck School of Medicine, University of Southern California, Los Angeles, CA 90033.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Neurology, Keck School of Medicine, University of Southern California, Los Angeles, CA 90033.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology, Keck School of Medicine, University of Southern California, Los Angeles, CA 90033.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Psychiatry, Keck School of Medicine, University of Southern California, Los Angeles, CA 90033.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Behavioral Sciences, Keck School of Medicine, University of Southern California, Los Angeles, CA 90033.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Bogdan</LastName><ForeName>Paul</ForeName><Initials>P</Initials><Identifier Source="ORCID">0000-0003-2118-0816</Identifier><AffiliationInfo><Affiliation>Ming Hsieh Department of Electrical and Computer Engineering, Viterbi School of Engineering, University of Southern California, Los Angeles, CA 90089.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Irimia</LastName><ForeName>Andrei</ForeName><Initials>A</Initials><Identifier Source="ORCID">0000-0002-9254-9388</Identifier><AffiliationInfo><Affiliation>Ethel Percy Andrus Gerontology Center, Leonard Davis School of Gerontology, University of Southern California, Los Angeles, CA 90089.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Corwin D. Denney Research Center, Department of Biomedical Engineering, Viterbi School of Engineering, University of Southern California, Los Angeles, CA 90089.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Quantitative &amp; Computational Biology, Dana &amp; David Dornsife College of Arts &amp; Sciences, University of Southern California, Los Angeles, CA 90089.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><CollectiveName>Alzheimer&#x2019;s Disease Neuroimaging Initiative</CollectiveName></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>03</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>Proc Natl Acad Sci U S A</MedlineTA><NlmUniqueID>7505876</NlmUniqueID><ISSNLinking>0027-8424</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D000328" MajorTopicYN="N">Adult</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D060825" MajorTopicYN="Y">Cognitive Dysfunction</DescriptorName><QualifierName UI="Q000473" MajorTopicYN="N">pathology</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D001921" MajorTopicYN="N">Brain</DescriptorName><QualifierName UI="Q000473" MajorTopicYN="N">pathology</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D000544" MajorTopicYN="Y">Alzheimer Disease</DescriptorName><QualifierName UI="Q000473" MajorTopicYN="N">pathology</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D008279" MajorTopicYN="N">Magnetic Resonance Imaging</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Alzheimer&#x2019;s disease</Keyword><Keyword MajorTopicYN="N">brain age</Keyword><Keyword MajorTopicYN="N">cognitive impairment</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>3</Day><Hour>15</Hour><Minute>32</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>4</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>6</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>ppublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36595679</ArticleId><ArticleId IdType="doi">10.1073/pnas.2214634120</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM"><PMID Version="1">36595270</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>03</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><ISSN IssnType="Electronic">1741-2552</ISSN><JournalIssue CitedMedium="Internet"><PubDate><Year>2022</Year><Month>Dec</Month><Day>13</Day></PubDate></JournalIssue><Title>Journal of neural engineering</Title><ISOAbbreviation>J Neural Eng</ISOAbbreviation></Journal><ArticleTitle>Multi-tasking deep network for tinnitus classification and severity prediction from multimodal structural MR images.</ArticleTitle><ELocationID EIdType="doi" ValidYN="Y">10.1088/1741-2552/acab33</ELocationID><Abstract><AbstractText>Subjective tinnitus is an auditory phantom perceptual disorder without an objective biomarker. Fast and efficient diagnostic tools will advance clinical practice by detecting or confirming the condition, tracking change in severity, and monitoring treatment response. Motivated by evidence of subtle anatomical, morphological, or functional information in magnetic resonance images (MRI) of the brain, we examine data-driven machine learning methods for joint tinnitus classification (tinnitus or no tinnitus) and tinnitus severity prediction. We propose a deep multi-task multimodal framework for joint functionalities using structural MRI (sMRI) data. To leverage cross-information multimodal neuroimaging data, we integrate two modalities of 3-dimensional sMRI - T1 weighted (T1w) and T2 weighted (T2w) images. To explore the key components in the MR images that drove task performance, we segment both T1w and T2w images into three different components - cerebrospinal fluid (CSF), grey matter (GM) and white matter (WM), and evaluate performance of each segmented image. Results demonstrate that our multimodal framework capitalizes on the information across both modalities (T1w and T2w) for the joint task of tinnitus classification and severity prediction. Our model outperforms existing learning-based and conventional methods in terms of accuracy, sensitivity, specificity, and negative predictive value.</AbstractText><CopyrightInformation>Creative Commons Attribution license.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Lin</LastName><ForeName>Chieh-Te</ForeName><Initials>CT</Initials><AffiliationInfo><Affiliation>University of California San Francisco, 513 Parnassus Ave, San Francisco, California, 94143, UNITED STATES.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Ghosh</LastName><ForeName>Sanjay</ForeName><Initials>S</Initials><Identifier Source="ORCID">0000-0002-0474-5072</Identifier><AffiliationInfo><Affiliation>Radiology and Biomedical Imaging, University of California San Francisco, 513 Parnassus Ave, San Francisco, California, 94143, UNITED STATES.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Hinkley</LastName><ForeName>Leighton B</ForeName><Initials>LB</Initials><AffiliationInfo><Affiliation>University of California San Francisco, 513 Parnassus Ave, San Francisco, California, 94143, UNITED STATES.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Dale</LastName><ForeName>Corby L</ForeName><Initials>CL</Initials><AffiliationInfo><Affiliation>University of California San Francisco, 513 Parnassus Ave, San Francisco, California, 94143, UNITED STATES.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Souza</LastName><ForeName>Ana C S</ForeName><Initials>ACS</Initials><AffiliationInfo><Affiliation>Federal University of Sao Joao del-Rei Midwest Campus Dona Lindu, Frei Orlando, 170, Sao Joao del Rei, MG, 36307, BRAZIL.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Sabes</LastName><ForeName>Jennifer H</ForeName><Initials>JH</Initials><AffiliationInfo><Affiliation>University of California San Francisco, 2380 Sutter St., San Francisco, California, 94115, UNITED STATES.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Hess</LastName><ForeName>Christopher P</ForeName><Initials>CP</Initials><AffiliationInfo><Affiliation>University of California San Francisco, 513 Parnassus Ave, San Francisco, California, 94143, UNITED STATES.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Adams</LastName><ForeName>Meredith E</ForeName><Initials>ME</Initials><AffiliationInfo><Affiliation>University of Minnesota Twin Cities, 516 Delaware St.,, Minneapolis, Minnesota, 55455, UNITED STATES.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Cheung</LastName><ForeName>Steven W</ForeName><Initials>SW</Initials><AffiliationInfo><Affiliation>University of California San Francisco, 2380 Sutter St.,, San Francisco, California, 94115, UNITED STATES.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Nagarajan</LastName><ForeName>Srikantan S</ForeName><Initials>SS</Initials><AffiliationInfo><Affiliation>University of California San Francisco, 513 Parnassus Ave, San Francisco, California, 94143, UNITED STATES.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>13</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>J Neural Eng</MedlineTA><NlmUniqueID>101217933</NlmUniqueID><ISSNLinking>1741-2552</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Deep learning</Keyword><Keyword MajorTopicYN="N">Neuroimaging biomarker</Keyword><Keyword MajorTopicYN="N">Structural MR Images</Keyword><Keyword MajorTopicYN="N">Tinnitus Classification</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>3</Day><Hour>11</Hour><Minute>34</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>4</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>4</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36595270</ArticleId><ArticleId IdType="doi">10.1088/1741-2552/acab33</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">36595239</PMID><DateCompleted><Year>2023</Year><Month>01</Month><Day>11</Day></DateCompleted><DateRevised><Year>2023</Year><Month>01</Month><Day>11</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">1361-6560</ISSN><JournalIssue CitedMedium="Internet"><Volume>68</Volume><Issue>2</Issue><PubDate><Year>2023</Year><Month>Jan</Month><Day>05</Day></PubDate></JournalIssue><Title>Physics in medicine and biology</Title><ISOAbbreviation>Phys Med Biol</ISOAbbreviation></Journal><ArticleTitle>Accelerated cardiac diffusion tensor imaging using deep neural network.</ArticleTitle><ELocationID EIdType="doi" ValidYN="Y">10.1088/1361-6560/acaa86</ELocationID><Abstract><AbstractText>Cardiac diffusion tensor imaging (DTI) is a noninvasive method for measuring the microstructure of the myocardium. However, its long scan time significantly hinders its wide application. In this study, we developed a deep learning framework to obtain high-quality DTI parameter maps from six diffusion-weighted images (DWIs) by combining deep-learning-based image generation and tensor fitting, and named the new framework FG-Net. In contrast to frameworks explored in previous deep-learning-based fast DTI studies, FG-Net generates inter-directional DWIs from six input DWIs to supplement the loss information and improve estimation accuracy for DTI parameters. FG-Net was evaluated using two datasets of<i>ex vivo</i>human hearts. The results showed that FG-Net can generate fractional anisotropy, mean diffusivity maps, and helix angle maps from only six raw DWIs, with a quantification error of less than 5%. FG-Net outperformed conventional tensor fitting and black-box network fitting in both qualitative and quantitative metrics. We also demonstrated that the proposed FG-Net can achieve highly accurate fractional anisotropy and helix angle maps in DWIs with different<i>b</i>-values.</AbstractText><CopyrightInformation>&#xa9; 2023 Institute of Physics and Engineering in Medicine.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Liu</LastName><ForeName>Shaonan</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Paul C. Lauterbur Research Centre for Biomedical Imaging, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, People's Republic of China.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Computer Science, Inner Mongolia University, Hohhot, People's Republic of China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Liu</LastName><ForeName>Yuanyuan</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Paul C. Lauterbur Research Centre for Biomedical Imaging, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, People's Republic of China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Xu</LastName><ForeName>Xi</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>Paul C. Lauterbur Research Centre for Biomedical Imaging, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, People's Republic of China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Chen</LastName><ForeName>Rui</ForeName><Initials>R</Initials><AffiliationInfo><Affiliation>Department of Radiology, Guangdong Provincial People's Hospital Guangdong Academy of Medical Sciences, Guangzhou, People's Republic of China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Liang</LastName><ForeName>Dong</ForeName><Initials>D</Initials><Identifier Source="ORCID">0000-0003-1358-9777</Identifier><AffiliationInfo><Affiliation>Paul C. Lauterbur Research Centre for Biomedical Imaging, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, People's Republic of China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Jin</LastName><ForeName>Qiyu</ForeName><Initials>Q</Initials><Identifier Source="ORCID">0000-0001-8639-233X</Identifier><AffiliationInfo><Affiliation>Department of Mathematical Science, Inner Mongolia University, Hohhot, People's Republic of China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Liu</LastName><ForeName>Hui</ForeName><Initials>H</Initials><AffiliationInfo><Affiliation>Department of Radiology, Guangdong Provincial People's Hospital Guangdong Academy of Medical Sciences, Guangzhou, People's Republic of China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Chen</LastName><ForeName>Guoqing</ForeName><Initials>G</Initials><AffiliationInfo><Affiliation>Department of Mathematical Science, Inner Mongolia University, Hohhot, People's Republic of China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhu</LastName><ForeName>Yanjie</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Paul C. Lauterbur Research Centre for Biomedical Imaging, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, People's Republic of China.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>National Center for Applied Mathematics Shenzhen, Shenzhen, Guangdong, People's Republic of China.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>01</Month><Day>05</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>Phys Med Biol</MedlineTA><NlmUniqueID>0401220</NlmUniqueID><ISSNLinking>0031-9155</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D056324" MajorTopicYN="Y">Diffusion Tensor Imaging</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D007091" MajorTopicYN="Y">Image Processing, Computer-Assisted</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D038524" MajorTopicYN="N">Diffusion Magnetic Resonance Imaging</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D016571" MajorTopicYN="N">Neural Networks, Computer</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D006321" MajorTopicYN="N">Heart</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D016880" MajorTopicYN="N">Anisotropy</DescriptorName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">cardiac diffusion tensor imaging (DTI)</Keyword><Keyword MajorTopicYN="N">convolutional neural network</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2022</Year><Month>5</Month><Day>23</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2022</Year><Month>12</Month><Day>9</Day></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>4</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>12</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>3</Day><Hour>11</Hour><Minute>33</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36595239</ArticleId><ArticleId IdType="doi">10.1088/1361-6560/acaa86</ArticleId></ArticleIdList></PubmedData></PubmedArticle><PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">36594904</PMID><DateRevised><Year>2023</Year><Month>01</Month><Day>04</Day></DateRevised><Article PubModel="Print-Electronic"><Journal><JournalIssue CitedMedium="Print"><Volume>13583</Volume><PubDate><Year>2022</Year><Month>Sep</Month></PubDate></JournalIssue><Title>Machine learning in medical imaging. MLMI (Workshop)</Title><ISOAbbreviation>Mach Learn Med Imaging</ISOAbbreviation></Journal><ArticleTitle>Understanding Clinical Progression of Late-Life Depression to Alzheimer's Disease Over 5 Years with Structural MRI.</ArticleTitle><Pagination><StartPage>259</StartPage><EndPage>268</EndPage><MedlinePgn>259-268</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1007/978-3-031-21014-3_27</ELocationID><Abstract><AbstractText>Previous studies have shown that late-life depression (LLD) may be a precursor of neurodegenerative diseases and may increase the risk of dementia. At present, the pathological relationship between LLD and dementia, in particularly Alzheimer's disease (AD) is unclear. Structural MRI (sMRI) can provide objective biomarkers for the computer-aided diagnosis of LLD and AD, providing a promising solution to understand the clinical progression of brain disorders. But few studies have focused on sMRI-based predictive analysis of clinical progression from LLD to AD. In this paper, we develop a deep learning method to predict the clinical progression of LLD to AD up to 5 years after baseline time using T1-weighted structural MRIs. We also analyze several important factors that limit the diagnostic performance of learning-based methods, including data imbalance, small-sample-size, and multi-site data heterogeneity, by leveraging a relatively large-scale database to aid model training. Experimental results on 308 subjects with sMRIs acquired from 2 imaging sites and the publicly available ADNI database demonstrate the potential of deep learning in predicting the clinical progression of LLD to AD. To the best of our knowledge, this is among the first attempts to explore the complex pathophysiological relationship between LLD and AD based on structural MRI using a deep learning method.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Lintao</ForeName><Initials>L</Initials><AffiliationInfo><Affiliation>School of Information Science and Engineering, Linyi University, Shandong, China.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Radiology and Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill 27599, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Yu</LastName><ForeName>Minhui</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Radiology and Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill 27599, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wang</LastName><ForeName>Lihong</ForeName><Initials>L</Initials><AffiliationInfo><Affiliation>Department of Psychiatry, University of Connecticut School of Medicine, University of Connecticut, Farmington, CT, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Steffens</LastName><ForeName>David C</ForeName><Initials>DC</Initials><AffiliationInfo><Affiliation>Department of Psychiatry, University of Connecticut School of Medicine, University of Connecticut, Farmington, CT, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wu</LastName><ForeName>Rong</ForeName><Initials>R</Initials><AffiliationInfo><Affiliation>Connecticut Convergence Institute for Translation in Regenerative Engineering, University of Connecticut Health, Farmington, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Potter</LastName><ForeName>Guy G</ForeName><Initials>GG</Initials><AffiliationInfo><Affiliation>Department of Psychiatry and Behavioral Sciences, Duke University Medical Center, Durham, NC, USA.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Liu</LastName><ForeName>Mingxia</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Department of Radiology and Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill 27599, USA.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><GrantList CompleteYN="Y"><Grant><GrantID>R01 AG041721</GrantID><Acronym>AG</Acronym><Agency>NIA NIH HHS</Agency><Country>United States</Country></Grant><Grant><GrantID>RF1 AG073297</GrantID><Acronym>AG</Acronym><Agency>NIA NIH HHS</Agency><Country>United States</Country></Grant></GrantList><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2022</Year><Month>12</Month><Day>16</Day></ArticleDate></Article><MedlineJournalInfo><Country>Germany</Country><MedlineTA>Mach Learn Med Imaging</MedlineTA><NlmUniqueID>101641981</NlmUniqueID></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Alzheimer&#x2019;s disease</Keyword><Keyword MajorTopicYN="N">Cognitive impairment</Keyword><Keyword MajorTopicYN="N">Late-life depression</Keyword><Keyword MajorTopicYN="N">Structural MRI</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>1</Month><Day>3</Day><Hour>10</Hour><Minute>53</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>1</Month><Day>4</Day><Hour>6</Hour><Minute>0</Minute></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>1</Month><Day>4</Day><Hour>6</Hour><Minute>1</Minute></PubMedPubDate></History><PublicationStatus>ppublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">36594904</ArticleId><ArticleId IdType="mid">NIHMS1859375</ArticleId><ArticleId IdType="pmc">PMC9805302</ArticleId><ArticleId IdType="doi">10.1007/978-3-031-21014-3_27</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Burke AD, Goldfarb D, Bollam P, Khokher S: Diagnosing and treating depression in patients with Alzheimer&#x2019;s disease. Neurol. Therapy 8(2), 325&#x2013;350 (2019)</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6858899</ArticleId><ArticleId IdType="pubmed">31435870</ArticleId></ArticleIdList></Reference><Reference><Citation>Rashidi-Ranjbar N, et al.: Frontal-executive and corticolimbic structural brain circuitry in older people with remitted depression, mild cognitive impairment, Alzheimer&#x2019;s dementia, and normal cognition. Neuropsychopharmacology 45(9), 1567&#x2013;1578 (2020)</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7360554</ArticleId><ArticleId IdType="pubmed">32422643</ArticleId></ArticleIdList></Reference><Reference><Citation>Lebedeva AK, et al.: MRI-based classification models in prediction of mild cognitive impairment and dementia in late-life depression. Front. Aging Neurosci 13, 1&#x2013;9 (2017)</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5288688</ArticleId><ArticleId IdType="pubmed">28210220</ArticleId></ArticleIdList></Reference><Reference><Citation>Butters MA, et al.: Pathways linking late-life depression to persistent cognitive impairment and dementia. Dialogues in Clinical Neuroscience (2022)</Citation><ArticleIdList><ArticleId IdType="pmc">PMC2872078</ArticleId><ArticleId IdType="pubmed">18979948</ArticleId></ArticleIdList></Reference><Reference><Citation>Joko T, et al.: Patterns of hippocampal atrophy differ among Alzheimer&#x2019;s disease, amnestic mild cognitive impairment, and late-life depression. Psychogeriatrics 16(6), 355&#x2013;361 (2016)</Citation><ArticleIdList><ArticleId IdType="pubmed">26756596</ArticleId></ArticleIdList></Reference><Reference><Citation>Alexopoulos GS: Mechanisms and treatment of late-life depression. Transl. Psych 9(1), 1&#x2013;16 (2019)</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6683149</ArticleId><ArticleId IdType="pubmed">31383842</ArticleId></ArticleIdList></Reference><Reference><Citation>Geerlings MI, Gerritsen L: Late-life depression, hippocampal volumes, and hypothalamic-pituitary-adrenal axis regulation: a systematic review and meta-analysis. Biol. Psychiat 82(5), 339&#x2013;350 (2017)</Citation><ArticleIdList><ArticleId IdType="pubmed">28318491</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhou T, Thung KH, Liu M, Shen D: Brain-wide genome-wide association study for Alzheimer&#x2019;s disease via joint projection learning and sparse regression model. IEEE Trans. Biomed. Eng 66(1), 165&#x2013;175 (2018)</Citation><ArticleIdList><ArticleId IdType="pmc">PMC6342004</ArticleId><ArticleId IdType="pubmed">29993426</ArticleId></ArticleIdList></Reference><Reference><Citation>Hermida AP, McDonald WM, Steenland K, Levey A: The association between late-life depression, mild cognitive impairment and dementia: Is inflammation the missing link? Expert Rev. Neurother 12(11), 1339&#x2013;1350 (2012)</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4404497</ArticleId><ArticleId IdType="pubmed">23234395</ArticleId></ArticleIdList></Reference><Reference><Citation>Teodorczuk A, et al.: Relationship between baseline white-matter changes and development of late-life depressive symptoms: 3-year results from the LADIS study. Psychol. Med 40(4), 603&#x2013;610 (2010)</Citation><ArticleIdList><ArticleId IdType="pubmed">19671212</ArticleId></ArticleIdList></Reference><Reference><Citation>Guan H, Liu Y, Xiao S, Yue L, Liu M: Cost-sensitive meta-learning for progress prediction of subjective cognitive decline with brain structural MRI. In: de Bruijne M, et al. (eds.) MICCAI 2021. LNCS, vol. 12905, pp. 248&#x2013;258. Springer, Cham: (2021). 10.1007/978-3-030-87240-324</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-3-030-87240-324</ArticleId></ArticleIdList></Reference><Reference><Citation>Mousavian M, Chen J, Greening S: Depression detection using feature extraction and deep learning from sMRI images. In: 18th IEEE International Conference On Machine Learning And Applications (ICMLA). IEEE 2019, pp. 1731&#x2013;1736 (2019)</Citation></Reference><Reference><Citation>Yang E, Wang L, Steffens D, Potter G, Liu M: Deep factor regression for computer-aided analysis of major depressive disorders with structural MRI data. In: IEEE 18th International Symposium on Biomedical Imaging (ISBI). IEEE 2021, pp. 208&#x2013;211 (2021)</Citation></Reference><Reference><Citation>Jack CR Jr, et al.: The Alzheimer&#x2019;s disease neuroimaging initiative (ADNI): MRI methods. J. Magn. Resonan. Imaging Official J. Int. Soc. Magn. Resonan. Med 27(4), 685&#x2013;691 (2008)</Citation><ArticleIdList><ArticleId IdType="pmc">PMC2544629</ArticleId><ArticleId IdType="pubmed">18302232</ArticleId></ArticleIdList></Reference><Reference><Citation>Steffens DC, et al.: Methodology and preliminary results from the neurocognitive outcomes of depression in the elderly study. J. Geriatr. Psychiatr. Neurol 17(4), 202&#x2013;211 (2004)</Citation><ArticleIdList><ArticleId IdType="pubmed">15533991</ArticleId></ArticleIdList></Reference><Reference><Citation>Steffens DC, Wang L, Manning KJ, Pearlson GD: Negative affectivity, aging, and depression: Results from the Neurobiology of Late-Life Depression (NBOLD) study. Am. J. Geriatr. Psychiatr 25(10), 1135&#x2013;1149 (2017)</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5600659</ArticleId><ArticleId IdType="pubmed">28457805</ArticleId></ArticleIdList></Reference><Reference><Citation>Jenkinson M, Beckmann CF, Behrens TE, Woolrich MW, Smith SM: FSL. Neuroimage 62(2), 782&#x2013;790 (2012)</Citation><ArticleIdList><ArticleId IdType="pubmed">21979382</ArticleId></ArticleIdList></Reference><Reference><Citation>Avants BB, Tustison N, Song G, et al.: Advanced normalization tools (ANTS). Insight J. 2(365), 1&#x2013;35 (2009)</Citation></Reference><Reference><Citation>Rolls ET, Huang CC, Lin CP, Feng J, Joliot M: Automated anatomical labelling atlas 3. Neuroimage 206, 116189 (2020)</Citation><ArticleIdList><ArticleId IdType="pubmed">31521825</ArticleId></ArticleIdList></Reference><Reference><Citation>He K, Zhang X, Ren S, Sun J: Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770&#x2013;778 (2016)</Citation></Reference><Reference><Citation>Chen S, Ma K, Zheng Y: Med3d: Transfer learning for 3D medical image analysis. arXiv preprint arXiv:1904.00625 (2019)</Citation></Reference><Reference><Citation>P&#xe9;rez-Garc&#xed;a F, Sparks R, Ourselin S: Torchio: a python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning. Comput. Methods Programs Biomed 208, 106236 (2021)</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8542803</ArticleId><ArticleId IdType="pubmed">34311413</ArticleId></ArticleIdList></Reference><Reference><Citation>Van der Maaten L, Hinton G: Visualizing data using t-SNE. J. Mach. Learn. Res 9(11), 1&#x2013;27 (2008)</Citation></Reference><Reference><Citation>Vilalta R, Drissi Y: A perspective view and survey of meta-learning. Artif. Intell. Rev 18(2), 77&#x2013;95 (2002)</Citation></Reference><Reference><Citation>Guan H, Liu M: Domain adaptation for medical image analysis: a survey. IEEE Trans. Biomed. Eng 69(3), 1173&#x2013;1185 (2021)</Citation><ArticleIdList><ArticleId IdType="pmc">PMC9011180</ArticleId><ArticleId IdType="pubmed">34606445</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle></PubmedArticleSet>